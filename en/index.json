[{"content":"前言 研究 hugo 建站之初是打算采用 Github Pages 来发布静态博客\n优点 仅需一个github账号和简单配置即可将静态博客发布到 github pages 没有维护的时间成本，可以将精力更多的放到博客内容本身上去 无需备案 无需ssl证书 缺点 访问速度较慢 访问速度较慢 访问速度较慢 虽说访问速度较慢可以通过各家的cdn加速来解决，但由于刚开始建立 blog 选择的是 wordpress ，域名、服务器、备案、证书等都已经一应俱全，且之前的架构采用 docker，添加一台 nginx 来跑 hugo 的静态网站是很方便的\n一键将hugo博客部署到阿里云 虽说标题带有一键，但还是有一定的门槛的，需要对 dokcer docker-compose nginx 有一定了解\n配置文件下载 下载完将目录上传到自己的服务器，重命名为 blog (当然你可以用其他名字)\n确保服务器网络、ssl证书申请、服务器公网ip、服务器安全组权限(80/443)等基础配置已经一应俱全 确保服务器安装了 docker 和 docker-compose 修改blog/conf/nginx-hugo/nginx.conf和blog/conf/nginx-proxy/default.conf，需要修改的地方在文件中已经标注出来了 将你的ssl证书放到hugo-blog-dockercompose/ssl/目录下 在blog目录下执行docker-compose up -d即可启动容器 将hugo生成的public目录上传到服务器blog/data/hugo/中，参考下文 在域名提供商处为你的域名添加A记录，指向服务器的公网ip地址(主域名和twikoo域名都要配置) 都配置完后 参考下文 配置twikoo 至此已经配置完成，应该可以通过域名访问hugo站点了，后续更新内容只需要hugo生成静态文件上传到服务即可\n所有的配置、应用数据、日志都保存在blog目录下，你可以在不同的服务器上快速迁移hugo环境，无需担心后续想要迁移新服务器时遇到的各种问题\nworkflow 编辑文章 采用 typora + picgo + 七牛云图床流程，参考我的另一篇文章：typora+picgo+七牛云上传图片\n生成静态文件 hugo -F --cleanDestinationDir 后面两个参数表示会先删除之前生成的 public 目录，保证每次生成的 public 都是新的\n上传静态文件 将mobaxterm的命令添加到用户环境变量中，以实现 git bash vscode windows terminal 中运行一些 mobaxterm 本地终端附带的命令，也就无需再专门打开一次 mobaxterm 去上传文件了\nrsync -avuz --progress --delete public/ root@lvbibir.cn:/root/blog/data/hugo/ 归档备份 沿用搭建Github pages时使用的 github仓库 ，来作为我博客的归档管理，也可以方便家里电脑和工作电脑之间的数据同步\ntwikoo评论 所有部署方式：https://twikoo.js.org/quick-start.html\nvercel+mongodb+github部署方式参考：https://www.sulvblog.cn/posts/blog/hugo_twikoo/\n记录一下账号关系：mongodb使用google账号登录，vercel使用github登录\n私有部署(docker) 如果是使用 一键将hugo博客部署到阿里云 中的步骤部署了twikoo，这步直接忽略，配置前端代码即可\ndocker run --name twikoo -e TWIKOO_THROTTLE=1000 -p 8080:8080 -v ${PWD}/data:/app/data -d imaegoo/twikoo 部署完成后看到如下结果即成功\n[root@lvbibir ~]# curl http://localhost:8080 {\u0026#34;code\u0026#34;:100,\u0026#34;message\u0026#34;:\u0026#34;Twikoo 云函数运行正常，请参考 https://twikoo.js.org/quick-start.html#%E5%89%8D%E7%AB%AF%E9%83%A8%E7%BD%B2 完成前端的配置\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;1.6.7\u0026#34;} 后续最好套上反向代理，加上域名和证书\n前端代码 创建或者修改 layouts\\partials\\comments.html\n\u0026lt;!-- Twikoo --\u0026gt; \u0026lt;div\u0026gt; \u0026lt;div class=\u0026#34;pagination__title\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;pagination__title-h\u0026#34; style=\u0026#34;font-size: 20px;\u0026#34;\u0026gt;💬评论\u0026lt;/span\u0026gt; \u0026lt;hr /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;tcomment\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.staticfile.org/twikoo/{{ .Site.Params.twikoo.version }}/twikoo.all.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; twikoo.init({ envId: \u0026#34;\u0026#34;, //填自己的，例如：https://example.com el: \u0026#34;#tcomment\u0026#34;, lang: \u0026#39;zh-CN\u0026#39;, path: window.TWIKOO_MAGIC_PATH||window.location.pathname, }); \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; 调用上述twikoo代码的位置：layouts/_default/single.html\n\u0026lt;article class=\u0026#34;post-single\u0026#34;\u0026gt; // 其他代码...... {{- if (.Param \u0026#34;comments\u0026#34;) }} {{- partial \u0026#34;comments.html\u0026#34; . }} {{- end }} \u0026lt;/article\u0026gt; 在站点配置文件config中加上版本号\nparams: twikoo: version: 1.6.7 更新 拉取新版本docker pull imaegoo/twikoo 停止旧版本容器docker stop twikoo 删除旧版本容器docker rm twikoo 部署新版本容器docker-compose up -d 在hugo配置文件 config.yml 中修改 twikoo版本 修改数据 直接修改blog/data/twikoo/目录下的文件后重启容器，❗慎重修改\n修改smms图床的api地址 已于 1.6.12 新版本修复，https://github.com/imaegoo/twikoo/releases/tag/1.6.12\n由于sm.ms域名国内无法访问，twikoo官方还没有出具体的修改方式，自己修改容器配置文件进行修改\n# 复制配置文件 [root@lvbibir blog]# docker cp twikoo:/app/node_modules/twikoo-func/utils/image.js /root/blog/conf/twikoo/ # 修改配置文件，原来的配置是 https://sm.ms/api.v2/upload [root@lvbibir blog]# grep smms conf/twikoo/image.js } else if (config.IMAGE_CDN === \u0026#39;smms\u0026#39;) { const uploadResult = await axios.post(\u0026#39;https://smms.app/api/v2/upload\u0026#39;, formData, { # 将配置文件映射进容器内，重启容器即可 [root@lvbibir blog]# grep twikoo docker-compose.yml twikoo: image: imaegoo/twikoo container_name: twikoo - $PWD/data/twikoo:/app/data - $PWD/conf/twikoo/image.js:/app/node_modules/twikoo-func/utils/image.js 自定义字体 可以使用一些在线的字体，可能会比较慢，推荐下载想要的字体放到自己的服务器或者cdn上\n修改assets\\css\\extended\\fonts.css，添加@font-face\n@font-face { font-family: \u0026#34;LXGWWenKaiLite-Bold\u0026#34;; src: url(\u0026#34;https://your.domain.com/fonts/test.woff2\u0026#34;) format(\u0026#34;woff2\u0026#34;); font-display: swap; } 修改assets\\css\\extended\\blank.css，推荐将英文字体放在前面，可以实现英文和中文使用不同字体。\n.post-content { font-family: Consolas, \u0026#34;LXGWWenKaiLite-Bold\u0026#34;; //修改 } body { font-family: Consolas, \u0026#34;LXGWWenKaiLite-Bold\u0026#34;; //修改 } 修改链接颜色 在 hugo+papermod 默认配置下，链接颜色是黑色字体带下划线的组合，个人非常喜欢typora-vue的渲染风格hugo官方文档给出了通过render hooks覆盖默认的markdown渲染link的方式\n新建layouts/_default/_markup/render-link.html文件，内容如下。在官方给出的示例中添加了 style=\u0026quot;color:#42b983，颜色可以自行修改\n\u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34;{{ with .Title}} title=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ if strings.HasPrefix .Destination \u0026#34;http\u0026#34; }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; style=\u0026#34;color:#42b983\u0026#34;;{{ end }}\u0026gt;{{ .Text | safeHTML }}\u0026lt;/a\u0026gt; Artitalk说说 官方文档\n需要注意的是如果使用的是国际版的LeadCloud，需要绑定自定义域名后才能正常访问\n记录一下账号关系：LeadCloud使用163邮箱登录\nleancloud配置 前往 LeanCloud 国际版，注册账号。 注册完成之后根据 LeanCloud 的提示绑定手机号和邮箱。 绑定完成之后点击创建应用，应用名称随意，接着在结构化数据中创建 class，命名为 shuoshuo。 在你新建的应用中找到结构化数据下的用户。点击添加用户，输入想用的用户名及密码。 回到结构化数据中，点击 class 下的 shuoshuo。找到权限，在 Class 访问权限中将 add_fields 以及 create 权限设置为指定用户，输入你刚才输入的用户名会自动匹配。为了安全起见，将 delete 和 update 也设置为跟它们一样的权限。 然后新建一个名为atComment的class，权限什么的使用默认的即可。 点击 class 下的 _User 添加列，列名称为 img，默认值填上你这个账号想要用的发布说说的头像url，这一项不进行配置，说说头像会显示为默认头像 —— Artitalk 的 logo。 在最菜单栏中找到设置-\u0026gt; 应用 keys，记下来 AppID 和 AppKey ，一会会用。 最后将 _User 中的权限全部调为指定用户，或者数据创建者，为了保证不被篡改用户数据以达到强制发布说说。 在设置-\u0026gt;域名绑定中绑定自定义域名 ❗ 关于设置权限的这几步\n这几步一定要设置好，才可以保证不被 “闲人” 破解发布说说的验证\nhugo配置 新增 content/talk.md 页面，内容如下，注意修改标注的内容，front-matter 的内容自行修改\n--- title: \u0026#34;💬 说说\u0026#34; date: 2021-08-31 hidemeta: true description: \u0026#34;胡言乱语\u0026#34; comments: true reward: false showToc: false TocOpen: false showbreadcrumbs: false --- \u0026lt;body\u0026gt; \u0026lt;!-- 引用 artitalk --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://unpkg.com/artitalk\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 存放说说的容器 --\u0026gt; \u0026lt;div id=\u0026#34;artitalk_main\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; new Artitalk({ appId: \u0026#39;**********\u0026#39;, // Your LeanCloud appId appKey: \u0026#39;************\u0026#39;, // Your LeanCloud appKey serverURL: \u0026#39;*********\u0026#39; // 绑定的自定义域名 }) \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; 这个时候已经可以直接访问了，https://your.domain.com/talk\n输入 leancloud配置 步骤中的第4步配置的用户名密码登录后就可以发布说说了\nshortcode ppt、bilibili、youtube、豆瓣阅读和电影卡片\nhttps://www.sulvblog.cn/posts/blog/shortcodes/\nmermaid\nhttps://www.sulvblog.cn/posts/blog/hugo_mermaid/\n图片画廊\nhttps://github.com/liwenyip/hugo-easy-gallery/\nhttps://www.liwen.id.au/heg/\n自定义footer 自定义页脚内容\n添加完下面的页脚内容后要修改 assets\\css\\extended\\blank.css 中的 --footer-height 的大小，具体数字需要考虑到行数和字体大小\n自定义徽标 徽标功能源自：https://shields.io/ 考虑到访问速度，可以在生成完徽标后放到自己的cdn上\n在 layouts\\partials\\footer.html 中的 \u0026lt;footer\u0026gt; 添加如下\n\u0026lt;a href=\u0026#34;https://gohugo.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://img.shields.io/static/v1?\u0026amp;style=plastic\u0026amp;color=308fb5\u0026amp;label=Power by\u0026amp;message=hugo\u0026amp;logo=hugo\u0026#34; style=\u0026#34;display: unset;\u0026#34;\u0026gt; \u0026lt;/a\u0026gt; 网站运行时间 在 layouts\\partials\\footer.html 中的 \u0026lt;footer\u0026gt; 添加如下\n起始时间自行修改\n\u0026lt;span id=\u0026#34;runtime_span\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt;function show_runtime(){window.setTimeout(\u0026#34;show_runtime()\u0026#34;,1000);X=new Date(\u0026#34;7/13/2021 1:00:00\u0026#34;);Y=new Date();T=(Y.getTime()-X.getTime());M=24*60*60*1000;a=T/M;A=Math.floor(a);b=(a-A)*24;B=Math.floor(b);c=(b-B)*60;C=Math.floor((b-B)*60);D=Math.floor((c-C)*60);runtime_span.innerHTML=\u0026#34;网站已运行\u0026#34;+A+\u0026#34;天\u0026#34;+B+\u0026#34;小时\u0026#34;+C+\u0026#34;分\u0026#34;+D+\u0026#34;秒\u0026#34;}show_runtime();\u0026lt;/script\u0026gt; 访问人数统计 统计功能源自：http://busuanzi.ibruce.info/\n在 layouts\\partials\\footer.html 中的 \u0026lt;footer\u0026gt; 添加如下\n\u0026lt;script async src=\u0026#34;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;span id=\u0026#34;busuanzi_container\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css\u0026#34;\u0026gt; 总访客数: \u0026lt;i class=\u0026#34;fa fa-user\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;span id=\u0026#34;busuanzi_value_site_uv\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; | 总访问量: \u0026lt;i class=\u0026#34;fa fa-eye\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;span id=\u0026#34;busuanzi_value_site_pv\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; | 本页访问量: \u0026lt;i class=\u0026#34;fa fa-eye\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;span id=\u0026#34;busuanzi_value_page_pv\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt; 其他修改 其他 css样式修改 基本都是通过 f12控制台 一点点摸索改的，不太规范且比较琐碎就不单独记录了，其实我根本已经忘记还改了哪些东西\n","permalink":"https://www.lvbibir.cn/en/posts/blog/hello-hugo/","summary":"前言 研究 hugo 建站之初是打算采用 Github Pages 来发布静态博客 优点 仅需一个github账号和简单配置即可将静态博客发布到 github pages 没有维护的时间成本，可以将精力更多的放到博客内容本身上去 无需备案 无需ssl证书 缺点 访问速度较慢 访问速度较慢 访问速度较慢 虽说访问速度较慢可以通过各家的cdn加速来解决，但由于","title":"【置顶】Hello,hugo!"},{"content":"0. 前言 最近注意到 windows 系统中当 onedrive 和 clash 同时开机自启时会导致 onedrive 无法自动登录, 需要退出 onedrive 重新启动一下才能正常登录.\n出现这个问题的原因是 onedrive 启动速度要比 clash 快, 导致 onedrive 启动时访问不到 clash. 其实只要将这两个其中一个不设置为开机自启即可解决, 但是这两个都是刚需, 放下任何一个都会不舒服.\n一番 google 下来, 大部分的解决方案都是添加 windows 的计划任务, 我尝试了半天也没办法无法成功, 最后终于找到了满足需求的解决方案, 使用 EarlyStart 实现在 windows explorer 启动前就启动自定义的软件.\n同时还能顺便解决之前感觉有点不舒服的两个问题:\nTranslucentTB: 自启动时会慢一拍, 刚进系统时任务栏没有透明, 等个几秒启动后才能正常 utools: 同样的, 进系统后第一时间不能使用, 得等几秒启动后才行 1. 安装 EarlyStart 下载 EarlyStart.zip\n解压后使用管理员打开 powershell 并进入安装目录\ncd D:\\software\\1-portable\\EarlyStart # 安装 C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\InstallUtil.exe .\\EarlyStart.exe # 卸载 C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\InstallUtil.exe /u .\\EarlyStart.exe 2. 配置启动项 首先将要配置快速启动的应用默认的开机自启给关掉\n在用户目录 C:\\Users\\\u0026lt;username\u0026gt; 创建一个名为 .earlystart 的文件, 每一行输入一个 exe 的路径\n然后还需要修改一下账户配置\n配置完成之后重启系统即可\n以上.\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-early-auto-start/","summary":"0. 前言 最近注意到 windows 系统中当 onedrive 和 clash 同时开机自启时会导致 onedrive 无法自动登录, 需要退出 onedrive 重新启动一下才能正常登录. 出现这个问题的原因是 onedrive 启动速度要比 clash 快, 导致 onedrive 启动时访问不到 clash. 其实只要将这两个其中一个不设置为开机自启即可解决, 但是这两个都是刚需, 放下任何一个都会不舒服. 一番 google 下来, 大部分的","title":"windows | 自定义开机快速启动项"},{"content":"0. 前言 python 虚拟环境的重要性已经无需多言了, 目前所有支持 python 虚拟环境的工具中最好用的应该就是 conda 了, 最重要的一点是可以一键创建不同版本的 python 环境以适应不同的需求.\nAnaconda 比较臃肿, 本文使用无 GUI 的 miniconda.\n环境:\nwin10 miniconda3-py11-23.5.2-0 1. 安装 安装前需要确认一下系统及用户的环境变量中不要存在中文, 在 CMD 中直接执行 path 或者 git-bash 中执行 echo $PATH 进行确认, 这个问题当时被折磨疯了, 还给 conda 项目提了 issue.\n最新版下载地址\n选好路径直接下一步即可, 没有需要注意的自定义配置项\n2. 配置 2.1 环境变量 在用户环境变量 PATH 添加如下项, 我的安装路径是 D:\\miniconda, 按实际情况修改\nD:\\software\\miniconda D:\\software\\miniconda\\Scripts D:\\software\\miniconda\\Library\\bin 添加完后重启系统, 让系统重新读取一下环境变量\n2.2 conda 配置 参考链接\nminiconda 默认没有 .condarc 配置文件, 需要生成一下\nconda config --set show_channel_urls yes .condarc 会生成到用户目录下\n$ cat .condarc channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ # 不自动激活 base 环境 auto_activate_base: false # 虚拟环境存放路径 envs_dirs: - D:\\software\\python\\envs # pkg 存放路径 pkgs_dirs: - D:\\software\\python\\pkgs 上述配置文件中主要配置了三项: conda 的清华国内源, 虚拟环境和 pkg 的存储路径\n如不配置创建虚拟环境时可能会生成到用户目录下, 导致系统盘臃肿, 建议新建一个目录专门存放\n2.3 pip 配置 系统中直接安装的 python, 其 pip 的配置文件一般存放在用户目录的 .pip/pip.ini, 使用 conda 创建的虚拟环境的 pip 则不同, 可以使用如下命令查看, 这个问题当时也折磨了我很久\n$ pip -v config list For variant \u0026#39;global\u0026#39;, will try loading \u0026#39;C:\\ProgramData\\pip\\pip.ini\u0026#39; For variant \u0026#39;user\u0026#39;, will try loading \u0026#39;C:\\Users\\lvbibir\\pip\\pip.ini\u0026#39; For variant \u0026#39;user\u0026#39;, will try loading \u0026#39;C:\\Users\\lvbibir\\AppData\\Roaming\\pip\\pip.ini\u0026#39; For variant \u0026#39;site\u0026#39;, will try loading \u0026#39;D:\\software\\miniconda\\pip.ini\u0026#39; 这里我们使用用户目录存放配置文件, 默认也是没有的\n$ cat pip/pip.ini [global] timeout = 6000 index-url = http://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn proxy=http://127.0.0.1:7890 配置 pip 使用国内的清华源, 最后一条 proxy 可以不写, 这个问题是因为我常开代理, pip 默认用 https 访问系统代理, 导致 pip 报错.\n2.4 管理虚拟环境 上述步骤做完后就可以正式使用 conda 创建虚拟环境了\n用管理员打开 powershell 使用如下命令初始化 conda\nconda init powershell conda init cmd conda init bash 之后重新打开终端, 创建你的虚拟环境, -n 表示虚拟环境的名字, 不指定 python 版本默认最新\nconda create -n py37 python=3.7 激活虚拟环境\nconda activate py37 退出虚拟环境\nconda deactivate 查看虚拟环境列表\nconda env list 删除虚拟环境\nconda env remove -n py37 --all 3. 其他 conda 最为人诟病的点应该是包管理跟 pip 可能会产生一些冲突, conda 官方给出的最佳方案是\n全程使用 conda install 来安装模块, 实在不行再用 pip\n使用 conda 创建完虚拟环境后, 一直用 pip 来管理模块\npip 应使用 –upgrade-strategy only-if-needed 参数运行, 以防止通过 conda 安装的软件包进行不必要的升级. 这是运行 pip 时的默认设置, 不应更改\n不要将 pip 与 –user 参数一起使用，避免所有用户安装\n总结一下就是不要来回地用 pip 和 conda.\n以上.\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-miniconda/","summary":"0. 前言 python 虚拟环境的重要性已经无需多言了, 目前所有支持 python 虚拟环境的工具中最好用的应该就是 conda 了, 最重要的一点是可以一键创建不同版本的 python 环境以适应不同的需求. Anaconda 比较臃肿, 本文使用无 GUI 的 miniconda. 环境: win10 miniconda3-py11-23.5.2-0 1. 安装 安装前需要确认一下系统及用户的环境变量中不要存在中文, 在 CMD 中直接执行 path 或者 git-bash 中执行 echo","title":"windows 使用 miniconda 配置 python 虚拟环境"},{"content":"0. 前言 分享一下如何监控某个主机上的网卡到指定 ip 的流量大小, 测试环境已安装 tcpdump 并配置了 zabbix_agent\n被检测端 ip 为 1.1.1.11, 要检测到 1.1.1.12-17 这些 ip 的出口流量\n大致流程为:\n创建一个监控脚本, 分析 1 分钟内指定网卡发送到指定 ip 的数据包大小并输出到日志文件\n将该脚本放到 crontab 中, 每分钟执行一次\n配置 zabbix-agent\n创建数据采集脚本, 提取日志文件中的内容 添加自定义配置, 创建采集的键值 配置 zabbix-server\n添加监控项 添加触发器 添加仪表盘 1. 监控脚本 添加 /opt/traffic_monitor.sh\n#!/bin/bash export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin set -e # 检查是否安装了tcpdump命令 if which tcpdump \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then # 如果已安装，则不进行任何提示 : else echo \u0026#34;系统中未安装 tcpdump 命令，请先安装 tcpdump。\u0026#34; exit 1 fi # 检查是否有 tcpdump 残留进程 existing_tcpdump_pids=$(pgrep -f \u0026#34;tcpdump -i ens32 -nn dst\u0026#34;) || true # 检查 tcpdump 进程数量 tcpdump_count=$(echo \u0026#34;${existing_tcpdump_pids}\u0026#34; | wc -w) if [ \u0026#34;$tcpdump_count\u0026#34; -gt 6 ]; then # 如果数量大于 6 视为之前的进程未正确关闭, 杀死所有 tcpdump 进程 kill -9 ${existing_tcpdump_pids} fi IPLIST=(\u0026#34;1.1.1.12\u0026#34; \u0026#34;1.1.1.13\u0026#34; \u0026#34;1.1.1.14\u0026#34; \u0026#34;1.1.1.15\u0026#34; \u0026#34;1.1.1.16\u0026#34; \u0026#34;1.1.1.17\u0026#34;) LOG_DIRECTORY=/var/log/traffic_monitor LOG_FILE=${LOG_DIRECTORY}/traffic_monitor.log [ -d \u0026#34;${LOG_DIRECTORY}\u0026#34; ] || mkdir -p \u0026#34;${LOG_DIRECTORY}\u0026#34; [ -f \u0026#34;${LOG_FILE}\u0026#34; ] || touch \u0026#34;${LOG_FILE}\u0026#34; # 获取文件大小（以字节为单位） log_file_size=$(du -b \u0026#34;$LOG_FILE\u0026#34; | cut -f1) # 设置100M对应的字节数 limit_size=$((100 * 1024 * 1024)) # 检查文件大小是否大于100M if [ \u0026#34;$log_file_size\u0026#34; -gt \u0026#34;$limit_size\u0026#34; ]; then # 清空文件 echo \u0026#34;\u0026#34; \u0026gt; \u0026#34;$LOG_FILE\u0026#34; fi start_time=$(date +\u0026#34;%Y%m%d-%H%M%S\u0026#34;) for ip in \u0026#34;${IPLIST[@]}\u0026#34;; do ( # 开始 tcpdump 抓包 output_file=/tmp/monitor-${ip}-${start_time}.output nohup tcpdump -i ens32 -nn dst ${ip} and not icmp 2\u0026gt;/dev/null \u0026gt; ${output_file} \u0026amp; # 等待 60 秒后关闭 tcpdump tcpdump_pid=$! sleep 60 \u0026amp;\u0026amp; kill ${tcpdump_pid} stop_time=$(date +\u0026#34;%Y%m%d-%H%M%S\u0026#34;) # 分析流量大小, 以 KB 为单位 traffic_size=$(cat ${output_file} | awk -F\u0026#39;length \u0026#39; \u0026#39;{print $2}\u0026#39; | awk \u0026#39;{sum+=$1} END {printf \u0026#34;%.2f\u0026#34;, sum / 1024}\u0026#39;) # 删除 tcpdump 的输出文件 rm -f ${output_file} echo \u0026#34;${ip} ==== ${start_time} ----\u0026gt; ${stop_time} ===== (KB) ${traffic_size}\u0026#34; \u0026gt;\u0026gt; ${LOG_FILE} ) \u0026amp; done # 等待所有后台任务完成 wait exit 0 放到 crontab 中\n* * * * * /bin/bash /opt/traffic_monitor.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 日志文件应有类似如下输出\n$ tail -12 /var/log/traffic_monitor/traffic_monitor.log 1.1.1.14 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1964.99 1.1.1.12 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 0.23 1.1.1.17 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.16 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.15 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.13 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.12 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 1029.49 1.1.1.14 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 0.00 1.1.1.15 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 0.00 1.1.1.16 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 1029.35 1.1.1.13 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 0.00 1.1.1.17 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 3086.44 2. 配置 zabbix-agent 添加 /opt/zabbix_traffic_monitor.sh, 根据 ip 筛选最后一个匹配项的数值\n#!/bin/bash LOG_DIRECTORY=/var/log/traffic_monitor LOG_FILE=${LOG_DIRECTORY}/traffic_monitor.log grep \u0026#34;$1\u0026#34; \u0026#34;${LOG_FILE}\u0026#34; | awk \u0026#39;{last_column=$NF} END {print last_column}\u0026#39; 添加 /etc/zabbix/zabbix_agentd.d/get_traffic_monitor.conf 配置文件\nUserParameter=get_traffic_monitor[*],/opt/zabbix_traffic_monitor.sh $1 重启 zabbix-agent\nsystemctl restart zabbix-agent 3. 配置 zabbix-server 创建监控项, 有几个 ip 创建几个监控项\n监控项测试, 此处应有值\n创建触发器, 同样的, 有几个 ip 创建几个\n仪表盘添加图形\n4. 测试 找一台服务器配置多 ip\nIPADDR=1.1.1.12 NETMASK=255.255.255.0 GATEWAY=1.1.1.254 DNS1=8.8.8.8 DNS2=114.114.114.114 IPADDR1=1.1.1.13 NETMASK1=255.255.255.0 IPADDR2=1.1.1.14 NETMASK2=255.255.255.0 IPADDR3=1.1.1.15 NETMASK3=255.255.255.0 IPADDR4=1.1.1.16 NETMASK4=255.255.255.0 IPADDR5=1.1.1.17 NETMASK5=255.255.255.0 重启 network\n配置 1.1.1.11 到 1.1.1.12-17 的免密登录\nssh-keygen ssh-copy-id root@1.1.1.12 # 每个 ip 都 ssh 一下, 添加一下 hotkey ssh root@1.1.1.13 ssh root@1.1.1.14 ssh root@1.1.1.15 ssh root@1.1.1.16 ssh root@1.1.1.17 运行一个脚本模拟网络流量\n#!/bin/bash # 设置目标IP地址列表 ip_list=(\u0026#34;1.1.1.12\u0026#34; \u0026#34;1.1.1.13\u0026#34; \u0026#34;1.1.1.14\u0026#34; \u0026#34;1.1.1.15\u0026#34; \u0026#34;1.1.1.16\u0026#34; \u0026#34;1.1.1.17\u0026#34;) dd if=/dev/zero of=/tmp/test bs=1M count=1 while true; do # 生成一个随机数，范围为 0 到 5 random_index=$((RANDOM % 6)) # 随机选择一个IP target_ip=\u0026#34;${ip_list[random_index]}\u0026#34; echo ${target_ip} /usr/bin/scp /tmp/test root@${target_ip}:/tmp/ # 等待10秒 sleep 10 done 运行脚本, 应有如下输出\n过段时间后查看仪表盘, 能看到流量数据\n触发器也应正常工作\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/zabbix-ip-traffic-monitor/","summary":"0. 前言 分享一下如何监控某个主机上的网卡到指定 ip 的流量大小, 测试环境已安装 tcpdump 并配置了 zabbix_agent 被检测端 ip 为 1.1.1.11, 要检测到 1.1.1.12-17 这些 ip 的出口流量 大致流程为: 创建一个监控脚本, 分析 1 分钟内指定网卡发送到指定 ip 的数据包大小并输出到日志文件 将该脚本放到 crontab 中, 每分钟执行一次 配置 zabbix-agent 创建数据采集脚本, 提取日志文","title":"Zabbix 监控主机到指定 ip 的流量大小"},{"content":"前言 以 centos7 为例, 通常我们新装完操作系统后需要进行配置 yum 源, iptables, selinux, ntp 以及优化 kernel 等操作, 现分享一些较为通用的配置. 同时博主将这些配置整理成了脚本, 可以一键执行.\n常用配置 iptables \u0026amp; selinux sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 iptables -F systemctl disable --now firewalld PS1 终端美化 cat \u0026gt; /etc/profile.d/PS1_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export PS1=\u0026#34;\\n[\\[\\e[31m\\]\\u\\[\\e[m\\]@\\[\\e[32m\\]\\h\\[\\e[m\\]] -\\$?- \\[\\e[33m\\]\\$(pwd)\\[\\e[m\\] \\[\\e[34m\\]\\$(date +\u0026#39;%F %T\u0026#39;)\\[\\e[m\\] \\n(\\#)$ \u0026#34; EOF source /etc/profile.d/PS1_conf.sh history 格式化 cat \u0026gt; /etc/profile.d/history_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 写入文件 export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups 不记录“重复”的命令；连续且相同 方为“重复” export PROMPT_COMMAND=\u0026#34;history -a\u0026#34; # 设置每条命令执行完立即写入HISTFILE(默认等待退出会话写入) export HISTTIMEFORMAT=\u0026#34;$(whoami) %F %T \u0026#34; # 设置命令执行时间格式，记录文件增加时间戳 shopt -s histappend # 防止会话退出时覆盖其他会话写到HISTFILE的内容 EOF source /etc/profile.d/history_conf.sh ssh 公钥 mkdir /root/.ssh || true chmod 700 /root/.ssh cat \u0026gt; /root/.ssh/authorized_keys \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCeQZmPg93SNx6zzR/l4RiPnHtFPbDTSOL7AtJOIvrlMm300x1OM8a48VqYuKEx7B7WM7UhszVndg8efJv9UdOtOaa0o8L0Wd2uujn2rFKKok69c5i7c/jmU1my9MkEsKpkx1MHQWVZTFqayv/DB9L5GaE/ShChsTSlXoQ6rc6JC4k1zgSsoNSTLwPrbZcDOZWprt/AOhqCklf9mL1E50WTx9XsjxBLqJIwwVEzmHAhzIiVowjBKjJpQ6hEvygCz67gNVn0vAvHPvCz3amrkCQa333Z9r8tbY7mJpq2Anj4qWtlnL9kHreVK6YoKGvM8+DrbVoT5/zM7wMZ+tdLmreUsu4OhgDkE4IgUMHWQ3T1GyD1EjCkqCdSfJbrLaAR8v7g92uDXO5irIyYMc/iQJ8v4okus9Iid61zFF0SPgZEykOVfT7jJqH0a/630D41uD0TK90v5PicVdh1FfEfok8P4F4UHGLUly2jRVBESQ/TXVGPaMITHPEtYEpmT3kmnOk= 15810243114@163.com EOF chmod 600 /root/.ssh/authorized_keys 加速 ssh 连接 echo \u0026#34;UseDNS no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 配置 yum 源 以实测最快的清华源为例\nmkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true cat \u0026gt; /etc/yum.repos.d/centos-tuna.repo \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [base] name=CentOS-$releasever - Base baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ gpgcheck=0 [updates] name=CentOS-$releasever - Updates baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ gpgcheck=0 [extras] name=CentOS-$releasever - Extras baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ gpgcheck=0 EOF yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion ntpdate 时间配置 timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com limit cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; * soft nofile 65535 * hard nofile 65535 * soft nproc 65535 * hard nproc 65535 EOF proxy cat \u0026gt; /root/proxy \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash case \u0026#34;$1\u0026#34; in set) export http_proxy=\u0026#34;http://1.1.1.1:7890\u0026#34; export https_proxy=\u0026#34;http://1.1.1.1:7890\u0026#34; export all_proxy=\u0026#34;socks5://1.1.1.1:7890\u0026#34; export ALL_PROXY=\u0026#34;socks5://1.1.1.1:7890\u0026#34; ;; unset) unset http_proxy unset https_proxy unset all_proxy unset ALL_PROXY ;; *) echo \u0026#34;Usage: source $0 {set|unset}\u0026#34; ;; esac EOF kernel cat \u0026gt;\u0026gt; /etc/sysctl.d/99-sysctl.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # 关闭ipv6 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 # 避免放大攻击 net.ipv4.icmp_echo_ignore_broadcasts = 1 # 开启恶意icmp错误消息保护 net.ipv4.icmp_ignore_bogus_error_responses = 1 # 开启反向路径过滤 net.ipv4.conf.all.rp_filter = 1 net.ipv4.conf.default.rp_filter = 1 # 关闭sysrq功能 kernel.sysrq = 0 # core文件名中添加pid作为扩展名 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 # 修改消息队列长度 kernel.msgmnb = 65536 kernel.msgmax = 65536 # 设置最大内存共享段大小bytes kernel.shmmax = 68719476736 kernel.shmall = 4294967296 # timewait的数量，默认180000 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.core.netdev_max_backlog = 262144 # 限制仅仅是为了防止简单的DoS 攻击 net.ipv4.tcp_max_orphans = 3276800 # 收到客户端确认信息的连接请求的最大值 net.ipv4.tcp_max_syn_backlog = 262144 net.ipv4.tcp_timestamps = 0 # 内核放弃建立连接之前发送SYNACK 包的数量 net.ipv4.tcp_synack_retries = 1 # 内核放弃建立连接之前发送SYN 包的数量 net.ipv4.tcp_syn_retries = 1 # 启用timewait 快速回收 net.ipv4.tcp_tw_recycle = 1 # 开启重用。允许将TIME-WAIT sockets 重新用于新的TCP连接 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_fin_timeout = 1 # 当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时 net.ipv4.tcp_keepalive_time = 30 # 修改防火墙表大小，默认65536 #net.netfilter.nf_conntrack_max=655350 #net.netfilter.nf_conntrack_tcp_timeout_established=1200 EOF sysctl -p 一键脚本 #!/bin/bash set -e if [ \u0026#34;$(id -u)\u0026#34; -ne 0 ]; then echo \u0026#34;当前用户不是管理员。请使用管理员权限运行此脚本。\u0026#34; exit 1 fi echo \u0026#34;========start=============\u0026#34; function disable_selinux_firewalld() { echo \u0026#34;========selinux===========\u0026#34; sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 echo \u0026#34;========firewalld=========\u0026#34; iptables -F systemctl disable --now firewalld } function format_history() { echo \u0026#34;========history format========\u0026#34; cat \u0026gt; /etc/profile.d/history_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 写入文件 export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups 不记录“重复”的命令；连续且相同 方为“重复” export PROMPT_COMMAND=\u0026#34;history -a\u0026#34; # 设置每条命令执行完立即写入HISTFILE(默认等待退出会话写入) export HISTTIMEFORMAT=\u0026#34;$(whoami) %F %T \u0026#34; # 设置命令执行时间格式，记录文件增加时间戳 shopt -s histappend # 防止会话退出时覆盖其他会话写到HISTFILE的内容 EOF source /etc/profile.d/history_conf.sh } function setup_ssh() { echo \u0026#34;========add ssh key========\u0026#34; mkdir /root/.ssh || true chmod 700 /root/.ssh cat \u0026gt; /root/.ssh/authorized_keys \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCeQZmPg93SNx6zzR/l4RiPnHtFPbDTSOL7AtJOIvrlMm300x1OM8a48VqYuKEx7B7WM7UhszVndg8efJv9UdOtOaa0o8L0Wd2uujn2rFKKok69c5i7c/jmU1my9MkEsKpkx1MHQWVZTFqayv/DB9L5GaE/ShChsTSlXoQ6rc6JC4k1zgSsoNSTLwPrbZcDOZWprt/AOhqCklf9mL1E50WTx9XsjxBLqJIwwVEzmHAhzIiVowjBKjJpQ6hEvygCz67gNVn0vAvHPvCz3amrkCQa333Z9r8tbY7mJpq2Anj4qWtlnL9kHreVK6YoKGvM8+DrbVoT5/zM7wMZ+tdLmreUsu4OhgDkE4IgUMHWQ3T1GyD1EjCkqCdSfJbrLaAR8v7g92uDXO5irIyYMc/iQJ8v4okus9Iid61zFF0SPgZEykOVfT7jJqH0a/630D41uD0TK90v5PicVdh1FfEfok8P4F4UHGLUly2jRVBESQ/TXVGPaMITHPEtYEpmT3kmnOk= 15810243114@163.com EOF chmod 600 /root/.ssh/authorized_keys echo \u0026#34;=========setup ssh========\u0026#34; echo \u0026#34;UseDNS no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config } function setup_yum() { echo \u0026#34;====backup repo===========\u0026#34; mkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true echo \u0026#34;====configure tuna repo====\u0026#34; cat \u0026gt; /etc/yum.repos.d/centos-tuna.repo \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [base] name=CentOS-$releasever - Base baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ gpgcheck=0 [updates] name=CentOS-$releasever - Updates baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ gpgcheck=0 [extras] name=CentOS-$releasever - Extras baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ gpgcheck=0 EOF echo \u0026#34;====upgrade yum============\u0026#34; yum clean all yum makecache fast echo \u0026#34;====dowload tools=========\u0026#34; yum install -y wget net-tools vim bash-completion ntpdate } function time_limit_proxy() { echo \u0026#34;=======setup timezone and ntp======\u0026#34; timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com echo \u0026#34;=======modify limit=========\u0026#34; cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; * soft nofile 65535 * hard nofile 65535 * soft nproc 65535 * hard nproc 65535 EOF echo \u0026#34;========http proxy==========\u0026#34; cat \u0026gt; /root/proxy \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash case \u0026#34;$1\u0026#34; in set) export http_proxy=\u0026#34;http://1.1.1.1:7890\u0026#34; export https_proxy=\u0026#34;http://1.1.1.1:7890\u0026#34; export all_proxy=\u0026#34;socks5://1.1.1.1:7890\u0026#34; export ALL_PROXY=\u0026#34;socks5://1.1.1.1:7890\u0026#34; ;; unset) unset http_proxy unset https_proxy unset all_proxy unset ALL_PROXY ;; *) echo \u0026#34;Usage: source $0 {set|unset}\u0026#34; ;; esac EOF } function setup_kernel() { echo \u0026#34;========Optimize kernel========\u0026#34; cat \u0026gt;\u0026gt; /etc/sysctl.d/99-sysctl.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # 关闭ipv6 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 # 避免放大攻击 net.ipv4.icmp_echo_ignore_broadcasts = 1 # 开启恶意icmp错误消息保护 net.ipv4.icmp_ignore_bogus_error_responses = 1 # 开启反向路径过滤 net.ipv4.conf.all.rp_filter = 1 net.ipv4.conf.default.rp_filter = 1 # 关闭sysrq功能 kernel.sysrq = 0 # core文件名中添加pid作为扩展名 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 # 修改消息队列长度 kernel.msgmnb = 65536 kernel.msgmax = 65536 # 设置最大内存共享段大小bytes kernel.shmmax = 68719476736 kernel.shmall = 4294967296 # timewait的数量，默认180000 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.core.netdev_max_backlog = 262144 # 限制仅仅是为了防止简单的DoS 攻击 net.ipv4.tcp_max_orphans = 3276800 # 收到客户端确认信息的连接请求的最大值 net.ipv4.tcp_max_syn_backlog = 262144 net.ipv4.tcp_timestamps = 0 # 内核放弃建立连接之前发送SYNACK 包的数量 net.ipv4.tcp_synack_retries = 1 # 内核放弃建立连接之前发送SYN 包的数量 net.ipv4.tcp_syn_retries = 1 # 启用timewait 快速回收 net.ipv4.tcp_tw_recycle = 1 # 开启重用。允许将TIME-WAIT sockets 重新用于新的TCP连接 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_fin_timeout = 1 # 当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时 net.ipv4.tcp_keepalive_time = 30 # 修改防火墙表大小，默认65536 #net.netfilter.nf_conntrack_max=655350 #net.netfilter.nf_conntrack_tcp_timeout_established=1200 EOF sysctl -p } disable_selinux_firewalld format_history setup_ssh setup_yum time_limit_proxy setup_kernel echo \u0026#34;=========finish============\u0026#34; exit 0 ","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-centos-init/","summary":"前言 以 centos7 为例, 通常我们新装完操作系统后需要进行配置 yum 源, iptables, selinux, ntp 以及优化 kernel 等操作, 现分享一些较为通用的配置. 同时博主将这些配置整理成了脚本, 可以一键执行. 常用配置 iptables \u0026amp; selinux sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 iptables -F systemctl disable --now firewalld PS1 终端美化 cat \u0026gt; /etc/profile.d/PS1_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export PS1=\u0026#34;\\n[\\[\\e[31m\\]\\u\\[\\e[m\\]@\\[\\e[32m\\]\\h\\[\\e[m\\]] -\\$?- \\[\\e[33m\\]\\$(pwd)\\[\\e[m\\] \\[\\e[34m\\]\\$(date +\u0026#39;%F %T\u0026#39;)\\[\\e[m\\] \\n(\\#)$ \u0026#34; EOF source /etc/profile.d/PS1_conf.sh history 格式化 cat \u0026gt; /etc/profile.d/history_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 写入文件 export HISTSIZE=1000 # hist","title":"shell | centos 初始化"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0\n这次部署的 loki 整体架构如下, loki 使用 statefulset 的方式运行, promtail 以 daemonset 的方式运行在 k8s 集群的每个节点.\n1. promtail 1.1 部署 namespace\napiVersion: v1 kind: Namespace metadata: name: logging rbac\napiVersion: v1 kind: ServiceAccount metadata: name: loki-promtail labels: app: promtail namespace: logging --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: app: promtail name: promtail-clusterrole namespace: logging rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: promtail-clusterrolebinding labels: app: promtail namespace: logging subjects: - kind: ServiceAccount name: loki-promtail namespace: logging roleRef: kind: ClusterRole name: promtail-clusterrole apiGroup: rbac.authorization.k8s.io configmap\napiVersion: v1 kind: ConfigMap metadata: name: loki-promtail namespace: logging labels: app: promtail data: promtail.yaml: | client: # 配置Promtail如何连接到Loki的实例 backoff_config: # 配置当请求失败时如何重试请求给Loki max_period: 5m max_retries: 10 min_period: 500ms batchsize: 1048576 # 发送给Loki的最大批次大小(以字节为单位) batchwait: 1s # 发送批处理前等待的最大时间（即使批次大小未达到最大值） external_labels: {} # 所有发送给Loki的日志添加静态标签 timeout: 10s # 等待服务器响应请求的最大时间 positions: filename: /run/promtail/positions.yaml server: http_listen_port: 3101 target_config: sync_period: 10s scrape_configs: - job_name: kubernetes-pods-name pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_label_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-app pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ source_labels: - __meta_kubernetes_pod_label_name - source_labels: - __meta_kubernetes_pod_label_app target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-direct-controllers pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ separator: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_label_name - __meta_kubernetes_pod_label_app - action: drop regex: \u0026#39;[0-9a-z-.]+-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name - source_labels: - __meta_kubernetes_pod_controller_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-indirect-controller pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ separator: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_label_name - __meta_kubernetes_pod_label_app - action: keep regex: \u0026#39;[0-9a-z-.]+-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name - action: replace regex: \u0026#39;([0-9a-z-.]+)-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-static pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror - action: replace source_labels: - __meta_kubernetes_pod_label_component target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror - __meta_kubernetes_pod_container_name target_label: __path__ daemonset\napiVersion: apps/v1 kind: DaemonSet metadata: name: loki-promtail namespace: logging labels: app: promtail spec: selector: matchLabels: app: promtail updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: promtail spec: serviceAccountName: loki-promtail containers: - name: promtail image: grafana/promtail:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/etc/promtail/promtail.yaml - -client.url=http://loki:3100/loki/api/v1/push env: - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName volumeMounts: - mountPath: /etc/promtail name: config - mountPath: /run/promtail name: run - mountPath: /var/lib/docker/containers name: docker readOnly: true - mountPath: /var/log/pods name: pods readOnly: true ports: - containerPort: 3101 name: http protocol: TCP securityContext: readOnlyRootFilesystem: true runAsGroup: 0 runAsUser: 0 readinessProbe: failureThreshold: 5 httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 tolerations: - operator: Exists volumes: - name: config configMap: defaultMode: 420 name: loki-promtail - name: run hostPath: path: /run/promtail type: \u0026#34;\u0026#34; - name: docker hostPath: path: /var/lib/docker/containers - name: pods hostPath: path: /var/log/pods 1.2 scrape_config 配置详解 主要解释一下 promtail 中的匹配规则, 因为采集的日志可以说非常地杂乱, 如何将应用日志分类就尤为重要, 可以说匹配规则是 promtail 的核心所在\n通常我们分类 pod 的手段基本为 namespace + labels + controller , 在 loki 中也一样, 在上述 configmap 的配置中将 k8s 中的所有 pod 分为了五类:\n定义了 label_name 未定义 label_name, 定义了 label_app 未定义 label_name \u0026amp; label_app, 由 Daemonset 控制 未定义 label_name \u0026amp; label_app, 由非 Daemonset 控制 未定义 label_name \u0026amp; label_app, 由 kubelet 直接控制 对应上述 configmap 中配置的五个 job:\nkubernetes-pods-name job=namespace/label_name kubernetes-pods-app job=namespace/label_app kubernetes-pods-direct-controllers job=namespace/controller kubernetes-pods-indirect-controllers job=namespace/controller kubernetes-pods-static job=namespace/label_component 每个指标数据将由上述规则分类, 添加一个 job 的 label\n然后基于指标数据对应 pod 的所有 label 附加到指标数据上\n- action: labelmap regex: __meta_kubernetes_pod_label_(.+) 再加上指标数据本身携带的一些 label, 我们就可以对 pod 日志做一个十分细致的区分\n2. loki rbac\napiVersion: v1 kind: ServiceAccount metadata: name: loki namespace: logging --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: loki namespace: logging rules: - apiGroups: - extensions resourceNames: - loki resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: loki namespace: logging roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: loki subjects: - kind: ServiceAccount name: loki configmap\napiVersion: v1 kind: ConfigMap metadata: name: loki namespace: logging labels: app: loki data: loki.yaml: | auth_enabled: false ingester: chunk_idle_period: 3m # 如果块没有达到最大的块大小，那么在刷新之前，块应该在内存中不更新多长时间 chunk_block_size: 262144 chunk_retain_period: 1m # 块刷新后应该在内存中保留多长时间 max_transfer_retries: 0 # Number of times to try and transfer chunks when leaving before falling back to flushing to the store. Zero = no transfers are done. lifecycler: # 配置ingester的生命周期，以及在哪里注册以进行发现 ring: kvstore: store: inmemory # 用于ring的后端存储，支持consul、etcd、inmemory replication_factor: 1 # 写入和读取的ingesters数量，至少为1（为了冗余和弹性，默认情况下为3) limits_config: enforce_metric_name: false reject_old_samples: true # 旧样品是否会被拒绝 reject_old_samples_max_age: 168h # 拒绝旧样本的最大时限 schema_config: # 配置从特定时间段开始应该使用哪些索引模式 configs: - from: 2020-10-24 # 创建索引的日期。如果这是唯一的schema_config，则使用过去的日期，否则使用希望切换模式时的日期 store: boltdb-shipper # 索引使用哪个存储，如：cassandra, bigtable, dynamodb，或boltdb object_store: filesystem # 用于块的存储，如：gcs, s3， inmemory, filesystem, cassandra，如果省略，默认值与store相同 schema: v11 index: # 配置如何更新和存储索引 prefix: index_ # 所有周期表的前缀 period: 24h # 表周期 server: http_listen_port: 3100 storage_config: # 为索引和块配置一个或多个存储 boltdb_shipper: active_index_directory: /data/loki/boltdb-shipper-active cache_location: /data/loki/boltdb-shipper-cache cache_ttl: 24h shared_store: filesystem filesystem: directory: /data/loki/chunks chunk_store_config: # 配置如何缓存块，以及在将它们保存到存储之前等待多长时间 max_look_back_period: 0s # 限制查询数据的时间，默认是禁用的，这个值应该小于或等于table_manager.retention_period中的值 table_manager: retention_deletes_enabled: true # 日志保留周期开关，用于表保留删除 retention_period: 48h # 日志保留周期，保留期必须是索引/块的倍数 compactor: working_directory: /data/loki/boltdb-shipper-compactor shared_store: filesystem ruler: storage: type: local local: directory: /etc/loki/rules/rules1.yaml rule_path: /tmp/loki/rules-temp alertmanager_url: http://alertmanager-main.monitoring.svc:9093 ring: kvstore: store: inmemory enable_api: true enable_alertmanager_v2: true statefulset service, 注意修改 storageClass 为自己的\n--- apiVersion: v1 kind: Service metadata: name: loki namespace: logging labels: app: loki spec: type: ClusterIP ports: - port: 3100 protocol: TCP name: http targetPort: http selector: app: loki --- apiVersion: apps/v1 kind: StatefulSet metadata: name: loki namespace: logging labels: app: loki spec: podManagementPolicy: OrderedReady replicas: 1 selector: matchLabels: app: loki serviceName: loki updateStrategy: type: RollingUpdate template: metadata: labels: app: loki spec: serviceAccountName: loki securityContext: fsGroup: 10001 runAsGroup: 10001 runAsNonRoot: true runAsUser: 10001 initContainers: [] containers: - name: loki image: grafana/loki:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/etc/loki/loki.yaml volumeMounts: - name: config mountPath: /etc/loki - name: storage mountPath: /data ports: - name: http-metrics containerPort: 3100 protocol: TCP livenessProbe: httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 45 timeoutSeconds: 1 periodSeconds: 10 successThreshold: 1 failureThreshold: 3 readinessProbe: httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 45 timeoutSeconds: 1 periodSeconds: 10 successThreshold: 1 failureThreshold: 3 securityContext: readOnlyRootFilesystem: true terminationGracePeriodSeconds: 4800 volumes: - name: config configMap: defaultMode: 420 name: loki volumeClaimTemplates: - metadata: name: storage labels: app: loki annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;nfs\u0026#34; # 注意修改 storageClass 名称 spec: accessModes: - ReadWriteOnce resources: requests: storage: \u0026#34;2Gi\u0026#34; 应用所有配置文件, 上述配置是 loki 针对 k8s 的一套比较标准的配置, 所以目前的配置仅能抓取 k8s 中所有 pod 发送到 stdout 和 stderr 的信息, 如果需要抓取日志文件还需另外配置.\n[root@k8s-node1 ~]# kubectl apply -f /opt/loki/ [root@k8s-node1 ~]# kubectl get pods -n logging NAME READY STATUS RESTARTS AGE loki-0 1/1 Running 0 3m29s loki-promtail-4kskw 1/1 Running 0 3m36s loki-promtail-p7qzr 1/1 Running 0 3m36s loki-promtail-wc5f7 1/1 Running 0 3m37s [root@k8s-node1 ~]# kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loki ClusterIP 10.105.115.178 \u0026lt;none\u0026gt; 3100/TCP 4m26s 3. Grafana grafana 部署请参考 prometheus 系列文章\n3.1 配置 在 grafana 中添加 loki 作为 data source, 这里我的 grafana 是直接部署在 k8s 中的, 所以可以通过 \u0026lt;svc-name\u0026gt;.\u0026lt;namespace\u0026gt; 访问到 loki\n在 Explore =\u0026gt; loki =\u0026gt; {job=\u0026quot;kube-system/kube-apiserver\u0026quot;} 可以看到 k8s 的 api-server 相关日志\n3.1 光标跳动问题 在 grafana 中手动写 logQL 查询数据时总会出现光标要么一直往行首跳, 要么一直往行尾跳, Github 也有很多人遇到了同样的问题, 社区仍未解决该问题, 目前可以通过 F12 控制台输入一条指令单次地修复这个问题, 指令如下\ndocument.querySelectorAll(\u0026#34;.slate-query-field \u0026gt; div\u0026#34;)[0][\u0026#39;style\u0026#39;].removeProperty(\u0026#39;-webkit-user-modify\u0026#39;); 4. dashboard 示例 4.1 traefik traefik 部署参考 traefik 系列文章\n如下图所示, 已经可以看到收集到的 traefik 日志\n我们还可以通过 dashboard 实时展示 traefik 的信息, 在 grafana 导入 13713 号模板\n此 dashboard 默认的 traefik 的采集语句是 {job=\u0026quot;/var/log/traefik.log\u0026quot;} , 我们需要按照实际情况进行修改, 这里我改成了 {app=\u0026quot;traefik/traefik\u0026quot;}\n导入修改好的 yaml, 选择数据源\n可以看到已经可以正常展示数据了\n但是还有一个小报错, 是因为这个 dashboard 依赖 grafana-piechart-panel 这个插件, 我们在 grafana 容器内执行安装插件\n[root@k8s-node1 manifests]# kubectl exec -it grafana-78bb4557f5-7rbbq -n monitoring -- grafana-cli plugins install grafana-piechart-panel ✔ Downloaded grafana-piechart-panel v1.6.4 zip successfully Please restart Grafana after installing plugins. Refer to Grafana documentation for instructions if necessary. [root@k8s-node1 manifests]# kubectl delete pod grafana-78bb4557f5-7rbbq -n monitoring pod \u0026#34;grafana-78bb4557f5-7rbbq\u0026#34; deleted 等待重建 pod, 可以看到这里已经可以正常显示了\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-loki-2-deploy/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0 这次部署的 loki 整体架构如下, loki 使用 statefulset 的方式运行, promtail 以 daemonset 的方式运行在 k8s 集群的每个节点. 1. promtail 1.1 部署 namespace apiVersion: v1 kind: Namespace metadata: name: logging rbac apiVersion: v1 kind: ServiceAccount metadata: name: loki-promtail labels: app: promtail namespace: logging --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: app: promtail name: promtail-clusterrole namespace: logging rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: promtail-clusterrolebinding labels: app: promtail namespace: logging subjects: - kind: ServiceAccount name: loki-promtail namespace: logging roleRef: kind: ClusterRole name: promtail-clusterrole apiGroup: rbac.authorization.k8s.io configmap apiVersion: v1","title":"loki (二) 部署"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0\n1. 简介 项目地址 官方文档\nLoki 是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，专门为 Prometheus 和 Kubernetes 用户做了相关优化。该项目受 Prometheus 启发，官方的介绍就是： Like Prometheus, But For Logs.\n2. 优缺点 与其他日志聚合系统相比， Loki 具有下面的一些特性:\n低索引开销\n不对日志进行全文索引。通过存储压缩非结构化日志和仅索引元数据，Loki 操作起来会更简单，更省成本。 这样做可以大幅降低索引资源开销, es无论你查不查，巨大的索引开销必须时刻承担 并发查询\n为了弥补没有全文索引带来的查询降速使用，Loki 将把查询分解成较小的分片，可以理解为并发的 grep 和 prometheus 采用相同的标签，对接 alertmanager\nLoki 和 Prometheus 之间的标签一致是 Loki 的超级能力之一 受到 Grafana 原生支持\n避免 kibana 和 grafana 来回切换 服务发现\n支持与 prometheus 一样的服务发现功能, 特别适合储存 Kubernetes Pod 日志 无需使用日志落盘或者 sidecar 当然, 它也有一定的缺点:\n技术比较新颖，相对应的论坛不是非常活跃。\n功能单一，只针对日志的查看，筛选有好的表现，对于数据的处理以及清洗没有 ELK 强大，同时与 ELK 相比，对于后期，ELK 可以连用各种技术进行日志的大数据处理，但是 loki 不行。\n3. 架构 3.1 整体架构 在 Loki 架构中有以下几个概念：\nGrafana：相当于 EFK 中的 Kibana ，用于 UI 的展示。 Loki：相当于 EFK 中的 ElasticSearch ，用于存储日志和处理查询。 Promtail：相当于 EFK 中的 Filebeat/Fluentd ，用于采集日志并将其发送给 Loki 。 LogQL：Loki 提供的日志查询语言，类似 Prometheus 的 PromQL，而且 Loki 支持 LogQL 查询直接转换为 Prometheus 指标。 3.2 promtail 官方文档\npromtail 是 loki 架构中最常用的采集器, 相当于 EFK 中的 filebeat/fluentd\n它的主要工作流程:\n使用 fsnotify 监听指定目录下（例如：/var/log/*.log）的文件创建与删除 对每个活跃的日志文件起一个 goroutine 进行类似 tail -f 的读取，读取到的内容发送给 channel 有一个单独的 goroutine 会读取 channel 中的日志行，分批并附加上标签后推送给 Loki 3.3 loki Loki 采用读写分离架构，关键组件有：\nDistributor 分发器：日志数据传输的“第一站”，Distributor 分发器接收到日志数据后，根据元数据和 hash 算法，将日志分批并行地发送到多个 Ingester 接收器上 Ingester 接收器：接收器是一个有状态的组件，在日志进入时对其进行 gzip 压缩操作，并负责构建和刷新 chunck 块，当 chunk 块达到一定的数量或者时间后，就会刷新 chunk 块和对应的 Index 索引存储到数据库中 Querier 查询器：给定一个时间范围和标签选择器，Querier 查询器可以从数据库中查看 Index 索引以确定哪些 chunck 块匹配，并通过 greps 将结果显示出来，它还会直接从 Ingester 接收器获取尚未刷新的最新数据 Query frontend 查询前端：查询前端是一个可选的组件，运行在 Querier 查询器之前，起到缓存，均衡调度的功能，用于加速日志查询 Loki 提供了两种部署方式：\n单体模式，ALL IN ONE：Loki 支持单一进程模式，可在一个进程中运行所有必需的组件。单进程模式非常适合测试 Loki 或以小规模运行。不过尽管每个组件都以相同的进程运行，但它们仍将通过本地网络相互连接进行组件之间的通信（grpc）。使用 Helm 部署就是采用的该模式。 微服务模式：为了实现水平可伸缩性，Loki 支持组件拆分为单独的组件分开部署，从而使它们彼此独立地扩展。每个组件都产生一个用于内部请求的 gRPC 服务器和一个用于外部 API 请求的 HTTP 服务，所有组件都带有 HTTP 服务器，但是大多数只暴露就绪接口、运行状况和指标端点。 4. 日志告警 Loki 支持三种模式创建日志告警：\n在 Promtail 中的 pipeline 管道的 metrics 的阶段，根据需求增加一个监控指标，然后使用 Prometheus 结合 Alertmanager 完成监控报警。 通过 Loki 自带的报警功能（ Ruler 组件）可以持续查询一个 rules 规则，并将超过阈值的事件推送给 AlertManager 或者其他 Webhook 服务。 将 LogQL 查询转换为 Prometheus 指标。可以通过 Grafana 自带的 Alert rules \u0026amp; notifications，定义有关 LogQL 指标的报警，推送到 Notification channels（ Prometheus Alertmanager ， Webhook 等）。 ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-loki-1-jianjie/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0 1. 简介 项目地址 官方文档 Loki 是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，专门为 Prometheus 和 Kubernetes 用户做了相关优化。该项目受 Prometheus 启发，官方的介绍就是： Like Prometheus, But","title":"loki (一) 简介"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1. alertmanager prometheus 架构中采集数据和发送告警是独立出来的, 告警触发后将信息转发到独立的组件 alertmanager, 由 alertmanager 对报警进行统一处理, 最后通过接收器 recevier 发送给指定用户\n1.1 工作机制 Alertmanager 收到告警信息后:\n进行分组 group 通过定义好的路由 routing 转发到正确的接收器 recevier recevier 通过 email dingtalk wechat 等方式通知给定义好的接收人 1.2 四大功能 分组(Grouping): 将同类型的告警进行分组, 合并多条告警到一个通知中\n抑制(Inhibition): 当某条告警已经发送, 停止重复发送由此告警引起的其他异常或者故障\n静默(Silences): 根据标签快速对告警进行静默处理, 如果告警符合静默的配置, Alertmanager则不会发送告警通知\n路由(route): 用于配置 Alertmanager 如何处理传入的特定类型的告警通知\n1.3 配置详解 global: # 经过此时间后，如果尚未更新告警，则将告警声明为已恢复。(即 prometheus 没有向 alertmanager 发送告警了) resolve_timeout: 5m # 配置发送邮件信息 smtp_smarthost: \u0026#39;smtp.qq.com:465\u0026#39; smtp_from: \u0026#39;742899387@qq.com\u0026#39; smtp_auth_username: \u0026#39;742899387@qq.com\u0026#39; smtp_auth_password: \u0026#39;password\u0026#39; smtp_require_tls: false # 读取告警通知模板的目录。 templates: - \u0026#39;/etc/alertmanager/template/*.tmpl\u0026#39; # 所有报警都会进入到这个根路由下，可以根据根路由下的子路由设置报警分发策略 route: # 先解释一下分组，分组就是将多条告警信息聚合成一条发送，这样就不会收到连续的报警了。 # 将传入的告警按标签分组(标签在 prometheus 中的 rules 中定义)，例如： # 接收到的告警信息里面有许多具有 cluster=A 和 alertname=LatencyHigh 的标签，这些个告警将被分为一个组。 # # 如果不想使用分组，可以这样写group_by: [...] group_by: [\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;] # 第一组告警发送通知需要等待的时间，这种方式可以确保有足够的时间为同一分组获取多个告警，然后一起触发这个告警信息。 group_wait: 30s # 发送第一个告警后，等待\u0026#34;group_interval\u0026#34;发送一组新告警。 group_interval: 5m # 分组内发送相同告警的时间间隔。这里的配置是每3小时发送告警到分组中。举个例子：收到告警后，一个分组被创建，等待5分钟发送组内告警，如果后续组内的告警信息相同,这些告警会在3小时后发送，但是3小时内这些告警不会被发送。 repeat_interval: 3h # 这里先说一下，告警发送是需要指定接收器的，接收器在receivers中配置，接收器可以是email、webhook、pagerduty、wechat等等。一个接收器可以有多种发送方式。 # 指定默认的接收器 receiver: team-X-mails # 下面配置的是子路由，子路由的属性继承于根路由(即上面的配置)，在子路由中可以覆盖根路由的配置 # 下面是子路由的配置 routes: # 使用正则的方式匹配告警标签 - match_re: # 这里可以匹配出标签含有 service=foo1 或 service=foo2 或 service=baz 的告警 service: ^(foo1|foo2|baz)$ # 指定接收器为 team-X-mails receiver: team-X-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出 severity=critical 的告警，并使用 team-X-pager 接收器发送告警，没有匹配到的告警会由父路由进行处理。 routes: - match: severity: critical receiver: team-X-pager # 这里也是一条子路由，会匹配出标签含有 service=files 的告警，并使用 team-Y-mails 接收器发送告警 - match: service: files receiver: team-Y-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出 severity=critical 的告警，并使用 team-Y-pager 接收器发送告警，没有匹配到的会由父路由进行处理。 routes: - match: severity: critical receiver: team-Y-pager # 该路由处理来自数据库服务的所有警报。如果没有团队来处理，则默认为数据库团队。 - match: # 首先匹配标签service=database service: database # 指定接收器 receiver: team-DB-pager # 根据受影响的数据库对告警进行分组 group_by: [alertname, cluster, database] routes: - match: owner: team-X receiver: team-X-pager # 告警是否继续匹配后续的同级路由节点，默认false，下面如果也可以匹配成功，会向两种接收器都发送告警信息(猜测。。。) continue: true - match: owner: team-Y receiver: team-Y-pager # 下面是关于inhibit(抑制)的配置，先说一下抑制是什么：抑制规则允许在另一个警报正在触发的情况下使一组告警静音。其实可以理解为告警依赖。比如一台数据库服务器掉电了，会导致db监控告警、网络告警等等，可以配置抑制规则如果服务器本身down了，那么其他的报警就不会被发送出来。 inhibit_rules: #下面配置的含义：当有多条告警在告警组里时，并且他们的标签alertname,cluster,service都相等，如果severity: \u0026#39;critical\u0026#39;的告警产生了，那么就会抑制severity: \u0026#39;warning\u0026#39;的告警。 - source_match: # 源告警(我理解是根据这个报警来抑制target_match中匹配的告警) severity: \u0026#39;critical\u0026#39; # 标签匹配满足severity=critical的告警作为源告警 target_match: # 目标告警(被抑制的告警) severity: \u0026#39;warning\u0026#39; # 告警必须满足标签匹配severity=warning才会被抑制。 equal: [\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;] # 必须在源告警和目标告警中具有相等值的标签才能使抑制生效。(即源告警和目标告警中这三个标签的值相等\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;) # 下面配置的是接收器 receivers: # 接收器的名称、通过邮件的方式发送、 - name: \u0026#39;team-X-mails\u0026#39; email_configs: # 发送给哪些人 - to: \u0026#39;team-X+alerts@example.org\u0026#39; # 是否通知已解决的警报 send_resolved: true # 接收器的名称、通过邮件和pagerduty的方式发送、发送给哪些人，指定pagerduty的service_key - name: \u0026#39;team-X-pager\u0026#39; email_configs: - to: \u0026#39;team-X+alerts-critical@example.org\u0026#39; pagerduty_configs: - service_key: \u0026lt;team-X-key\u0026gt; # 接收器的名称、通过邮件的方式发送、发送给哪些人 - name: \u0026#39;team-Y-mails\u0026#39; email_configs: - to: \u0026#39;team-Y+alerts@example.org\u0026#39; # 接收器的名称、通过pagerduty的方式发送、指定pagerduty的service_key - name: \u0026#39;team-Y-pager\u0026#39; pagerduty_configs: - service_key: \u0026lt;team-Y-key\u0026gt; # 一个接收器配置多种发送方式 - name: \u0026#39;ops\u0026#39; webhook_configs: - url: \u0026#39;http://prometheus-webhook-dingtalk.kube-ops.svc.cluster.local:8060/dingtalk/webhook1/send\u0026#39; send_resolved: true email_configs: - to: \u0026#39;742899387@qq.com\u0026#39; send_resolved: true - to: \u0026#39;soulchild@soulchild.cn\u0026#39; send_resolved: true 1.4 Alertmanager CRD Prometheus Operator 为 alertmanager 抽象了两个 CRD资源:\nalertmanager CRD: 基于 statefulset, 实现 alertmanager 的部署以及扩容缩容 alertmanagerconfig CRD: 实现模块化修改 alertmanager 的配置 通过 alertManager CRD 部署的实例配置文件由 secret/alertmanager-main-generated 提供\n# kubectl get pod alertmanager-main-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;config-volume\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;name\u0026#34;: \u0026#34;config-volume\u0026#34;, \u0026#34;secret\u0026#34;: { \u0026#34;defaultMode\u0026#34;: 420, \u0026#34;secretName\u0026#34;: \u0026#34;alertmanager-main-generated\u0026#34; } } # kubectl get secret alertmanager-main-generated -n monitoring -o jsonpath=\u0026#39;{.data.alertmanager\\.yaml}\u0026#39; | base64 --decode \u0026#34;global\u0026#34;: \u0026#34;resolve_timeout\u0026#34;: \u0026#34;5m\u0026#34; \u0026#34;inhibit_rules\u0026#34;: - \u0026#34;equal\u0026#34;: - \u0026#34;namespace\u0026#34; - \u0026#34;alertname\u0026#34; \u0026#34;source_matchers\u0026#34;: ...... secret alertmanager-main-generated 是自动生成的, 基于 secret alertmanager-main 和 CRD alertmanagerConfig\n[root@k8s-node1 manifests]# kubectl explain alertmanager.spec.configSecret DESCRIPTION: ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for this Alertmanager instance. Defaults to \u0026#39;alertmanager-\u0026lt;alertmanager-name\u0026gt;\u0026#39; The secret is mounted into /etc/alertmanager/config. 综上, 修改 alertmanager 配置可以修改 secret alertmanager-main 或者 CRD alertmanagerconfig\n2. 示例 2.1 secret 新建 alertmanager.yaml 配置文件\nglobal: resolve_timeout: 5m smtp_from: \u0026#39;15810243114@163.com\u0026#39; smtp_smarthost: \u0026#39;smtp.163.com:25\u0026#39; smtp_auth_username: \u0026#39;15810243114@163.com\u0026#39; smtp_auth_password: \u0026#39;******\u0026#39; smtp_require_tls: false smtp_hello: \u0026#39;163.com\u0026#39; route: receiver: Default group_by: - namespace continue: false routes: - receiver: Critical matchers: - severity=\u0026#34;critical\u0026#34; continue: false group_wait: 30s group_interval: 5m repeat_interval: 12h inhibit_rules: - source_matchers: - severity=\u0026#34;critical\u0026#34; target_matchers: - severity=~\u0026#34;warning|info\u0026#34; equal: - namespace - alertname - source_matchers: - severity=\u0026#34;warning\u0026#34; target_matchers: - severity=\u0026#34;info\u0026#34; equal: - namespace - alertname receivers: - name: Default email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true - name: Critical email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true 修改 secret alertmanager-main\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 查看生成的 secret alertmanager-main-generated\nkubectl get secret alertmanager-main-generated -n monitoring -o jsonpath=\u0026#39;{.data.alertmanager\\.yaml}\u0026#39; | base64 --decode 之后 prometheus-operator 会自动更新 alertmanager 的配置\n# kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus-operator | tail -1 level=info ts=2023-04-30T11:43:01.104579363Z caller=operator.go:741 component=alertmanageroperator key=monitoring/main msg=\u0026#34;sync alertmanager\u0026#34; 2.2 alertmanagerconfig 默认情况下配置 alertmanager 是无法获取到的, 我们需要先修改一下 alertmanager 实例, 添加标签选择器\napiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: spec: alertmanagerConfigSelector: matchLabels: alertmanager: main 创建 alertmanager CRD 资源\napiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: dinghook namespace: monitoring labels: alertmanager: main spec: receivers: - name: web webhookConfigs: - url: http://dingtalk-hook-web sendResolved: true - name: db webhookConfigs: - url: http://dingtalk-hook-db sendResolved: true route: groupBy: [\u0026#34;app\u0026#34;] groupWait: 30s groupInterval: 5m repeatInterval: 12h continue: false receiver: web routes: - matchers: - name: app value: nginx receiver: web - matchers: - name: app value: mysql receiver: db 同样的, prometheus-operator 会更新 alertmanager 配置\n# kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus-operator | tail -1 level=info ts=2023-04-30T11:55:00.309492873Z caller=operator.go:741 component=alertmanageroperator key=monitoring/main msg=\u0026#34;sync alertmanager\u0026#34; 查看最后生成的配置\n# kubectl get secret alertmanager-main-generated -n monitoring -o jsonpath=\u0026#39;{.data.alertmanager\\.yaml}\u0026#39; | base64 --decode global: resolve_timeout: 5m smtp_from: 15810243114@163.com smtp_hello: 163.com smtp_smarthost: smtp.163.com:25 smtp_auth_username: 15810243114@163.com smtp_auth_password: ********* smtp_require_tls: false route: receiver: Default group_by: - namespace routes: - receiver: monitoring-dinghook-web group_by: - app matchers: - namespace=\u0026#34;monitoring\u0026#34; # 指定匹配了 namespace continue: true # continue 也没有按照预设配置 routes: - receiver: monitoring-dinghook-web match: app: nginx - receiver: monitoring-dinghook-db match: app: mysql group_wait: 30s group_interval: 5m repeat_interval: 12h - receiver: Critical matchers: - severity=\u0026#34;critical\u0026#34; group_wait: 30s group_interval: 5m repeat_interval: 12h inhibit_rules: - target_matchers: - severity=~\u0026#34;warning|info\u0026#34; source_matchers: - severity=\u0026#34;critical\u0026#34; equal: - namespace - alertname - target_matchers: - severity=\u0026#34;info\u0026#34; source_matchers: - severity=\u0026#34;warning\u0026#34; equal: - namespace - alertname receivers: - name: Default email_configs: - send_resolved: true to: lvbibir@foxmail.com - name: Critical email_configs: - send_resolved: true to: lvbibir@foxmail.com - name: monitoring-dinghook-web webhook_configs: - send_resolved: true url: http://dingtalk-hook-web - name: monitoring-dinghook-db webhook_configs: - send_resolved: true url: http://dingtalk-hook-db templates: [] 目前 alertmanagerconfig 这个 CRD 使用起来感觉有点麻烦, 一级 route 目前只能按照 namespace 筛选, 而且 continue 也只能设置成 false , 而且无法指定其他配置中的 receiver, 比如全局配置中的 Default\n[root@k8s-node1 ~]# kubectl explain alertmanagerconfig.spec.route.continue DESCRIPTION: Boolean indicating whether an alert should continue matching subsequent sibling nodes. It will always be overridden to true for the first-level route by the Prometheus operator. [root@k8s-node1 ~]# kubectl explain alertmanagerconfig.spec.route.matchers DESCRIPTION: List of matchers that the alert’s labels should match. For the first level route, the operator removes any existing equality and regexp matcher on the `namespace` label and adds a `namespace: \u0026lt;object namespace\u0026gt;` matcher. 2.3 告警模板 alertmanager 收到的告警大概长这个样子\nalertmanager CRD 支持 configMaps 参数, 会自动挂载到 /etc/alertmanager/configmaps 目录, 我们可以将模板文件配置成 configmap\n[root@k8s-node1 ~]# kubectl explain alertmanager.spec.configMaps DESCRIPTION: ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods. The ConfigMaps are mounted into /etc/alertmanager/configmaps/\u0026lt;configmap-name\u0026gt;. 创建模板文件 email.tmpl\n{{ define \u0026#34;email.html\u0026#34; }} {{- if gt (len .Alerts.Firing) 0 -}} {{- range $index, $alert := .Alerts -}} ========= ERROR ==========\u0026lt;br\u0026gt; 告警名称：{{ .Labels.alertname }}\u0026lt;br\u0026gt; 告警级别：{{ .Labels.severity }}\u0026lt;br\u0026gt; 告警机器：{{ .Labels.instance }} {{ .Labels.device }}\u0026lt;br\u0026gt; 告警详情：{{ .Annotations.summary }}\u0026lt;br\u0026gt; 告警时间：{{ (.StartsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;br\u0026gt; ========= END ==========\u0026lt;br\u0026gt; {{- end }} {{- end }} {{- if gt (len .Alerts.Resolved) 0 -}} {{- range $index, $alert := .Alerts -}} ========= INFO ==========\u0026lt;br\u0026gt; 告警名称：{{ .Labels.alertname }}\u0026lt;br\u0026gt; 告警级别：{{ .Labels.severity }}\u0026lt;br\u0026gt; 告警机器：{{ .Labels.instance }}\u0026lt;br\u0026gt; 告警详情：{{ .Annotations.summary }}\u0026lt;br\u0026gt; 告警时间：{{ (.StartsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;br\u0026gt; 恢复时间：{{ (.EndsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;br\u0026gt; ========= END ==========\u0026lt;br\u0026gt; {{- end }} {{- end }} {{- end }} 创建 configmap\nkubectl create configmap alertmanager-templates --from-file=email.tmpl --dry-run -o yaml -n monitoring \u0026gt; alertmanager-configmap-templates.yaml kubectl apply -f alertmanager-configmap-templates.yaml 更新 alertmanager 示例, 添加 configmap\napiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: spec: alertmanagerConfigSelector: matchLabels: alertmanager: main configMaps: - alertmanager-templates 查看挂载\n# kubectl get pod -n monitoring alertmanager-main-0 -o jsonpath=\u0026#34;{.spec.volumes[?(@.name==\u0026#39;configmap-alertmanager-templates\u0026#39;)]}\u0026#34; | python -m json.tool { \u0026#34;configMap\u0026#34;: { \u0026#34;defaultMode\u0026#34;: 420, \u0026#34;name\u0026#34;: \u0026#34;alertmanager-templates\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;configmap-alertmanager-templates\u0026#34; } 查看容器内的路径\n# kubectl exec -it alertmanager-main-0 -n monitoring -- sh /alertmanager $ cat /etc/alertmanager/configmaps/alertmanager-templates/email.tmpl 修改 alertmanager.yaml 配置文件, 指定模板文件\nreceivers: - name: Default email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true html: \u0026#39;{{ template \u0026#34;email.html\u0026#34; . }}\u0026#39; # 添加 与模板中的 define 对应 - name: Critical email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true html: \u0026#39;{{ template \u0026#34;email.html\u0026#34; . }}\u0026#39; # 添加 与模板中的 define 对应 templates: - \u0026#39;/etc/alertmanager/configmaps/alertmanager-templates/*.tmpl\u0026#39; 更新 secret\nkubectl create secret generic alertmanager-main -n monitoring --from-file=alertmanager.yaml --dry-run -oyaml \u0026gt; alertmanager-main-secret.yaml kubectl apply -f alertmanager-main-secret.yaml 查看配置是否生效, 在 webUI 界面查看\n查看新生成的告警邮件\n告警邮件 恢复邮件 ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-6-alertmanager/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1. alertmanager prometheus 架构中采集数据和发送告警是独立出来的, 告警触发后将信息转发到独立的组件 alertmanager, 由 alertmanager 对报警进行统一处理, 最后通过接收器 recevier 发送给指定用户 1.1 工作机制 Alertmanager 收到告警信息后: 进行分组 group 通过定义好的路由 routing 转发到正确的接收器 recevier recevier 通过 email dingtalk wechat 等方式通知给定义好的接收人 1.2 四大功能 分组","title":"prometheus (六) Alertmanager"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1. 告警规则 prometheus 支持两种类型的规则, 记录规则 recording rule 和告警规则 alerting rule\n1.1 recording rule 记录规则: 允许预先计算经常需要或计算量大的表达式，并将其结果保存为一组新的时间序列。查询预先计算的结果通常比每次需要时都执行原始表达式要快得多。这对于每次刷新时都需要重复查询相同表达式的仪表板特别有用。\n如下示例, 将统计 cpu 个数的表达式存为一个新的时间序列 instance:node_num_cpu:sum\ngroups: - name: node-exporter.rules rules: - record: instance:node_num_cpu:sum expr: | count without (cpu, mode) ( node_cpu_seconds_total{job=\u0026#34;node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;} ) 原始表达式结果\n新表达式结果\n1.2 alerting rule 告警规则: 当满足指定的触发条件时发送告警\nalert: 告警规则的名称 expr: 告警触发条件, 基于 PromQL 表达式, 如果表达式执行结果为 True 则推送告警 for: 等待评估时间, 可选参数. 表示当触发条件持续一定时间后才发送告警, 在等待期间告警的状态为 pending labels: 自定义标签 annotaions: 指定一组附加信息, 可以使用 $labels $externalLabels $value 格式化信息. $labels 储存报警实例的时序数据; $externalLabels 储存 prometheus 中 global.external_labels 配置的标签; $value 保存报警实例的评估值 description: 详细信息 summary: 描述信息 如下示例, 当节点的某个文件系统剩余空间不足 10% 达到 30 分钟后将发送告警\ngroups: - name: test rules: - alert: NodeFilesystemAlmostOutOfSpace expr: node_filesystem_avail_bytes{job=\u0026#34;node-exporter\u0026#34;,fstype!=\u0026#34;\u0026#34;} / node_filesystem_size_bytes{job=\u0026#34;node-exporter\u0026#34;,fstype!=\u0026#34;\u0026#34;} * 100 \u0026lt; 10 for: 30m labels: severity: warning annotations: description: \u0026#39; {{ $labels.instance }} 节点 {{ $labels.device }} 文件系统剩余空间: {{ printf \u0026#34;%.2f\u0026#34; $value }}% \u0026#39; summary: \u0026#39;文件系统剩余空间不足 10%\u0026#39; 1.3 prometheusrule CRD Prometheus Operator 抽象出来一个 prometheusrule CRD 资源, 通过管理这个 CRD 资源实现告警规则的统一管理\nkube-prometheus 默认帮我们创建了一些告警规则\n# kubectl get prometheusrule -A NAMESPACE NAME AGE monitoring alertmanager-main-rules 8d monitoring kube-prometheus-rules 8d monitoring kube-state-metrics-rules 8d monitoring kubernetes-monitoring-rules 8d monitoring node-exporter-rules 8d monitoring prometheus-k8s-prometheus-rules 8d monitoring prometheus-operator-rules 8d prometheusrule 定义一系列报警规则\napiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: name: demo namespace: monitoring spec: groups: - name: group1 rules: - alert: alert1 annotations: description: alert-1 summary: alert-1 expr: up == 0 for: 15m labels: severity: critical - alert: alert2 ...... - name: group2 rules: - alert: alert3 ...... 对于 prometheusrule 的更新操作(create, delete, update)都会被 watch 到, 然后更新到统一的一个 configmap 中, 然后 prometheus 自动重载配置\n每个 prometheusrule 会作为 configmap prometheus-k8s-rulefiles-0 中的一个 data , data 的命名规则为 \u0026lt;namespace\u0026gt;-\u0026lt;rulename\u0026gt;-ruleuid\n# kubectl get cm prometheus-k8s-rulefiles-0 -n monitoring NAME DATA AGE prometheus-k8s-rulefiles-0 7 41m # prometheus 实例的挂载信息 # kubectl get pod prometheus-k8s-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;prometheus-k8s-rulefiles-0\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;configMap\u0026#34;: { \u0026#34;defaultMode\u0026#34;: 420, \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-rulefiles-0\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-rulefiles-0\u0026#34; } # prometheus 中实际的存储路径 # kubectl exec -it prometheus-k8s-0 -n monitoring -- ls /etc/prometheus/rules/prometheus-k8s-rulefiles-0/ monitoring-alertmanager-main-rules-79a2aba8-1a50-4bbc-b201-e9c8ee43e6aa.yaml monitoring-kube-prometheus-rules-9867eba7-cd4c-4677-b931-4268744ae5e7.yaml monitoring-kube-state-metrics-rules-b787fea0-dba2-4d6d-9fd6-0b470ce45059.yaml monitoring-kubernetes-monitoring-rules-b1939032-6a22-4ce1-b0ce-6482db094018.yaml monitoring-node-exporter-rules-0140bdd4-b858-4672-85be-930eabdc95eb.yaml monitoring-prometheus-k8s-prometheus-rules-87a80a69-f3be-4d3e-8a26-e1da2ade3a0a.yaml monitoring-prometheus-operator-rules-8688aa7b-a157-4ddc-bd09-21781f8ac567.yaml prometheus 的配置中定义了 rule_files 路径\n2. 示例 2.1 磁盘使用率 当磁盘可用空间少于 50% 时触发告警\n创建 prometheusrule\napiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: demo namespace: monitoring spec: groups: - name: demo rules: - alert: nodeDiskUsage annotations: description: | 节点 {{$labels.instance }} 挂载目录 {{ $labels.mountpoint }} 当前可用空间 {{ printf \u0026#34;%.2f\u0026#34; $value }}% summary: | 挂载目录可用空间低于 50% expr: | node_filesystem_avail_bytes{fstype!=\u0026#34;\u0026#34;,job=\u0026#34;node-exporter\u0026#34;} / node_filesystem_size_bytes{fstype!=\u0026#34;\u0026#34;,job=\u0026#34;node-exporter\u0026#34;} * 100 \u0026lt; 50 for: 1m labels: severity: warning 查看生成的告警规则, 当前状态是 inactive\nnode1 当前状态, / 目录总容量 45G, 可用空间为 82%\n[root@k8s-node1 manifests]# df -hT | head -1 \u0026amp;\u0026amp; df -hT | grep -E \u0026#34;/$\u0026#34; Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/centos_one-root xfs 45G 7.8G 38G 18% / 接下来用 dd 手动创建一个 25G 的大文件, 此时剩余空间仅剩 27%\n[root@k8s-node1 manifests]# dd if=/dev/zero of=/tmp/demo bs=1G count=25 [root@k8s-node1 manifests]# df -hT | head -1 \u0026amp;\u0026amp; df -hT | grep -E \u0026#34;/$\u0026#34; Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/centos_one-root xfs 45G 33G 13G 73% / 此时告警规则已经进入 pending 状态了, 我们设置了 1m 的评估等待时间\n一分钟过后进入 firing 状态, 正式发出告警, 此时我们设置的 $label 还没有解析\n我们去 alertmanager 看一下, 成功收到了告警, 且 $labels 和 $value 也已经正常解析了\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-5-rule/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1. 告警规则 prometheus 支持两种类型的规则, 记录规则 recording rule 和告警规则 alerting rule 1.1 recording rule 记录规则: 允许预先计算经常需要或计算量大的表达式，并将其结果保存为一组新的时间序列。查询预先计算的结果通常比每次需要时都执行原始表达式要快得多。这对于每次刷新时都需要重复查询相同表达式的仪表板特别","title":"prometheus (五) 记录规则与告警规则"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1. 简介 Probe 的 API 文档\n1.1 白盒监控vs黑盒监控 白盒监控: 我们监控主机的资源用量、容器的运行状态、数据库中间件的运行数据等等，这些都是支持业务和服务的基础设施，通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化\n黑盒监控: 以用户的身份测试服务的外部可见性，常见的黑盒监控包括 HTTP 探针 TCP 探针 等用于检测站点或者服务的可访问性，以及访问效率等。\n黑盒监控相较于白盒监控最大的不同在于黑盒监控是以故障为导向的. 当故障发生时，黑盒监控能快速发现故障，而白盒监控则侧重于主动发现或者预测潜在的问题。一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题。\n1.2 blackbox exporter Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过 HTTP HTTPS DNS TCP ICMP 以及 gPRC 的方式对 endpoints 端点进行探测。\n在 kube-prometheus 的默认配置中已经部署了 Blackbox Exporter 以供用户使用\n[root@k8s-node1 kube-prometheus]# ls manifests/blackboxExporter-* | cat manifests/blackboxExporter-clusterRoleBinding.yaml manifests/blackboxExporter-clusterRole.yaml manifests/blackboxExporter-configuration.yaml manifests/blackboxExporter-deployment.yaml manifests/blackboxExporter-serviceAccount.yaml manifests/blackboxExporter-serviceMonitor.yaml manifests/blackboxExporter-service.yaml 1.3 Probe CRD prometheus-operator 提供了一个 Probe CRD 对象，可以用来进行黑盒监控，具体的探测功能由 Blackbox-exporter 实现。\nProbe 支持 staticConfig 和 ingress 两种配置方式, 使用 ingress 时可以自动发现 ingress 代理的 url 并进行探测\n大概步骤:\n首先，用户创建一个 Probe CRD 对象，对象中指定探测方式、探测目标等参数； 然后，prometheus-operator watch 到 Probe 对象创建，然后生成对应的 prometheus 拉取配置，reload 到prometheus 中； 最后，prometheus 使用 url=/probe?target={探测目标}\u0026amp;module={探测方式}，拉取 blackbox-exporter ，此时 blackbox-exporter 会对目标进行探测，并以 metrics 格式返回探测结果； 2. probe 示例 2.1 staticConfig 2.1.1 kube-dns 使用黑盒监控监测 kube-dns 的可用性\n默认配置下的 blackbox exporter 未开启 dns 模块, 我们手动开启一下\n修改 blackboxExporter-configuration.yaml 文件, 添加 dns 模块\napiVersion: v1 data: config.yml: |- \u0026#34;modules\u0026#34;: \u0026#34;dns\u0026#34;: # DNS 检测模块 \u0026#34;prober\u0026#34;: \u0026#34;dns\u0026#34; \u0026#34;dns\u0026#34;: \u0026#34;transport_protocol\u0026#34;: \u0026#34;tcp\u0026#34; # 默认是 udp \u0026#34;preferred_ip_protocol\u0026#34;: \u0026#34;ip4\u0026#34; # 默认是 ip6 \u0026#34;query_name\u0026#34;: \u0026#34;kubernetes.default.svc.cluster.local\u0026#34; 更新 configmap 配置文件, prometheus-opertor 会 watch 到更新然后通过 pod 中的 module-configmap-reloader 容器通知 blackbox-exporter 重载配置\n# 每个 blackbox-exporter POD 中有三个 container [root@k8s-node1 manifests]# kubectl get pods -n monitoring -l app.kubernetes.io/name=blackbox-exporter \\ -o jsonpath=\u0026#39;{.items[*].spec.containers[*].name}{\u0026#34;\\n\u0026#34;}\u0026#39; blackbox-exporter module-configmap-reloader kube-rbac-proxy [root@k8s-node1 manifests]# kubectl apply -f blackboxExporter-configuration.yaml configmap/blackbox-exporter-configuration configured [root@k8s-node1 manifests]# kubectl logs -n monitoring -l app.kubernetes.io/name=blackbox-exporter | tail -2 level=info ts=2023-04-23T02:23:49.614Z caller=tls_config.go:191 msg=\u0026#34;TLS is disabled.\u0026#34; http2=false level=info ts=2023-04-28T06:40:48.168Z caller=main.go:278 msg=\u0026#34;Reloaded config file\u0026#34; 创建 Probe CRD 资源\napiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: blackbox-kube-dns namespace: monitoring spec: jobName: blackbox-kube-dns interval: 10s module: dns prober: # 指定blackbox的地址 url: blackbox-exporter:19115 # blackbox-exporter 的 地址 和 端口 path: /probe # 路径 targets: staticConfig: static: - kube-dns.kube-system:53 # 要检测的 url 查看生成的 target\n现在可以通过:\nprobe_success{job=\u0026quot;blackbox-kube-dns\u0026quot;} 查看服务状态是否可用 probe_dns_lookup_time_seconds{job='blackbox-kube-dns'} DNS解析耗时 查看 blackbox exporter 一次 dns 探测生成的 metrics 指标\n2.2.1 http http 探测一般使用 http_2xx 模块, 虽然默认有这个模块, 但是默认的配置不太合理, 我们修改一下\napiVersion: v1 data: config.yml: |- \u0026#34;modules\u0026#34;: \u0026#34;dns\u0026#34;: # DNS 检测模块 \u0026#34;prober\u0026#34;: \u0026#34;dns\u0026#34; \u0026#34;dns\u0026#34;: \u0026#34;transport_protocol\u0026#34;: \u0026#34;tcp\u0026#34; # 默认是 udp \u0026#34;preferred_ip_protocol\u0026#34;: \u0026#34;ip4\u0026#34; # 默认是 ip6 \u0026#34;query_name\u0026#34;: \u0026#34;kubernetes.default.svc.cluster.local\u0026#34; \u0026#34;http_2xx\u0026#34;: \u0026#34;http\u0026#34;: \u0026#34;preferred_ip_protocol\u0026#34;: \u0026#34;ip4\u0026#34; \u0026#34;valid_status_codes\u0026#34;: \u0026#34;[200]\u0026#34; # 最好加上状态码, 方便 grafana 展示数据 \u0026#34;valid_http_versions\u0026#34;: [\u0026#34;HTTP/1.1\u0026#34;, \u0026#34;HTTP/2\u0026#34;] \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34; \u0026#34;prober\u0026#34;: \u0026#34;http\u0026#34; 更新配置\n[root@k8s-node1 manifests]# kubectl apply -f blackboxExporter-configuration.yaml configmap/blackbox-exporter-configuration configured 快速创建一个 nginx 应用\nkubectl create deployment nginx --image=nginx:1.22.1 --port=80 kubectl expose deployment nginx --name=nginx --port=80 --target-port=80 创建 Probe 资源\napiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: blackbox-http-nginx namespace: monitoring spec: jobName: blackbox-http-nginx prober: url: blackbox-exporter:19115 path: /probe module: http_2xx # 配置文件中的检测模块 targets: staticConfig: static: - nginx.default 查看生成的 target\n查看一次 http 探测生成的 metrics 指标\n2.2 ingress自动发现 接下来使用 ingrss 自动发现实现集群内的 ingress 并进行黑盒探测\n先创建两个 web 应用\nkubectl create deployment web-1 --image=nginx:1.22.1 --port=80 kubectl expose deployment web-1 --name=web-1 --port=80 --target-port=80 kubectl create deployment web-2 --image=nginx:1.22.1 --port=80 kubectl expose deployment web-2 --name=web-2 --port=80 --target-port=80 2.2.1 创建 ingress 包含三个路径: http://web1.test.com http://web1.test.com/test http://web2.test.com\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: demo-web namespace: default labels: prometheus.io/http-probe: \u0026#34;true\u0026#34; # 用于监测 annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/router.entrypoints: web spec: rules: - host: web1.test.com http: paths: - pathType: Prefix path: / backend: service: name: web-1 port: number: 80 - pathType: Prefix path: /test backend: service: name: web-1 port: number: 80 - host: web2.test.com http: paths: - pathType: Prefix path: / backend: service: name: web-2 port: number: 80 部署并访问测试\n[root@k8s-node1 manifests]# echo \u0026#34;1.1.1.1 web1.test.com web2.test.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts [root@k8s-node1 manifests]# curl -sI web1.test.com | head -1 HTTP/1.1 200 OK [root@k8s-node1 manifests]# curl -sI web1.test.com/test | head -1 HTTP/1.1 404 Not Found [root@k8s-node1 manifests]# curl -sI web2.test.com | head -1 HTTP/1.1 200 OK 2.2.2 创建 probe 可以使用 label 或者 annotation 两种方式筛选监测的 ingress, 不配置监测所有 ingress\napiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: blackbox-ingress namespace: monitoring spec: jobName: blackbox-ingress prober: url: blackbox-exporter:19115 path: /probe module: http_2xx targets: ingress: namespaceSelector: # 监测所有 namespace # any: true # 只监测指定 namespace 的 ingress matchNames: - default - monitoring # 只监测配置了标签 prometheus.io/http-probe: true 的 ingress selector: matchLabels: prometheus.io/http-probe: \u0026#34;true\u0026#34; # 只监测配置了注解 prometheus.io/http_probe: true 的 ingress # relabelingConfigs: # - action: keep # sourceLabels: # - __meta_kubernetes_ingress_annotation_prometheus_io_http_probe # regex: \u0026#34;true\u0026#34; 查看 targets\n不过由于 ingress 配置的域名无法解析, 所以监测到的状态是失败的:\n2.2.3 配置 coredns 我们为 coredns 添加自定义 hosts, 实现 blackbox 可以解析到我们的自定义域名\n# kubectl edit cm coredns -n kube-system apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } hosts { # 添加 hosts 配置 1.1.1.1 web1.test.com 1.1.1.1 web2.test.com fallthrough } cache 30 loop reload loadbalance } kind: ConfigMap 等待 coredns 的配置生效后, 我们重新再看一下 target 状态\n3. additionalScrapeConfigs 目前 prometheus operator 只支持 ingress 方式的自动发现, 而且自定义配置其实不是很多, 更推荐使用 additionalScrapeConfigs 静态配置的方式实现\n3.2 service自动发现 沿用 2.2 章节中创建的两个 service\n修改 service web-1, 添加 annotation\n# kubectl edit svc web-1 metadata: annotations: prometheus.io/http-probe: \u0026#34;true\u0026#34; # 控制是否监测 prometheus.io/http-probe-path: / # 控制监测路径 prometheus.io/http-probe-port: \u0026#34;80\u0026#34; # 控制监测端口 修改静态配置\n# vim prometheus-additional.yaml - job_name: \u0026#34;kubernetes-services\u0026#34; metrics_path: /probe params: module: - \u0026#34;http_2xx\u0026#34; ## 使用 Kubernetes 动态服务发现,且使用 Service 类型的发现 kubernetes_sd_configs: - role: service relabel_configs: ## 设置只监测 Annotation 里配置了 prometheus.io/http_probe: true 的 service - action: keep source_labels: [__meta_kubernetes_service_annotation_prometheus_io_http_probe] regex: \u0026#34;true\u0026#34; - action: replace source_labels: - \u0026#34;__meta_kubernetes_service_name\u0026#34; - \u0026#34;__meta_kubernetes_namespace\u0026#34; - \u0026#34;__meta_kubernetes_service_annotation_prometheus_io_http_probe_port\u0026#34; - \u0026#34;__meta_kubernetes_service_annotation_prometheus_io_http_probe_path\u0026#34; target_label: __param_target regex: (.+);(.+);(.+);(.+) replacement: $1.$2:$3$4 - target_label: __address__ replacement: blackbox-exporter:19115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name 更新 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 确保 prometheus CRD 实例配置了 secret\n[root@k8s-node1 manifests]# grep -A2 additionalScrapeConfigs prometheus-prometheus.yaml additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 查看 target, 可以看到只有 web-1 发现成功, 因为 web-2 没有配置 annotation\n查看一下监测状态, 直接使用 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt; 访问, 在集群内是可以正常解析的, 所以这里 http 状态码为正常的 200\n3.1 ingress自动发现 依旧使用 2.2 章节中创建的 ingress\n为 ingress 添加 annotation 注解\n# kubectl edit ingress demo-web annotations: # 添加如下项 prometheus.io/http-probe: \u0026#34;true\u0026#34; # 用于控制是否监测 prometheus.io/http-probe-port: \u0026#34;80\u0026#34; # 用于控制监测端口 修改静态配置\n# cat prometheus-additional.yaml - job_name: \u0026#34;kubernetes-ingresses\u0026#34; metrics_path: /probe params: module: - \u0026#34;http_2xx\u0026#34; ## 使用 Kubernetes 动态服务发现,且使用 ingress 类型的发现 kubernetes_sd_configs: - role: ingress relabel_configs: ## 设置只监测 Annotation 里配置了 prometheus.io/http_probe: true 的 ingress - action: keep source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_http_probe] regex: \u0026#34;true\u0026#34; - action: replace source_labels: - \u0026#34;__meta_kubernetes_ingress_scheme\u0026#34; - \u0026#34;__meta_kubernetes_ingress_host\u0026#34; - \u0026#34;__meta_kubernetes_ingress_annotation_prometheus_io_http_probe_port\u0026#34; - \u0026#34;__meta_kubernetes_ingress_path\u0026#34; target_label: __param_target regex: (.+);(.+);(.+);(.+) replacement: ${1}://${2}:${3}${4} - target_label: __address__ replacement: blackbox-exporter:19115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: ingress_name 更新 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 确保 prometheus CRD 实例配置了 secret\n[root@k8s-node1 manifests]# grep -A2 additionalScrapeConfigs prometheus-prometheus.yaml additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 查看 target\n查看监测状态, 与预期配置一样\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-4-blackbox-probe/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1. 简介 Probe 的 API 文档 1.1 白盒监控vs黑盒监控 白盒监控: 我们监控主机的资源用量、容器的运行状态、数据库中间件的运行数据等等，这些都是支持业务和服务的基础设施，通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优","title":"prometheus (四) 黑盒监控"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1. 简介 手动添加 job 配置未免过于繁琐, prometheus 支持很多种方式的服务发现, 在 k8s 中是通过 kubernetes_sd_config 配置实现的. 通过抓取 k8s REST API 自动发现我们部署在 k8s 集群中的 exporter 实例\n在 Prometheus Operator 中, 我们无需手动编辑配置文件添加 kubernetes_sd_config 配置, Prometheus Operator 提供了下述资源:\nserviceMonitor: 创建 endpoints 级别的服务发现 podMonitor: 创建 pod 级别的服务发现 probe: 创建 ingress 级别的服务发现(用于黑盒监控) 通过对这三种 CRD 资源的管理实现 prometheus 动态的服务发现.\n1.1 kubernetes_sd_config kubernets_sd_config 官方文档\nkubernetes_sd_config 支持 node service pod endpoints ingress 5 种服务发现模式.\nnode 适用于与主机相关的监控资源，如 Kubernetes 组件状态、节点上运行的容器状态等； service 和 ingress 用于黑盒监控的场景，如对服务的可用性以及服务质量的监控； endpoints 和 pod 用于获取 Pod 的监控数据，如监控用户部署的支持 Prometheus 的应用。 每种发现模式都支持很多 label, prometheus 可以通过 relabel_config 分析这些标签进行标签重写或者丢弃 target\n在 kube-prometheus 的模板配置中, 所有的 target 都是通过 endpoints 模式实现的.\nendpoints 模式的自动发现会添加 endpoints 后端所有 pod 暴露出来的所有 port. 如下所示\n# 共有 10 个 endpoints, 后端包含 15 个 pod, 所有 ip+port 的组合有 44 个 endpoints ===\u0026gt; pods ===\u0026gt; pod-ip+port 10 15 44 同样, 在 prometheus 后端看到的 targets 将会是 44 个, 然后按照 relabel 规则在这些所有的 target 中选择合适的 target 并进行 active\n2. serviceMonitor CRD 2.1 node-exporter 以上节部署的 kube-prometheus 为例, 学习 prometheus 如何通过 endpoints 模式的服务发现来添加我们创建的 node-exporter 为 target\n需要注意的是, 与一般部署的 node-exporter 不同, kube-prometheus 额外创建了一个 headless service, 随着 service 创建的 endpoints 将用于 prometheus 的自动发现.\n# cat manifests/nodeExporter-service.yaml apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 1.3.1 name: node-exporter namespace: monitoring spec: clusterIP: None ports: - name: https port: 9100 targetPort: https selector: app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus 查看 endpoints\n[root@k8s-node1 ~]# kubectl get ep -n monitoring -l app.kubernetes.io/name=node-exporter NAME ENDPOINTS AGE node-exporter 1.1.1.1:9100,1.1.1.2:9100,1.1.1.3:9100 10d 查看 serviceMonitor\n# cat manifests/nodeExporter-serviceMonitor.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 1.3.1 name: node-exporter namespace: monitoring spec: endpoints: - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token interval: 15s port: https relabelings: - action: replace regex: (.*) replacement: $1 sourceLabels: - __meta_kubernetes_pod_node_name targetLabel: instance scheme: https tlsConfig: insecureSkipVerify: true jobLabel: app.kubernetes.io/name selector: matchLabels: # 标签匹配规则, 符合 label 条件的 target 才会被 active app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus 上述的 serviceMonitor 将会为 prometheus 生成一个 job, 使用了 endpoints 模式的 kubernetes_sd_config, 用于自动发现集群内符合条件的 node-exporter\n##### job 基础信息 ######## - job_name: serviceMonitor/monitoring/node-exporter/0 honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: https authorization: type: Bearer credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: insecure_skip_verify: true follow_redirects: true ##### kubernetes_sd_configs 自动发现的配置 ######### kubernetes_sd_configs: - role: endpoints kubeconfig_file: \u0026#34;\u0026#34; follow_redirects: true namespaces: names: - monitoring ##### 开始执行 relabel, 第一个规则是替换任务名 ################ relabel_configs: - source_labels: [job] separator: ; regex: (.*) target_label: __tmp_prometheus_job_name replacement: $1 action: replace ###### 以下是匹配规则, 如果不满足 label 匹配规则就丢弃 target, 对应 serviceMonitor 配置的 matchlabels ###### - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_component, __meta_kubernetes_service_labelpresent_app_kubernetes_io_component] separator: ; regex: (exporter);true replacement: $1 action: keep # 如果不满足, 丢弃此 target - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name, __meta_kubernetes_service_labelpresent_app_kubernetes_io_name] separator: ; regex: (node-exporter);true replacement: $1 action: keep - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_part_of, __meta_kubernetes_service_labelpresent_app_kubernetes_io_part_of] separator: ; regex: (kube-prometheus);true replacement: $1 action: keep - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: https replacement: $1 action: keep ##### 以下是 replace 替换标签的操作, 可以使我们的 target 标签有更好的可读性 ##### - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Node;(.*) target_label: node replacement: ${1} action: replace - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Pod;(.*) target_label: pod replacement: ${1} action: replace - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: service replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_container_name] separator: ; regex: (.*) target_label: container replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: job replacement: ${1} action: replace - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name] separator: ; regex: (.+) target_label: job replacement: ${1} action: replace - separator: ; regex: (.*) target_label: endpoint replacement: https action: replace - source_labels: [__meta_kubernetes_pod_node_name] separator: ; regex: (.*) target_label: instance replacement: $1 action: replace - source_labels: [__address__] separator: ; regex: (.*) modulus: 1 target_label: __tmp_hash replacement: $1 action: hashmod - source_labels: [__tmp_hash] separator: ; regex: \u0026#34;0\u0026#34; replacement: $1 action: keep 在 prometheus 的服务发现界面可以看到采集到的所有 target, 每个 target 就对应了一个 pod-ip+Port ,每个 target 含有许多原始标签, relebal_config 就是针对这些标签进行筛选和重写等其他操作.\nendpoints 级别的标签\nservice 和 pod 级别的标签\n查看自动注册到 prometheus 的 node-exporter\n可以发现:\n经过 keep 规则成功从 44 个 target 中筛选到了对应的 node-exporter 经过 replace 规则之后 target-labels 有了更好的可读性 2.2 traefik 接下来演示一下通过创建 serviceMonitor 实现采集 traefik 的 metrics 指标, traefik 安装请参考 traefik系列文章\n在配置中开启 metric\n访问测试\n[root@k8s-node1 ~]# kubectl get svc traefik-metrics -n traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-metrics ClusterIP 10.103.102.23 \u0026lt;none\u0026gt; 9100/TCP 19s [root@k8s-node1 ~]# [root@k8s-node1 ~]# curl -s 10.103.102.23:9100/metrics | head -5 # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 4.8217e-05 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 7.3819e-05 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000203355 2.2.1 rbac 创建一个用于访问 traefik 命名空间的 role\n修改 manifests/prometheus-roleSpecificNamespaces.yaml, 新增如下配置\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.32.1 name: prometheus-k8s namespace: traefik rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - pods verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch 将上一步创建的 role 与 serviceAccount prometheus-k8s 绑定\n修改 manifests/prometheus-roleBindingSpecificNamespaces.yaml, 新增如下配置\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.32.1 name: prometheus-k8s namespace: traefik roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring 验证\n[root@k8s-node1 kube-prometheus]# kubectl get role,rolebinding -n traefik NAME CREATED AT role.rbac.authorization.k8s.io/prometheus-k8s 2023-04-26T06:50:22Z NAME ROLE AGE rolebinding.rbac.authorization.k8s.io/prometheus-k8s Role/prometheus-k8s 75m 2.2.2 serviceMonitor 创建 serviceMonitor\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: traefik namespace: monitoring labels: app.kubernetes.io/name: traefik spec: jobLabel: app.kubernetes.io/name endpoints: - interval: 15s port: metrics # endpoint(service) 中定义的 portName path: /metrics # metrics 访问路径 namespaceSelector: # 指定 namespace matchNames: - traefik selector: matchLabels: app: traefik-metrics # endpoint 的 label 筛选 部署\nkubectl apply -f manifests/traefik-serviceMonitor.yml 2.2.3 验证 prometheus 的 configuration 界面自动生成的配置如下\n- job_name: serviceMonitor/monitoring/traefik/0 honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http follow_redirects: true relabel_configs: - source_labels: [job] separator: ; regex: (.*) target_label: __tmp_prometheus_job_name replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_label_app, __meta_kubernetes_service_labelpresent_app] separator: ; regex: (traefik-metrics);true replacement: $1 action: keep - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: metrics replacement: $1 action: keep - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Node;(.*) target_label: node replacement: ${1} action: replace - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Pod;(.*) target_label: pod replacement: ${1} action: replace - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: service replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_container_name] separator: ; regex: (.*) target_label: container replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: job replacement: ${1} action: replace - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name] separator: ; regex: (.+) target_label: job replacement: ${1} action: replace - separator: ; regex: (.*) target_label: endpoint replacement: metrics action: replace - source_labels: [__address__] separator: ; regex: (.*) modulus: 1 target_label: __tmp_hash replacement: $1 action: hashmod - source_labels: [__tmp_hash] separator: ; regex: \u0026#34;0\u0026#34; replacement: $1 action: keep kubernetes_sd_configs: - role: endpoints kubeconfig_file: \u0026#34;\u0026#34; follow_redirects: true namespaces: names: - traefik Service Discovery 界面\nTargets 界面\n3. podMonitor CRD 3.1 calico-node 以 calico 为例, 使用 podMonitor 资源监控 calico-node\ncalico 中核心的组件是 Felix，它负责设置路由表和 ACL 规则，同时还负责提供网络健康状况的数据；这些数据会被写入etcd。\n监控 calico 的核心便是监控 felix，felix 相当于 calico 的大脑。\n如下所示, 一般 calico-node 都是使用 daemonset 方式部署在集群中的\n[root@k8s-node1 ~]# kubectl get pods -n kube-system -l k8s-app=calico-node -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES calico-node-8p9w5 1/1 Running 2 (5d16h ago) 7d23h 1.1.1.2 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-bw2kb 1/1 Running 2 (5d16h ago) 7d23h 1.1.1.1 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-gt688 1/1 Running 2 (5d16h ago) 7d23h 1.1.1.3 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node 默认是没有打开 metrics 监听的, 我们修改 calico 的 daemonset 配置文件, 可以直接修改部署时的 yaml, 也可以 kubectl edit ds calico-node -n kube-system\nkind: DaemonSet spec: template: spec: containers: - name: calico-node env: # 添加如下配置, 开启 FELIX 的 metrics 的监听端口为 9101 (9100 端口被 node-exporter 占据了) - name: FELIX_PROMETHEUSMETRICSENABLED value: \u0026#34;True\u0026#34; - name: FELIX_PROMETHEUSMETRICSPORT value: \u0026#34;9101\u0026#34; ports: - containerPort: 9101 name: metrics protocol: TCP 修改完等待 calico-node 的 pod 重新部署, 然后访问测试\n[root@k8s-node1 ~]# curl -s k8s-node2:9101/metrics | head -6 # HELP felix_active_local_endpoints Number of active endpoints on this host. # TYPE felix_active_local_endpoints gauge felix_active_local_endpoints 9 # HELP felix_active_local_policies Number of active policies on this host. # TYPE felix_active_local_policies gauge felix_active_local_policies 0 由于 kube-prometheus 默认已经帮我们创建了基于 kube-sysetm 命名空间的授权, 我们无需再额外配置\n创建 podMonitor\napiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: labels: app.kubernetes.io/name: calico-node name: calico-node namespace: monitoring spec: jobLabel: app.kubernetes.io/name podMetricsEndpoints: - interval: 15s path: /metrics port: metrics # 确保与 calico-node 的 containerPort 名称一致 namespaceSelector: matchNames: - kube-system # 确保命名空间正确 selector: matchLabels: k8s-app: calico-node # 确保 label 配置正确 查看 prometheus\n4. 集群范围的自动发现 当我们的 k8s 集群中 service 和 pod 达到一定规模后手动一个一个创建 serviceMonitor 和 podMonitor 不免又麻烦了起来, 我们可以使用不限制 namespace 的 kubernetes_sd_configs 实现集群范围内自动发现所有的 exporter 实例\n接下来的演示中我们监控集群范围内的所有 endpoints, 并且将带有 prometheus.io/scrape=true 这个 annotations 的 service 注册到 prometheus\n4.1 rbac 由于需要访问访问集群范围内的资源对象, 继续使用 role+roleBinding 模式显然不适合, prometheus-k8s 这个 serviceAccount 还绑定一个名为 prometheus-k8s 的 clusterRole, 该 clusterRole 默认权限是不够的, 添加需要的权限:\n# vim manifests/prometheus-clusterRole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.32.1 name: prometheus-k8s namespace: monitoring rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes - services - endpoints - pods - nodes/metrics verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get # kubectl apply -f manifests/prometheus-clusterRole.yaml 4.2 自动发现配置 通过上一篇文章中的 additionalScrapeConfigs 添加自动发现配置\n# vim prometheus-additional.yaml - job_name: \u0026#39;kubernetes-service-endpoints\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name 修改 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 确保 prometheus CRD 中添加了 additionalScrapeConfigs 配置\nkind: Prometheus spec: additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 4.3 验证 service kube-dns 默认有 prometheus.io/scrape=true 这个注解, 已经成功注册:\n创建一个示例应用\n# vim node-exporter-deploy-svc.yml apiVersion: v1 kind: Namespace metadata: name: test --- apiVersion: v1 kind: Service metadata: name: test-node-exporter namespace: test labels: app: test-node-exporter annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;9200\u0026#34; spec: selector: app: test-node-exporter ports: - name: metrics port: 9200 targetPort: metrics --- apiVersion: apps/v1 kind: Deployment metadata: name: test-node-exporter namespace: test labels: app: test-node-exporter spec: replicas: 1 selector: matchLabels: app: test-node-exporter template: metadata: labels: app: test-node-exporter spec: containers: - args: - --web.listen-address=:9200 image: registry.cn-hangzhou.aliyuncs.com/lvbibir/node-exporter:v1.3.1 name: node-exporter ports: - name: metrics containerPort: 9200 # kubectl apply -f node-exporter-deploy-svc.yml 如下, 我们部署的 node-exporter 已经成功注册, 只要 service 设置了 annotations 即可\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-3-servicemonitor-podmonitor/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1. 简介 手动添加 job 配置未免过于繁琐, prometheus 支持很多种方式的服务发现, 在 k8s 中是通过 kubernetes_sd_config 配置实现的. 通过抓取 k8s REST API 自动发现我们部署在 k8s 集群中的 exporter 实例 在 Prometheus Operator 中, 我们无需手动编辑配置文件添加 kubernetes_sd_config 配置, Prometheus Operator 提供了下述资源: serviceMonitor: 创建 endpoints 级别的服务发现 podMonitor: 创建 pod 级别的服务发现 probe: 创建 ingress 级别的","title":"prometheus (三) 服务发现"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1. 简介 使用原生的 prometheus 时, 我们创建 job 直接修改配置文件即可, 然而在 prometheus-operator 中所有的配置都抽象成了 k8s CRD 资源, 手动配置 job 需要:\n创建 secret 在 prometheus CRD 资源中配置 additionalScrapeConfigs additional-scrape-config 官方示例\n2. 示例 2.1 node-exporter 添加 k8s 集群外的 node-exporter metrics\n在 1.1.1.4 部署 node-exporter\ndocker run -d --name node-exporter \\ -p 9102:9100 \\ -v \u0026#34;/proc:/host/proc:ro\u0026#34; \\ -v \u0026#34;/sys:/host/sys:ro\u0026#34; \\ -v \u0026#34;/:/rootfs:ro\u0026#34; \\ registry.cn-hangzhou.aliyuncs.com/lvbibir/node-exporter:v1.3.1 \\ --path.sysfs=/host/sys \\ --path.rootfs=/roofs # 验证可用性 [root@1-1-1-4 ~]# curl -s 1.1.1.4:9100/metrics | head -5 # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 0.000326036 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 0.000326036 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000714158 创建 job 配置 prometheus-additional.yaml\n- job_name: \u0026#34;node-exporter\u0026#34; static_configs: - targets: - \u0026#34;1.1.1.4:9100\u0026#34; 创建 secret additional-scrape-configs.yaml\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 修改 prometheus 资源 prometheus-prometheus.yaml , 添加 additionalScrapeConfigs\nkind: Prometheus spec: # 添加如下三行 additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 更新一下 prometheus\n[root@k8s-node1 demo]# kubectl apply -f ../prometheus-prometheus.yaml 查看结果\n3. 动态更新 后续所有的自定义配置直接更新现有的 secret 即可\n比如在之前的 node-exporter 的 job 中新增一个 target\n修改 prometheus-additional.yaml\n- job_name: \u0026#34;node-exporter\u0026#34; static_configs: - targets: - \u0026#34;1.1.1.4:9100\u0026#34; - \u0026#34;192.168.17.99:59100\u0026#34; 更新 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml prometheus 会自动重载配置\n4. 更新失败排查 如果修改了 secret 不生效一定要注意 secret 部署的 namespace 是不是 monitoring\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-2-additionalscrapeconfig/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1. 简介 使用原生的 prometheus 时, 我们创建 job 直接修改配置文件即可, 然而在 prometheus-operator 中所有的配置都抽象成了 k8s CRD 资源, 手动配置 job 需要: 创建 secret 在 prometheus CRD 资源中配置 additionalScrapeConfigs additional-scrape-config 官方示例 2. 示例 2.1 node-exporter 添加 k8s 集群外的 node-exporter metrics 在 1.1.1.4 部署 node-exporter docker run -d --name node-exporter \\ -p 9102:9100 \\ -v \u0026#34;/proc:/host/proc:ro\u0026#34; \\ -v \u0026#34;/sys:/host/sys:ro\u0026#34; \\ -v \u0026#34;/:/rootfs:ro\u0026#34; \\ registry.cn-hangzhou.aliyuncs.com/lvbibir/node-exporter:v1.3.1 \\ --path.sysfs=/host/sys \\ --path.rootfs=/roofs # 验证可用性 [root@1-1-1-4 ~]# curl -s 1.1.1.4:9100/metrics | head -5 # HELP","title":"prometheus (二) 静态配置"},{"content":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1. 简介 1.1 prometheus operator Prometheus Operator: 在 Kubernetes 上管理 Prometheus 集群。该项目的目的是简化和自动化基于 Prometheus 的 Kubernetes 集群监控堆栈的配置。\n所有 CRD 资源的 API 文档\nPrometheus Operator 的核心特性是 watch Kubernetes API 服务器对特定对象的更改，并确保当前 Prometheus 部署与这些对象匹配。\nmonitoring.coreos.com/v1:\nprometheus 相关\nPrometheus: 配置 Prometheus statefulset 及 Prometheus 的一些配置。 ServiceMonitor: 用于通过 Service 对 K8S 中的资源进行监控，推荐首选 ServiceMonitor. 它声明性地指定了 Kubernetes service 应该如何被监控。 PodMonitor: 用于对 Pod 进行监控，推荐首选 ServiceMonitor. PodMonitor 声明性地指定了应该如何监视一组 pod。 Probe: 它声明性地指定了应该如何监视 ingress 或静态目标组. 一般用于黑盒监控. PrometheusRule: 用于管理 Prometheus 告警规则；它定义了一套所需的 Prometheus 警报和/或记录规则。可以被 Prometheus 实例挂载使用。 Alertmanager 相关\nAlertmanager: 配置 AlertManager statefulset 及 AlertManager 的一些配置。 AlertmanagerConfig: 用于管理 AlertManager 配置文件；它声明性地指定 Alertmanager 配置的子部分，允许将警报路由到自定义接收器，并设置禁止规则。 其他\nThanosRuler: 管理 ThanosRuler deployment； 1.2 kube-prometheus kube-prometheus 提供了一个基于 Prometheus 和 Prometheus Operator 的完整集群监控堆栈的示例配置。这包括部署多个 Prometheus 和 Alertmanager 实例、用于收集节点指标的指标导出器（如 node_exporters)、将 Prometheus 链接到各种指标端点的目标配置，以及用于通知集群中潜在问题的示例警报规则。\n2. 部署 kubernets 与 kube-prometheus 的兼容性关系如下\nkube-prometheus stack Kubernetes 1.21 Kubernetes 1.22 Kubernetes 1.23 Kubernetes 1.24 Kubernetes 1.25 release-0.9 ✔ ✔ ✗ ✗ ✗ release-0.10 ✗ ✔ ✔ ✗ ✗ release-0.11 ✗ ✗ ✔ ✔ ✗ release-0.12 ✗ ✗ ✗ ✔ ✔ kube-prometheus 项目提供的 yaml 中使用的镜像大部分是 quay.io 或者 k8s.gcr.io 等外网仓库的镜像，博主已经将所需镜像上传到了阿里云，且 fork 官方仓库后修改了 yaml 中的镜像仓库地址，可以直接拉取我修改后的 yaml\n这里我的 k8s 测试集群版本是 1.22.3，部署 release-0.10 版本的 kube-prometheus\n[root@k8s-node1 opt]# cd /opt/ \u0026amp;\u0026amp; git clone https://github.com/lvbibir/kube-prometheus -b release-0.10 [root@k8s-node1 kube-prometheus]# cd /opt/kube-prometheus/ [root@k8s-node1 kube-prometheus]# kubectl create -f manifests/setup/ [root@k8s-node1 kube-prometheus]# kubectl create -f manifests/ 验证\n[root@k8s-node1 kube-prometheus]# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 5m16s alertmanager-main-1 2/2 Running 0 5m16s alertmanager-main-2 2/2 Running 0 5m16s blackbox-exporter-7c8787786-r9lmv 3/3 Running 0 8m12s grafana-795c6dd64b-8cspz 1/1 Running 0 8m11s kube-state-metrics-56f79b8fdc-9p97x 3/3 Running 0 8m11s node-exporter-4scm6 2/2 Running 0 8m11s node-exporter-7hlrp 2/2 Running 0 8m11s node-exporter-pph2d 2/2 Running 0 8m11s prometheus-adapter-5595dcc894-nmn2w 1/1 Running 0 8m10s prometheus-adapter-5595dcc894-rzdhv 1/1 Running 0 8m10s prometheus-k8s-0 2/2 Running 0 5m15s prometheus-k8s-1 2/2 Running 0 5m15s prometheus-operator-7575c94984-7r4l9 2/2 Running 0 8m10s [root@k8s-node1 kube-prometheus]# kubectl get svc -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-main NodePort 10.104.94.103 \u0026lt;none\u0026gt; 9093:39093/TCP,8080:10422/TCP 9m12s alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 6m16s blackbox-exporter ClusterIP 10.103.168.130 \u0026lt;none\u0026gt; 9115/TCP,19115/TCP 9m12s grafana NodePort 10.108.134.168 \u0026lt;none\u0026gt; 3000:33000/TCP 9m11s kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 9m11s node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 9m11s prometheus-adapter ClusterIP 10.111.22.126 \u0026lt;none\u0026gt; 443/TCP 9m10s prometheus-k8s NodePort 10.111.183.173 \u0026lt;none\u0026gt; 9090:39090/TCP,8080:43263/TCP 9m11s prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 6m15s prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 9m10s 可以通过 nodePort 访问，也可以通过 ingress 将 grafana 暴露到外部\ngrafana 默认用户名密码为 admin/admin\n2.1 组件介绍 kube-prometheus 中部署了如下组件:\nCRD 资源: manifests/setup 目录中的内容 实际的组件部署和配置: manifests 目录中的内容 exporter 数据采集器: node-exporter blackbox-exporter 用于黑盒监控 prometheus 实例 grafana 实例 alertmanager 实例 adapter 实例, 替代原始 metrics-server 组件, 实现自定义 HPA 指标, 参考 3. 数据持久化 3.1 prometheus prometheus 默认的数据文件使用的是 emptydir 方式进行的持久化, 我们改为 nfs\n修改 manifests/prometheus-prometheus.yaml\n在文件最后新增配置\nretention: 15d # 监控数据保存的时间为 15 天 storage: # 存储配置, 使用 nfs 的 storageClass volumeClaimTemplate: spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 应用后查看新建的 pod 中的 volumes 信息\n# pod.spec.container.volumeMounts [root@k8s-node1 ~]# kubectl get pod prometheus-k8s-0 -n monitoring -o jsonpath=\u0026#39;{.spec.containers[?(@.name==\u0026#34;prometheus\u0026#34;)].volumeMounts[?(@.name==\u0026#34;prometheus-k8s-db\u0026#34;)]}{\u0026#34;\\n\u0026#34;}\u0026#39; | python -m json.tool { \u0026#34;mountPath\u0026#34;: \u0026#34;/prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-db\u0026#34;, \u0026#34;subPath\u0026#34;: \u0026#34;prometheus-db\u0026#34; } # pod.spec.volumes [root@k8s-node1 ~]# kubectl get pod prometheus-k8s-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;prometheus-k8s-db\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-db\u0026#34;, \u0026#34;persistentVolumeClaim\u0026#34;: { \u0026#34;claimName\u0026#34;: \u0026#34;prometheus-k8s-db-prometheus-k8s-0\u0026#34; } } 与传统 statefulset 不同的是, prometheus 识别到 .spec.storage.volumeClaimTemplate 配置后会自动将 prometheus 的数据文件挂载到自动创建的 pvc 上, 无需手动指定 name 然后挂载\n3.2 alertmanager 与 prometheus 类似, 这里就不赘述了\nmanifests/alertmanager-alertmanager.yaml\nstorage: # 存储配置, 使用 nfs 的 storageClass volumeClaimTemplate: spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 重新 apply 之后, 查看新生成 pod 的 volumes\n# pod.spec.container.volumeMounts [root@k8s-node1 ~]# kubectl get pod alertmanager-main-0 -n monitoring -o jsonpath=\u0026#39;{.spec.containers[?(@.name==\u0026#34;alertmanager\u0026#34;)].volumeMounts[?(@.name==\u0026#34;alertmanager-main-db\u0026#34;)]}{\u0026#34;\\n\u0026#34;}\u0026#39; | python -m json.tool { \u0026#34;mountPath\u0026#34;: \u0026#34;/alertmanager\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;alertmanager-main-db\u0026#34;, \u0026#34;subPath\u0026#34;: \u0026#34;alertmanager-db\u0026#34; } # pod.spec.volumes [root@k8s-node1 ~]# kubectl get pod alertmanager-main-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;alertmanager-main-db\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;name\u0026#34;: \u0026#34;alertmanager-main-db\u0026#34;, \u0026#34;persistentVolumeClaim\u0026#34;: { \u0026#34;claimName\u0026#34;: \u0026#34;alertmanager-main-db-alertmanager-main-0\u0026#34; } } 3.3 grafana grafana 就是一个普通的 deployment 应用, 直接修改 yaml 中的 volume 配置即可\n[root@k8s-node1 ~]# mkdir /nfs/kubernetes/grafana-data [root@k8s-node1 ~]# chmod -R 777 /nfs/kubernetes/grafana-data 修改 manifests/grafana-deployment.yaml 直接将默认的 emptydir 修改为 nfs 即可\nvolumes: - name: grafana-storage nfs: server: k8s-node1 path: /nfs/kubernetes/grafana-data ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-1-deploy/","summary":"0. 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1. 简介 1.1 prometheus operator Prometheus Operator: 在 Kubernetes 上管理 Prometheus 集群。该项目的目的是简化和自动化基于 Prometheus 的 Kubernetes 集群监控堆栈的配置。 所有 CRD 资源的 API 文档 Prometheus Operator 的核心特性是 watch Kubernetes API 服务器对特定对象的更改，并确保当前 Prometheus 部署与这些对象匹配。 monitoring.coreos.com/v1: prometheus 相关 Prometheus: 配置 Prometheus statefulset 及 Prometheus 的一些配置。 ServiceMonitor: 用于通过 Service 对 K8S 中的资源进行监控，推荐首","title":"prometheus (一) 简介及部署"},{"content":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示\n1. 简介 traefik 的路由规则就可以实现 4 层和 7 层的基本负载均衡操作，使用 IngressRoute IngressRouteTCP IngressRouteUDP 资源即可。但是如果想要实现 加权轮询、流量复制 等高级操作，traefik抽象出了一个 TraefikService 资源。此时整体流量走向为：外部流量先通过 entryPoints 端口进入 traefik，然后由 IngressRoute/IngressRouteTCP/IngressRouteUDP 匹配后进入 TraefikService，在 TraefikService 这一层实现加权轮循和流量复制，最后将请求转发至kubernetes的service。\n除此之外traefik还支持7层的粘性会话、健康检查、传递请求头、响应转发、故障转移等操作。\n2. 灰度发布(加权轮询) 官方文档\n灰度发布也称为金丝雀发布，让一部分即将上线的服务发布到线上，观察是否达到上线要求，主要通过加权轮询的方式实现。\n创建 traefikService 和 inressRoute 资源，实现 wrr 加权轮询\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroutewrr namespace: default spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/`) kind: Rule services: - name: wrr namespace: default kind: TraefikService --- apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: wrr namespace: default spec: weighted: services: - name: myapp1 port: 80 weight: 1 # 定义权重 kind: Service # 可选，默认就是 Service - name: myapp2 port: 80 weight: 2 测试结果如下，可以看到每 3 次访问会有 1 次流量落到 v1 应用， 2 次流量落到 v2 应用\n[root@k8s-node1 ~]# for i in {1..9}; do curl http://myapp.test.com \u0026amp;\u0026amp; sleep 1; done Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 3. 会话保持(粘性会话) 官方文档\n会话保持功能依赖加权轮询功能\n当我们使用 traefik 的负载均衡时，默认情况下轮循多个 k8s 的 service 服务，如果用户对同一内容的多次请求，可能被转发到了不同的后端服务器。假设用户发出请求被分配至服务器 A，保存了一些信息在 session 中，该用户再次发送请求被分配到服务器 B，要用之前保存的信息，若服务器 A 和 B 之间没有 session 粘滞，那么服务器 B 就拿不到之前的信息，这样会导致一些问题。traefik 同样也支持粘性会话，可以让用户在一次会话周期内的所有请求始终转发到一台特定的后端服务器上。\n创建 traefikervie 和 ingressRoute，实现基于 cookie 的会话保持\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroute-sticky namespace: default spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/`) kind: Rule services: - name: sticky namespace: default kind: TraefikService --- apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: sticky namespace: default spec: weighted: services: - name: myapp1 port: 80 weight: 1 # 定义权重 - name: myapp2 port: 80 weight: 2 sticky: # 开启粘性会话 cookie: # 基于cookie区分客户端 name: test-cookie # 指定客户端请求时，包含的cookie名称 客户端访问测试，携带 cookie\n[root@k8s-node1 ~]# for i in {1..5}; do curl -b \u0026#34;test-cookie=default-myapp2-80\u0026#34; http://myapp.test.com; done Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# for i in {1..5}; do curl -b \u0026#34;test-cookie=default-myapp1-80\u0026#34; http://myapp.test.com; done Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 4. 流量复制 官方文档\n所谓的流量复制，也称为镜像服务是指将请求的流量按规则复制一份发送给其它服务，并且会忽略这部分请求的响应，这个功能在做一些压测或者问题复现的时候很有用。\n创建 traefikService 和 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroute-mirror namespace: default spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/`) kind: Rule services: - name: mirror-from-service namespace: default kind: TraefikService --- apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: mirror-from-service namespace: default spec: mirroring: name: myapp1 # 发送 100% 的请求到 myapp1 port: 80 mirrors: - name: myapp2 # 然后复制 10% 的请求到 myapp2 port: 80 percent: 10, 测试如下，可以看到只有 myapp1 应用会有数据返回\n[root@k8s-node1 ~]# for i in {1..20}; do curl http://myapp.test.com \u0026amp;\u0026amp; sleep 1; done Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; ....... myapp2 应用同样收到了请求，与预期相同，收到了 10% 的流量，\n[root@k8s-node1 ~]# kubectl logs -l app=bar .... 10.244.36.64 - - [20/Apr/2023:07:04:33 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 65 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; 10.244.36.64 - - [20/Apr/2023:07:04:43 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 65 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-4-traefikservice/","summary":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示 1. 简介 traefik 的路由规则就可以实现 4 层和 7 层的基本负载均衡操作，使用 IngressRoute IngressRouteTCP IngressRouteUDP 资源即可。但是如果想要实现 加权轮询、流量复制 等高级操作，t","title":"traefik (四) 服务(TraefikService)"},{"content":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示\n1. 简介 官方文档\nTraefik Middlewares 是一个处于路由和后端服务之前的中间件，在外部流量进入 Traefik，且路由规则匹配成功后，将流量发送到对应的后端服务前，先将其发给中间件进行一系列处理（类似于过滤器链 Filter，进行一系列处理），例如，添加 Header 头信息、鉴权、流量转发、处理访问路径前缀、IP 白名单等等，经过一个或者多个中间件处理完成后，再发送给后端服务，这个就是中间件的作用。 Traefik内置了很多不同功能的Middleware，主要是针对HTTP和TCP，这里挑选几个比较常用的进行演示。\n1.1 重定向-redirectScheme 官方文档\n定义一个 ingressroute，包含一个自动将 http 跳转到 https 的中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp2 spec: entryPoints: - web routes: - match: Host(`myapp2.test.com`) kind: Rule services: - name: myapp2 port: 80 middlewares: - name: redirect-https-middleware # 指定使用RedirectScheme中间件，完成http强制跳转至https --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: redirect-https-middleware spec: redirectScheme: scheme: https # 自动跳转到 https 测试，可以看到访问 http 自动 307 重定向到了 https\n[root@k8s-node1 ~]# curl -I http://myapp2.test.com HTTP/1.1 307 Temporary Redirect Location: https://myapp2.test.com/ Date: Wed, 19 Apr 2023 07:58:34 GMT Content-Length: 18 Content-Type: text/plain; charset=utf-8 1.2 去除请求路径前缀-stripPrefix 官方文档\n假设现在有这样一个需求，当访问 http://myapp.test.com/v1 时，流量调度至 myapp1。当访问 http://myapp.test.com/v2 时，流量调度至 myapp2。这种需求是非常常见的，在 NGINX 中，我们可以配置多个 Location 来定制规则，使用 Traefik 也可以这么做。但是定制不同的前缀后，由于应用本身并没有这些前缀，导致请求返回 404，这时候我们就需要对请求的 path 进行处理。\n创建一个 IngressRoute，并设置两条规则，根据不同的访问路径代理至相对应的 service\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/v1`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: prefix-url-middleware - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/v2`) kind: Rule services: - name: myapp2 port: 80 middlewares: - name: prefix-url-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: prefix-url-middleware spec: stripPrefix: # 去除前缀的中间件 stripPrefix，指定将请求路径中的v1、v2去除。 prefixes: - /v1 - /v2 部署测试\n[root@k8s-node1 ~]# curl http://myapp.test.com/v1 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://myapp.test.com/v2 Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# kubectl logs -l app=myapp1 | tail -2 # 未添加插件的访问路径为 /v1/ 10.244.36.64 - - [19/Apr/2023:08:02:03 +0000] \u0026#34;GET /v1/ HTTP/1.1\u0026#34; 404 169 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; # 添加插件后的访问路径为 / 10.244.36.64 - - [19/Apr/2023:08:04:31 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 65 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; 1.3 白名单-IPWhiteList 官方文档\n为提高安全性，通常情况下一些管理员界面会设置 ip 访问白名单，只希望个别用户可以访问。\n示例\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: ip-white-list-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: ip-white-list-middleware spec: ipWhiteList: sourceRange: - 127.0.0.1/32 - 1.1.1.253 测试\n# 白名单外主机 [root@k8s-node1 ~]# curl -I http://myapp1.test.com HTTP/1.1 403 Forbidden # 白名单内主机 Admin@BJLPT0152 MINGW64 ~ $ ipconfig | grep 1.1.1.253 IPv4 地址 . . . . . . . . . . . . : 1.1.1.253 Admin@BJLPT0152 MINGW64 ~ $ curl -i http://myapp1.test.com Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 1.4 基础用户认证-basicAuth 官方文档\n通常企业安全要求规范除了要对管理员页面限制访问ip外，还需要添加账号密码认证，而 traefik 默认没有提供账号密码认证功能，此时就可以通过BasicAuth 中间件完成用户认证，只有认证通过的授权用户才可以访问页面。\n安装 htpasswd 工具生成密码文件\n[root@k8s-node1 ~]# yum install -y httpd [root@k8s-node1 ~]# htpasswd -bc basic-auth-secret-lvbibir lvbibir 123 Adding password for user lvbibir [root@k8s-node1 ~]# kubectl create secret generic basic-auth-lvbibir --from-file=basic-auth-secret-lvbibir secret/basic-auth-lvbibir created 创建 ingressroute，使用 basicAuth 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: basic-auth-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: basic-auth-middleware spec: basicAuth: secret: basic-auth-lvbibir 访问测试，可以看到弹出界面提示需要输入用户名和密码，输入后回车显示正常页面\n1.5 修改请求/响应头信息-headers 官方文档\n为了提高业务的安全性，安全团队会定期进行漏洞扫描，其中有些 web 漏洞就需要通过修改响应头处理，traefik 的 Headers 中间件不仅可以修改返回客户端的响应头信息，还能修改反向代理后端 service 服务的请求头信息。\n例如对 https://myapp2.test.com 提高安全策略，强制启用HSTS HSTS：即 HTTP 严格传输安全响应头，收到该响应头的浏览器会在 63072000s（约 2 年）的时间内，只要访问该网站，即使输入的是 http，浏览器会自动跳转到 https。（HSTS 是浏览器端的跳转，之前的HTTP 重定向到 HTTPS是服务器端的跳转）\n创建 ingressRoute 和 headers 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp2-tls spec: entryPoints: - web - websecure routes: - match: Host(`myapp2.test.com`) kind: Rule services: - name: myapp2 port: 80 middlewares: - name: hsts-header-middleware tls: secretName: myapp2-tls # 指定tls证书名称 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: hsts-header-middleware spec: headers: customResponseHeaders: Strict-Transport-Security: \u0026#39;max-age=63072000\u0026#39; 访问测试\n[root@k8s-node1 ~]# curl -kI https://myapp2.test.com HTTP/1.1 200 OK Accept-Ranges: bytes Content-Length: 65 Content-Type: text/html Date: Wed, 19 Apr 2023 08:37:07 GMT Etag: \u0026#34;5a9251f0-41\u0026#34; Last-Modified: Sun, 25 Feb 2018 06:04:32 GMT Server: nginx/1.12.2 Strict-Transport-Security: max-age=63072000 # headers 插件添加的响应头 1.6 限流-rateLimit 官方文档\n在实际生产环境中，流量限制也是经常用到的，它可以用作安全目的，比如可以减慢暴力密码破解的速率。通过将传入请求的速率限制为真实用户的典型值，并标识目标URL地址(通过日志)，还可以用来抵御 DDOS 攻击。更常见的情况，该功能被用来保护下游应用服务器不被同时太多用户请求所压垮。\n创建 ingressRoute 和 rateLimit 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: rate-limit-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: rate-limit-middleware spec: rateLimit: # 指定 1s 内请求数平均值不大于 10 个，高峰最大值不大于 50 个。 burst: 10 average: 50 压力测试，使用ab工具进行压力测试，一共请求 100 次，每次并发 10。测试结果失败的请求为 72 次，总耗时 0.409 秒\n[root@k8s-node1 ~]# ab -n 100 -c 10 \u0026#34;http://myapp1.test.com/\u0026#34; Concurrency Level: 10 Time taken for tests: 0.409 seconds Complete requests: 100 Failed requests: 72 (Connect: 0, Receive: 0, Length: 72, Exceptions: 0) Non-2xx responses: 72 1.7 熔断-circuitBreaker 官方文档\n服务熔断的作用类似于保险丝，当某服务出现不可用或响应超时的情况时，为了防止整个系统出现雪崩，暂时停止对该服务的调用。\n熔断器三种状态\nClosed：关闭状态，所有请求都正常访问。 Open：打开状态，所有请求都会被降级。traefik 会对请求情况计数，当一定时间内失败请求百分比达到阈值，则触发熔断，断路器会完全打开。 Recovering：半开恢复状态，open 状态不是永久的，打开后会进入休眠时间。随后断路器会自动进入半开状态。此时会释放部分请求通过，若这些请求都是健康的，则会完全关闭断路器，否则继续保持打开，再次进行休眠计时 服务熔断原理(断路器的原理) 统计用户在指定的时间范围（默认10s）之内的请求总数达到指定的数量之后，如果不健康的请求(超时、异常)占总请求数量的百分比（50%）达到了指定的阈值之后，就会触发熔断。触发熔断，断路器就会打开(open),此时所有请求都不能通过。在5s之后，断路器会恢复到半开状态(half open)，会允许少量请求通过，如果这些请求都是健康的，那么断路器会回到关闭状态(close).如果这些请求还是失败的请求,断路器还是恢复到打开的状态(open).\ntraefik支持的触发器\nNetworkErrorRatio：网络错误率 ResponseCodeRatio：状态代码比率 LatencyAtQuantileMS：分位数的延迟（以毫秒为单位） 创建 ingressRoute ，添加 circuitBreaker 中间件，指定 50% 的请求比例响应时间大于 1MS 时熔断。\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: circuit-breaker-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: circuit-breaker-middleware spec: circuitBreaker: expression: LatencyAtQuantileMS(50.0) \u0026gt; 1 压力测试，一共请求 1000 次，每次并发 100 次。触发熔断机制，测试结果失败的请求为 999 次，总耗时 1.742 秒。\n[root@k8s-node1 traefik]# ab -n 1000 -c 100 \u0026#34;http://myapp1.test.com/\u0026#34; Concurrency Level: 100 Time taken for tests: 1.742 seconds Complete requests: 1000 Failed requests: 999 (Connect: 0, Receive: 0, Length: 2, Exceptions: 0) Write errors: 0 Non-2xx responses: 999 1.8 自定义错误页-errorPages 官方文档\n在实际的业务中，肯定会存在 4XX 5XX 相关的错误异常，如果每个应用都开发一个单独的错误页，无疑大大增加了开发成本，traefik 同样也支持自定义错误页，但是需要注意的是，错误页面不是由 traefik 存储处理，而是通过定义中间件，将错误的请求重定向到其他的页面。\n首先，我们先创建一个应用。这个web应用的功能是：\n当请求 / 时，返回状态码为 200 当请求 /400 时，返回 400 状态码 当请求 /500 时，返回 500 状态码 创建 deployment svc\napiVersion: apps/v1 kind: Deployment metadata: name: flask spec: selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: cuiliang0302/request-code:v2.0 imagePullPolicy: IfNotPresent resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: flask spec: type: ClusterIP selector: app: flask ports: - port: 5000 targetPort: 5000 创建 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: flask spec: entryPoints: - web routes: - match: Host(`flask.test.com`) kind: Rule services: - name: flask port: 5000 访问测试，模拟 400 500 错误\n[root@k8s-node1 ~]# curl -I http://flask.test.com HTTP/1.1 200 OK [root@k8s-node1 ~]# curl -I http://flask.test.com/400 HTTP/1.1 400 Bad Request [root@k8s-node1 ~]# curl -I http://flask.test.com/500 HTTP/1.1 500 Internal Server Error [root@k8s-node1 ~]# curl -I http://flask.test.com/404 HTTP/1.1 404 Not Found 现在提出一个新的需求，当我访问flask项目时，如果错误码为400，返回myapp1的页面，如果错误码为500，返回myapp2的页面(前提是myapp1和myapp2服务已创建)。\n创建 errors 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: errors5 spec: errors: status: - \u0026#34;500-599\u0026#34; # query: /{status}.html # 可以为每个页面定义一个状态码，也可以指定5XX使用统一页面返回 query : / # 指定返回myapp2的请求路径 service: name: myapp2 port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: errors4 spec: errors: status: - \u0026#34;400-499\u0026#34; # query: /{status}.html # 可以为每个页面定义一个状态码，也可以指定5XX使用统一页面返回 query : / # 指定返回myapp1的请求路径 service: name: myapp1 port: 80 修改 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: flask spec: entryPoints: - web routes: - match: Host(`flask.test.com`) kind: Rule services: - name: flask port: 5000 middlewares: - name: errors4 - name: errors5 访问测试，可以看到 400 页面和 500 页面已经成功重定向了\n[root@k8s-node1 ~]# curl http://flask.test.com/ \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;flask\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;hello flask\u0026lt;/h1\u0026gt; \u0026lt;img src=\u0026#34;/static/photo.jpg\u0026#34; alt=\u0026#34;photo\u0026#34;\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@k8s-node1 ~]# curl http://flask.test.com/400 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://flask.test.com/500 Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 1.9 数据压缩-compress 官方文档\n有时候客户端和服务器之间会传输比较大的报文数据，这时候就占用较大的网络带宽和时长。为了节省带宽，加速报文的响应速速，可以将传输的报文数据先进行压缩，然后再进行传输，traefik也同样支持数据压缩。\ntraefik 默认只对大于 1024 字节，且请求标头包含 Accept-Encoding gzip 的资源进行压缩。可以指定排除特定类型不启用压缩或者根据内容大小来决定是否压缩。\n继续使用上面创建的flask应用，现在创建中间件并修改 ingressRoute，使用默认配置策略即可。\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: flask spec: entryPoints: - web routes: - match: Host(`flask.test.com`) kind: Rule services: - name: flask port: 5000 middlewares: - name: compress --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: compress spec: compress: {} 访问测试\nhtml 文件小于 1024 字节，未开启压缩\n图片资源大于 1024 字节，开启了压缩\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-3-middleware/","summary":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示 1. 简介 官方文档 Traefik Middlewares 是一个处于路由和后端服务之前的中间件，在外部流量进入 Traefik，且路由规则匹配成功后，将流量发送到对应的","title":"traefik (三) 中间件(Middleware)"},{"content":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n1. 简介 官方文档\n1.1 三种方式 Traefik 创建路由规则有多种方式，比如：\n原生 Ingress 写法 使用 CRD IngressRoute 方式 使用 GatewayAPI 的方式 相较于原生 Ingress 写法，ingressRoute 是 2.1 以后新增功能，简单来说，他们都支持路径 (path) 路由和域名 (host) HTTP 路由，以及 HTTPS 配置，区别在于 IngressRoute 需要定义 CRD 扩展，但是它支持了 TCP、UDP 路由以及中间件等新特性，强烈推荐使用 ingressRoute\n1.2 匹配规则 规则 描述 Headers(key, value) 检查headers中是否有一个键为key值为value的键值对 HeadersRegexp(key, regexp) 检查headers中是否有一个键位key值为正则表达式匹配的键值对 Host(example.com, …) 检查请求的域名是否包含在特定的域名中 HostRegexp(example.com, {subdomain:[a-z]+}.example.com, …) 检查请求的域名是否包含在特定的正则表达式域名中 Method(GET, …) 检查请求方法是否为给定的methods(GET、POST、PUT、DELETE、PATCH)中 Path(/path, /articles/{cat:[a-z]+}/{id:[0-9]+}, …) 匹配特定的请求路径，它接受一系列文字和正则表达式路径 PathPrefix(/products/, /articles/{cat:[a-z]+}/{id:[0-9]+}) 匹配特定的前缀路径，它接受一系列文字和正则表达式前缀路径 Query(foo=bar, bar=baz) 匹配查询字符串参数，接受key=value的键值对 ClientIP(10.0.0.0/16, ::1) 如果请求客户端 IP 是给定的 IP/CIDR 之一，则匹配。它接受 IPv4、IPv6 和网段格式。 2. dashboard 案例 之前的部署章节中我们是以 nodePort 和 service nodePort 的方式访问的 traefik 的 dashboard，接下来以三种方式演示通过域名访问 dashboard\n2.1 ingress apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: traefik-dashboard namespace: traefik annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/router.entrypoints: web spec: rules: - host: ingress.test.com http: paths: - pathType: Prefix path: / backend: service: name: traefik port: number: 9000 访问: http://ingress.test.com\n2.2 ingressRoute apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard namespace: traefik spec: entryPoints: - web routes: - match: Host(`traefik.test.com`) kind: Rule services: - name: api@internal kind: TraefikService namespace: traefik 访问：http://traefik.test.com\n2.3 Gateway API 目前，Traefik 对 Gateway APIs 的实现是基于 v1alpha1 版本的规范，目前最新的规范是 v1alpha2，所以和最新的规范可能有一些出入的地方。\n创建 gatewayClass\napiVersion: networking.x-k8s.io/v1alpha1 kind: GatewayClass metadata: name: traefik spec: controller: traefik.io/gateway-controller 创建 gateway\napiVersion: networking.x-k8s.io/v1alpha1 kind: Gateway metadata: name: http-gateway namespace: kube-system spec: gatewayClassName: traefik listeners: - protocol: HTTP port: 80 routes: kind: HTTPRoute namespaces: from: All selector: matchLabels: app: traefik 创建 httproute\napiVersion: networking.x-k8s.io/v1alpha1 kind: HTTPRoute metadata: name: traefik-dashboard namespace: kube-system labels: app: traefik spec: hostnames: - \u0026#34;gateway.test.com\u0026#34; rules: - matches: - path: type: Prefix value: / forwardTo: - serviceName: traefik port: 9000 weight: 1 访问：http://gateway.test.com\n3. myapp 环境准备 myapp1\napiVersion: apps/v1 kind: Deployment metadata: name: myapp1 spec: selector: matchLabels: app: myapp1 template: metadata: labels: app: myapp1 spec: containers: - name: myapp1 image: ikubernetes/myapp:v1 resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp1 spec: type: ClusterIP selector: app: myapp1 ports: - port: 80 targetPort: 80 myapp2\napiVersion: apps/v1 kind: Deployment metadata: name: myapp2 spec: selector: matchLabels: app: myapp2 template: metadata: labels: app: myapp2 spec: containers: - name: myapp2 image: ikubernetes/myapp:v2 resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp2 spec: type: ClusterIP selector: app: myapp2 ports: - port: 80 targetPort: 80 创建资源并访问测试\n[root@k8s-node1 ~]# vim demo/app/myapp1.yml [root@k8s-node1 ~]# vim demo/app/myapp2.yml [root@k8s-node1 ~]# kubectl apply -f demo/app/ deployment.apps/myapp1 created service/myapp1 created deployment.apps/myapp2 created service/myapp2 created [root@k8s-node1 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 12d myapp1 ClusterIP 10.100.229.135 \u0026lt;none\u0026gt; 80/TCP 33s myapp2 ClusterIP 10.96.56.49 \u0026lt;none\u0026gt; 80/TCP 33s [root@k8s-node1 ~]# curl 10.100.229.135 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl 10.96.56.49 Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 4. ingressRoute 4.1 http 路由 实现目标：集群外部用户通过访问 http://myapp1.test.com 域名时，将请求代理至myapp1应用。\n创建 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web # 与 configmap 中定义的 entrypoint 名字相同 routes: - match: Host(`myapp1.test.com`) # 域名 kind: Rule services: - name: myapp1 # 与svc的name一致 port: 80 # 与svc的port一致 部署\n[root@k8s-node1 ~]# vim demo/ingressroute/http-myapp1.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressroute/http-myapp1.yml ingressroute.traefik.containo.us/myapp1 created 访问测试\n4.2 https 路由 自签名证书\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=myapp2.test.com\u0026#34; 创建 tls 类型的 secret\nkubectl create secret tls myapp2-tls --cert=tls.crt --key=tls.key 创建 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp2 spec: entryPoints: - websecure # 监听 websecure 这个入口点，也就是通过 443 端口来访问 routes: - match: Host(`myapp2.test.com`) kind: Rule services: - name: myapp2 port: 80 tls: secretName: myapp2-tls # 指定tls证书名称 部署\n[root@k8s-node1 ~]# vim demo/ingressroute/https-myapp2.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressroute/https-myapp2.yml ingressroute.traefik.containo.us/myapp2 created 访问测试，由于是自签名证书，所以会提示不安全\n5. ingressRouteTCP ingreeRouteTCP 官方文档\n5.1 不带 TLS 证书 部署mysql\napiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql namespace: default data: my.cnf: | [mysqld] character-set-server = utf8mb4 collation-server = utf8mb4_unicode_ci skip-character-set-client-handshake = 1 default-storage-engine = INNODB max_allowed_packet = 500M explicit_defaults_for_timestamp = 1 long_query_time = 10 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: mysql name: mysql spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD value: abc123 ports: - containerPort: 3306 volumeMounts: - mountPath: /etc/mysql/conf.d/my.cnf subPath: my.cnf name: cm volumes: - name: cm configMap: name: mysql --- apiVersion: v1 kind: Service metadata: name: mysql namespace: default spec: ports: - port: 3306 protocol: TCP targetPort: 3306 selector: app: mysql ingressRouteTCP\nSNI为服务名称标识，是 TLS 协议的扩展。因此，只有 TLS 路由才能使用该规则指定域名。非 TLS 路由使用带有 * 的规则来声明每个非 TLS 请求都将由路由进行处理。\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: mysql namespace: default spec: entryPoints: - tcpep # 9200 端口 routes: - match: HostSNI(`*`) # 由于 Traefik 中使用 TCP 路由配置需要 SNI，而 SNI 又是依赖 TLS 的，所以我们需要配置证书才行，如果没有证书的话，我们可以使用通配符*(适配ip)进行配置 services: - name: mysql port: 3306 部署\n[root@k8s-node1 ~]# vim demo/ingressrouteTCP/mysql.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressrouteTCP/mysql.yml configmap/mysql created deployment.apps/mysql created service/mysql created [root@k8s-node1 ~]# vim demo/ingressrouteTCP/route.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressrouteTCP/route.yml ingressroutetcp.traefik.containo.us/mysql created 集群外主机验证\n添加 hosts (mysql.test.com) 以 root \u0026amp; abc123 访问 9200 端口 5.2 带 TLS 证书 大多数情况下 tcp 路由不需要配置 TLS ，下面仅演示两个关键步骤\n创建 tls 类型的 secret\nkubectl create secret tls redis-tls --key=redis.key --cert=redis.crt 创建 ingressRouteTCP，需要携带 secret\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: redis spec: entryPoints: - redisep routes: - match: HostSNI(`redis.test.com`) services: - name: redis port: 6379 tls: secretName: redis-tls 6. ingressRouteUDP 创建应用\nkind: Deployment apiVersion: apps/v1 metadata: name: whoamiudp labels: app: whoamiudp spec: replicas: 2 selector: matchLabels: app: whoamiudp template: metadata: labels: app: whoamiudp spec: containers: - name: whoamiudp image: traefik/whoamiudp:latest ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: whoamiudp spec: ports: - port: 8080 protocol: UDP selector: app: whoamiudp 配置 ingressRouteUDP\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteUDP metadata: name: whoamiudp spec: entryPoints: - udpep routes: - services: - name: whoamiudp port: 8080 直接访问 svc 验证\n[root@k8s-node1 traefik]# kubectl get svc whoamiudp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE whoamiudp ClusterIP 10.96.119.116 \u0026lt;none\u0026gt; 8080/UDP 2m22s [root@k8s-node1 traefik]# echo \u0026#34;WHO\u0026#34; | socat - udp4-datagram:10.96.119.116:8080 Hostname: whoamiudp-6ff7dd6fb9-8qfc7 IP: 127.0.0.1 IP: 10.244.169.174 [root@k8s-node1 traefik]# echo \u0026#34;test\u0026#34; | socat - udp4-datagram:10.96.119.116:8080 Received: test 访问 udp 路由验证\n[root@k8s-node1 traefik]# echo \u0026#34;WHO\u0026#34; | socat - udp4-datagram:k8s-node1:9300 Hostname: whoamiudp-6ff7dd6fb9-5l8rd IP: 127.0.0.1 IP: 10.244.107.243 [root@k8s-node1 traefik]# echo \u0026#34;test\u0026#34; | socat - udp4-datagram:1.1.1.1:9300 Received: test 7. 负载均衡 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web # 与 configmap 中定义的 entrypoint 名字相同 routes: - match: Host(`lb.test.com`) # 域名 kind: Rule services: - name: myapp1 # 与svc的name一致 port: 80 # 与svc的port一致 - name: myapp2 # 与svc的name一致 port: 80 # 与svc的port一致 部署\n[root@k8s-node1 ~]# vim demo/lb/lb.yml [root@k8s-node1 ~]# kubectl apply -f demo/lb/lb.yml ingressroute.traefik.containo.us/myapp1 created 访问测试，可以发现循环相应 myapp1 和 myapp2 的内容\n[root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-2-router/","summary":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 1. 简介 官方文档 1.1 三种方式 Traefik 创建路由规则有多种方式，比如： 原生 Ingress 写法 使用 CRD IngressRoute 方式 使用 GatewayAPI 的方式 相较于原生 Ingress 写法，ingressRoute 是 2.1 以后新增功能，简单来说，他们都支持路径 (path)","title":"traefik (二) 路由(ingressRoute)"},{"content":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n参考：https://www.cuiliangblog.cn/detail/section/29427812\n1. 简介 1.1 Traefik 简介 Traefik 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。\n它是一个边缘路由器，它会拦截外部的请求并根据逻辑规则选择不同的操作方式，这些规则决定着这些请求到底该如何处理。Traefik 提供自动发现能力，会实时检测服务，并自动更新路由规则。\n1.2 Traefik 核心组件 从上图可知，当请求 Traefik 时，请求首先到 entrypoints，然后分析传入的请求，查看他们是否与定义的 Routers 匹配。如果匹配，则会通过一系列 middlewares 处理，再到 traefikServices 上做流量转发，最后请求到 kubernetes的services上 。\n这就涉及到以下几个重要的核心组件:\nProviders 是基础组件，Traefik 的配置发现是通过它来实现的，它可以是协调器，容器引擎，云提供商或者键值存储。Traefik 通过查询 Providers 的 API 来查询路由的相关信息，一旦检测到变化，就会动态的更新路由。 Entrypoints 是 Traefik 的网络入口，它定义接收请求的接口，以及是否监听TCP或者UDP。 Routers 主要用于分析请求，并负责将这些请求连接到对应的服务上去，在这个过程中，Routers还可以使用Middlewares来更新请求，比如在把请求发到服务之前添加一些Headers。 Services 负责配置如何到达最终将处理传入请求的实际服务。 Middlewares 用来修改请求或者根据请求来做出一些判断（authentication, rate limiting, headers, \u0026hellip;），中间件被附件到路由上，是一种在请求发送到你的服务之前（或者在服务的响应发送到客户端之前）调整请求的一种方法。 1.3 Traefik CRD资源 官方文档\ntraefik通过自定义资源实现了对traefik资源的创建和管理，支持的crd资源类型如下所示：\nkind 功能 IngressRoute HTTP路由配置 Middleware HTTP中间件配置 TraefikService HTTP负载均衡/流量复制配置 IngressRouteTCP TCP路由配置 MiddlewareTCP TCP中间件配置 IngressRouteUDP UDP路由配置 TLSOptions TLS连接参数配置 TLSStores TLS存储配置 ServersTransport traefik与后端之间的传输配置 2. Traefik 部署 traefik 是支持 helm 部署的，但是查看 helm 包的 value.yaml 配置发现总共有 500 多行配置，当需要修改配置项或者对 traefik 做一下自定义配置时，并不灵活。如果只是使用 traefik 的基础功能，推荐使用 helm 部署。如果想深入研究使用 traefik 的话，推荐使用自定义方式部署。\n2.1 crd rbac serviceaccount crd\n[root@k8s-node1 ~]# mkdir /opt/traefik [root@k8s-node1 ~]# cd /opt/traefik [root@k8s-node1 traefik]# wget https://raw.githubusercontent.com/traefik/traefik/v2.9/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml [root@k8s-node1 traefik]# kubectl apply -f kubernetes-crd-definition-v1.yml rbac\n[root@k8s-node1 traefik]# wget https://raw.githubusercontent.com/traefik/traefik/v2.9/docs/content/reference/dynamic-configuration/kubernetes-crd-rbac.yml [root@k8s-node1 traefik]# kubectl apply -f kubernetes-crd-rbac.yml clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created serviceaccount.yml\napiVersion: v1 kind: Namespace metadata: name: traefik --- apiVersion: v1 kind: ServiceAccount metadata: namespace: traefik name: traefik-ingress-controller --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: traefik 2.2 configmap 在 Traefik 中有三种方式定义静态配置：在配置文件中、在命令行参数中、通过环境变量传递，由于 Traefik 配置很多，通过 CLI 定义不是很方便，一般时候选择将其配置选项放到配置文件中，然后存入 ConfigMap，将其挂入 traefik 中。\nconfigmap.yml 文件内容：\napiVersion: v1 kind: ConfigMap metadata: name: traefik-config namespace: traefik data: traefik.yaml: |- global: checkNewVersion: false # 周期性的检查是否有新版本发布 sendAnonymousUsage: false # 周期性的匿名发送使用统计信息 serversTransport: insecureSkipVerify: true # Traefik忽略验证代理服务的TLS证书 api: insecure: true # 允许HTTP 方式访问API dashboard: true # 启用Dashboard debug: false # 启用Debug调试模式 metrics: prometheus: # 配置Prometheus监控指标数据，并使用默认配置 addRoutersLabels: true # 添加routers metrics entryPoint: \u0026#34;metrics\u0026#34; # 指定metrics监听地址 entryPoints: web: address: \u0026#34;:80\u0026#34; # 配置80端口，并设置入口名称为 web forwardedHeaders: insecure: true # 信任所有的forward headers websecure: address: \u0026#34;:443\u0026#34; # 配置443端口，并设置入口名称为 websecure forwardedHeaders: insecure: true traefik: address: \u0026#34;:9000\u0026#34; # 配置9000端口为 dashboard 的端口，不设置默认值为 8080 metrics: address: \u0026#34;:9100\u0026#34; # 配置9100端口，作为metrics收集入口 tcpep: address: \u0026#34;:9200\u0026#34; # 配置9200端口，作为tcp入口 udpep: address: \u0026#34;:9300/udp\u0026#34; # 配置9300端口，作为udp入口 providers: kubernetesIngress: \u0026#34;\u0026#34; # 启用 Kubernetes Ingress 方式来配置路由规则 kubernetesGateway: \u0026#34;\u0026#34; # 启用 Kubernetes Gateway API kubernetesCRD: # 启用Kubernetes CRD方式来配置路由规则 ingressClass: \u0026#34;\u0026#34; # 指定traefik的ingressClass名称 allowCrossNamespace: true #允许跨namespace allowEmptyServices: true #允许空endpoints的service log: filePath: \u0026#34;/etc/traefik/logs/traefik.log\u0026#34; # 设置调试日志文件存储路径，如果为空则输出到控制台 level: \u0026#34;DEBUG\u0026#34; # 设置调试日志级别 format: \u0026#34;json\u0026#34; # 设置调试日志格式 accessLog: filePath: \u0026#34;/etc/traefik/logs/access.log\u0026#34; # 设置访问日志文件存储路径，如果为空则输出到 stdout 和 stderr format: \u0026#34;json\u0026#34; # 设置访问调试日志格式 bufferingSize: 0 # 设置访问日志缓存行数 fields: # 设置访问日志中的字段是否保留（keep保留、drop不保留） defaultMode: keep # 设置默认保留访问日志字段 names: # 针对访问日志特别字段特别配置保留模式 ClientUsername: drop StartUTC: drop # 禁用日志timestamp使用UTC headers: # 设置Header中字段是否保留 defaultMode: keep # 设置默认保留Header中字段 names: # 针对Header中特别字段特别配置保留模式 # User-Agent: redact# 可以针对指定agent Authorization: drop Content-Type: keep 设置节点 label，用于控制在哪些节点部署 Traefik，这里我们使用 k8s-node1(master) 节点作为边缘节点部署\nkubectl label node k8s-node1 IngressProxy=true 2.3 deployment service 使用 DeamonSet 或者 Deployment 均可，此处使用 Deployment 方式部署 Traefik，调度至含有 IngressProxy=true 的边缘节点\n同时使用 podAntiAffinity 避免多个 traefik 实例运行在同一节点造成单点故障.\nkubectl apply -f deployment.yml\napiVersion: apps/v1 kind: Deployment metadata: name: traefik namespace: traefik labels: app: traefik spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: name: traefik labels: app: traefik spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - traefik topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 5 # 等待容器优雅退出的时长 tolerations: # 设置容忍所有污点，防止节点被设置污点 - operator: \u0026#34;Exists\u0026#34; nodeSelector: # 设置node筛选器，在特定label的节点上启动 IngressProxy: \u0026#34;true\u0026#34; # 调度至IngressProxy: \u0026#34;true\u0026#34;的节点 containers: - name: traefik image: traefik:v2.9 env: - name: KUBERNETES_SERVICE_HOST # 手动指定k8s api,避免网络组件不稳定。 value: \u0026#34;1.1.1.1\u0026#34; - name: KUBERNETES_SERVICE_PORT_HTTPS # API server端口 value: \u0026#34;6443\u0026#34; - name: KUBERNETES_SERVICE_PORT # API server端口 value: \u0026#34;6443\u0026#34; - name: TZ # 指定时区 value: \u0026#34;Asia/Shanghai\u0026#34; ports: - name: web containerPort: 80 - name: websecure containerPort: 443 - name: dashboard containerPort: 9000 # Traefik Dashboard 端口 - name: metrics containerPort: 9100 - name: tcpep containerPort: 9200 # tcp端口 - name: udpep containerPort: 9300 # udp端口 securityContext: # 只开放网络权限 capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --configfile=/etc/traefik/config/traefik.yaml volumeMounts: - mountPath: /etc/traefik/config name: config - mountPath: /etc/traefik/logs name: logdir - mountPath: /etc/localtime name: timezone readOnly: true resources: requests: memory: \u0026#34;5Mi\u0026#34; cpu: \u0026#34;10m\u0026#34; limits: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; volumes: - name: config # traefik配置文件 configMap: name: traefik-config - name: logdir # traefik日志目录 hostPath: path: /var/log/traefik type: \u0026#34;DirectoryOrCreate\u0026#34; - name: timezone # 挂载时区文件 hostPath: path: /etc/localtime type: File service kubectl apply -f service.yml\napiVersion: v1 kind: Service metadata: labels: app: traefik name: traefik # 实际提供服务的 service, 使用 NodePort 模式 namespace: traefik spec: type: NodePort selector: app: traefik ports: - name: web protocol: TCP port: 80 targetPort: 80 nodePort: 80 - name: websecure protocol: TCP port: 443 targetPort: 443 nodePort: 443 - name: dashboard protocol: TCP port: 9000 targetPort: 9000 nodePort: 9000 - name: tcpep protocol: TCP port: 9200 targetPort: 9200 nodePort: 9200 - name: udpep protocol: UDP port: 9300 targetPort: 9300 nodePort: 9300 --- apiVersion: v1 kind: Service metadata: labels: app: traefik-metrics name: traefik-metrics # metrics 用于给集群内的 prometheus 提供数据 namespace: traefik spec: selector: app: traefik ports: - name: metrics protocol: TCP port: 9100 targetPort: 9100 2.4 验证 [root@k8s-node1 traefik]# kubectl get pod,cm,sa,svc -n traefik |grep traefik pod/traefik-69bd67497f-v27qp 1/1 Running 0 12m configmap/traefik-config 1 7d5h serviceaccount/traefik-ingress-controller 1 7d5h service/traefik NodePort 10.101.142.158 \u0026lt;none\u0026gt; 80:80/TCP,443:443/TCP,9000:9000/TCP,9200:9200/TCP,9300:9300/UDP 11m service/traefik-metrics ClusterIP 10.98.89.13 \u0026lt;none\u0026gt; 9100/TCP 12m 可以直接通过 http://1.1.1.1:9000 访问到 dashboard\n2.5 其他配置 2.5.1 强制使用TLS v1.2+ 如今，TLS v1.0 和 v1.1 因为存在安全问题，现在已被弃用。为了保障系统安全，所有入口路由都应该强制使用TLS v1.2 或更高版本。\n参考文档：https://doc.traefik.io/traefik/user-guides/crd-acme/#force-tls-v12\n[root@k8s-node1 traefik]# tee traefik-tlsoption.yml \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; apiVersion: traefik.containo.us/v1alpha1 kind: TLSOption metadata: name: default namespace: traefik spec: minVersion: VersionTLS12 cipherSuites: - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # TLS 1.2 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 # TLS 1.2 - TLS_AES_256_GCM_SHA384 # TLS 1.3 - TLS_CHACHA20_POLY1305_SHA256 # TLS 1.3 curvePreferences: - CurveP521 - CurveP384 sniStrict: true EOF [root@k8s-node1 traefik]# kubectl apply -f traefik-tlsoption.yml tlsoption.traefik.containo.us/default created 2.5.2 日志切割 官方并没有日志轮换的功能，但是 traefik 收到 USR1 信号后会重建日志文件，因此可以通过 logrotate 实现日志轮换\nmkdir -p /etc/logrotate.d/traefik tee /etc/logrotate.d/traefik/config \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; /var/log/traefik/*.log { daily rotate 15 missingok notifempty compress dateext dateyesterday dateformat .%Y-%m-%d create 0644 root root postrotate docker kill --signal=\u0026#34;USR1\u0026#34; $(docker ps | grep traefik |grep -v pause| awk \u0026#39;{print $1}\u0026#39;) endscript } EOF 创建定时任务\ncrontab -e 0 0 * * * /usr/sbin/logrotate -f /etc/logrotate.d/traefik/config \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 2.6 多控制器 有的业务场景下可能需要在一个集群中部署多个 traefik，例如：避免单个traefik配置规则过多导致加载处理缓慢。每个namespace部署一个traefik。或者traefik生产与测试环境区分等场景，需要不同的实例控制不同的 IngressRoute 资源对象，要实现该功能有两种方法\n2.6.1 annotations 注解筛选 首先在 traefik 配置文件中的 providers 下增加 Ingressclass 参数，指定具体的值\nproviders: kubernetesCRD: # 启用Kubernetes CRD方式来配置路由规则 ingressClass: \u0026#34;traefik-v2.9\u0026#34; # 指定traefik的ingressClass实例名称 allowCrossNamespace: true #允许跨namespace allowEmptyServices: true #允许空endpoints的service 接下来在 IngressRoute 资源对象中的 annotations 参数中添加 kubernetes.io/ingress.class: traefik-v2.9 即可\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard namespace: traefik annotations: kubernetes.io/ingress.class: traefik-v2.9 # 因为静态配置文件指定了ingressclass，所以这里的annotations 要指定，否则访问会404 spec: entryPoints: - web routes: - match: Host(`traefik.test.com`) kind: Rule services: - name: api@internal kind: TraefikService namespace: traefik 2.6.2 label 标签选择器筛选 首先在traefik配置文件中的providers下增加labelSelector参数，指定具体的标签键值。\nproviders: kubernetesCRD: # 启用Kubernetes CRD方式来配置路由规则 # ingressClass: \u0026#34;traefik-v2.9\u0026#34; # 指定traefik的ingressClass名称 labelSelector: \u0026#34;app=traefik-v2.9\u0026#34; # 通过标签选择器指定traefik标签 allowCrossNamespace: true #允许跨namespace allowEmptyServices: true #允许空endpoints的service 然后在 IngressRoute 资源对象中添加labels标签选择器，选择 app: traefik-v2.9 这个标签即可\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard labels: # 通过标签选择器，该IngressRoute资源由配置了app=traefik-v2.9的traefik处理 app: traefik-v2.9 # annotations: # kubernetes.io/ingress.class: traefik-v2.9 spec: entryPoints: - web routes: - match: Host(`traefik.test.com`) kind: Rule services: - name: api@internal kind: TraefikService namespace: traefik ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-1-deploy/","summary":"0. 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 参考：https://www.cuiliangblog.cn/detail/section/29427812 1. 简介 1.1 Traefik 简介 Traefik 是一个为了让部署微服务更加便捷而诞生的现代HTTP反","title":"traefik (一) 简介、部署和配置"},{"content":"1. 简介 Gateway API（之前叫 Service API）是由 SIG-NETWORK 社区管理的开源项目，项目地址：https://gateway-api.sigs.k8s.io/。\nIngress 资源对象不能很好的满足网络需求，很多场景下 Ingress 控制器都需要通过定义 annotations 或者 crd 来进行功能扩展，这对于使用标准和支持是非常不利的，新推出的 Gateway API 旨在通过可扩展的面向角色的接口来增强服务网络。\nGateway API 是 Kubernetes 中的一个 API 资源集合，包括 GatewayClass、Gateway、HTTPRoute、TCPRoute、Service 等，这些资源共同为各种网络用例构建模型。\n2. 部署 2.1 crd 内容较长，直接复制官网yaml\n[root@k8s-node1 traefik]# kubectl apply -f gateway-api-crd.yml customresourcedefinition.apiextensions.k8s.io/gatewayclasses.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/gateways.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/httproutes.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/tcproutes.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/tlsroutes.networking.x-k8s.io created 2.2 rbac --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: gateway-role rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - networking.x-k8s.io resources: - gatewayclasses - gateways - httproutes - tcproutes - tlsroutes verbs: - get - list - watch - apiGroups: - networking.x-k8s.io resources: - gatewayclasses/status - gateways/status - httproutes/status - tcproutes/status - tlsroutes/status verbs: - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: gateway-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: gateway-role subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: default 应用yaml\n[root@k8s-node1 traefik]# kubectl apply -f gateway-api-rbac.yml clusterrole.rbac.authorization.k8s.io/gateway-role created clusterrolebinding.rbac.authorization.k8s.io/gateway-controller created ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-gatewayapi/","summary":"1. 简介 Gateway API（之前叫 Service API）是由 SIG-NETWORK 社区管理的开源项目，项目地址：https://gateway-api.sigs.k8s.io/。 Ingress 资源对象不能很好的满足网络需求，很多场景下 Ingress 控制器都需要通过定义 annotations 或者 crd 来进行功能扩展，这对于使用标准和支持是非常不利的，新推出的 Gateway API 旨在通过可","title":"kubernetes | Gateway API 简介及部署"},{"content":"简介 dns配置文件 /etc/resolv.conf 中常看到有 search 设置，以前以为是根据search 中的域去指定nameserver，其实不是这样用的。它的一个用处是程序只需要知道主机名就可以解析到 ip，不必知道域名后缀 domain 是什么\nFQDN (Fully Qualified Domain Name) 含义是完整的域名. 例如, 一台机器主机名(hostname)是 www, 域名后缀(domain)是baidu.com, 那么该主机的FQDN应该是 www.baidu.com. 最后是以 . 来结尾的, 但是大部分的应用和服务器都允许忽略最后这个点 . 所有大家直接输入 www.baidu.com 也可以识别\nhttps://www.man7.org/linux/man-pages/man5/resolv.conf.5.html\nsearch 下面以几个示例演示一下 search 是如何工作的\n/etc/resolv.conf 配置文件内容\nnameserver 8.8.8.8 search foo.local bar.local 解析 test ，优先以 hostname 的形式拼接到 search 中配置的 domain 上进行查询，如果失败直接以 FQDN 的形式查询\n[root@k8s-node1 ~]# host -a test Trying \u0026#34;test.foo.local\u0026#34; Trying \u0026#34;test.bar.local\u0026#34; Trying \u0026#34;test\u0026#34; Host test not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 53 ms 解析 test.hello ，优先以 FQDN 的形式查询，如果失败则以 hostname 的形式拼接到 search 中配置的 domain 上进行查询\n[root@k8s-node1 ~]# host -a test.hello Trying \u0026#34;test.hello\u0026#34; Received 103 bytes from 8.8.8.8#53 in 48 ms Trying \u0026#34;test.hello.foo.local\u0026#34; Trying \u0026#34;test.hello.bar.local\u0026#34; Host test.hello not found: 3(NXDOMAIN) Received 113 bytes from 8.8.8.8#53 in 49 ms 解析 test. ，直接认定为 FQDN ，以 FQDN 的形式查询，不会进行拼接查询\n[root@k8s-node1 ~]# host -a test. Trying \u0026#34;test\u0026#34; Host test. not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 54 ms options ndots 可以发现，配置了 search 之后，除非以最后一种形式查询，总会将 hostname 和 search 进行拼接查询\n其实它是由 options ndots:[number] 选项控制的：当查询的域名有 \u0026gt;= number 个 . 时，优先以 FQDN 的形式查询，如果失败再拼接查询\n配置 /etc/resolv.conf\nnameserver 8.8.8.8 search foo.local bar.local options ndots:2 观察下述示例\ntest\n[root@k8s-node1 ~]# host -a test Trying \u0026#34;test.foo.local\u0026#34; Trying \u0026#34;test.bar.local\u0026#34; Trying \u0026#34;test\u0026#34; Host test not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 45 ms test.hello\n[root@k8s-node1 ~]# host -a test.hello Trying \u0026#34;test.hello.foo.local\u0026#34; Trying \u0026#34;test.hello.bar.local\u0026#34; Trying \u0026#34;test.hello\u0026#34; Host test.hello not found: 3(NXDOMAIN) Received 103 bytes from 8.8.8.8#53 in 46 ms test.hello.world\n[root@k8s-node1 ~]# host -a test.hello.world Trying \u0026#34;test.hello.world\u0026#34; Received 119 bytes from 8.8.8.8#53 in 57 ms Trying \u0026#34;test.hello.world.foo.local\u0026#34; Trying \u0026#34;test.hello.world.bar.local\u0026#34; Host test.hello.world not found: 3(NXDOMAIN) Received 119 bytes from 8.8.8.8#53 in 45 ms test.\n[root@k8s-node1 ~]# host -a test. Trying \u0026#34;test\u0026#34; Host test. not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 45 ms ","permalink":"https://www.lvbibir.cn/en/posts/tech/dns-search/","summary":"简介 dns配置文件 /etc/resolv.conf 中常看到有 search 设置，以前以为是根据search 中的域去指定nameserver，其实不是这样用的。它的一个用处是程序只需要知道主机名就可以解析到 ip，不必知道域名后缀 domain 是什么 FQDN (Fully Qualified Domain Name) 含义是完整的域名. 例如, 一台机器主机名(hostname)是 www, 域名后缀(dom","title":"dns配置文件中search和options ndots详解"},{"content":"基础概念 StatefulSet 应用场景：分布式应用、集群\n部署有状态应用\n解决Pod独立生命周期，保持Pod启动顺序和唯一性\n稳定，唯一的网络标识符，持久存储 有序，优雅的部署和扩展、删除和终止 有序，滚动更新 StatefulSet 控制器的优势\n稳定的存储\nStatefulSet的存储卷使用VolumeClaimTemplate创建，称为卷申请模板，当StatefulSet使用VolumeClaimTemplate创建一个PersistentVolume时，同样也会为每个Pod分配并创建一个编号的PVC。该PVC和PV不会随着StatefulSet的删除而删除 稳定的网络ID\nStatefulSet 中的每个 POD 名称固定：\u0026lt;statefulset-name\u0026gt;-\u0026lt;number\u0026gt; 通过 serviceName 字段指定 Headless Service ，可以为每个 POD 分配一个固定的 DNS 解析，重启或者重建 POD 时虽然 ip 有所变动，但 DNS 解析会保持稳定 示例yaml\napiVersion: v1 kind: Service metadata: name: statefulset-nginx labels: app: statefulset-nginx spec: selector: app: statefulset-nginx clusterIP: None ports: - name: web port: 80 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: statefulset-nginx spec: serviceName: \u0026#34;statefulset-nginx\u0026#34; replicas: 2 selector: matchLabels: app: statefulset-nginx template: metadata: labels: app: statefulset-nginx spec: terminationGracePeriodSeconds: 5 containers: - name: statefulset-nginx image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - name: web containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 稳定的存储 可以看到与deployment不同，statefulset中的每个pod都分配到了独立的pv，且重启pod后存储对应关系不变\n[root@k8s-node1 ~]# kubectl get pod,pvc,pv | awk \u0026#39;{print $1}\u0026#39; # pod NAME pod/nfs-client-provisioner-66d6cb77fd-47hsf pod/statefulset-nginx-0 pod/statefulset-nginx-1 # pvc NAME persistentvolumeclaim/www-statefulset-nginx-0 persistentvolumeclaim/www-statefulset-nginx-1 # pv NAME persistentvolume/pvc-17751fde-1b23-4535-98bb-a70342ddd6fe persistentvolume/pvc-b7519f46-b2af-42e4-b66d-d7459be2e87c [root@k8s-node1 ~]# ls /nfs/ default-www-statefulset-nginx-0-pvc-17751fde-1b23-4535-98bb-a70342ddd6fe default-www-statefulset-nginx-1-pvc-b7519f46-b2af-42e4-b66d-d7459be2e87c 稳定的网络ID 手动删除pod后除了pod的ip会变动，主机名和dns解析都正常\n# POD名字固定 [root@k8s-node1 ~]# kubectl get pods -l app=statefulset-nginx NAME READY STATUS RESTARTS AGE statefulset-nginx-0 1/1 Running 0 5m18s statefulset-nginx-1 1/1 Running 0 5m17s # 主机名固定 [root@k8s-node1 ~]# for i in 0 1; do kubectl exec \u0026#34;statefulset-nginx-$i\u0026#34; -- hostname; done statefulset-nginx-0 statefulset-nginx-1 # DNS解析固定 [root@k8s-node1 ~]# kubectl run -it --rm --restart=Never --image busybox:1.28 dns-test -- nslookup statefulset-nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: statefulset-nginx Address 1: 10.244.107.230 statefulset-nginx-1.statefulset-nginx.default.svc.cluster.local Address 2: 10.244.169.157 statefulset-nginx-0.statefulset-nginx.default.svc.cluster.local pod \u0026#34;dns-test\u0026#34; deleted 暴露应用 由于使用的是 Headless Service ，无法使用 NodePort 的方式暴露应用端口，我们可以单独创建 service 来暴露特定 pod 应用\nStatefulSet 控制器中的 pod 名称都是固定的： \u0026lt;statefulset-name\u0026gt;-\u0026lt;number\u0026gt; ，可以通过 statefulset.kubernetes.io/pod-name 标签固定 pod\n示例如下\napiVersion: v1 kind: Service metadata: name: ss-nginx-0 labels: app: ss-nginx-0 spec: selector: statefulset.kubernetes.io/pod-name: statefulset-nginx-0 type: NodePort ports: - name: web port: 80 targetPort: 80 nodePort: 30003 验证\n[root@k8s-node1 ~]# kubectl get svc ss-nginx-0 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ss-nginx-0 NodePort 10.111.69.1 \u0026lt;none\u0026gt; 80:30003/TCP 3h53m [root@k8s-node1 ~]# kubectl get ep ss-nginx-0 NAME ENDPOINTS AGE ss-nginx-0 10.244.169.188:80 3h53m ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-statefulset/","summary":"基础概念 StatefulSet 应用场景：分布式应用、集群 部署有状态应用 解决Pod独立生命周期，保持Pod启动顺序和唯一性 稳定，唯一的网络标识符，持久存储 有序，优雅的部署和扩展、删除和终止 有序，滚动更新 StatefulSet 控制器的优势 稳定的存储 StatefulSet的存储卷使用VolumeClaimTemplate创建","title":"kubernetes | statefulset控制器详解"},{"content":"command args 如果指定了 containers.command，Dockerfile 中的 ENTRYPOINT 会被覆盖且 CMD 指令被忽略\n如果指定了 containers.args，Dockerfile 中的 ENTRYPOINT 继续执行， CMD指令 被覆盖\nENTRYPOINT CMD command args finally [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] ep-1 foo bar [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] ep-2 [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-1 zoo boo [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-2 zoo boo CMD ENTRYPOINT 我们大概可以总结出下面几条规律：\n如果 ENTRYPOINT 使用了 shell 模式，CMD 指令会被忽略。 如果 ENTRYPOINT 使用了 exec 模式，CMD 指定的内容被追加为 ENTRYPOINT 指定命令的参数。 如果 ENTRYPOINT 使用了 exec 模式，CMD 也应该使用 exec 模式。 还有一点需要注意，如果使用 docker run --entrypoint 覆盖了 Dockerfile 中的 ENTRYPOINT , 同时 CMD 指令也会被忽略\n真实的情况要远比这三条规律复杂，好在 docker 给出了官方的解释，如下图所示：\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-command-args-docker-entrypoint-cmd/","summary":"command args 如果指定了 containers.command，Dockerfile 中的 ENTRYPOINT 会被覆盖且 CMD 指令被忽略 如果指定了 containers.args，Dockerfile 中的 ENTRYPOINT 继续执行， CMD指令 被覆盖 ENTRYPOINT CMD command args finally [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] ep-1 foo bar [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] ep-2 [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-1 zoo boo [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-2 zoo boo CMD ENTRYPOINT 我们大概可以总结","title":"kubernetes中的command和args 与 dockerfile中的ENTRYPOINT和CMD的关系"},{"content":"前言 在使用 Docker 的过程中，编写 Dockerfile 是非常重要的一部分工作。合理编写 Dockerfile 会使我们构建出来的 Docker image 拥有更佳的性能和健壮性\n目标:\n更快的构建速度 更小的 Docker 镜像大小 更少的 Docker 镜像层 充分利用镜像缓存 增加 Dockerfile 可读性 让 Docker 容器使用起来更简单 总结\n编写.dockerignore 文件 容器只运行单个应用 将多个 RUN 指令合并为一个 基础镜像的标签不要用 latest 每个 RUN 指令后删除多余文件 选择合适的基础镜像(alpine 版本最好) 设置 WORKDIR 和 CMD 使用 ENTRYPOINT (可选) 在 entrypoint 脚本中使用 exec COPY 与 ADD 优先使用前者 合理调整 COPY 与 RUN 的顺序 设置默认的环境变量，映射端口和数据卷 使用 LABEL 设置镜像元数据 添加 HEALTHCHECK 可以说每条 Dockerfile 指令都有相关的优化项，这里就不一一赘述了，下面仅列举一些常见且重要的设置\n参考内容：\nhttps://blog.fundebug.com/2017/05/15/write-excellent-dockerfile/ 容器的优雅退出 众所周知，docker容器本质上是一个个进程，进程的优雅退出需要考虑的是如何正确处理 SIGTERM 信号，关于这点在我的另一篇博文中介绍过 kill命令详解以及linux中的信号\n无论是 docker stop 还是在 kubernetes 中使用容器，一般关闭容器都是向容器内的 1 号进程发送 SIGTERM 信号，等待容器自行进行资源清理等操作，等待时间 docker 默认 10s，k8s 默认 30s，如果容器仍未退出，则发送 SIGKILL 信号强制杀死进程\n综上，我们只需要考虑 2 点\n应用程序如何处理信号\n这就需要在应用程序中定义对信号的处理逻辑了，包括对每个信号如何处理如何转发给子进程等。\n应用程序如何获取信号\ndocker 容器的一号进程是由 CMD ENTRYPOINT 这两个指令决定的，所以正确使用这两个指令十分关键\nCMD 和 ENTRYPOINT 分别都有 exec 和 shell 两种格式：\n使用 exec 格式时，我们执行的命令就是一号进程 使用 shell 格式时，实际会以 /bin/sh -c command arg... 的方式运行，这种情况下容器的一号进程将会是 /bin/sh，当收到信号时 /bin/sh 不会将信号转发给我们的应用程序，导致意料之外的错误，所以十分不推荐使用 shell 格式 我们还可以使用 tini 作为 init 系统管理进程\n官方地址：https://github.com/krallin/tini\nTini (Tiny but Independent) 是一个小型的、可执行的程序，它的主要目的是作为一个 init 系统的替代品，用于在容器中启动应用程序。\n在容器中启动应用程序时，通常会使用 init 系统来管理进程。然而，由于容器的特殊性，传统的 init 系统可能无法完全满足容器化应用程序的需求。Tini 作为一个小巧而独立的程序，可以帮助解决容器启动时可能遇到的各种问题，如僵尸进程、信号处理等。\n在 Docker 中使用 Tini 的主要意义在于提高容器的稳定性和可靠性。Tini 可以确保容器中的应用程序在启动和退出时正确处理信号，避免僵尸进程和其它常见问题的出现。此外，Tini 还可以有效地限制容器中的资源使用，避免应用程序崩溃或者占用过多的系统资源，从而提高容器的可用性和可维护性。\n总之，使用 Tini 可以让容器中的应用程序更加健壮、稳定和可靠，这对于运行生产环境中的应用程序非常重要。\n使用示例\nFROM nginx ENV TINI_VERSION=v0.19.0 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [\u0026#34;/tini\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;/docker-entrypoint.sh\u0026#34;] CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] Alpine Linux\nRUN apk add --no-cache tini # Tini is now available at /sbin/tini ENTRYPOINT [\u0026#34;/sbin/tini\u0026#34;, \u0026#34;--\u0026#34;] NixOS\nnix-env --install tini Debian\napt-get install tini Arch Linux\npacaur -S tini RUN指令 RUN 指令一般用于安装配置软件包等操作，通常需要比较多的步骤，如果每条命令都单独用 RUN 指令去跑会导致镜像层数非常多，所以尽可能将所有 RUN 指令拼接起来是当前的事实标准\n也要将 RUN 指令中生产的一些附属文件删除以缩小最终镜像的大小\n如下示例\nFROM debian:stretch RUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y $buildDeps \\ \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\ \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026amp;\u0026amp; make -C /usr/src/redis \\ \u0026amp;\u0026amp; make -C /usr/src/redis install \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm redis.tar.gz \\ \u0026amp;\u0026amp; rm -r /usr/src/redis \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps 多阶段构建 很多时候我们的应用容器会包含 构建 和 运行 两大功能，而运行所需要的依赖数量明显少于构建时的依赖，我们最终的 image 交付物有运行环境就足够了\n在很多的场景中，我们都会制作两个 Dockerfile 分别用于构建和运行，文件交付起来十分麻烦\n在 Docker Engine 17.05 中引入了多阶段构建，以此降低构建复杂度，同时使缩小镜像尺寸更为简单\n如下示例，go 程序编译完后几乎不需要任何依赖环境即可运行\n# 阶段1 FROM golang:1.16 WORKDIR /go/src COPY app.go ./ RUN go build app.go -o myapp # 阶段2，引用空镜像 scratch FROM scratch WORKDIR /server # 复制文件，通过编号引用，0 代表阶段 1 COPY --from=0 /go/src/myapp ./ CMD [\u0026#34;./myapp\u0026#34;] 上述例子可以修改一下，可读性更强\n# 阶段1命名为builder FROM golang:1.16 as builder WORKDIR /go/src COPY app.go ./ RUN go build app.go -o myapp # 阶段2，引用空镜像 scratch FROM scratch WORKDIR /server # 复制文件，通过名称引用 COPY --from=builder /go/src/myapp ./ CMD [\u0026#34;./myapp\u0026#34;] 只构建某个阶段\n构建镜像时，不一定需要构建整个 Dockerfile，我们可以通过--target参数指定某个目标阶段构建，比如我们开发阶段我们只构建builder阶段进行测试。\ndocker build --target builder -t builder_app:v1 . 使用外部镜像\nCOPY --from httpd:latest /usr/local/apache2/conf/httpd.conf ./httpd.conf 从上一阶段创建新的阶段\n# 阶段1命名为builder FROM golang:1.16 as builder WORKDIR /go/src COPY app.go ./ RUN go build app.go -o myapp # 阶段2，引用阶段1再进行一次构建 FROM builder as builder_ex ADD dest.tar ./ ... ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-dockerfile-optimization/","summary":"前言 在使用 Docker 的过程中，编写 Dockerfile 是非常重要的一部分工作。合理编写 Dockerfile 会使我们构建出来的 Docker image 拥有更佳的性能和健壮性 目标: 更快的构建速度 更小的 Docker 镜像大小 更少的 Docker 镜像层 充分利用镜像缓存 增加 Dockerfile 可读性 让 Docker 容器使用起来更简单 总结 编写.dockerignore 文件 容器只运行单个应用 将多个 RUN 指令合并为","title":"docker | dockerfile最佳实践"},{"content":"前言 Dockerfile用于构建docker镜像, 实际上就是把在linux下的命令操作写到了Dockerfile中, 通过Dockerfile去执行设置好的操作命令, 保证通过Dockerfile的构建镜像是一致的.\nDockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n参考内容：\nhttps://yeasy.gitbook.io/docker_practice/image/dockerfile FROM 指定基础镜像 命令格式\nFROM IMAGE[:TAG][@DIGEST] 我们可以用任意已存在的镜像为基础构建我们的自定义镜像\n比如:\n系统镜像: centos, ubuntu, debian, alpine\n应用镜像: nginx, redis, mongo, mysql, httpd\n运行环境镜像: php, java, golang\n工具镜像: busybox\n示例\n# tag 默认使用 latest FROM alpine # 指定 tag FROM alpine:3.17.3 # 指定 digest FROM alpine@sha256:b6ca290b6b4cdcca5b3db3ffa338ee0285c11744b4a6abaa9627746ee3291d8d # 同时指定 tag 和 digest FROM alpine:3.17.3@sha256:b6ca290b6b4cdcca5b3db3ffa338ee0285c11744b4a6abaa9627746ee3291d8d 除了选择现有镜像为基础镜像外，Docker还存在一个特殊的镜像，名为 scratch。这个镜像无法从别处拉取, 可以理解为是Docker自 1.5.0 版本开始的自带镜像, 它仅包含一个空的文件系统.\nscratch镜像一般用于构建基础镜像, 比如官方镜像Ubuntu\nCOPY 复制文件 格式:\nCOPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;源路径1\u0026gt; [源路径2] ... \u0026lt;目标路径\u0026gt; COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026quot;\u0026lt;源路径1\u0026gt;\u0026quot;, \u0026quot;[源路径2]\u0026quot;, ..., \u0026quot;\u0026lt;目标路径\u0026gt;\u0026quot;] COPY 指令将从构建上下文目录中 \u0026lt;源路径\u0026gt; 的文件/目录复制到新的镜像层内的 \u0026lt;目标路径\u0026gt; 位置.\n\u0026lt;源路径\u0026gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如：\nCOPY hom* /mydir/ COPY hom?.txt /mydir/ \u0026lt;目标路径\u0026gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。\n此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。\n在使用该指令的时候还可以加上 --chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt; 选项来改变文件的所属用户及所属组。\nCOPY --chown=55:mygroup files* /mydir/ COPY --chown=bin files* /mydir/ COPY --chown=1 files* /mydir/ COPY --chown=10:11 files* /mydir/ ADD 更高级的复制文件 ADD 指令和 COPY 的格式和性质基本一致。同样支持 --chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt; 指令修改属主和属组。\n但是在 COPY 基础上增加了一些功能:\n\u0026lt;源路径\u0026gt; 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 \u0026lt;目标路径\u0026gt; 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整. 如果 \u0026lt;源路径\u0026gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 \u0026lt;目标路径\u0026gt; 去。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。\n另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。\nRUN 执行命令 格式:\nshell格式:RUN [command] \u0026lt;parameter1\u0026gt; \u0026lt;parameter2\u0026gt; ..., 等价于在linux中执行/bin/sh -c \u0026quot;command parameter1 parameter2 ...\u0026quot;\nRUN ls -l exec格式:RUN [\u0026quot;command\u0026quot;, \u0026quot;parameter1\u0026quot;, \u0026quot;parameter2\u0026quot;...], 不会通过shell执行, 所以像$HOME这样的变量就无法获取.\nRUN [\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;] RUN [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls -l\u0026#34;] # 可以获取环境变量 RUN指令用于指定构建镜像时执行的命令, Dockerfile允许多个RUN指令, 并且每个RUN指令都会创建一个镜像层.\nRUN指令一般用于安装配置软件包等操作, 为避免镜像层数过多, 一般RUN指令使用shell格式且使用换行符来执行多个命令，且尽量将 RUN 指令产生的附属物删除以缩小镜像大小\n如下示例\nFROM debian:stretch RUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y $buildDeps \\ \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\ \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026amp;\u0026amp; make -C /usr/src/redis \\ \u0026amp;\u0026amp; make -C /usr/src/redis install \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm redis.tar.gz \\ \u0026amp;\u0026amp; rm -r /usr/src/redis \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps CMD 容器启动命令 CMD 指令的格式和 RUN 相似，也是两种格式：\nshell 格式：CMD [command] \u0026lt;parameters\u0026gt; exec 格式：CMD [\u0026quot;command\u0026quot;, \u0026quot;\u0026lt;parameter1\u0026gt;\u0026quot;, \u0026quot;parameter2\u0026quot;, ...] 参数列表格式：CMD [\u0026quot;参数1\u0026quot;, \u0026quot;参数2\u0026quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 CMD 指令用于设置容器启动时 默认执行 的指令，一般会设置为应用程序的启动脚本或者工具镜像的bash，设置了多条CMD指令时，只有最后一条 CMD 会被执行。\n在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，会输出系统版本信息。\n在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 \u0026quot;，而不要使用单引号。\n例如一般nginx容器的CMD指令:\nCMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] ENTRYPOINT 入口点 ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。\nshell 格式：ENTRYPOINT [command] \u0026lt;parameters\u0026gt; exec 格式：ENTRYPOINT [\u0026quot;command\u0026quot;, \u0026quot;\u0026lt;parameter1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;parameter2\u0026gt;\u0026quot;, ...] ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。\n当指定了 ENTRYPOINT 且使用的是 exec 格式时，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为：\nENTRYPOINT [\u0026#34;command\u0026#34;, \u0026#34;\u0026lt;parameter1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;parameter2\u0026gt;\u0026#34;, \u0026#34;CMD\u0026#34;] 以下示例将展示 CMD 指令作为参数传给 ENTRYPOINT 的场景\n场景一：我们自己构建了一个用于查看外网 ip 和归属地的镜像\nFROM alpine RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\u0026#39; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk --update add curl CMD [ \u0026#34;-s\u0026#34; ] ENTRYPOINT [ \u0026#34;curl\u0026#34;, \u0026#34;http://myip.ipip.net\u0026#34; ] 构建\ndocker build -t busybox-curl . 以两种方式运行\n# 容器中实际执行的指令为 curl http://myip.ipip.net -s [root@lvbibir learn]# docker run -it --rm busybox-curl 当前 IP：101.201.150.47 来自于：中国 北京 北京 阿里云 # 容器中实际执行的指令为 curl http://myip.ipip.net -i [root@lvbibir learn]# docker run -it --rm busybox-curl -i HTTP/1.1 200 OK Date: Mon, 10 Apr 2023 03:21:59 GMT Content-Type: text/plain; charset=utf-8 Content-Length: 72 Connection: keep-alive Node: ipip-myip5 X-Cache: BYPASS X-Request-Id: e309720b9197e8b94cec18b409c69d1d Server: WAF Connection: close Accept-Ranges: bytes 当前 IP：101.201.150.47 来自于：中国 北京 北京 阿里云 场景二：应用运行前的准备工作\n启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。\n比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。\n此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。\n这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 \u0026lt;CMD\u0026gt;）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的：\nFROM alpine:3.4 ... RUN addgroup -S redis \u0026amp;\u0026amp; adduser -S -G redis redis ... ENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;] EXPOSE 6379 CMD [ \u0026#34;redis-server\u0026#34; ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本：\n#!/bin/sh ... # allow the container to be started with `--user` if [ \u0026#34;$1\u0026#34; = \u0026#39;redis-server\u0026#39; -a \u0026#34;$(id -u)\u0026#34; = \u0026#39;0\u0026#39; ]; then find . \\! -user redis -exec chown redis \u0026#39;{}\u0026#39; + exec gosu redis \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; fi exec \u0026#34;$@\u0026#34; 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如：\n[root@lvbibir learn]# docker run -it redis id uid=0(root) gid=0(root) groups=0(root) ENV 设置环境变量 格式有两种：\nENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt;... ENV 用于设置环境变量，既可以在 Dockerfile 中调用，也可以在构建完的容器运行时中使用。\n支持的指令： ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN\n下面这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。\nENV VERSION=1.0 DEBUG=on \\ NAME=\u0026#34;Happy Feet\u0026#34; 示例\nFROM alpine ENV VERSION=1.0 \\ DEBUG=on \\ NAME=\u0026#34;Happy Feet\u0026#34; RUN echo \u0026#34;name: ${NAME}\u0026#34; \u0026gt; /test \\ \u0026amp;\u0026amp; echo \u0026#34;version: ${VERSION}\u0026#34; \u0026gt;\u0026gt; /test 构建\n[root@lvbibir learn]# docker build -t demo-env . 构建时调用了环境变量\n[root@lvbibir learn]# docker run -it --rm demo-env cat /test name: Happy Feet version: 1.0 构建后的容器运行时中调用，这里需要使用 /bin/sh -c 的方式，不然无法读取变量。且对 $ 进行转义，不然读取的将会是宿主机的变量\n[root@lvbibir learn]# docker run -it --rm demo-env /bin/sh -c \u0026#34;echo \\${DEBUG}\u0026#34; on ARG 构建参数 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。\nDockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg \u0026lt;参数名\u0026gt;=\u0026lt;值\u0026gt; 来覆盖。\n灵活的使用 ARG 指令，能够在不修改 Dockerfile 的情况下，构建出不同的镜像。\nARG 指令有生效范围，如果在 FROM 指令之前指定，那么只能用于 FROM 指令中，FROM 指令可以是多个\nARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine RUN set -x ; echo ${DOCKER_USERNAME} 使用上述 Dockerfile 会发现无法输出 ${DOCKER_USERNAME} 变量的值，要想正常输出，你必须在 FROM 之后再次指定 ARG，如下示例\n# 只在 FROM 中生效 ARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine # 要想在 FROM 之后使用，必须再次指定 ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} 如下示例，变量将会在每个 FROM 指令中生效\n# 这个变量在每个 FROM 中都生效 ARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine RUN set -x ; echo 1 FROM ${DOCKER_USERNAME}/alpine RUN set -x ; echo 2 如下示例，对于在各个阶段中使用的变量都必须在每个阶段分别指定\nARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine # 在FROM 之后使用变量，必须在每个阶段分别指定 ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} FROM ${DOCKER_USERNAME}/alpine # 在FROM 之后使用变量，必须在每个阶段分别指定 ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} VOLUME 定义匿名卷 格式为：\nVOLUME [\u0026quot;\u0026lt;路径1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;路径2\u0026gt;\u0026quot;...] VOLUME \u0026lt;路径\u0026gt; 容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中。\n为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据，从而保证了容器存储层的无状态化。\nVOLUME 创建的匿名卷会挂载到系统 /var/lib/docker/volumes/\u0026lt;CONTAINER-ID\u0026gt;/\u0026lt;_VOLUME\u0026gt; 目录下，且不会随着容器删除而删除，需要手动删除\n如下示例\nFROM alpine VOLUME /data 构建运行\n[root@lvbibir learn]# docker build -t demo-volume . [root@lvbibir learn]# docker run -itd --name=demo-volume demo-volume [root@lvbibir learn]# docker exec -it demo-volume ls -ld /data drwxr-xr-x 2 root root 4096 Apr 10 05:24 /data 查看挂载目录\n[root@lvbibir learn]# docker inspect --format=\u0026#39;{{json .Mounts}}\u0026#39; demo-volume | python -m json.tool [ { \u0026#34;Destination\u0026#34;: \u0026#34;/data\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;49cf915dd297292e3d0e4b2c7a66ead6875cfb0dbd010de15189040ab1158b3b\u0026#34;, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/49cf915dd297292e3d0e4b2c7a66ead6875cfb0dbd010de15189040ab1158b3b/_data\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34; } ] 如下示例，运行容器时，可以指定 -v 参数将目录挂载到指定位置\n[root@lvbibir learn]# docker run -itd -v /mydata:/data --name demo-volume-2 demo-volume [root@lvbibir learn]# docker inspect --format=\u0026#39;{{json .Mounts}}\u0026#39; demo-volume-2 | python -m json.tool [ { \u0026#34;Destination\u0026#34;: \u0026#34;/data\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Propagation\u0026#34;: \u0026#34;rprivate\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Source\u0026#34;: \u0026#34;/mydata\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;bind\u0026#34; } ] EXPOSE 暴露端口 格式为 EXPOSE \u0026lt;端口1\u0026gt; [端口2] ...\nEXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务\n在 Dockerfile 中写入这样的声明有两个好处：\n一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射； 另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p \u0026lt;宿主端口\u0026gt;:\u0026lt;容器端口\u0026gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。\nWORKDIR 指定工作目录 格式为 WORKDIR \u0026lt;路径\u0026gt;\n使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录\n如下示例，是一个常见的错误，world.txt 最终会在 /app 目录下，而不是期望的 /app/demo 目录\nWORKDIR /app RUN mkdir demo \u0026amp;\u0026amp; cd demo RUN echo \u0026#34;hello\u0026#34; \u0026gt; world.txt 上述需求可以进行如下优化，推荐使用第二种写法\nWORKDIR /app/demo RUN echo \u0026#34;hello\u0026#34; \u0026gt; world.txt # 或者 WORKDIR /app RUN mkdir demo \\ \u0026amp;\u0026amp; echo \u0026#34;hello\u0026#34; \u0026gt; demo/world.txt # 或者 WORKDIR /app RUN mkdir demo \\ \u0026amp;\u0026amp; cd demo \\ \u0026amp;\u0026amp; echo \u0026#34;hello\u0026#34; \u0026gt; demo/world.txt 如果你的 WORKDIR 指令使用的相对路径，那么所切换的路径与之前的 WORKDIR 有关\n如下示例，pwd 的输出将会是 /a/b/c\nWORKDIR /a WORKDIR b WORKDIR c RUN pwd USER 指定当前用户 格式：USER \u0026lt;用户名\u0026gt;[:\u0026lt;用户组\u0026gt;]\nUSER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。\n注意，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。\nRUN groupadd -r redis \u0026amp;\u0026amp; useradd -r -g redis redis USER redis RUN [ \u0026#34;redis-server\u0026#34; ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu\n不过更推荐的还是 上文 中提到过的通过 ENTRYPOINT 脚本的方式\n使用 gosu 示例\n# 建立 redis 用户，并使用 gosu 换另一个用户执行命令 RUN groupadd -r redis \u0026amp;\u0026amp; useradd -r -g redis redis # 下载 gosu RUN wget -O /usr/local/bin/gosu \u0026#34;https://github.com/tianon/gosu/releases/download/1.12/gosu-amd64\u0026#34; \\ \u0026amp;\u0026amp; chmod +x /usr/local/bin/gosu \\ \u0026amp;\u0026amp; gosu nobody true # 设置 CMD，并以另外的用户执行 CMD [ \u0026#34;exec\u0026#34;, \u0026#34;gosu\u0026#34;, \u0026#34;redis\u0026#34;, \u0026#34;redis-server\u0026#34; ] HEALTHCHECK 健康检查 格式：\nHEALTHCHECK [选项] CMD \u0026lt;命令\u0026gt;：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。\n在没有 HEALTHCHECK 指令前，Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。很多情况下这没问题，但是如果程序进入死锁状态，或者死循环状态，应用进程并不退出，但是该容器已经无法提供服务了。在 1.12 以前，Docker 不会检测到容器的这种状态，从而不会重新调度，导致可能会有部分容器已经无法提供服务了却还在接受用户请求。\nHEALTHCHECK 支持下列选项：\n--interval=\u0026lt;间隔\u0026gt;：两次健康检查的间隔，默认为 30 秒； --timeout=\u0026lt;时长\u0026gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； --retries=\u0026lt;次数\u0026gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。\n在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：\n0：成功； 1：失败； 2：保留，不要使用这个值。 如下示例，假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断\nFROM nginx HEALTHCHECK --interval=5s --timeout=3s --retries=3\\ CMD curl -fs http://localhost/ || exit 1 构建运行\n[root@lvbibir learn]# docker build -t myweb . [root@lvbibir learn]# docker run -d --name demo-myweb -p 800:80 myweb # 此时是 starting 状态 [root@lvbibir learn]# docker ps | grep myweb e6f585df60a6 myweb \u0026#34;/docker-entrypoint.…\u0026#34; About a minute ago Up 2 seconds (health: starting) 0.0.0.0:800-\u0026gt;80/tcp demo-myweb # 等待几秒变为 healthy 状态 [root@lvbibir learn]# docker ps | grep myweb e6f585df60a6 myweb \u0026#34;/docker-entrypoint.…\u0026#34; 2 minutes ago Up About a minute (healthy) 0.0.0.0:800-\u0026gt;80/tcp demo-myweb 删除 index.html 文件模拟故障\n[root@lvbibir learn]# docker exec -it demo-myweb rm -f /usr/share/nginx/html/index.html # 状态变为unhealthy [root@lvbibir learn]# docker ps | grep myweb e6f585df60a6 myweb \u0026#34;/docker-entrypoint.…\u0026#34; 6 minutes ago Up 5 minutes (unhealthy) 0.0.0.0:800-\u0026gt;80/tcp demo-myweb 为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。\n[root@lvbibir learn]# docker inspect --format \u0026#39;{{json .State.Health}}\u0026#39; demo-myweb | python -m json.tool { \u0026#34;FailingStreak\u0026#34;: 25, \u0026#34;Log\u0026#34;: [ { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:41:51.393698555+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 1, \u0026#34;Output\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:41:51.285647058+08:00\u0026#34; }, { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:41:56.504282619+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 1, \u0026#34;Output\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:41:56.401745529+08:00\u0026#34; }, ........... ], \u0026#34;Status\u0026#34;: \u0026#34;unhealthy\u0026#34; } 恢复文件\n[root@lvbibir learn]# docker exec -it demo-myweb /bin/bash root@e6f585df60a6:/# echo test \u0026gt; /usr/share/nginx/html/index.html root@e6f585df60a6:/# exit [root@lvbibir learn]# docker inspect --format \u0026#39;{{json .State.Health}}\u0026#39; demo-myweb | python -m json.tool { \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:48:30.482498808+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;test\\n\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:48:30.378197999+08:00\u0026#34; }, { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:48:35.599150547+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;test\\n\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:48:35.490433323+08:00\u0026#34; }, ....... ], \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34; } ONBUILD 为他人作嫁衣裳 格式：ONBUILD \u0026lt;其它指令\u0026gt;。\nONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。\nDockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。\n假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile：\nFROM node:slim WORKDIR /app COPY ./package.json /app RUN [ \u0026#34;npm\u0026#34;, \u0026#34;install\u0026#34; ] COPY . /app/ CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。\n如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。\n那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。\n基础镜像(my-node) Dockerfile\nFROM node:slim WORKDIR /app CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 应用镜像(my-app1) Dockerfile\nFROM my-node COPY ./package.json /app RUN [ \u0026#34;npm\u0026#34;, \u0026#34;install\u0026#34; ] COPY . /app/ 基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。\n那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。\nONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile:\nFROM node:slim WORKDIR /app ONBUILD COPY ./package.json /app ONBUILD RUN [ \u0026#34;npm\u0026#34;, \u0026#34;install\u0026#34; ] ONBUILD COPY . /app/ CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 应用镜像 Dcokerfile\nFROM my-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。\nLABEL 为镜像添加元数据 LABEL 指令用来给镜像以键值对的形式添加一些元数据（metadata）。\nLABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... 我们还可以用一些标签来申明镜像的作者、文档地址等：\nLABEL org.opencontainers.image.authors=\u0026#34;yeasy\u0026#34; LABEL org.opencontainers.image.documentation=\u0026#34;https://yeasy.gitbooks.io\u0026#34; 具体可以参考 https://github.com/opencontainers/image-spec/blob/master/annotations.md\nSHELL 指定shell 格式：SHELL [\u0026quot;executable\u0026quot;, \u0026quot;parameters\u0026quot;]\nSHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell，Linux 中默认为 `[\u0026quot;/bin/sh\u0026quot;, \u0026ldquo;-c\u0026rdquo;]\n如下示例，两个 RUN 运行同一命令，第二个 RUN 运行的命令会打印出每条命令并当遇到错误时退出。\nSHELL [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] RUN lll ; ls SHELL [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-cex\u0026#34;] RUN lll ; ls 如下示例，当 ENTRYPOINT CMD 以 shell 格式指定时，SHELL 指令所指定的 shell 也会成为这两个指令的 shell\nSHELL [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-cex\u0026#34;] # /bin/sh -cex \u0026#34;nginx\u0026#34; ENTRYPOINT nginx # /bin/sh -cex \u0026#34;nginx\u0026#34; CMD nginx ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-dockerfile-instruction/","summary":"前言 Dockerfile用于构建docker镜像, 实际上就是把在linux下的命令操作写到了Dockerfile中, 通过Dockerfile去执行设置好的操作命令, 保证通过Dockerfile的构建镜像是一致的. Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，","title":"docker | dockerfile指令详解"},{"content":"1. 简介 kill命令很容易让人产生误解, 以为仅仅是用来终止linux中的进程.\n在man手册中对kill命令的解释如下, 不难看出, kill命令是一个用于将指定的signal发送给进程的工具\nDESCRIPTION The command kill sends the specified signal to the specified process or process group. If no signal is specified, the TERM signal is sent. The TERM signal will kill processes which do not catch this signal. For other processes, it may be necessary to use the KILL (9) signal, since this signal cannot be caught. Most modern shells have a builtin kill function, with a usage rather similar to that of the command described here. The \u0026lsquo;-a\u0026rsquo; and \u0026lsquo;-p\u0026rsquo; options, and the possibility to specify processes by command name are a local extension. If sig is 0, then no signal is sent, but error checking is still performed.\n命令格式\nkill [-s signal|-p] [-q sigval] [-a] [--] pid... 常用参数\n-l # 列出所有支持的signal -s NAME # 使用NAME指定signal -NUM # 使用编号指定signal kill -s HUP 和 kill -1 效果一样 2. 支持的信号 [root@lvbibir ~]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 可以看到kill支持的信号非常多, 在这些信号中只有9) SIGKILL可以无条件地终止process, 其他信号都将依照process中定义的信号处理规则来进行忽略或者处理.\n上述信号中常用的其实很少, 如下表所示\n编号 名称 解释 1 SIGHUP 启动被终止的程序, 也可以让进程重新读取自己的配置文件, 类似重新启动 2 SIGINT 相当于输入 ctrl-c 来中断一个程序 9 SIGKILL 强制中断一个程序, 不会进行资源的清理工作. 如果该程序进行到一半, 可能会有半成品产生, 类似 vim 的 .filename.swp 保留下来 15 SIGTERM 以正常(优雅)的方式来终止进程, 由程序自身决定该如何终止 19 SIGSTOP 相当于输入 ctrl-z 来暂停一个程序 3. 常用命令 以正常的方式终止进程, 由于信号15是最常用也是最佳的程序退出方式, 所以 kill 命令不指定信号时, 默认使用的就是信号 15\nkill pid # 或者 kill -15 pid 强制终止进程\nkill -9 pid ","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-command-kill/","summary":"1. 简介 kill命令很容易让人产生误解, 以为仅仅是用来终止linux中的进程. 在man手册中对kill命令的解释如下, 不难看出, kill命令是一个用于将指定的signal发送给进程的工具 DESCRIPTION The command kill sends the specified signal to the specified process or process group. If no signal is specified, the TERM signal is sent. The TERM signal will kill processes which do not catch this signal. For other processes, it may be necessary to use the KILL","title":"linux | kill命令详解以及linux中的信号"},{"content":"报错详细信息\nException in thread \u0026#34;main\u0026#34; java.nio.file.NotDirectoryException: /usr/share/elasticsearch/plugins/plugin-descriptor.properties at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:439) at java.base/java.nio.file.Files.newDirectoryStream(Files.java:482) at java.base/java.nio.file.Files.list(Files.java:3793) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.getPluginInfo(BootstrapJvmOptions.java:49) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.bootstrapJvmOptions(BootstrapJvmOptions.java:34) at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:137) at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:86) 安装插件时直接将插件的zip解压到了 plugins目录 导致的，每个插件应以目录的形式存放在 plugins目录 中\n[root@21-centos-7 ~]# ls /data/elasticsearch/plugins/ commons-codec-1.9.jar commons-logging-1.2.jar config elasticsearch-analysis-ik-7.17.3.jar httpclient-4.5.2.jar httpcore-4.4.4.jar plugin-descriptor.properties plugin-security.policy 只需要为每个插件创建一个目录，并把插件解压到对应目录即可\nmkdir /data/elasticsearch/plugins/elasticsearch-analysis-ik/ unzip elasticsearch-analysis-ik-7.17.3.zip -d /data/elasticsearch/plugins/elasticsearch-analysis-ik/ 参考: https://github.com/medcl/elasticsearch-analysis-ik/issues/638\n","permalink":"https://www.lvbibir.cn/en/posts/tech/elasticsearch-plugin-error/","summary":"报错详细信息 Exception in thread \u0026#34;main\u0026#34; java.nio.file.NotDirectoryException: /usr/share/elasticsearch/plugins/plugin-descriptor.properties at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:439) at java.base/java.nio.file.Files.newDirectoryStream(Files.java:482) at java.base/java.nio.file.Files.list(Files.java:3793) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.getPluginInfo(BootstrapJvmOptions.java:49) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.bootstrapJvmOptions(BootstrapJvmOptions.java:34) at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:137) at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:86) 安装插件时直接将插件的zip解压到了 plugins目录 导致的，每个插件应以目录的形式存放在 plugins目录 中 [root@21-centos-7 ~]# ls /data/elasticsearch/plugins/ commons-codec-1.9.jar commons-logging-1.2.jar config elasticsearch-analysis-ik-7.17.3.jar httpclient-4.5.2.jar httpcore-4.4.4.jar plugin-descriptor.properties plugin-security.policy 只需要为每个插件创建一个目录，并把插件解压到对应目录即可 mkdir /data/elasticsearch/plugins/elasticsearch-analysis-ik/ unzip elasticsearch-analysis-ik-7.17.3.zip -d /data/elasticsearch/plugins/elasticsearch-analysis-ik/ 参考: https://github.com/medcl/elasticsearch-analysis-ik/issues/638","title":"elasticsearch 安装插件报错 java.nio.file.NotDirectoryException"},{"content":"1. 基本语法 if [ command ];then 符合该条件执行的语句 elif [ command ];then 符合该条件执行的语句 else 符合该条件执行的语句 fi 2. 字符串判断 表达式 解释 [ -z STRING ] 如果STRING的长度为零则为真 ，即判断是否为空，空即是真； [ -n STRING ] or [ STRING ] 如果STRING的长度非零则为真 ，即判断是否为非空，非空即是真； [ STRING1 = STRING2 ] 如果两个字符串相同则为真 ； [ STRING1 != STRING2 ] 如果字符串不相同则为真 ； 3. 数值判断 表达式 解释 [ INT1 -eq INT2 ] INT1和INT2两数相等为真，= [ INT1 -ne INT2 ] INT1和INT2两数不等为真，!= [ INT1 -gt INT2 ] INT1大于INT1为真，\u0026gt; [ INT1 -ge INT2 ] INT1大于等于INT2为真，\u0026gt;= [ INT1 -lt INT2 ] INT1小于INT2为真，\u0026lt; [ INT1 -le INT2 ] INT1小于等于INT2为真，\u0026lt;= 4. 文件/目录判断 表达式 解释 [ -b FILE ] 如果 FILE 存在且是一个块特殊文件则为真 [ -c FILE ] 如果 FILE 存在且是一个字特殊文件则为真 [ -d DIR ] 如果 FILE 存在且是一个目录则为真 [ -e FILE ] 如果 FILE 存在则为真 [ -f FILE ] 如果 FILE 存在且是一个普通文件则为真 [ -g FILE ] 如果 FILE 存在且已经设置了SGID则为真 [ -k FILE ] 如果 FILE 存在且已经设置了粘制位则为真 [ -p FILE ] 如果 FILE 存在且是一个名字管道(F如果O)则为真 [ -r FILE ] 如果 FILE 存在且是可读的则为真 [ -s FILE ] 如果 FILE 存在且大小不为0则为真 [ -t FD ] 如果文件描述符 FD 打开且指向一个终端则为真 [ -u FILE ] 如果 FILE 存在且设置了SUID (set user ID)则为真 [ -w FILE ] 如果 FILE存在且是可写的则为真 [ -x FILE ] 如果 FILE 存在且是可执行的则为真 [ -O FILE ] 如果 FILE 存在且属有效用户ID则为真 [ -G FILE ] 如果 FILE 存在且属有效用户组则为真 [ -L FILE ] 如果 FILE 存在且是一个符号连接则为真 [ -N FILE ] 如果 FILE 存在且自上次阅读以来已进行了修改则为真 [ -S FILE ] 如果 FILE 存在且是一个套接字则为真 [ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 更新，或者 FILE1 存在且 FILE2 不存在则为真 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 要老，或者 FILE2 存在且 FILE1 不存在则为真 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则为真 5. 与或非 -a \u0026amp;\u0026amp; 与，两个条件都满足 -o || 或，两个条件只满足一个条件 ! 非，两个条件都不满足 ","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-if/","summary":"1. 基本语法 if [ command ];then 符合该条件执行的语句 elif [ command ];then 符合该条件执行的语句 else 符合该条件执行的语句 fi 2. 字符串判断 表达式 解释 [ -z STRING ] 如果STRING的长度为零则为真 ，即判断是否为空，空即是真； [ -n STRING ] or [ STRING ] 如果STRING的长度非零则为真 ，即判断是否为非空，非空即是真； [ STRING1 = STRING2 ] 如果两个字","title":"shell | if条件判断"},{"content":"1. 阿里云构建 1.1 git仓库设置 1.1.1 创建仓库 用于存放Dockerfile\n1.1.2 上传Dockerfile #换成自己的仓库地址 git clone https://github.com/lvbibir/docker-images cd docker-images mkdir -p k8s.gcr.io/pause-3.2/ echo \u0026#34;FROM k8s.gcr.io/pause:3.2\u0026#34; \u0026gt; k8s.gcr.io/pause-3.2/Dockerfile git add . git commit -m \u0026#39;new image: k8s.gcr.io/pause:3.2\u0026#39; # 默认分支可能是main，取决于你的github设置 git push origin master 1.2 阿里云设置 登陆阿里云，访问阿里云容器镜像服务\n1.2.1 创建个人实例 1.2.2 进入个人实例创建命名空间 1.2.3 创建访问凭证 1.2.4 绑定github账号 1.2.5 新建镜像仓库 指定刚才创建的github仓库，记得勾选海外机器构建\n1.2.6 新建构建 手动触发构建，正常状况应该可以看到构建成功\n1.2.7 镜像下载 操作指南里可以看到如何下载镜像，标签即刚才新建构建时指定的镜像版本\n2. gcr.io_mirror 项目地址\n该项目通过 Github Actions 将 gcr.io、k8s.gcr.io、registry.k8s.io、quay.io、ghcr.io 镜像仓库的镜像搬运至dockerhub\n直接提交issue，在模板issue的[PORTER]后面添加想要搬运的镜像tag，也可以直接在关闭的issue列表中检索，可能也会有其他人搬运过，直接用就行了\n稍等一小会可以看到镜像已经搬运到dockerhub了\n3. Docker Playground Docker Playground 是一个免费的线上docker环境，由于是外网环境所以下载镜像、推送到dockerhub都很快，也可以直接推到阿里云的仓库\ndocker login --username=lvbibir registry.cn-hangzhou.aliyuncs.com docker pull \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; dokcer tag \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; registry.cn-hangzhou.aliyuncs.com/lvbibir/\u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; docker push registry.cn-hangzhou.aliyuncs.com/lvbibir/\u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; 4. http proxy 如果有代理软件可以在docker中配置代理实现\n{ \u0026#34;proxies\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;httpProxy\u0026#34;: \u0026#34;http://127.0.0.1:1080\u0026#34;, \u0026#34;httpsProxy\u0026#34;: \u0026#34;http://127.0.0.1:1080\u0026#34;, \u0026#34;noProxy\u0026#34;: \u0026#34;*.test.example.com,.example2.com\u0026#34; } } } 5. 使用国内现成的镜像站 这种方式的问题主要是镜像不全，且没有统一的管理，建议使用之前的四种方式\n阿里云仓库\ndocker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0 # 换成 docker pull registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.3.0 也可以使用 lank8s.cn，他们的对应关系 k8s.gcr.io –\u0026gt; lank8s.cn，gcr.io –\u0026gt; gcr.lank8s.cn\ndocker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0 # 换成 docker pull lank8s.cn/sig-storage/csi-node-driver-registrar:v2.3.0 中科大\ndocker image pull quay.io/kubevirt/virt-api:v0.45.0 # 换成 docker pull quay.mirrors.ustc.edu.cn/kubevirt/virt-api:v0.45.0 ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-download-foreign-images/","summary":"1. 阿里云构建 1.1 git仓库设置 1.1.1 创建仓库 用于存放Dockerfile 1.1.2 上传Dockerfile #换成自己的仓库地址 git clone https://github.com/lvbibir/docker-images cd docker-images mkdir -p k8s.gcr.io/pause-3.2/ echo \u0026#34;FROM k8s.gcr.io/pause:3.2\u0026#34; \u0026gt; k8s.gcr.io/pause-3.2/Dockerfile git add . git commit -m \u0026#39;new image: k8s.gcr.io/pause:3.2\u0026#39; # 默认分支可能是main，取决于你的github设置 git push origin master 1.2 阿里云设置 登陆阿里云，访问阿里云容器镜像服务 1.2.1 创建个人实例 1.2.2 进","title":"docker | 下载外网镜像的几种方式"},{"content":"原题如下\n鬼谷子随意从2-99中选取了两个数。他把这两个数的和告诉了庞涓， 把这两个数的乘积告诉了孙膑。但孙膑和庞涓彼此不知道对方得到的数。第二天， 庞涓很有自信的对孙膑说：虽然我不知道这两个数是什么，但我知道你一定也不知道。随后，孙膑说：那我知道了。庞涓说：那我也知道了。这两个数是什么？\n代码示例\n#!/usr/bin/env python # -*- coding: utf-8 -*- \u0026#39;\u0026#39;\u0026#39; 第一步 庞告诉孙，已知和Sum满足有至少两种ab组合，且任意一组ab的乘积Pro都满足至少有两种ab组合，通过isPang函数将可能的ab组合放入abList_1 第二步 孙告诉庞，abList_1中的ab组合乘积得pro，该pro满足至少有两种ab组合，且所有的ab组合有且仅有一组ab组合满足isPang函数，通过isSun函数将abList中满足条件的ab组合放入abList_2，ab组合的积放入proList 第三步 庞告诉孙，abList_2中的ab组合相加得Sum，该Sum满足至少有两种ab组合，且所有的ab组合有且仅有一组ab所得乘积pro在proList中，将满足条件的ab组合放入abList，即最终答案 \u0026#39;\u0026#39;\u0026#39; # 根据给出的sum，遍历所有可能的a和b的组合 def getCombinationSum(sum): combination = [] for a in range(2, 100): for b in range(2, 100): if a + b == sum and a \u0026lt;= b: combination.append((a, b)) return combination # 根据给出的pro，遍历所有可能的a和b的组合 def getCombinationPro(pro): combination = [] for a in range(2, 100): for b in range(2, 100): if a * b == pro and a \u0026lt;= b: combination.append((a, b)) return combination def isPang(sum): \u0026#39;\u0026#39;\u0026#39; 第一步，传入的sum满足以下条件返回True，否则False: 1. 可以拆分成若干组ab的加和 2. 每一组拆分出来的ab乘积运算得pro，该pro满足有至少两组ab的乘积 \u0026#39;\u0026#39;\u0026#39; if len(getCombinationSum(sum)) \u0026lt; 2: return False else: combinationSum = getCombinationSum(sum) for i in combinationSum: status = 0 pro = i[0] * i[1] # 有其中一组ab不满足就打断循环 if len(getCombinationPro(pro)) \u0026lt; 2: status = 1 break if status == 0: return True else: return False def isSun(pro): \u0026#39;\u0026#39;\u0026#39; 第二步，传入的pro满足以下条件返回一组ab组合(元组)，否则False 1. 可以拆分成若干组ab的乘积 2. 每一组拆分出来的ab相加运算得sum，所有ab加和的sum有且仅有一个满足第一步的条件(放入isPang函数后返回True) \u0026#39;\u0026#39;\u0026#39; combination = [] combinationPro = getCombinationPro(pro) if len(combinationPro) \u0026gt; 1: for i in combinationPro: sum = i[0] + i[1] if isPang(sum): combination.append(i) if len(combination) == 1: return combination else: return False if __name__ == \u0026#39;__main__\u0026#39;: # 第一步 abList_1 = [] for sum in range(4, 198+1): if isPang(sum): abList_1 += getCombinationSum(sum) # 第二步 abList_2 = [] proList = [] for i in abList_1: pro = i[0] * i[1] if isSun(pro): abList_2.append(i) proList.append(pro) # 第三步 abList = [] for i in abList_2: sum = i[0] + i[1] n = 0 for j in getCombinationSum(sum): if j[0] * j[1] in proList: n += 1 if n == 1: abList.append(i) print(abList) ","permalink":"https://www.lvbibir.cn/en/posts/tech/logic-problem-guiguzi/","summary":"原题如下 鬼谷子随意从2-99中选取了两个数。他把这两个数的和告诉了庞涓， 把这两个数的乘积告诉了孙膑。但孙膑和庞涓彼此不知道对方得到的数。第二天， 庞涓很有自信的对孙膑说：虽然我不知道这两个数是什么，但我知道你一定也不知道。随后，孙膑说：那我知道了。庞涓说：那我也知道了。这两个数是什","title":"逻辑题 | 鬼谷子数学问题"},{"content":"1 前言 挂刀是指从饰品交易平台购买游戏饰品，在steam市场出售以实现将人民币转换为steam阿根廷账号余额。\nsteam圣诞促销活动快结束了，买了几款游戏后发现阿根廷账号余额没多少了，挂刀过程又比较繁琐，故有此文记录一下挂刀搞余额的步骤。\n2 步骤 2.1 网易buff账号注册及绑定 buff账号使用手机号注册即可，绑定需要搞余额的steam账号，同时需要提供steam账号的API key和交易链接，这部分buff有教程，或者百度，很容易找到\n2.2 挂刀油猴脚本 脚本链接\n脚本安装成功进入网页版buff后，简单设置一下脚本，推荐设置了一下货币转换为阿根廷比索和默认排序规则\n2.3 脚本提供的信息 每个饰品需要关注的有如下信息：\n挂刀比例 越低代表售出后可获得的余额更多 左边是buff售价，右边是市场售价（阿根廷比索）\nsteam求购人数\n2.4 寻找合适饰品 以以下几个维度入手，选择合适的饰品：\n比例较低 求购人数多 价格合适，太高可能卖的慢，太低要达到自己的要求可能需要倒好几个甚至十几个饰品才够 选择好后直接购买即可，后续步骤按照buff教程来\n2.5 卖出饰品 脚本提供的收益只能参考，具体还是要在市场看，着急就按照最低价卖即可，别人购买后就可以愉快的买新游戏啦\n","permalink":"https://www.lvbibir.cn/en/posts/life/steam-guadao/","summary":"1 前言 挂刀是指从饰品交易平台购买游戏饰品，在steam市场出售以实现将人民币转换为steam阿根廷账号余额。 steam圣诞促销活动快结束了，买了几款游戏后发现阿根廷账号余额没多少了，挂刀过程又比较繁琐，故有此文记录一下挂刀搞余额的步骤。 2 步骤 2.1 网易buff账号注册及绑定 buff账","title":"steam挂刀教程"},{"content":"在/etc/prifile.d目录下新建一个文件，用户登录系统时自动生效\nvim /etc/profile.d/history_conf.sh source /etc/profile.d/history_conf.sh # 手动生效 文件内容\nexport HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 指定命令写入文件(默认~/.bash_history) export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups 不记录“重复”的命令；连续且相同 方为“重复” ； # ignorespace 不记录所有以空格开头的命令； # ignoreboth 表示ignoredups:ignorespace ,效果相当于以上两种的组合； # erasedups 删除重复命令； export PROMPT_COMMAND=\u0026#34;history -a\u0026#34; # 设置每条命令执行完立即写入HISTFILE(默认等待退出会话写入) export HISTTIMEFORMAT=\u0026#34;`whoami` %F %T \u0026#34; # 设置命令执行时间格式，记录文件增加时间戳 shopt -s histappend # 防止会话退出时覆盖其他会话写到HISTFILE的内容； 效果如下\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-history-format/","summary":"在/etc/prifile.d目录下新建一个文件，用户登录系统时自动生效 vim /etc/profile.d/history_conf.sh source /etc/profile.d/history_conf.sh # 手动生效 文件内容 export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 指定命令写入文件(默认~/.bash_history) export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；","title":"linux中history命令的格式化输出"},{"content":"流程图\n代码示例\n使用前需要登录harbor\n确保镜像的项目名在harbor中已存在\n格式三类型的镜像会推送到harbor的library项目中\n#!/bin/bash # author: Amadeus Liu # date: 2022-10-11 17:02:13 # version: 1.0 harbor_url=\u0026#34;local.harbor.com\u0026#34; log_file=\u0026#34;/var/log/push-harbor.log\u0026#34; image_id=$(docker images -q | sort -u) ls ${log_file} || touch ${log_file} echo \u0026#34;############# $(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34;) #############\u0026#34; \u0026gt;\u0026gt; ${log_file} get_image_tags () { docker inspect $1 --format=\u0026#39;{{.RepoTags}}\u0026#39; | sed \u0026#39;s/\\[//g\u0026#39; | sed \u0026#39;s/\\]//g\u0026#39; } image_tag_and_push () { docker tag $1 $2 \u0026amp;\u0026amp; echo \u0026#34;docker tag $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} docker push $2 \u0026amp;\u0026amp; echo \u0026#34;docker pull $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} } for i in ${image_id}; do # 判断镜像是否有harbor仓库的标签，有则视为harbor仓库中已有 if [[ $(get_image_tags $i) =~ ${harbor_url} ]]; then echo \u0026#34;已有${harbor_url}仓库标签-----$(get_image_tags $i)\u0026#34; else # 镜像的第一个完整标签 image_tag_first=$(echo $(get_image_tags $i) | awk -F\u0026#39; \u0026#39; \u0026#39;{print $1}\u0026#39;) # 镜像的第一个完整标签并去除版本 image_tag_first_delete_ver=$(echo ${image_tag_first} | awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) # 判断标签属于哪种格式 if [[ ${image_tag_first_delete_ver} =~ \u0026#34;/\u0026#34; ]]; then # 镜像的第一个完整标签的第一部分（\u0026#39;/\u0026#39;分割后的$1） image_tag_first_repo=$(echo ${image_tag_first_delete_ver}| awk -F\u0026#39;/\u0026#39; \u0026#39;{print $1}\u0026#39;) if [[ \u0026#34;${image_tag_first_repo}\u0026#34; =~ \u0026#34;.\u0026#34; ]]; then # 格式一 image_tag_harbor=\u0026#34;${harbor_url}/$(echo ${image_tag_first} | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $2}\u0026#39;)/$(echo ${image_tag_first} | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $3}\u0026#39;)\u0026#34; echo \u0026#34;${image_tag_first} \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;tag to\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${image_tag_harbor}\u0026#34; image_tag_and_push $i ${image_tag_harbor} else # 格式二 image_tag_harbor=\u0026#34;${harbor_url}/${image_tag_first}\u0026#34; echo \u0026#34;${image_tag_first} \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;tag to\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${image_tag_harbor}\u0026#34; image_tag_and_push $i ${image_tag_harbor} fi else # 格式三 image_tag_harbor=\u0026#34;${harbor_url}/library/${image_tag_first}\u0026#34; echo \u0026#34;${image_tag_first} \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;tag to\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${image_tag_harbor}\u0026#34; image_tag_and_push $i ${image_tag_harbor} fi fi done 腾讯云搬迁声明\n我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=3ielzwnut2qsg\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-push-harbor/","summary":"流程图 代码示例 使用前需要登录harbor 确保镜像的项目名在harbor中已存在 格式三类型的镜像会推送到harbor的library项目中 #!/bin/bash # author: Amadeus Liu # date: 2022-10-11 17:02:13 # version: 1.0 harbor_url=\u0026#34;local.harbor.com\u0026#34; log_file=\u0026#34;/var/log/push-harbor.log\u0026#34; image_id=$(docker images -q | sort -u) ls ${log_file} || touch ${log_file} echo \u0026#34;############# $(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34;) #############\u0026#34; \u0026gt;\u0026gt; ${log_file} get_image_tags () { docker inspect $1 --format=\u0026#39;{{.RepoTags}}\u0026#39; | sed \u0026#39;s/\\[//g\u0026#39; | sed \u0026#39;s/\\]//g\u0026#39; } image_tag_and_push () { docker tag $1 $2 \u0026amp;\u0026amp; echo \u0026#34;docker tag $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} docker push $2 \u0026amp;\u0026amp; echo \u0026#34;docker pull $1 $2\u0026#34; \u0026gt;\u0026gt;","title":"shell | 将本地镜像批量推送到harbor"},{"content":"之前本地做一些测试的时候多次修改过hosts文件，导致hosts文件出现了某些问题，按照网上很多方式自建hosts文件、修改编码格式、包括使用一些第三方工具修复都没有作用，记录一下成功修复hosts文件的步骤\n使用管理员权限打开命令提示符 执行如下代码 for /f %P in (\u0026#39;dir %windir%\\WinSxS\\hosts /b /s\u0026#39;) do copy %P %windir%\\System32\\drivers\\etc \u0026amp; echo %P \u0026amp; Notepad %P ","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-fix-hosts/","summary":"之前本地做一些测试的时候多次修改过hosts文件，导致hosts文件出现了某些问题，按照网上很多方式自建hosts文件、修改编码格式、包括使用一些第三方工具修复都没有作用，记录一下成功修复hosts文件的步骤 使用管理员权限打开命令提示符 执行如下代码 for /f %P in (\u0026#39;dir %windir%\\WinSxS\\hosts /b /s\u0026#39;) do copy %P %windir%\\System32\\drivers\\etc \u0026amp; echo %P \u0026amp;","title":"windows | hosts文件修复"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\nConfigMap 创建ConfigMap后，数据实际会存储在k8s中的Etcd中，然后通过创建pod时引用该数据。\n应用场景：应用程序配置\npod使用ConfigMap数据有两种方式：\n变量注入 数据卷挂载 可以通过读取目录或者文件快速创建 configmap\nkubectl create configmap \u0026lt;configmap-name\u0026gt; \\ --from-file=[key-name]=\u0026lt;path\u0026gt; \\ # key 不指定时使用文件名作为 key 文件内容作为 value，path 既可以文件也可以是目录 --from-env-file=\u0026lt;path\u0026gt; \\ # 文件内容应是 key=value 的形式，逐行读取 --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \\ # 通过指定的键值对创建 configmap yaml示例\napiVersion: v1 kind: ConfigMap metadata: name: configmap-demo data: abc: \u0026#34;123\u0026#34; cde: \u0026#34;456\u0026#34; redis.properties: | port: 6379 host: 1.1.1.4 password: 123456 --- apiVersion: v1 kind: Pod metadata: name: pod-configmap spec: containers: - name: demo image: nginx:1.19 env: - name: ABCD valueFrom: configMapKeyRef: name: configmap-demo key: abc - name: CDEF valueFrom: configMapKeyRef: name: configmap-demo key: cde volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: - name: config configMap: name: configmap-demo items: - key: \u0026#34;redis.properties\u0026#34; path: \u0026#34;redis.properties\u0026#34; # 挂载文件名 容器内验证\n[root@k8s-node1 ~]# kubectl exec -it pod-configmap -- bash root@pod-configmap:/# echo $ABCD 123 root@pod-configmap:/# echo $CDEF 456 root@pod-configmap:/# cat /config/redis.properties port: 6379 host: 1.1.1.4 password: 123456 Secret 与ConfigMap类似，区别在于Secret主要存储敏感数据，所有的数据都会经过base64编码。\nSecret支持三种数据类型：\ndocker-registry：存储镜像仓库认证信息 generic：从文件、目录或者字符串创建，例如存储用户名密码 tls：存储证书，例如HTTPS证书 示例\n将用户名和密码进行编码\n[root@k8s-node1 ~]# echo -n \u0026#39;admin\u0026#39; | base64 YWRtaW4= [root@k8s-node1 ~]# echo -n \u0026#39;123.com\u0026#39; | base64 MTIzLmNvbQ== secret.yaml\napiVersion: v1 kind: Secret metadata: name: db-pass type: Opaque data: username: YWRtaW4= password: MTIzLmNvbQ== pod-secret.yaml\napiVersion: v1 kind: Pod metadata: name: pod-secret-demo spec: containers: - name: demo image: nginx:1.19 env: - name: USER # 变量名 valueFrom: secretKeyRef: name: db-pass key: username - name: PASS # 变量名 valueFrom: secretKeyRef: name: db-pass key: password volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: - name: config secret: secretName: db-pass items: - key: password path: my-password # 挂载文件名 验证\n[root@k8s-node1 ~]# kubectl apply -f secret.yaml secret/db-pass created [root@k8s-node1 ~]# kubectl apply -f pod-secret.yaml pod/pod-secret-demo created [root@k8s-node1 ~]# kubectl exec -it pod-secret-demo -- bash root@pod-secret-demo:/# echo $USER admin root@pod-secret-demo:/# echo $PASS 123.com root@pod-secret-demo:/# cat /config/my-password 123.com ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-configmap-secret/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 ConfigMap 创建ConfigMap后，数据实际会存储在k8s中的Etcd中，然后通过创建pod时引用该数据。 应用场景：应用程序配置 pod使用ConfigMap数据有两种方式： 变量注入 数据卷挂载","title":"kubernetes | configmap \u0026 secret"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\nkubernetes安全框架 客户端要想访问 K8s 集群 API Server，一般需要 CA 证书、Token 或者用户名+密码 如果 Pod 访问，需要 ServiceAccount K8S 安全控制框架主要由下面3个阶段进行控制，每一个阶段都支持插件方式，通过 API Server 配置来启用插件。\nAuthentication（鉴权）\nAuthorization（授权）\nAdmission Control（准入控制）\n鉴权(Authentication) 三种客户端身份认证：\nHTTPS 证书认证：基于 CA 证书签名的数字证书认证 HTTP Token认证：通过一个 Token 来识别用户 HTTP Base认证：用户名+密码的方式认证 授权(Authorization) RBAC（Role-Based Access Control，基于角色的访问控制）：负责完成授权（Authorization）工作。\nRBAC根据API请求属性，决定允许还是拒绝。\n比较常见的授权维度：\nuser：用户名\ngroup：用户分组\n资源，例如pod、deployment\n资源操作方法：get，list，create，update，patch，watch，delete\n命名空间\nAPI组\n准入控制(Admission Control) Adminssion Control实际上是一个准入控制器插件列表，发送到API Server的请求都需要经过这个列表中的每个准入控制器插件的检查，检查不通过，则拒绝请求\nRBAC https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/\n基础概念 RBAC（Role-Based Access Control，基于角色的访问控制），允许通过Kubernetes API动态配置策略。\n角色\nRole：授权特定命名空间的访问权限\nClusterRole：授权所有命名空间的访问权限\n角色绑定\nRoleBinding：将角色绑定到主体（即subject）\nClusterRoleBinding：将集群角色绑定到主体\n主体（subject）\nUser：用户\nGroup：用户组\nServiceAccount：服务账号\n示例 新建用户并授权 为 Amadeus 用户授权 default 命名空间 Pod 读取权限\n新建用户 新建一个 k8s 用户大概可以分为以下几步：\n签发用户证书 生成用户的证书 key 通过用户的证书 key，生成用户的证书请求 (csr) 通过 k8s api 的 ca 证书去签发用户的证书请求，生成用户的证书 (crt) 生成 kubeconfig 配置文件 kubectl config set-cluster //集群配置 kubectl config set-credentials //用户配置 kubectl config set-context //context配置 kubectl config use-context //使用context 使用新创建的用户 kubectl \u0026ndash;kubecofig=path // 通过参数指定 KUBECONFIG=path kubectl // 通过环境变量指定，path 可以指定多个，用 : 连接，从而将多个配置文件合并在一起使用 签发用户证书 可以使用 openssl 或者 cfssl 进行签发，任选一种\n[root@k8s-node1 ~]# mkdir -p /etc/kubernetes/users/Amadeus [root@k8s-node1 ~]# cd /etc/kubernetes/users/Amadeus/ openssl # 创建用户证书 key [root@k8s-node1 Amadeus]# openssl genrsa -out Amadeus.key 2048 # 创建用户证书请求 (csr)，-subj 指定组和用户，其中 O 是组名，CN 是用户名 [root@k8s-node1 Amadeus]# openssl req -new -key Amadeus.key -out Amadeus.csr -subj \u0026#34;/O=hello/CN=Amadeus\u0026#34; # 生成用户的证书 (crt)，使用 k8s 的 ca 签发用户证书 [root@k8s-node1 Amadeus]# openssl x509 -req -in Amadeus.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out Amadeus.crt -days 3650 [root@k8s-node1 Amadeus]# ls Amadeus.crt Amadeus.csr Amadeus.key cfssl 下载cfssl工具\nwget --no-check-certificate https://github.com/cloudflare/cfssl/releases/download/1.2.0/cfssl_linux-amd64 wget --no-check-certificate https://github.com/cloudflare/cfssl/releases/download/1.2.0/cfssljson_linux-amd64 wget --no-check-certificate https://github.com/cloudflare/cfssl/releases/download/1.2.0/cfssl-certinfo_linux-amd64 chmod a+x cfssl* mv cfssl_linux-amd64 /usr/bin/cfssl mv cfssljson_linux-amd64 /usr/bin/cfssljson mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo [root@k8s-node1 ~]# cfssl version Version: 1.2.0 Revision: dev Runtime: go1.6 创建 ca-config.json 证书文件\ncat \u0026gt; ca-config.json \u0026lt;\u0026lt;EOF { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ], \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; } } } } EOF Amadeus-csr.json 证书文件\ncat \u0026gt; Amadeus-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;Amadeus\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } EOF 生成证书\n[root@k8s-node1 ~]# cfssl gencert -ca=/etc/kubernetes/pki/ca.crt -ca-key=/etc/kubernetes/pki/ca.key -config=ca-config.json -profile=kubernetes Amadeus-csr.json | cfssljson -bare Amadeus # 生成时会有警告，可以忽略，是因为提供的信息不是很全 [root@k8s-node1 ~]# ls Amadeus* # 生成如下三个文件 Amadeus.csr # csr Amadeus-key.pem # key Amadeus.pem # crt 配置 kubeconfig 配置文件 生成 kubeconfig 文件，并将 cluster 信息添加进去\nkubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=https://1.1.1.1:6443 \\ --kubeconfig=Amadeus.kubeconfig # --embed-certs=true 表示将证书写入etcd 为 kubeconfig 配置文件添加用户配置：设置用户证书(crt)和证书key\nkubectl config set-credentials Amadeus \\ --client-key=Amadeus-key.pem \\ --client-certificate=Amadeus.pem \\ --embed-certs=true \\ --kubeconfig=Amadeus.kubeconfig 为 kubeconfig 配置文件添加 context\nkubectl config set-context Amadeus@kubernetes \\ --cluster=kubernetes \\ --user=Amadeus \\ --kubeconfig=Amadeus.kubeconfig 为 kubeconfig 配置文件设置使用的context\nkubectl config use-context Amadeus@kubernetes --kubeconfig=Amadeus.kubeconfig 查看生成的配置文件\n# KUBECONFIG=./Amadeus.kubeconfig kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://1.1.1.1:6443 name: kubernetes contexts: - context: cluster: kubernetes user: Amadeus name: Amadeus@kubernetes current-context: Amadeus@kubernetes kind: Config preferences: {} users: - name: Amadeus user: client-certificate-data: REDACTED client-key-data: REDACTED 创建RBAC权限策略 使 Amadeus 用户有权限查看 default 命名空间下的 pod\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # api组，置空为核心组 resources: [\u0026#34;pods\u0026#34;] # 资源 verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] # 对资源的操作 --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: Amadeus # 绑定的用户名 apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io 测试验证 [root@k8s-node1 ~]# kubectl apply -f demo-rbac.yml role.rbac.authorization.k8s.io/pod-reader created rolebinding.rbac.authorization.k8s.io/read-pods created # pod可以正常查看 [root@k8s-node1 ~]# cp /etc/kubernetes/users/Amadeus/Amadeus.kubeconfig /root/ [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get pods -n default NAME READY STATUS RESTARTS AGE bar-664fbc5498-kz4sr 1/1 Running 0 18h bar-664fbc5498-r74vl 1/1 Running 0 18h bar-664fbc5498-smqxm 1/1 Running 0 18h ...... # 其他资源都没有权限 [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \u0026#34;Amadeus\u0026#34; cannot list resource \u0026#34;nodes\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get deployments Error from server (Forbidden): deployments.apps is forbidden: User \u0026#34;Amadeus\u0026#34; cannot list resource \u0026#34;deployments\u0026#34; in API group \u0026#34;apps\u0026#34; in the namespace \u0026#34;default\u0026#34; 给该用户增加查看、创建和删除deployment的权限，但pod的权限依旧只有查看\n[root@k8s-node1 ~]# vim demo-rbac.yml # 在rbac.yaml中增加如下规则 - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] [root@k8s-node1 ~]# kubectl apply -f demo-rbac.yml role.rbac.authorization.k8s.io/pod-reader configured rolebinding.rbac.authorization.k8s.io/read-pods unchanged # 操作 deployment [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl create deployment my-dep --image=nginx:1.22.1 --replicas=3 deployment.apps/my-dep created [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get pods -l app=my-dep NAME READY STATUS RESTARTS AGE my-dep-bc4cb798-4kbkq 1/1 Running 0 15s my-dep-bc4cb798-jdzq7 1/1 Running 0 15s my-dep-bc4cb798-lhm8p 1/1 Running 0 15s [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl delete deployment my-dep deployment.apps \u0026#34;my-dep\u0026#34; deleted 网络策略(Network Policy) https://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/\n基础概念 网络策略（Network Policy），用于限制Pod出入流量，提供Pod级别和Namespace级别网络访问控制。\n一些应用场景：\n应用程序间的访问控制。例如微服务A允许访问微服务B，微服务C不能访问微服务A\n开发环境命名空间不能访问测试环境命名空间Pod\n当Pod暴露到外部时，需要做Pod白名单\n多租户网络环境隔离\nPod网络入口方向隔离：\n基于Pod级网络隔离：只允许特定对象访问Pod（使用标签定义），允许白名单上的IP地址或者IP段访问Pod\n基于Namespace级网络隔离：多个命名空间，A和B命名空间Pod完全隔离。\nPod网络出口方向隔离：\n拒绝某个Namespace上所有Pod访问外部\n基于目的IP的网络隔离：只允许Pod访问白名单上的IP地址或者IP段\n基于目标端口的网络隔离：只允许Pod访问白名单上的端\n示例一 只允许default命名空间中携带run=client1标签的Pod访问default命名空间携带app=web标签的Pod的80端口，无法ping通\n[root@k8s-node1 ~]# kubectl create deployment web --image=nginx:1.22.1 [root@k8s-node1 ~]# kubectl run client1 --image=busybox:1.28 -- sleep 36000 [root@k8s-node1 ~]# kubectl run client2 --image=busybox:1.28 -- sleep 36000 [root@k8s-node1 ~]# kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS client1 1/1 Running 0 69s run=client1 client2 1/1 Running 0 62s run=client2 web-bc7cc9f65-5mg9d 1/1 Running 0 2m3s app=web,pod-template-hash=bc7cc9f65 networkpolicy.yaml示例\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: app: web policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: project: default - podSelector: matchLabels: run: client1 ports: - protocol: TCP port: 80 测试验证\n[root@k8s-node1 ~]# kubectl apply -f networkpolicy.yaml [root@k8s-node1 ~]# kubectl get networkpolicy [root@k8s-node1 ~]# kubectl get pods web-bc7cc9f65-hdhr2 -o jsonpath=\u0026#39;{.metadata.annotations.cni\\.projectcalico\\.org\\/podIP}\u0026#39; 10.244.169.169/32 [root@k8s-node1 ~]# kubectl exec -it client1 -- telnet 10.244.169.169 80 Connected to 10.244.169.169 [root@k8s-node1 ~]# kubectl exec -it client2 -- telnet 10.244.169.169 80 # 超时无法联通 [root@k8s-node1 ~]# kubectl delete -f networkpolicy.yaml 示例二 ns1命名空间下所有pod可以互相访问，也可以访问其他命名空间Pod，但其他命名空间不能访问ns1命名空间Pod。\n[root@k8s-node1 ~]# kubectl create ns ns1 [root@k8s-node1 ~]# kubectl run ns1-client1 --image=busybox -n ns1 -- sleep 36000 [root@k8s-node1 ~]# kubectl run ns1-client2 --image=busybox -n ns1 -- sleep 36000 [root@k8s-node1 ~]# kubectl get pods -n ns1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ns1-client1 1/1 Running 0 78s 10.244.169.168 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ns1-client2 1/1 Running 0 70s 10.244.107.212 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-node1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES client1 1/1 Running 0 51s 10.244.169.171 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; client2 1/1 Running 0 26m 10.244.107.238 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; networkpolicy.yaml\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-from-other-namespaces namespace: ns1 spec: podSelector: {} # 置空表示默认所有Pod policyTypes: - Ingress ingress: - from: - podSelector: {} # 置空表示拒绝所有 验证\n[root@k8s-node1 ~]# kubectl apply -f networkpolicy.yaml [root@k8s-node1 ~]# kubectl get networkpolicy -n ns1 # ns1命名空间内pod可以互通 [root@k8s-node1 ~]# kubectl exec -it ns1-client1 -n ns1 -- ping 10.244.107.212 # ns1-client2 PING 10.244.107.212 (10.244.107.212): 56 data bytes 64 bytes from 10.244.107.212: seq=0 ttl=62 time=0.900 ms 64 bytes from 10.244.107.212: seq=1 ttl=62 time=0.651 ms # default命名空间的pod无法访问ns1命名空间的pod [root@k8s-node1 ~]# kubectl exec -it client1 -- ping 10.244.107.212 # ns1-client2 # ns1命名空间的pod可以正常访问default命名空间的pod [root@k8s-node1 ~]# kubectl exec -it ns1-client1 -n ns1 -- ping 10.244.169.171 # client1 PING 10.244.169.171 (10.244.169.171): 56 data bytes 64 bytes from 10.244.169.171: seq=0 ttl=63 time=0.119 ms 64 bytes from 10.244.169.171: seq=1 ttl=63 time=0.067 ms [root@k8s-node1 ~]# kubectl delete -f networkpolicy.yaml ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-rbac-networkpolicy/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 kubernetes安全框架 客户端要想访问 K8s 集群 API Server，一般需要 CA 证书、Token 或者用户名+密码 如果 Pod 访问，需要 ServiceAccount K8S 安全控制框架主要由下面3个阶段进行控制，每一个阶段都支持","title":"kubernetes | RBAC鉴权和NetworkPolicy"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\nservice 基本概念 service存在的意义\n服务发现：防止Pod失联\n负载均衡：定义一组Pod的访问策略\nservice通过label-selector关联pod\nservice的三种类型\nClusterIP：集群内部使用\n默认**，**分配一个稳定的IP地址，即VIP，只能在集群内部访问（同Namespace内的Pod）\nNodePort：对外暴露应用\n在每个节点上启用一个端口(30000-32767)来暴露服务，可以在集群外部访问。也会分配一个稳定内部集群IP地址。\nLoadBalancer：对外暴露应用，适用公有云\n与NodePort类似，在每个节点上启用一个端口来暴露服务。除此之外，Kubernetes会请求底层云平台上的负载均衡器，将每个Node（[NodeIP]:[NodePort]）作为后端添加进去。\n示例\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: selector: app: nginx type: NodePort ports: - protocol: TCP port: 80 # service端口，内部访问端口 targetPort: 80 # 后端业务镜像实际暴露的端口 nodePort: 30002 # 内部访问端口映射到节点端口 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.22.1 ports: - containerPort: 80 代理模式 Iptables：\n灵活，功能强大\n规则遍历匹配和更新，呈线性时延\nIPVS：\n工作在内核态，有更好的性能\n调度算法丰富：rr，wrr，lc，wlc，ip hash\u0026hellip;\niptables模式 使用 iptables 模式时，根据 iptables 的 --mode random --probability 来匹配每一条请求，每个 pod 收到的流量趋近于平衡，不是完全的轮询\n这种模式，kube-proxy 会监听 Kubernetes 对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会安装 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 任意一组 backend pod 中。对于每个 Endpoints 对象，它也会安装iptables规则，这个规则会选择一个 backend pod 组合。\nk8s默认采用的代理模式是iptables，可以通过查看kube-proxy组件的日志可得\n[root@k8s-node1 ~]# kubectl logs kube-proxy-8mf2l -n kube-system | grep Using I0412 02:02:29.634610 1 server_others.go:212] Using iptables Proxier. 创建一个上述示例中的 yaml ，查看 iptables 规则\n[root@k8s-node1 ~]# kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx NodePort 10.105.220.154 \u0026lt;none\u0026gt; 80:30002/TCP 4m40s # SVC当前共关联三个POD [root@k8s-node1 ~]# kubectl get pod -o wide -l app=nginx NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-55f4d8c85-l29wx 1/1 Running 0 4m57s 10.244.169.133 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-55f4d8c85-lf5dj 1/1 Running 0 4m57s 10.244.107.205 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-55f4d8c85-q4gsx 1/1 Running 0 4m57s 10.244.107.203 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-node1 ~]# iptables-save |grep -i nodeport |grep 30002 # NODEPORTS 根据端口将流量转发到 SVC 链 -A KUBE-NODEPORTS -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp --dport 30002 -j KUBE-SVC-2CMXP7HKUVJN7L6M [root@k8s-node1 ~]# iptables-save |grep KUBE-SVC-2CMXP7HKUVJN7L6M # ClusterIP 相关 -A KUBE-SERVICES -d 10.109.98.33/32 -p tcp -m comment --comment \u0026#34;default/nginx cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-2CMXP7HKUVJN7L6M # 转发到具体 POD 链，每条 POD 链都有一样的概率获取到流量 -A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment \u0026#34;default/nginx\u0026#34; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-ONLOYCYPTBL5FQH5 -A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment \u0026#34;default/nginx\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JABTJNSPARJARZOW -A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-SEP-TZBGLRHUI2CFM5CU # POD链中定义了转发到具体的POD地址， [root@k8s-node1 ~]# iptables-save |grep KUBE-SEP-ONLOYCYPTBL5FQH5 -A KUBE-SEP-ONLOYCYPTBL5FQH5 -s 10.244.107.252/32 -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-ONLOYCYPTBL5FQH5 -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp -j DNAT --to-destination 10.244.107.252:80 [root@k8s-node1 ~]# iptables-save |grep KUBE-SEP-JABTJNSPARJARZOW -A KUBE-SEP-JABTJNSPARJARZOW -s 10.244.169.135/32 -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-JABTJNSPARJARZOW -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp -j DNAT --to-destination 10.244.169.135:80 [root@k8s-node1 ~]# iptables-save |grep KUBE-SEP-TZBGLRHUI2CFM5CU -A KUBE-SEP-TZBGLRHUI2CFM5CU -s 10.244.169.136/32 -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-TZBGLRHUI2CFM5CU -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp -j DNAT --to-destination 10.244.169.136:80 ipvs模式 ipvsadm安装配置(所有节点都要配置)\n[root@k8s-node1 ~]# yum install ipvsadm [root@k8s-node1 ~]# cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt; EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF [root@k8s-node1 ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules [root@k8s-node1 ~]# source /etc/sysconfig/modules/ipvs.modules 修改 service 使用的代理模式为 ipvs\n[root@k8s-node1 ~]# kubectl edit configmap kube-proxy -n kube-system mode: \u0026#34;ipvs\u0026#34; ipvs: scheduler: \u0026#34;rr\u0026#34; #rr, wrr, lc, wlc, ip hash等 # 删除所有 kube-proxy，k8s 会重新创建 [root@k8s-node1 ~]# kubectl delete pod -n kube-system -l k8s-app=kube-proxy [root@k8s-node1 ~]# kubectl logs kube-proxy-8z86w -n kube-system | grep Using I0412 08:30:21.169231 1 server_others.go:274] Using ipvs Proxier. 查看 ipvs 规则\n[root@k8s-node1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 172.17.0.1:30002 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 TCP 1.1.1.1:30002 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 TCP 10.244.36.64:30002 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 TCP 10.109.98.33:80 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 Headless Service Headless Service 相比普通 Service 只是将 spec.clusterIP 定义为 None\nHeadless Service 几大特点：\n不分配 clusterIP\n没有负载均衡的功能(kube-proxy 不会安装 iptables 规则)\n可以通过解析 service 的 DNS，返回所有 Pod 的 IP 和 DNS (statefulSet部署的Pod才有DNS)\n[root@k8s-node1 ~]# kubectl run -it --rm --restart=Never --image busybox:1.28 dns-test -- nslookup statefulset-nginx.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: statefulset-nginx.default.svc.cluster.local Address 1: 10.244.107.200 statefulset-nginx-1.statefulset-nginx.default.svc.cluster.local Address 2: 10.244.169.188 statefulset-nginx-0.statefulset-nginx.default.svc.cluster.local pod \u0026#34;dns-test\u0026#34; deleted Headless Services 应用场景\n自主选择权，client 可以通过查询DNS来获取 Real Server 的信息，自己来决定使用哪个Real Server\nHeadless Service 的对应的每一个 Endpoints，即每一个Pod，都会有对应的DNS域名，这样Pod之间就可以互相访问\nDNS解析名称：\npod：\u0026lt;pod-name\u0026gt;.\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local\nservice: \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local\nIngress 基本概念 NodePort的不足\n一个端口只能一个服务使用，端口需提前规划 只支持4层负载均衡 Ingress是什么？\nIngress 公开了从集群外部到集群内服务的HTTP和HTTPS路由。流量路由由Ingress资源上定义的规则控制。\n下面是一个将所有流量都发送到同一Service的简单Ingress示例：\nIngress Controller\nIngress管理的负载均衡器，为集群提供全局的负载均衡能力。\nIngress Contronler怎么工作的？\nIngress Contronler通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段 Nginx 配置，应用到管理的Nginx服务，然后热加载生效。\n以此来达到Nginx负载均衡器配置及动态更新的问题\n使用流程：\n部署Ingress Controller\n创建Ingress规则\nIngress Contorller主流控制器：\ningress-nginx-controller: nginx 官方维护的控制器\nTraefik： HTTP反向代理、负载均衡工具\nIstio：服务治理，控制入口流量\n这里使用 Nginx 官方维护的，Github：https://github.com/kubernetes/ingress-nginx\n安装部署 下载 yaml 文件\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/baremetal/1.22/deploy.yaml --no-check-certificate 修改\n# 修改kind, 将原先的Deployment修改为DaemontSet，实现所有物理节点访问 kind: DaemonSet spec: template: spec: # 新增 hostNetwork, 将ingress-nginx-controller的端口直接暴露在宿主机上 hostNetwork: true containers: # 修改 image 为国内地址 image: registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.3.0 # 新增污点容忍，允许在 master 节点创建pod tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; --- kind: Job spec: template: spec: containers: # 修改 image 为国内地址 image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 --- kind: Job spec: template: spec: containers: # 修改 image 为国内地址 image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 部署\n[root@k8s-node1 ~]# kubectl apply -f deploy.yaml [root@k8s-node1 opt]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-admission-create--1-zfwrz 0/1 Completed 0 12m 10.244.169.135 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-admission-patch--1-8rhjr 0/1 Completed 0 12m 10.244.169.134 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-bb2kd 1/1 Running 0 12m 1.1.1.3 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-bp588 1/1 Running 0 12m 1.1.1.2 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-z2782 1/1 Running 0 12m 1.1.1.1 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 如果出现内部访问报错：failed calling webhook \u0026#34;validate.nginx.ingress.kubernetes.io\u0026#34; [root@k8s-node1 ~]# kubectl get ValidatingWebhookConfiguration [root@k8s-node1 ~]# kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission 测试 测试 url 跳转，创建三套 nginx 应用 : test | foo | bar\n需要注意的是，代理路径假如是 /foo 的话，后端真实路径也是 /foo\ntest 应用示例，foo 和 bar 的自行修改\napiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: test spec: replicas: 3 selector: matchLabels: app: test template: metadata: labels: app: test spec: terminationGracePeriodSeconds: 5 containers: - name: test image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www persistentVolumeClaim: claimName: pvc-test --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-test spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 2Gi --- apiVersion: v1 kind: Service metadata: name: test labels: app: test spec: selector: app: test ports: - protocol: TCP port: 80 targetPort: 80 创建 ingress\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: demo-nginx annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: test.com http: paths: - path: / pathType: Prefix backend: service: name: test port: number: 80 - path: /foo pathType: Prefix backend: service: name: foo port: number: 80 - path: /bar pathType: Prefix backend: service: name: bar port: number: 80 查看创建的 ingress\n[root@k8s-node1 ~]# kubectl describe ingress demo-nginx Name: demo-nginx Namespace: default Address: 1.1.1.1,1.1.1.2,1.1.1.3 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- test.com / test:80 (10.244.107.209:80,10.244.107.212:80,10.244.169.140:80) /foo foo:80 (10.244.107.210:80,10.244.169.138:80,10.244.169.144:80) /bar bar:80 (10.244.107.211:80,10.244.169.131:80,10.244.169.143:80) Annotations: kubernetes.io/ingress.class: nginx Events: \u0026lt;none\u0026gt; 修改 index.html\n[root@k8s-node1 ~]# echo \u0026#34;test\u0026#34; \u0026gt; /nfs/default-pvc-test-pvc-93a7df14-90f2-4466-8655-6ef42549b760/index.html [root@k8s-node1 ~]# mkdir /nfs/default-pvc-foo-pvc-75e73500-1a70-4305-8253-d1e7d8c88b49/foo [root@k8s-node1 ~]# echo \u0026#34;foo\u0026#34; \u0026gt; /nfs/default-pvc-foo-pvc-75e73500-1a70-4305-8253-d1e7d8c88b49/foo/index.html [root@k8s-node1 ~]# mkdir /nfs/default-pvc-bar-pvc-73d12b15-7c53-46ee-a1b6-d0cb2c25e7e6/bar/ [root@k8s-node1 ~]# echo \u0026#34;bar\u0026#34; \u0026gt; /nfs/default-pvc-bar-pvc-73d12b15-7c53-46ee-a1b6-d0cb2c25e7e6/bar/index.html 访问测试\n[root@k8s-node1 ~]# curl http://1.1.1.1/ -H \u0026#34;Host: test.com\u0026#34; test [root@k8s-node1 ~]# curl http://1.1.1.2/foo/ -H \u0026#34;Host: test.com\u0026#34; foo [root@k8s-node1 ~]# curl http://1.1.1.3/bar/ -H \u0026#34;Host: test.com\u0026#34; bar ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-service-ingress/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 service 基本概念 service存在的意义 服务发现：防止Pod失联 负载均衡：定义一组Pod的访问策略 service通过label-selector关联pod service的三种类型 Clust","title":"kubernetes | service \u0026 ingress"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n为什么需要数据卷\n启动时需要的初始数据，录入配置文件\n启动过程中产生的临时数据，该临时数据需要多个容器间共享\n启动过程中产生的持久化数据，例如mysql的data\n数据卷概述\nkubernetes中的volume提供了在容器中挂载外部存储的能力 Pod需要设置卷来源（spec.volume）和挂载点（spec.containers.volumeMounts）两个信息后才可以使用相应的Volume 常用的数据卷：\n本地（hostPath，emptyDir）\n网络（NFS，Ceph，GlusterFS）\n公有云（AWS EBS）\nK8S资源（configmap，secret）\nemptyDir（临时存储卷） emptyDir卷：是一个临时存储卷，与Pod生命周期绑定一起，如果Pod删除了卷也会被删除。\n应用场景：Pod中容器之间数据共享\nemptyDir的实际存储路径在pod所在节点的/var/lib/kubelet/pods/\u0026lt;pod-id\u0026gt;/volumes/kubernetes.io~empty-dir目录下\n查看pod的uid\nkubectl get pod \u0026lt;pod-name\u0026gt; -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39; 示例如下\napiVersion: v1 kind: Pod metadata: name: demo-emptydir spec: terminationGracePeriodSeconds: 5 containers: - name: write image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;for i in $(seq 100); do echo $i \u0026gt;\u0026gt; /data/hello; sleep 1; done\u0026#34;] volumeMounts: - name: data mountPath: /data - name: read image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;tail -f /data/hello\u0026#34;] volumeMounts: - name: data mountPath: /data volumes: - name: data emptyDir: {} 查看日志\n[root@k8s-node1 ~]# kubectl logs -f demo-emptydir -c read 1 2 3 ... hostPath（节点存储卷） hostPath卷：挂载Node文件系统（Pod所在节点）上文件或者目录到Pod中的容器。\n应用场景：Pod中容器需要访问宿主机文件\n示例yaml\napiVersion: v1 kind: Pod metadata: name: pod-hostpath spec: terminationGracePeriodSeconds: 5 containers: - name: busybox image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 36000\u0026#34;] volumeMounts: - name: data mountPath: /data volumes: - name: data hostPath: path: /tmp type: Directory NFS（网络存储卷） NFS卷提供对NFS挂载支持，可以自动将NFS共享路径挂载到Pod中\n配置nfs服务端，\nyum install nfs-utils # nfs-utils包每个节点都需安装 mkdir -p /nfs echo \u0026#34;/nfs 1.1.1.0/24(rw,async,no_root_squash)\u0026#34; \u0026gt;\u0026gt; /etc/exports # 格式：NFS共享的目录 客户端地址1(参数1,参数2,...) 客户端地址2(参数1,参数2,...)) systemctl enable --now nfs systemctl enable --now rpcbind 常用选项： ro：客户端挂载后，其权限为只读，默认选项； rw:读写权限； sync：同时将数据写入到内存与硬盘中； async：异步，优先将数据保存到内存，然后再写入硬盘； Secure：要求请求源的端口小于1024 用户映射： root_squash:当NFS客户端使用root用户访问时，映射到NFS服务器的匿名用户； no_root_squash:当NFS客户端使用root用户访问时，映射到NFS服务器的root用户； all_squash:全部用户都映射为服务器端的匿名用户； anonuid=UID：将客户端登录用户映射为此处指定的用户uid； anongid=GID：将客户端登录用户映射为此处指定的用户gid 客户端测试\n[root@k8s-node2 ~]# mount -t nfs k8s-node1:/nfs /mnt/ [root@k8s-node2 ~]# df -hT | grep k8s-node1 k8s-node1:/nfs nfs4 44G 4.0G 41G 9% /mnt 示例yaml\napiVersion: apps/v1 kind: Deployment metadata: name: demo-nfs labels: app: demo-nfs spec: replicas: 3 selector: matchLabels: app: demo-nfs template: metadata: labels: app: demo-nfs spec: terminationGracePeriodSeconds: 5 containers: - name: demo-nfs image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www nfs: server: k8s-node1 path: /nfs/ --- apiVersion: v1 kind: Service metadata: name: demo-nfs labels: app: demo-nfs spec: selector: app: demo-nfs ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30003 type: NodePort 验证\n[root@k8s-node1 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-nfs NodePort 10.97.209.119 \u0026lt;none\u0026gt; 80:30003/TCP 5m41s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5d2h [root@k8s-node1 ~]# echo \u0026#34;hello, nfs\u0026#34; \u0026gt; /nfs/index.html [root@k8s-node1 ~]# curl 10.97.209.119 hello, nfs pv和pvc（持久存储卷） 基础概念 PersistentVolume（PV）：存储资源创建和使用抽象化，使得存储作为集群中的资源管理\nPersistentVolumeClaim（PVC）：让用户不需要关心具体的Volume实现细节\npvc如何匹配到pv\n存储空间的请求\n匹配最接近的pv，如果没有满足条件的pv，则pod处于pending状态\n访问模式的设置\n存储空间字段能否限制实际可用容量\n不能，存储空间字段只用于匹配到pv，具体可用容量取决于网络存储 pv生命周期 AccessModes（访问模式）：\nAccessModes 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\nReadWriteOnce（RWO）：可被一个 node 读写挂载\nReadOnlyMany（ROX）：可被多个 node 只读挂载\nReadWriteMany（RWX）：可被多个 node 读写挂载\nRECLAIM POLICY（回收策略）：\n目前 PV 支持的策略有三种：\nRetain（保留）： 保留数据，需要管理员手工清理数据\nRecycle（回收）：清除 PV 中的数据，效果相当于执行 rm -rf /ifs/kuberneres/*\nDelete（删除）：与 PV 相连的后端存储同时删除\nSTATUS（状态）：\n一个 PV 的生命周期中，可能会处于4中不同的阶段：\nAvailable（可用）：表示可用状态，还未被任何 PVC 绑定\nBound（已绑定）：表示 PV 已经被 PVC 绑定\nReleased（已释放）：PVC 被删除，但是资源还未被集群重新声明\nFailed（失败）： 表示该 PV 的自动回收失败\npv示例\napiVersion: v1 kind: PersistentVolume metadata: name: demo-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: server: k8s-node1 path: /nfs pvc示例\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: demo-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi deployment \u0026amp; service 示例\napiVersion: apps/v1 kind: Deployment metadata: name: demo-pvc labels: app: demo-pvc spec: replicas: 3 selector: matchLabels: app: demo-pvc template: metadata: labels: app: demo-pvc spec: terminationGracePeriodSeconds: 5 containers: - name: demo-pvc image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www-pvc mountPath: /usr/share/nginx/html/ volumes: - name: www-pvc persistentVolumeClaim: claimName: demo-pvc --- apiVersion: v1 kind: Service metadata: name: demo-pvc labels: app: demo-pvc spec: selector: app: demo-pvc ports: - protocol: TCP port: 80 targetPort: 80 验证\n[root@k8s-node1 ~]# echo \u0026#34;pvc for NFS is successful\u0026#34; \u0026gt; /nfs/index.html [root@k8s-node1 ~]# kubectl get svc -l app=demo-pvc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-pvc ClusterIP 10.97.28.93 \u0026lt;none\u0026gt; 80/TCP 102s [root@k8s-node1 ~]# curl 10.97.28.93 pvc for NFS is successful pv动态供给 之前的PV使用方式称为静态供给，需要K8s运维工程师提前创建一堆PV，供开发者使用\n因此，K8s开始支持PV动态供给，使用StorageClass对象实现。\n查看k8s原生支持的共享存储：https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner\n基于NFS实现自动创建pv插件\n自动创建的pv挂载路径为\u0026lt;nfs-path\u0026gt;/\u0026lt;namespace\u0026gt;-\u0026lt;pvc-name\u0026gt;-\u0026lt;pv-name\u0026gt;\npvc-name：默认情况下为yaml中自定义的pvc-name，使用statefulset控制器时pvc的名字为\u0026lt;volumeClaimTemplates-name\u0026gt;-\u0026lt;pod-name\u0026gt; pv-name：pv的名字为pvc-\u0026lt;pvc-uid\u0026gt; k8s-1.20版本后默认禁止使用selfLink，需要打开一下\n修改k8s的apiserver参数，改完 apiserver 会自动重启\n[root@k8s-node1 ~]# vi /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 ··· - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --feature-gates=RemoveSelfLink=false # 添加这个配置 部署NFS插件 此组件是对 nfs-client-provisioner 的扩展，nfs-client-provisioner 已经不提供更新，且 nfs-client-provisioner 的 Github 仓库已经迁移到 NFS-Subdir-External-Provisioner 的仓库\nrbac 创建 nfs-rbac.yml\napiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumeclaims\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;] resources: [\u0026#34;storageclasses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: kube-system roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: kube-system roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io nfs-subdir-external-provisioner 创建 nfs-provisioner-deploy.yml\napiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate # 设置升级策略为删除再创建(默认为滚动更新) selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner #image: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.0 image: registry.cn-hangzhou.aliyuncs.com/lvbibir/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME # Provisioner的名称,以后设置的storageclass要和这个保持一致 value: nfs-client - name: NFS_SERVER # NFS服务器地址,需和valumes参数中配置的保持一致 value: 1.1.1.1 - name: NFS_PATH # NFS服务器数据存储目录,需和valumes参数中配置的保持一致 value: /nfs/kubernetes volumes: - name: nfs-client-root nfs: server: 1.1.1.1 # NFS服务器地址 path: /nfs/kubernetes # NFS服务器数据存储目录 storageClass 创建 nfs-sc.yml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; ## 是否设置为默认的storageclass provisioner: nfs-client ## 动态卷分配者名称，必须和deployment的PROVISIONER_NAME变量中设置的Name一致 parameters: archiveOnDelete: \u0026#34;false\u0026#34; ## 设置为\u0026#34;false\u0026#34;时删除PVC不会保留数据,\u0026#34;true\u0026#34;则保留数据 mountOptions: - hard ## 指定为硬挂载方式 - nfsvers=4 ## 指定NFS版本,这个需要根据NFS Server版本号设置 创建上述资源\n[root@k8s-node1 nfs]# mkdir /nfs/kubernetes/ [root@k8s-node1 nfs]# kubectl apply -f . deployment.apps/nfs-client-provisioner created serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created storageclass.storage.k8s.io/nfs created 示例 apiVersion: apps/v1 kind: Deployment metadata: name: demo-auto-pv labels: app: demo-auto-pv spec: replicas: 3 selector: matchLabels: app: demo-auto-pv template: metadata: labels: app: demo-auto-pv spec: terminationGracePeriodSeconds: 5 containers: - name: demo-auto-pv image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www persistentVolumeClaim: claimName: pvc-auto --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-auto spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 测试验证\n[root@k8s-node1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE pod/demo-auto-pv-7c974689b4-4cwr6 1/1 Running 0 47s pod/demo-auto-pv-7c974689b4-bb9v8 1/1 Running 0 47s pod/demo-auto-pv-7c974689b4-p525n 1/1 Running 0 47s pod/nfs-client-provisioner-66d6cb77fd-47hsf 1/1 Running 0 4m15s [root@k8s-node1 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc-auto Bound pvc-22b65e10-ab97-47eb-aaa1-6c354a749a55 2Gi RWO managed-nfs-storage 47s [root@k8s-node1 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-22b65e10-ab97-47eb-aaa1-6c354a749a55 2Gi RWO Delete Bound default/pvc-auto managed-nfs-storage 47s [root@k8s-node1 ~]# ls -l /nfs/ total 4 drwxrwxrwx. 2 root root 6 Apr 11 17:37 default-pvc-auto-pvc-22b65e10-ab97-47eb-aaa1-6c354a749a55 ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-storage/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 为什么需要数据卷 启动时需要的初始数据，录入配置文件 启动过程中产生的临时数据，该临时数据需要多个容器间共享 启动过程中产生的持久化数据，例如mysql的data 数据卷概述 kubernet","title":"kubernetes | 存储"},{"content":"0. 前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1. 滚动升级 滚动升级的实现机制\n两个replicaset控制器分别控制旧版本的pod和新版本pod，replicaset2启动一个新版版本pod，相应的replicaset1停止一个旧版本pod，从而实现滚动升级。在这过程中，无法保证业务流量完全不丢失。\n升级\nkubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [options] # 示例 kubectl set image deployment/demo-rollout nginx=nginx:1.15 --record=true # --record=true 表示将升级的命令记录到升级记录中 回滚\n# 上次升级状态 kubectl rollout status deployment demo-rollout # 升级记录 kubectl rollout history deployment demo-rollout # 回滚至上个版本 kubectl rollout undo deployment demo-rollout # 回滚至指定版本 kubectl rollout undo deployment demo-rollout --to-revision=2 1.1 升级 在所有work节点先创建几个busybox镜像的tag用于升级演示\n[root@k8s-node3 ~]# for i in {1..3}; do docker tag busybox:latest busybox:v${i}; done [root@k8s-node3 ~]# docker images | grep busybox busybox latest 7cfbbec8963d 3 weeks ago 4.86MB busybox v1 7cfbbec8963d 3 weeks ago 4.86MB busybox v2 7cfbbec8963d 3 weeks ago 4.86MB busybox v3 7cfbbec8963d 3 weeks ago 4.86MB 创建v1版本的deployment\napiVersion: apps/v1 kind: Deployment metadata: name: demo-rollout labels: app: demo-rollout spec: replicas: 3 selector: matchLabels: app: demo-rollout template: metadata: labels: app: demo-rollout spec: containers: - name: busybox image: busybox:v1 command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;sleep 36000\u0026#39;] 也可以使用命令创建\n[root@k8s-node1 ~]# kubectl create deployment demo-rollout --image=busybox:v1 --replicas=3 -- sleep 3600 deployment.apps/demo-rollout created [root@k8s-node1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE demo-rollout-5d847fd86c-678pr 1/1 Running 0 4s demo-rollout-5d847fd86c-9mj4v 1/1 Running 0 4s demo-rollout-5d847fd86c-xhvf7 1/1 Running 0 4s 升级至v2和v3\n# 升级 [root@k8s-node1 ~]# kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true [root@k8s-node1 ~]# kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true # 查看升级状态 [root@k8s-node1 ~]# kubectl rollout status deployment demo-rollout deployment \u0026#34;demo-rollout\u0026#34; successfully rolled out [root@k8s-node1 ~]# kubectl rollout history deployment demo-rollout deployment.apps/demo-rollout REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true 3 kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true # 查看实际镜像版本 [root@k8s-node1 ~]# kubectl get deployment demo-rollout -o jsonpath=\u0026#39;{.spec.template.spec.containers}\u0026#39; [root@k8s-node1 ~]# kubectl describe deployment demo-rollout | grep -i image: Image: busybox:v3 1.2 回滚 回滚至v1版本\n[root@k8s-node1 ~]# kubectl rollout undo deployment/demo-rollout --to-revision=1 deployment.apps/demo-rollout rolled back [root@k8s-node1 ~]# kubectl describe deployment demo-rollout | grep -i image: Image: busybox:v1 [root@k8s-node1 ~]# kubectl rollout history deployment demo-rollout deployment.apps/demo-rollout REVISION CHANGE-CAUSE 2 kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true 3 kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true 4 \u0026lt;none\u0026gt; 可以看到 rollout history 删除了第一次的记录, 重新记录到第四条\n恢复到v2版本\n[root@k8s-node1 ~]# kubectl rollout undo deployment/demo-rollout --to-revision=2 deployment.apps/demo-rollout rolled back [root@k8s-node1 ~]# kubectl describe deployment demo-rollout | grep -i image: Image: busybox:v2 [root@k8s-node1 ~]# kubectl rollout history deployment demo-rollout deployment.apps/demo-rollout REVISION CHANGE-CAUSE 3 kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true 4 \u0026lt;none\u0026gt; 5 kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true 2. 自动伸缩 手动扩容 kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME) [options] # 示例 kubectl scale deployment demo-rollout --replicas=10 自动扩容 实现自动扩容需满足两个条件：\n运行了metric-server\npod设置了request资源\nHorizontal Pod Autoscaling: pod水平扩容，k8s中的一个api资源，使用autoscale时会创建一个hpa资源\nHPA基本原理:\n查询指定的资源中所有Pod的资源平均使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能. HPA自动伸缩副本时会使POD的资源使用率趋近于预设的target值 比如只有一个POD时, 资源使用率达到了 180%/70%, HPA会将POD数量扩容到3个, 此时资源使用率将会是 60%/70%. 当pod资源使用率回到正常水平, controller-manager会默认等待5分钟的时间再缩容pod,以免再次出现突发流量. kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [options] # 基于cpu指标进行扩容 kubectl autoscale deployment demo-rollout --min=3 --max=10 --cpu-percent=10 # 查看hpa kubectl get hpa # replicaset控制器记录了pod的详细伸缩记录 kubectl get rs kubectl describe rs demo-rollout-54fdcc5676 2.1 基于CPU 创建deployment资源\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-demo spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 resources: requests: memory: 50Mi cpu: 50m --- apiVersion: v1 kind: Service metadata: name: hpa-demo labels: app: nginx spec: selector: app: nginx type: NodePort ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30002 创建hpa资源\ncpu使用率 = 已使用 / request\n--cpu-percent=60代表所有pod的平均cpu使用率达到百分之60时触发扩容\n[root@k8s-node1 ~]# kubectl autoscale deployment hpa-demo --cpu-percent=60 --min=1 --max=10 horizontalpodautoscaler.autoscaling/hpa-demo autoscaled [root@k8s-node1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo Deployment/hpa-demo 0%/60% 1 10 1 18s 压测\n[root@k8s-node1 ~]# yum install -y httpd-tools [root@k8s-node1 ~]# ab -n 1000000 -c 200 http://1.1.1.1:30002/ hpa自动扩容, pod数量增加到了10个\n[root@k8s-node1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo Deployment/hpa-demo 160%/10% 1 10 10 50m [root@k8s-node1 ~]# kubectl describe hpa hpa-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetScale 17m (x8 over 19m) horizontal-pod-autoscaler deployments/scale.apps \u0026#34;hpa-demo\u0026#34; not found Normal SuccessfulRescale 11m horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 11m horizontal-pod-autoscaler New size: 8; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 10m horizontal-pod-autoscaler New size: 10; reason: [root@k8s-node1 ~]# kubectl describe deployment hpa-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 16m deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 1 Normal ScalingReplicaSet 10m deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 4 Normal ScalingReplicaSet 9m53s deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 8 Normal ScalingReplicaSet 9m38s deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 10 压测结束后也并不会立即减少pod数量，会等一段时间后减少pod数量，防止流量再次激增。默认时间大概是5分钟左右\n2.2 基于内存 使用busybox容器测试, 另挂载一个configMap用于内存压力测试, 由于用到了mount命令, 还需要将container声明为特权模式.\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-mem spec: selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 36000\u0026#34;] volumeMounts: - name: increase-mem-script mountPath: /opt/ resources: requests: memory: 50Mi cpu: 50m securityContext: privileged: true volumes: - name: increase-mem-script configMap: name: increase-mem-config --- apiVersion: v1 kind: ConfigMap metadata: name: increase-mem-config data: increase-mem.sh: | #!/bin/sh mkdir /tmp/memory mount -t tmpfs -o size=40M tmpfs /tmp/memory dd if=/dev/zero of=/tmp/memory/block sleep 60 rm /tmp/memory/block umount /tmp/memory rmdir /tmp/memory 获取hpa的模板yaml文件\n[root@k8s-node1 ~]# kubectl autoscale deployment hpa-mem --min=1 --max=10 --dry-run=client -o yaml \u0026gt; hpa-mem-hpa.yml [root@k8s-node1 ~]# vim hpa-mem-hpa.yml 使用yaml创建hpa, 默认使用的是autoscaling/v1版本的api, 它不支持基于内存的自动扩容, 需要修改为 autoscaling/v2beta1\napiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: hpa-mem spec: maxReplicas: 10 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-mem metrics: - type: Resource resource: name: memory targetAverageUtilization: 60 执行脚本进行压测, 随着脚本执行, hpa自动将副本数扩容到了两个\n[root@k8s-node1 ~]# kubectl exec -it hpa-mem-c6c7d4957-fpsfb -- /bin/sh /opt/increase-mem.sh [root@k8s-node1 ~]# kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-mem Deployment/hpa-mem 0%/60% 1 10 1 2m55s hpa-mem Deployment/hpa-mem 80%/60% 1 10 1 4m1s hpa-mem Deployment/hpa-mem 80%/60% 1 10 2 4m16s 脚本执行60s后会使内存使用率自动恢复正常, 副本数过段时间也会自动恢复\n2.3 基于自定义指标 待续\u0026hellip;..\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-auto-scale/","summary":"0. 前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1. 滚动升级 滚动升级的实现机制 两个replicaset控制器分别控制旧版本的pod和新版本pod，replicaset2启动一个新版版本pod，相应的replicaset1停止一个旧","title":"kubernetes | 滚动升级和自动伸缩"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\ncontrollers作用：\n管理pod对象 使用标签与pod关联 负责滚动更新、伸缩、副本管理、维持pod状态等 daemonset ingress statefulset replicaset ReplicaSet：副本集\n协助Deployment做事\nPod副本数量管理，不断对比当前Pod数量与期望Pod数量\nDeployment每次发布都会创建一个RS作为记录，用于实现回滚\ndeployment deployment用于网站、API、微服务等，功能特性：\n管理pod和replicaset 具有上线部署、副本设定、滚动升级、回滚等功能 提供声明式更新，例如只更新一个新的image 示例\napiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19 ports: - containerPort: 80 ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-controllers/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 controllers作用： 管理pod对象 使用标签与pod关联 负责滚动更新、伸缩、副本管理、维持pod状态等 daemonset ingress statefulset replicaset ReplicaSet：副本集 协助Deployment做事 Pod副本","title":"kubernetes | 控制器"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\nkubelet logs命令的流程\nkubectl logs --请求--\u0026gt; apiserver --请求--\u0026gt; kubelet --读取--\u0026gt; container日志 k8s日志包含两大类：\nk8s系统的组件日志\nk8s集群中部署的应用程序的日志\n标准输出\n日志文件\n组件日志 journalctl -u kubelet kubectl logs kube-proxy -n kube-system /var/log/messages pod 日志 标准输出 实时查看pod标准输出日志\nkubectl logs [options] \u0026lt;podname\u0026gt; kubectl logs -f \u0026lt;podname\u0026gt; kubectl logs -f \u0026lt;podname\u0026gt; -c \u0026lt;containername\u0026gt; kubectl logs --previous \u0026lt;podname\u0026gt; # 查看pod上次重启的日志 k8s 会将每个 pod 中每个 container 的日志记录到 pod 所在 node 的 /var/log/pods 目录中, 日志文件其实是 docker 保存的日志文件的一个软连接.\nk8s 会为每个 pod 的每个 container 日志保留 2 份, 一份为 container 当前状态的日志, 另一份是 container 上一次生命周期的日志, 日志保留数量应该是由 k8s 的 gc 机制管控.\n# k8s 日志 /var/log/pods/\u0026lt;pod namespace\u0026gt;_\u0026lt;pod name\u0026gt;_\u0026lt;pod uid\u0026gt;/\u0026lt;容器名称\u0026gt;/容器重启次数.log # docker日志 /var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log 示例\n# 可以看到 calico-kube-controllers 这个 pod 重启了 7 次 [root@k8s-node1 ~]# kubectl get pods -n kube-system -l k8s-app=calico-kube-controllers NAME READY STATUS RESTARTS AGE calico-kube-controllers-6c76574f75-69flf 1/1 Running 7 (41h ago) 4d1h # k8s 分别保存了编号 6 和 7 两个日志文件 [root@k8s-node3 ~]# ls -l /var/log/pods/kube-system_calico-kube-controllers-6c76574f75-69flf_066e1042-97a4-4547-8d2d-6580cbad40c5/calico-kube-controllers/ lrwxrwxrwx. 1 root root 165 Apr 20 14:31 6.log -\u0026gt; /var/lib/docker/containers/8ed4865daa5d984d9b7e3412f61251ce1a5e12e295fce1f14ee341c3f79b1afe/8ed4865daa5d984d9b7e3412f61251ce1a5e12e295fce1f14ee341c3f79b1afe-json.log lrwxrwxrwx. 1 root root 165 Apr 23 10:23 7.log -\u0026gt; /var/lib/docker/containers/c30d353ee464efd853968dcd1524933aa214294303e5a7cd7828b0e86f0e94ec/c30d353ee464efd853968dcd1524933aa214294303e5a7cd7828b0e86f0e94ec-json.log # 7 号日志文件是当前生命周期的日志 [root@k8s-node1 ~]# kubectl logs --tail=1 calico-kube-controllers-6c76574f75-69flf -n kube-system 2023-04-23 03:05:54.256 [INFO][1] resources.go 350: Main client watcher loop [root@k8s-node3 calico-kube-controllers]# tail -n 1 7.log | python -m json.tool { \u0026#34;log\u0026#34;: \u0026#34;2023-04-23 03:05:54.256 [INFO][1] resources.go 350: Main client watcher loop\\n\u0026#34;, \u0026#34;stream\u0026#34;: \u0026#34;stderr\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2023-04-23T03:05:54.257447874Z\u0026#34; } # 6 号日志文件是上一次生命周期的日志 [root@k8s-node1 ~]# kubectl logs --tail=1 --previous calico-kube-controllers-6c76574f75-69flf -n kube-system 2023-04-21 08:57:20.637 [INFO][1] resources.go 350: Main client watcher loop [root@k8s-node3 calico-kube-controllers]# tail -n 1 6.log | python -m json.tool { \u0026#34;log\u0026#34;: \u0026#34;2023-04-21 08:57:20.637 [INFO][1] resources.go 350: Main client watcher loop\\n\u0026#34;, \u0026#34;stream\u0026#34;: \u0026#34;stderr\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2023-04-21T08:57:20.637780663Z\u0026#34; } 日志文件 比如nginx应用的日志一般保存在accesss.log和error.log日志中，这些日志是不会输出到标准输出的，可以采用如下两种方式进行采集\nemptyDir数据卷 创建pod时挂载emptyDIr类型的数据卷，用以持久化自定义的日志文件\n需要先找到pod分配的节点\nKubectl get pods -o wide 再查看pod的id\ndocker ps | grep pod-name # 或者 kubectl get pod \u0026lt;podname\u0026gt; -n \u0026lt;namespace\u0026gt; -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39; pod日志文件路径\n/var/lib/kubelet/pods/\u0026lt;pod-id\u0026gt;/volumes/kubernetes.io~empty-dir 示例\napiVersion: v1 kind: Pod metadata: name: pod-logs-emptydir spec: containers: - name: web image: nginx volumeMounts: - name: logs mountPath: /var/log/nginx/ volumes: - name: logs emptyDir: {} sidecar边车容器 通过创建边车容器实现将应用原本的日志文件输出到标准输出\n示例：\napiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: web image: nginx:1.22.1 volumeMounts: - name: logs mountPath: /var/log/nginx/ - name: accesslog image: busybox:1.28 command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;tail -f /opt/access.log\u0026#39;] volumeMounts: - name: logs mountPath: /opt volumes: - name: logs emptyDir: {} ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-logs/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 kubelet logs命令的流程 kubectl logs --请求--\u0026gt; apiserver --请求--\u0026gt; kubelet --读取--\u0026gt; container日志 k8s日志包含两大类： k8s系统的组件日志 k8s集群中部署的应用程序的日","title":"kubernetes | 日志"},{"content":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n创建pod的工作流程 kubectl run nginx \u0026ndash;image=nginx kubectl将创建pod的请求发送到apiserver apiserver将请求信息写入etcd apiserver通知scheduler，收到请求信息后根据调度算法将pod分配到合适节点 scheduler给pod标记调度结果，并返回给apiserver apiserver收到后写入etcd 对应节点的kubelet收到创建pod的事件，从apiserver获取到pod的相关信息 kubelet调用docker api创建pod所需的容器 创建完成之后将pod状态汇报给apiserver apiserver将收到的pod状态写入apiserver kubectl get pods即可收到相关信息 资源限制对pod调度的影响 容器资源限制：\nresources.limits.cpu\nresources.limits.memory\n容器使用的最小资源需求，并不是实际占用，是预留资源：\nresources.requests.cpu\nresources.requests.memory\napiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: web image: nginx resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; #cpu单位也可以写浮点数，例如0.25=250m，代表四分之一核cpu limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; nodeSelector nodeSelector用于将Pod调度到匹配Label的Node上，如果没有匹配的标签会调度失败。\n先创建pod后打标签，起始出于pending状态，打好标签后，pod会正常分配\n给节点打标签：\nkubectl label nodes [node] key=value # 打lable, value可以是空 kubectl label nodes [node] key- # 删除label kubectl get nodes -l key=value # 根据label筛选 # 示例 kubectl label nodes k8s-node1 disktype=ssd kubectl label nodes k8s-node1 disktype- 示例\napiVersion: v1 kind: Pod metadata: name: pod-nodeselector spec: containers: - name: nginx image: nginx:1.19 nodeSelector: disktype: \u0026#34;ssd\u0026#34; nodeAffinity 节点亲和性概念上类似于 nodeSelector， 它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。 节点亲和性有两种：\nrequiredDuringSchedulingIgnoredDuringExecution： 调度器只有在规则被满足的时候才能执行调度。此功能类似于 nodeSelector， 但其语法表达能力更强。 preferredDuringSchedulingIgnoredDuringExecution： 调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。 先创建pod后打标签起始出于pending状态，打好标签后，pod会正常分配\nIgnoredDuringExecution 意味着如果节点标签在 Kubernetes 调度 Pod 后发生了变更，Pod 仍将继续运行。\n操作符：In、NotIn、Exists、DoesNotExist、Gt、Lt\n示例\napiVersion: v1 kind: Pod metadata: name: with-affinity-anti-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - windows preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: label-1 operator: In values: - key-1 - weight: 50 preference: matchExpressions: - key: label-2 operator: In values: - key-2 containers: - name: with-node-affinity image: registry.k8s.io/pause:2.0 Taint(污点) Taints：避免Pod调度到特定Node上\n应用场景：\n专用节点，例如配备了特殊硬件的节点\n基于Taint的驱逐\n设置污点：\nkubectl taint node [node] key=value:[effect] # 其中[effect]可取值： # - NoSchedule ：一定不能被调度。 # - PreferNoSchedule：尽量不要调度。 # - NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。 去掉污点：\nkubectl taint node [node] key:[effect]- 示例\n[root@k8s-node1 ~]# kubectl label node k8s-node2 disktype=ssd node/k8s-node2 labeled [root@k8s-node1 ~]# kubectl taint node k8s-node2 disktype=ssd:NoSchedule node/k8s-node2 tainted [root@k8s-node1 ~]# kubectl describe node k8s-node2 | grep -i taints Taints: disktype=ssd:NoSchedule Tolerations（污点容忍）\n允许Pod调度到持有Taints的Node上，但不是绝对分配到指定的标签，搭配nodeSelector或者nodeAffinity使用，实现将pod分配到特定污点的节点上\ntolerations: #设置容忍所有污点，防止节点被设置污点 - operator: \u0026#34;Exists\u0026#34; 示例\n[root@k8s-node1 ~]# kubectl describe node k8s-node2 | grep -i taints Taint Taints: disktype=ssd:NoSchedule [root@k8s-node1 ~]# kubectl apply -f pod-tolerations.yaml [root@k8s-node1 ~]# kubectl get pods pod-tolerations -o wide pod-tolerations 1/1 Running 0 13s 10.244.169.183 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yaml\napiVersion: v1 kind: Pod metadata: name: pod-tolerations spec: containers: - name: nginx image: nginx:1.19 nodeSelector: disktype: \u0026#34;ssd\u0026#34; tolerations: - key: \u0026#34;disktype\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;ssd\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeName 指定节点名称，用于将Pod调度到指定的Node上，不经过调度器scheduler，所以无视污点\n示例\n[root@k8s-node1 ~]# kubectl describe node k8s-node2| grep Taint Taints: disktype=ssd:NoSchedule [root@k8s-node1 ~]# kubectl apply -f pod-nodename.yaml pod/pod-nodename created [root@k8s-node1 ~]# kubectl get pod pod-nodename -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-nodename 1/1 Running 0 27s 10.244.169.184 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yaml\napiVersion: v1 kind: Pod metadata: name: pod-nodename spec: containers: - name: nginx image: nginx nodeName: k8s-node2 DaemonSet控制器 DaemonSet功能：\n在每一个Node上运行一个Pod\n新加入的Node也同样会自动运行一个Pod\n应用场景：网络插件、监控Agent、日志Agent，比如k8s的calico-node和kube-proxy组件\n示例\n[root@k8s-node1 ~]# kubectl apply -f daemonset-filebeat.yaml [root@k8s-node1 ~]# kubectl get pods -n kube-system -o wide |grep filebeat filebeat-2c6p4 1/1 Running 0 90s 10.244.107.246 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; filebeat-4ffcx 1/1 Running 0 90s 10.244.36.65 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; filebeat-h7959 1/1 Running 0 90s 10.244.169.186 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yaml\napiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: kube-system spec: selector: matchLabels: name: filebeat template: metadata: labels: name: filebeat spec: containers: - name: log image: elastic/filebeat:7.3.2 volumeMounts: - mountPath: /log/ name: log volumes: - name: log hostPath: path: /var/lib/docker/containers/ type: Directory tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-scheduler/","summary":"前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 创建pod的工作流程 kubectl run nginx \u0026ndash;image=nginx kubectl将创建pod的请求发送到apiserver apiserver将请求信息写入etcd apiserver通知scheduler，收到请求信息后根","title":"kubernetes | 调度"},{"content":"0. 前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1. 简介 基本概念\n最小部署单元\n一组容器的集合\n一个Pod中的容器共享网络命名空间\nPod是短暂的\n存在意义\nPod为亲密性应用而存在。\n亲密性应用场景：\n两个应用之间发生文件交互\n两个应用需要通过127.0.0.1或者socket通信（典型组合：nginx+php）\n两个应用需要发生频繁的调用\n2. pod中的容器分类 Infrastructure Container：基础容器，维护整个Pod网络空间\nInitContainers：初始化容器，先于业务容器开始执行\nContainers：业务容器，并行启动\nInfrastructure Container\npod中总会多一个pause容器，这个容器就是实现将pod中的所有容器的网络命名空间进行统一，a容器在localhost或者127.0.0.1的某个端口提供了服务，b容器访问localhost或者127.0.0.1加端口也可以访问到\npause容器主要为每个业务容器提供以下功能：\nPID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。\n网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围。\nIPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信。\nUTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）。\nInit container：\n基本支持所有普通容器特征\n优先普通容器执行\n应用场景：\n控制普通容器启动，初始容器完成后才会启动业务容器\n初始化配置，例如下载应用配置文件、注册信息等\n示例\napiVersion: v1 kind: Pod metadata: name: pod-init spec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox command: [\u0026#34;wget\u0026#34;, \u0026#34;-O\u0026#34;, \u0026#34;/work-dir/index.html\u0026#34;, \u0026#34;http://www.baidu.com/index.html\u0026#34;] volumeMounts: - name: workdir mountPath: \u0026#34;/work-dir\u0026#34; volumes: - name: workdir emptyDir: {} 3. 静态pod 静态Pod特点：\nPod由特定节点上的kubelet管理\n不能使用控制器\nPod名称标识当前节点名称\n在kubelet配置文件启用静态Pod：\nvi /var/lib/kubelet/config.yaml ... staticPodPath: /etc/kubernetes/manifests ... 将部署的pod yaml放到该目录会由kubelet自动创建\n4. 重启策略 Pod 的 spec 中包含一个 restartPolicy 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。\nrestartPolicy 适用于 Pod 中的所有容器。\nAlways：当容器终止退出后，总是重启容器，默认策略。\nOnFailure：当容器异常退出（退出状态码非0）时，才重启容器。\nNever：当容器终止退出，从不重启容器。\n5. 健康检查 5.1 三种探针 kubernetes包含以下三种探针\nlivenessProbe(存活探针): 如果检查失败, 根据Pod的restartPolicy来决定是否重启container. readinessProbe(就绪探针): 如果检查失败, 会把Pod暂时从service endpoints中剔除. startupProbe(启动探针): 如果检查失败, 根据Pod的restartPolicy来决定是否重启container. 用于启动非常慢的应用. 需要注意的是, 如果容器未配置以上三种探针, 则视为三种探针皆为成功, liveness和readiness探针的initialDelaySeconds配置代表startup探针成功后等待多少秒再去初始化 liveness和readiness 探针.\n5.1.1 检查方法 支持以下四种检查方法：\nhttpGet：对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 exec：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 tcpSocket：对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。 gRPC：使用 gRPC 执行一个远程过程调用。需要应用程序支持，参考 5.1.2 检查结果 Success(成功)\nFailure(失败)\nUnknown(未知): 不会执行任何操作.\n5.1.3 探针配置 initialDelaySeconds: 容器启动后(startup探针成功)要等待多少秒后存活和就绪探测器才被初始化, 默认是0秒, 最小值是0. periodSeconds: 执行探测的时间间隔.默认是10秒, 最小值是1. timeoutSeconds: 探测的超时后等待多少秒. 默认值是1秒. 最小值是1. successThreshold: 探测器在失败后, 被视为成功的最小连续成功数. 默认值是1. 存活探测的这个值必须是1。最小值是1. failureThreshold: 当Pod启动了并且探测到失败的重试次数. 存活探测情况下的放弃就意味着重新启动容器. 就绪探测情况下的放弃Pod会被打上未就绪的标签, 默认值是3, 最小值是1. 5.2 示例 5.2.1 liveness linveness实际触发重启需要的时间 = 失败次数 * 间隔时间 + 等待容器优雅退出的宽限期(默认30s，docker默认是10s)\nfailureThreshold * periodSeconds + terminationGracePeriodSeconds\nlivenessProbe示例\napiVersion: v1 kind: Pod metadata: name: pod-livenessprobe namespace: default spec: restartPolicy: OnFailure terminationGracePeriodSeconds: 10 containers: - name: liveness image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;touch /tmp/healthy; sleep 10; rm -rf /tmp/healthy; sleep 600\u0026#34;] livenessProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/tmp/healthy\u0026#34;] initialDelaySeconds: 5 periodSeconds: 5 failureThreshold: 2 运行结果可以看到在两分钟的时间里重启了4次，每次30s\n[root@k8s-node1 opt]# kubectl get pods NAME READY STATUS RESTARTS AGE liveness-pod 1/1 Running 4 (2s ago) 2m2s POD运行的前10s检查一直成功 在POD启动的第15s第一次检查失败 第20s第二次检查失败，给容器发送停止信号 等待10s后强制重启容器 5.2.2 liveness-with-startup 示例:\napiVersion: v1 kind: Pod metadata: name: pod-probes namespace: default spec: terminationGracePeriodSeconds: 10 containers: - name: liveness-with-startup image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 5; touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\u0026#34;] startupProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/tmp/healthy\u0026#34;] periodSeconds: 5 failureThreshold: 10 livenessProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/tmp/healthy\u0026#34;] initialDelaySeconds: 15 periodSeconds: 5 failureThreshold: 3 重启过程:\nPOD启动成功,触发startup探针 第10秒startup探针成功 第25秒后初始化liveness探针 第40秒liveness探针第一次失败 第50秒liveness探针第三次失败, 触发重启, 等待容器优雅退出 第60秒强制重启container 6. lifecycle 6.1 postStart 和 preStop 如下示例\napiVersion: v1 kind: Pod metadata: name: lifecycle-demo-pod namespace: default labels: test: lifecycle spec: containers: - name: lifecycle-demo image: nginx:1.22.1 imagePullPolicy: IfNotPresent lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello from the postStart handler\u0026#39; \u0026gt;\u0026gt; /var/log/nginx/message\u0026#34;] preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello from the preStop handler\u0026#39; \u0026gt;\u0026gt; /var/log/nginx/message\u0026#34;] volumeMounts: - name: message-log mountPath: /var/log/nginx/ readOnly: false # 读写挂载方式，默认为读写模式false initContainers: - name: init-myservice image: busybox:1.28 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello initContainers\u0026#39; \u0026gt;\u0026gt; /var/log/nginx/message\u0026#34;] volumeMounts: - name: message-log mountPath: /var/log/nginx/ readOnly: false # 读写挂载方式，默认为读写模式false volumes: - name: message-log hostPath: path: /data/volumes/nginx/log/ type: DirectoryOrCreate # 表示如果宿主机没有此目录则会自动创建 效果如下\n[root@k8s-node1 ~]# kubectl delete pod lifecycle-demo-pod [root@k8s-node2 log]# cat message Hello initContainers Hello from the postStart handler Hello from the preStop handler ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-pod/","summary":"0. 前言 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1. 简介 基本概念 最小部署单元 一组容器的集合 一个Pod中的容器共享网络命名空间 Pod是短暂的 存在意义 Pod为亲密性应用而存在。 亲密性应用场景： 两个应用之间发生文件交互 两个应用需要通过1","title":"kubernetes | pod"},{"content":"kubectl命令的自动补全 yum install bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) 镜像拉取策略 imagePullPolicy: Always|Never|IfNotPresent 修改nodePort范围 vim /etc/kubernetes/manifests/kube-apiserver.yaml # spec.containers.command 增加 - --service-node-port-range=1-65535 command 和 args containers.command 等同于 Dockerfile 中的 ENTRYPOINT\ncontainers.args 等同于 Dockerfile 中的 CMD\n如果 Dockerfile 中默认的 ENTRYPOINT 被覆盖，则默认的 CMD 指令同时也会被覆盖\nlabel标签选择运算符 https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels\n基于等值 有三种运算符 = == !=\n前两种是等效的，示例\n[root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname=k8s-node1 NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname!=k8s-node1 NAME STATUS ROLES AGE VERSION k8s-node2 Ready \u0026lt;none\u0026gt; 21d v1.22.3 k8s-node3 Ready \u0026lt;none\u0026gt; 21d v1.22.3 基于集合 同样三种运算符 in notin exists\nexists只用于判断 key 是否存在\nhello in (foo, bar) # 所有包含了 hello 标签且值等于 foo 或者 bar 的资源 hello notin (foo, bar) # 所有包含了 hello 标签且值不等于 foo 或者 bar 的资源；以及没有 hello 标签的资源 hello # 所有包含了 hello 标签的资源；不校验值 !hello # 所有未包含 hello 标签的资源；不校验值 示例\n[root@k8s-node1 ~]# kubectl get nodes -l \u0026#34;kubernetes.io/hostname in (k8s-node1, k8s-node2)\u0026#34; NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d v1.22.3 k8s-node2 Ready \u0026lt;none\u0026gt; 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l \u0026#34;kubernetes.io/hostname notin (k8s-node1, k8s-node2)\u0026#34; NAME STATUS ROLES AGE VERSION k8s-node3 Ready \u0026lt;none\u0026gt; 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d v1.22.3 k8s-node2 Ready \u0026lt;none\u0026gt; 21d v1.22.3 k8s-node3 Ready \u0026lt;none\u0026gt; 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l \\!kubernetes.io/hostname No resources found 常见报错 NodeNotReady Image garbage collection failed once 参考地址\n报错：\n# kubectl describe node k8s-node01 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 11m kubelet Starting kubelet. Normal NodeHasSufficientMemory 11m kubelet Node k8s-node01 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 11m kubelet Node k8s-node01 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 11m kubelet Node k8s-node01 status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 11m kubelet Updated Node Allocatable limit across pods # journalctl -u kubelet | grep garbage Mar 06 09:50:33 k8s-node01 kubelet[45471]: E0306 09:50:33.106476 45471 kubelet.go:1343] \u0026#34;Image garbage collection failed once. Stats initialization may not have completed yet\u0026#34; err=\u0026#34;failed to get imageFs info: unable to find data in memory cache\u0026#34; 解决：\n未部署CNI组件\ndocker镜像或容器未能正确删除导致的\ndocker system prune systemctl stop kubelet systemctl stop docker systemctl start docker systemctl start kubelet node无法ping通pod 所有calico的pod运行都是running状态, 使用calicoctl node status看到的网卡绑定也是没问题的.\ncalico的pod有如下报错\n[root@k8s-node1 ~]# kubectl logs calico-node-l66pn -n kube-system 2023-04-08 04:28:47.660 [INFO][65] felix/int_dataplane.go 1600: Received interface update msg=\u0026amp;intdataplane.ifaceUpdate{Name:\u0026#34;tunl0\u0026#34;, State:\u0026#34;down\u0026#34;, Index:4} bird: Netlink: Network is down bird: Netlink: Network is down bird: Netlink: Network is down bird: Netlink: Network is down 我这里是通过关闭NetworkManager解决的.关闭后pod日志立即就恢复正常了\n[root@k8s-node1 ~]# systemctl stop NetworkManager [root@k8s-node1 ~]# systemctl disable NetworkManager 虚拟机挂起导致calico网络不可用 出现在我的虚拟机测试机群上，挂起虚拟机过段时间后重新启动虚拟机，发现集群状态是正常的(node 是 ready 状态)，然而 calico-kube-controllers metric-server nfs-provisiner 等功能组件陷入了 CrashLoopBackOff 状态，报错基本上都是无法连接到 api-server\n但是 calicoctl 看到 calico 集群是没什么问题的，之前遇到几次都是暴躁重启 docker 解决的，后面发现重启 calico 相关容器就可以了，具体原因还没找到，估计与 vmware 虚拟机挂起操作有关。\nkubectl delete pods -n kube-system -l \u0026#34;k8s-app in (calico-node, calico-kube-controllers)\u0026#34; kubectl命令 一些常用命令\n# 查看某个资源的详细信息 kubectl describe \u0026lt;type\u0026gt; \u0026lt;name\u0026gt; -n \u0026lt;namespace\u0026gt; # 查看pod的日志 kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; # 查看当前支持的api版本 kubectl api-versions get options:\n-w/--watch: # 实时更新，类似tail的-f选项 -o wide: # 查看更为详细的信息，比如ip和分配的节点 -o json: # 以json格式输出 -o jsonpath=\u0026#39;{}\u0026#39; # 输出指定的json内容 -l key=vaule # 根据lable筛选 --show-lables # 显示资源的所有label 示例:\n# 查看所有支持的资源 kubectl api-resources # 查看service映射的pod的端口和ip kubectl get cp/endpoints # 查看pod kubectl get pod \u0026lt;podname\u0026gt; -n \u0026lt;namespace\u0026gt; -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39; # 查看pod的id # 查看指定pod的事件 kubectl get events --field-selector involvedObject.name=demo-probes create kubectl create \u0026lt;resource\u0026gt; [Options] --dry-run=client: 仅尝试运行，不实际运行 -o, --output=\u0026#39;\u0026#39;: 输出为指定的格式 快速创建一系列资源\n[root@k8s-node1 ~]# kubectl create namespace test namespace/test created [root@k8s-node1 ~]# kubectl create deployment my-dep --image=nginx:1.22. --replicas=3 -n test deployment.apps/my-dep created [root@k8s-node1 ~]# kubectl expose deployment my-dep --port=80 --target-port=8080 --type=NodePort -n test service/my-dep exposed [root@k8s-node1 ~]# kubectl get pods,deployment,svc -n test NAME READY STATUS RESTARTS AGE pod/my-dep-5f8dfc8c78-7w5nz 1/1 Running 0 41s pod/my-dep-5f8dfc8c78-gt65r 1/1 Running 0 41s pod/my-dep-5f8dfc8c78-n4vjd 1/1 Running 0 41s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/my-dep 3/3 3 3 41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-dep NodePort 10.110.205.138 \u0026lt;none\u0026gt; 80:31890/TCP 17s expose kubectl expose deployment my-dep --port=80 --target-port=8080 --type=NodePort -n test # --port 表示service暴露的端口 # --target-port 表示后端镜像实际提供服务的端口 label kubectl label nodes [node] key=value # 打lable, value可以是空 kubectl label nodes [node] key- # 删除label kubectl get nodes -l key=value # 根据label筛选 kubectl get nodes --show-labesl # 显示资源的所有标签 run kubectl run -it test --image busybox --rm -- ping 10.244.107.207 calicoctl 下载地址\n# 查看集群信息 DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes # 使用配置文件的方式 [root@k8s-node1 ~]# mkdir /etc/calico [root@k8s-node1 ~]# cat \u0026gt; /etc/calico/calicoctl.cfg \u0026lt;\u0026lt;EOF \u0026gt; apiVersion: projectcalico.org/v3 \u0026gt; kind: CalicoAPIConfig \u0026gt; metadata: \u0026gt; spec: \u0026gt; datastoreType: \u0026#34;kubernetes\u0026#34; \u0026gt; kubeconfig: \u0026#34;/root/.kube/config\u0026#34; \u0026gt; EOF # 查看集群信息 [root@k8s-node1 ~]# calicoctl --allow-version-mismatch get nodes NAME k8s-node1 k8s-node2 k8s-node3 [root@k8s-node1 ~]# calicoctl --allow-version-mismatch node status IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 1.1.1.2 | node-to-node mesh | up | 03:53:45 | Established | | 1.1.1.3 | node-to-node mesh | up | 03:53:51 | Established | +--------------+-------------------+-------+----------+-------------+ namespace k8s与docker的namespace不同\ndocker中的namespace用于容器间的资源隔离\nk8s中的namespace用于\nk8s的抽象资源间的资源隔离，比如pods、控制器、service等\n资源隔离后，对这一组资源进行权限控制\nyaml编写 通过创建资源获取yaml\nkubectl create deployment web --image=nginx:1.19 --dry-run=client -o yaml \u0026gt; deploy.yaml 通过已有资源获取yaml\nkubectl get deployment nginx-deployment -o yaml \u0026gt; deploy2.yaml 查看api中的资源及解释\nkubectl explain pods.spec.container kubectl explain deployment yaml报错排查\nerror: error parsing pod-configmap.yaml: error converting YAML to JSON: yaml: line 19: did not find expected \u0026#39;-\u0026#39; indicator 解决\n由于yaml文件列表对齐不统一导致的\nyaml文件格式要对齐，同一级别的对象要放在同一列，几个空格不重要，不要用tab制表符\n# 格式1 ports: - port: 80 # 格式2 ports: - port: 80 ","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-notes/","summary":"kubectl命令的自动补全 yum install bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) 镜像拉取策略 imagePullPolicy: Always|Never|IfNotPresent 修改nodePort范围 vim /etc/kubernetes/manifests/kube-apiserver.yaml # spec.containers.command 增加 - --service-node-port-range=1-65535 command 和 args containers.command 等同于 Dockerfile 中的 ENTRYPOINT containers.args 等同于 Dockerfile 中的 CMD 如果 Dockerfile 中默认的 ENTRYPOINT 被覆盖，则默认的 CMD 指令同时也会被覆盖 label标签选择运算符 https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels 基于等值 有三种运算符 = == != 前两种是等效的，示例 [root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname=k8s-node1 NAME","title":"kubernetes | 杂记"},{"content":"Bash有一个内置的set命令，可以用来查看、设置、取消shell选项\nset设置的选项无法被继承，仅对当前的bash环境有效，bash命令也可以直接使用set的单字符选项来开启一个自定义参数的子bash环境，比如执行的脚本\n查看： echo $- 和 set -o 和 echo ${SHELLOPTS} 设置： set -abefhkmnptuvxBCHP 和 set -o options-name 取消： set +abefhkmnptuvxBCHP 和 set +o options-name set -和set +设置单字符选项，使用echo $-查看当前shell开启的单字符选项\nset -o 和set +o 设置多字符选项，使用set -o查看当前shell所有的多字符选项的状态(开启或关闭)\n使用echo ${SHELLOPTS}查看当前shell开启的长格式选项\n所有的短格式选项都可以找到对应的长格式选项，长格式选项多了emacs、history、ignoreeof、nolog、pipefail、posix、vi。详见set命令的man手册\n例如 set -B 和set -o braceexpand 是等效的，注意这里的设置和取消有点反常识：设置用 -，关闭反而是用 +\n[root@lvbibir ~]# echo $- himBH # set + 方式去除B选项，相应的 set -o 中的 braceexpand 选项也关闭了 [root@lvbibir ~]# set +B [root@lvbibir ~]# echo $- himH [root@lvbibir ~]# set -o | grep braceexpand braceexpand off # set -o 开启 braceexpand 选项，相应的 echo $- 中的 B 选项也开启了 [root@lvbibir ~]# set -o braceexpand [root@lvbibir ~]# echo $- himBH [root@lvbibir ~]# set -o | grep braceexpand braceexpand on ","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-command-set/","summary":"Bash有一个内置的set命令，可以用来查看、设置、取消shell选项 set设置的选项无法被继承，仅对当前的bash环境有效，bash命令也可以直接使用set的单字符选项来开启一个自定义参数的子bash环境，比如执行的脚本 查看： echo $- 和 set -o 和 echo ${SHELLOPTS} 设置： set -abefhkmnptuvxBCHP 和 set -o options-name 取消： set +abefhkmnptuvxBCHP 和 set +o options-name","title":"linux | set命令详解"},{"content":"前言 安装过程中会替换相当一部分系统内置的软件包，不建议用于生产环境\ncephadm依赖python3.6，而此版本的openeuler内置版本为3.7，且不支持platform-python\n参考：openeuler的gitee社区issue\nceph：v16.2（pacific）\n操作系统：openEuler-20.03-LTS-SP3\n内核版本：4.19.90-2112.8.0.0131.oe1.x86_64\n集群角色：\nip 主机名 角色 1.1.1.101 ceph-node1 cephadm,mgr,mon,osd 1.1.1.102 ceph-node2 osd,mgr,mon 1.1.1.103 ceph-node3 osd,mgr,mon 基础环境配置(所有节点) 防火墙 systemctl stop firewalld systemctl disable firewalld 修改主机名 hostnamectl set-hostname ceph-node1 hostnamectl set-hostname ceph-node2 hostnamectl set-hostname ceph-node3 vi /etc/hosts # 添加 1.1.1.101 ceph-node1 1.1.1.102 ceph-node2 1.1.1.103 ceph-node3 配置 yum\u0026amp;epel 源 rpm -e openEuler-release-20.03LTS_SP3-52.oe1.x86_64 wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum install epel-release rm -f /etc/yum.repos.d/CentOS-Linux-* yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装 python3.6 yum install python3-pip-wheel python3-setuptools-wheel wget http://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/x86_64/os/Packages/python3-libs-3.6.8-41.el8.x86_64.rpm wget http://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/x86_64/os/Packages/libffi-3.1-22.el8.x86_64.rpm rpm -ivh libffi-3.1-22.el8.x86_64.rpm --force cp /usr/lib64/libpython3.so /usr/lib64/libpython3.so-3.7.4 rpm -ivh python3-libs-3.6.8-41.el8.x86_64.rpm --force --nodeps mv /lib64/libpython3.so /lib64/python3.so-3.6.8 ln -s /usr/lib64/libpython3.so /lib64/libpython3.so yum install platform-python yum install python3-pip vi /usr/bin/yum # 将 #!/usr/bin/python3 改成 #!/usr/bin/python3.7 yum install python3-prettytable-0.7.2-14.el8 yum install python3-gobject-base-3.28.3-2.el8 rpm -e --nodeps firewalld-doc-0.6.6-4.oe1.noarch yum install firewalld-0.9.3-7.el8 安装 docker yum install docker-ce systemctl start docker systemctl status docker systemctl enable docker 安装 cephadm \u0026amp; ceph-common curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release pacific yum install cephadm rpm -e --nodeps libicu-62.1-6.oe1.x86_64 yum install ceph-common-16.2.9-0.el8 ceph集群配置 集群初始化 cephadm bootstrap --mon-ip 1.1.1.101 出现如下提示说明安装成功\n...... Generating a dashboard self-signed certificate... Creating initial admin user... Fetching dashboard port number... Ceph Dashboard is now available at: URL: https://ceph-node1:8443/ User: admin Password: dkk08l0czz Enabling client.admin keyring and conf on hosts with \u0026#34;admin\u0026#34; label Enabling autotune for osd_memory_target You can access the Ceph CLI as following in case of multi-cluster or non-default config: sudo /usr/sbin/cephadm shell --fsid aac4d9ba-3be0-11ed-b415-000c29211f5f -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring Or, if you are only running a single cluster on this host: sudo /usr/sbin/cephadm shell Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/en/pacific/mgr/telemetry/ Bootstrap complete. 访问：https://1.1.1.101:8443/\n第一次访问 dashboard 需要修改初始账号密码\n添加主机 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node3 ceph orch host add ceph-node2 1.1.1.102 --labels _admin ceph orch host add ceph-node3 1.1.1.103 --labels _admin 添加磁盘 # 单盘添加 ceph orch daemon add osd ceph-node1:/dev/vdb # 查看所有可用设备 ceph orch device ls # 自动添加所有可用设备 ceph orch apply osd --all-available-devices ","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v16-cpehadm-openeuler/","summary":"前言 安装过程中会替换相当一部分系统内置的软件包，不建议用于生产环境 cephadm依赖python3.6，而此版本的openeuler内置版本为3.7，且不支持platform-python 参考：openeuler的gitee社区issue ceph：v16.2（pacific） 操作","title":"cephadm 安装 ceph-v16 (pacific) (openeuler)"},{"content":"前言 适用于 Centos8/openeuler + docker\n安装 cephadm、ceph-common 的过程就不赘述了，主要探讨如何实现 cephadm 离线安装 ceph v16.2.8\n一、离线 rpm 包和 docker 镜像的获取 找一台有外网的测试机（尽量跟生产系统的环境一致）通过 yum 安装 cephadm、ceph-common、docker 等需要的 rpm 包，注意使用 downloadonly 参数先下载好 rpm 包和对应的依赖，然后再通过 yum localinstall 安装\n使用 cephadm bootstrap 初始化单节点 ceph 集群，过程中会下载好需要的 docker 镜像\n初始化完成后就可以使用 cephadm rm-cluster --force --zap-osds --fsid \u0026lt;fsid\u0026gt; 把现在的集群删除了，暂时用不到\n二、修改 docker 镜像 我们需要修改的镜像只有 quay.io/ceph/ceph:v16 这个镜像，采用 docker commit 的方式修改\n先运行一个容器用于修改文件\n[root@node-128 ~]# docker run -itd --name test quay.io/ceph/ceph:v16 520af9cf98688d1eb1f572c28c4c60db4f231e4dbf6b3594c54c3892494e5d6c [root@node-128 ~]# docker exec -it test /bin/bash # 容器操作 [root@520af9cf9868 /]# find /usr/ -name serve.py /usr/share/ceph/mgr/cephadm/serve.py /usr/lib/python3.6/site-packages/pecan/commands/serve.py [root@520af9cf9868 /]# vi /usr/share/ceph/mgr/cephadm/serve.py 如下，注释三行，大约 937 行\n如下，三处修改大约位于 1342 行\n注释 if 语句\n修改 cepadm 命令的 pull 为 inspect-image\n获取 container 数据改为直接写死\n至此，已修改完毕，将容器提交为新的镜像\ndocker commit -m \u0026#34;修改 /usr/share/ceph/mgr/cephadm/serve.py 文件\u0026#34; -a \u0026#34;lvbibir\u0026#34; test ceph:v16 [root@node-128 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ceph v16 c654e94b4c3f 3 days ago 1.23GB quay.io/ceph/ceph v16 e8311b759ac3 3 months ago 1.23GB quay.io/ceph/ceph-grafana 8.3.5 dad864ee21e9 4 months ago 558MB quay.io/prometheus/prometheus v2.33.4 514e6a882f6e 5 months ago 204MB quay.io/prometheus/node-exporter v1.3.1 1dbe0e931976 8 months ago 20.9MB quay.io/prometheus/alertmanager v0.23.0 ba2b418f427c 11 months ago 57.5MB 然后将原先的镜像删除，将修改后的镜像改为原先的镜像 tag\ndocker rmi quay.io/ceph/ceph:v16 docker tag ceph:v16 quay.io/ceph/ceph:v16 docker rmi ceph:v16 [root@ceph-x86-node3 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/ceph/ceph v16 c654e94b4c3f 4 days ago 1.23GB quay.io/ceph/ceph-grafana 8.3.5 dad864ee21e9 4 months ago 558MB quay.io/prometheus/prometheus v2.33.4 514e6a882f6e 5 months ago 204MB quay.io/prometheus/node-exporter v1.3.1 1dbe0e931976 8 months ago 20.9MB quay.io/prometheus/alertmanager v0.23.0 ba2b418f427c 11 months ago 57.5MB 在 本博客另一篇文章 有脚本可以方便的批量导入导出镜像\n三、测试 将之前下载的 rpm 包和导出的 docker 镜像进行归档压缩，上传至无法访问外网的环境，之后就与在线部署 ceph 集群的步骤一样了\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v16-cpehadm-openuler-offline/","summary":"前言 适用于 Centos8/openeuler + docker 安装 cephadm、ceph-common 的过程就不赘述了，主要探讨如何实现 cephadm 离线安装 ceph v16.2.8 一、离线 rpm 包和 docker 镜像的获取 找一台有外网的测试机（尽量跟生产系统的环境一致）通过 yum 安装 cephadm、ceph-common、docker 等需要的 rpm 包，注意使用 downloadonly 参数先下载好 rpm","title":"cephadm 离线安装 ceph-v16 (Pacific) (openeuler)"},{"content":"前言 在 pxe 的一般场景下，通常在只需要在 dhcp 服务中配置一个通用的 filename 来指定客户端在 tftp 服务端获取的引导程序，但是在略微复杂的场景中，比如可能有些服务器默认是 legacy 模式，而有些服务器是 UEFI 模式，这两种模式使用的引导程序是不同的，但我们又不想频繁的去修改 dhcp 配置文件。本文主要探讨的就是这个问题，如何配置 dhcp 来应对复杂的服务器环境\n难点主要有两个，一个是区分某些 dhcp 客户端是否需要 pxe 引导程序，另外一个是如何区分不同的模式和架构来去分配对应的 pxe 引导程序\nRFC Request For Comments（RFC），是一系列以编号排定的文件。文件收集了有关互联网相关信息，以及UNIX和互联网社区的软件文件。RFC文件是由Internet Society（ISOC）赞助发行。基本的互联网通信协议都有在RFC文件内详细说明。RFC文件还额外加入许多在标准内的论题，例如对于互联网新开发的协议及发展中所有的记录。因此几乎所有的互联网标准都有收录在RFC文件之中。\ndhcp option 60 DHCP Option 60 Vendor class identifier为厂商类标识符。这个选项作用于客户端可选地识别客户端厂商类型和配置。这个信息是N个8位编码，由DHCP服务端解析。厂商可能会为客户端选择定义特殊的厂商类标识符信息，以便表达特殊的配置或者其他关于客户端的信息。比如：这个标识符可能编码了客户端的硬件配置。客户端发送过来的服务器不能解析的类规范信息必须被忽略（尽管可能会有报告）。\ndhcp option 93 dhcp-options 的 man 手册中有提到对于架构类型在 RFC 4578 中有一套标准，可通过 if 语句判断 dhcp 客户端的Arch代码来提供不同的PXE引导程序给客户端\n# man dhcp-options option pxe-system-type uint16 [, uint16 ... ]; A list of one ore more 16-bit integers which allows a client to specify its pre-boot architecture type(s). This option is included based on RFC 4578. 下述为 RFC 4578 标准中对 arch 代码制定的标准，name 字段包含启动模式和 cpu 架构信息（自己的猜测，这里没找到对于 name 更详细的解释）\nType Architecture Name ---- ----------------- 0 Intel x86PC 1 NEC/PC98 2 EFI Itanium 3 DEC Alpha 4 Arc x86 5 Intel Lean Client 6 EFI IA32 7 EFI BC 8 EFI Xscale 9 EFI x86-64 抓包获取arch代码 通过前文描述，我们得知 arch 代码主要是由硬件厂商定义好的，配置好 pxe 服务，arch 代码的获取至关重要，去咨询硬件厂商效率太慢，这里通过更为方便的抓包获取\n抓包主要获取提供 dhcp 服务的网卡的数据包，需服务端开启 dhcp 服务，客户端通过网卡启动\nwindows端通过 wireshark 来完成\nlinux服务端使用 tcpdump -i \u0026lt;interface\u0026gt; -w \u0026lt;file\u0026gt; 生成到文件然后用 wireshark 分析\n以下提供几个 dhcp option 60 和 dhcp option 93 报文示例：\nAMD Ryzen 7 4800U with Radeon Graphics (x86)\nvmware workstation v16 平台\nUEFI 模式下\n这里获取到的 arch 代码为 7\nAMD Ryzen 7 4800U with Radeon Graphics (x86)\nvmware workstation v16 平台\nlegacy 模式下\n这里获取到的 arch 代码为 0\nkunpeng 920 （aarch64）\nkvm 平台\nUEFI 模式下\n这里获取到的 arch 代码为 11\n以上抓包都是在网络引导的环境下进行的，在使用已安装操作系统中的网卡去发送 dhcp 请求时，整个数据包传输过程都没有 option 60 和 option 93 这两个选项的参与，我猜测这两个选项只有在网络引导的环境下才会去参与\ndhcp 配置文件示例 在上述论证基础之上，我们就可以通过配置 dhcp 服务来使 pxe 足以应对复杂的网络环境和硬件环境\n解决前言中提到的两个难点分别通过 option 60 和 option 93 分别解决\n# 这里应该是将 option 93 的值格式化成 16 进制，用于下面的 if 判断（猜测） option arch code 93 = unsigned integer 16; class \u0026#34;pxeclients\u0026#34; { # 这里判断 option 60 选项的值的前9个字符是否是 PXEClient match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; next-server 10.17.25.17; # 这里通过 if 判断 arch 代码来决定如何去分配对应的 pxe 引导程序 if option arch = 00:07 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else if option arch = 00:09 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else { filename \u0026#34;/pxelinux.0\u0026#34;; } } 较为详细的配置文件示例，后面有简化版\n# 启用 PXE 支持 allow booting; allow bootp; # PXE 定义命名空间 option space PXE; option PXE.mtftp-ip code 1 = ip-address; option PXE.mtftp-cport code 2 = unsigned integer 16; option PXE.mtftp-sport code 3 = unsigned integer 16; option PXE.mtftp-tmout code 4 = unsigned integer 8; option PXE.mtftp-delay code 5 = unsigned integer 8; option arch code 93 = unsigned integer 16; # RFC4578 authoritative; one-lease-per-client true; # 不使用DNS动态更新 ddns-update-style none; # 忽略客户端DNS更新 ignore client-updates; # 不使用 PXE 的网络 shared-network main { subnet 10.17.25.0 netmask 255.255.255.0 { option routers 10.17.25.254; option subnet-mask 255.255.255.0; option domain-name \u0026#34;zhijie-liu.com\u0026#34;; # 在此网络关闭PXE支持 deny bootp; pool { range 10.17.25.200 10.17.25.210; host nagios-test { hardware ethernet 00:0d:56:66:82:c3; fixed-address 10.17.25.200; } } } } # 使用 PXE 的网络 shared-network pxe { subnet 10.17.15.0 netmask 255.255.255.0 { option routers 10.17.15.254; option subnet-mask 255.255.255.0; option domain-name \u0026#34;xiyang-liu.com\u0026#34;; option domain-name-servers 10.17.26.88, 8.8.8.8; default-lease-time 86400; max-lease-time 172800; pool { range 10.17.15.1 10.17.15.20; class \u0026#34;pxeclient\u0026#34; { match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; next-server 10.17.25.17; if option arch = 00:07 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else if option arch = 00:09 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else { filename \u0026#34;/pxelinux.0\u0026#34;; } } # 根据 MAC 地址单独分配地址和指定的 PXE 引导程序 host gpxelinux { option host-name \u0026#34;gpxelinux.zhijie-liu.com\u0026#34;; hardware ethernet 00:50:56:24:0B:30; fixed-address 10.17.15.8; filename \u0026#34;/gpxelinux.0\u0026#34; } } } } 简化版（仅kvm平台测试通过）\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; option arch code 93 = unsigned integer 16; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; class \u0026#34;pxeclients\u0026#34; { match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; next-server 1.1.1.21; if option arch = 00:11 { filename \u0026#34;/grubaa64.efi\u0026#34;; } } } 参考 https://blog.csdn.net/u012145252/article/details/125405273\nhttps://www.cnblogs.com/boowii/p/6475921.html\nhttps://www.rfc-editor.org/rfc/rfc4578.html\n","permalink":"https://www.lvbibir.cn/en/posts/tech/pxe-dhcp-legacy-uefi-archs/","summary":"前言 在 pxe 的一般场景下，通常在只需要在 dhcp 服务中配置一个通用的 filename 来指定客户端在 tftp 服务端获取的引导程序，但是在略微复杂的场景中，比如可能有些服务器默认是 legacy 模式，而有些服务器是 UEFI 模式，这两种模式使用的引导程序是不同的，但我们又不想频繁的去修改 dhcp 配置文件。本文主要探讨的就是这个问题，如何配","title":"pxe 如何应对复杂的服务器硬件环境"},{"content":"示例代码\nimport os # 输入文件夹地址 path = \u0026#34;C://Users//lvbibir//Desktop//lvbibir.github.io//content//posts//read//\u0026#34; files = os.listdir(path) # 输出所有文件名，只是为了确认一下 for file in files: print(file) # 获取旧名和新名 i = 0 for file in files: # 旧名称的信息 old = path + os.sep + files[i] # 新名称的信息 new = path + os.sep + file.replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # 新旧替换 print(new) os.rename(old,new) i+=1 ","permalink":"https://www.lvbibir.cn/en/posts/tech/python-rename-file/","summary":"示例代码 import os # 输入文件夹地址 path = \u0026#34;C://Users//lvbibir//Desktop//lvbibir.github.io//content//posts//read//\u0026#34; files = os.listdir(path) # 输出所有文件名，只是为了确认一下 for file in files: print(file) # 获取旧名和新名 i = 0 for file in files: # 旧名称的信息 old = path + os.sep + files[i] # 新名称的信息 new = path + os.sep + file.replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # 新旧替换 print(new) os.rename(old,new) i+=1","title":"python批量修改目录下文件名"},{"content":"前言 书名《微习惯》，作者斯提芬·盖斯[美]，江西人民出版社，译者桂君\n微习惯是一种非常微小的积极行为，你需要每天强迫自己完成它。微习惯太小，小到不可能失败。正是因为这个特性，它不会给你造成任何负担，而且具有超强的“欺骗性”，它因此成了极具优势的习惯养成策略。\n微习惯策略的科学原理表明了人们无法长期坚持大多数主流成长策略的原因，也解释了人们长期坚持微习惯策略的可能性。人们无法让改变的效果持久时，往往认为原因在于自己，但其实有问题的并不是他们本身，而是他们采用的策略。当你开始用微习惯策略教你的方法按照大脑的规律做事情时，持久改变其实很容易。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n一 微习惯是什么 千里之行，始于足下。 ——老子\n欢迎来到微习惯的世界，首先陈述两个事实：\n哪怕是一点点行动，也比毫不作为强无数倍（在数学意义上如此，实际生活中也是如此）。 相比某一天做很多事，每天做一点事的影响力会更大。 几乎每个人都经历过瓶颈期，竭尽全力想提升自己却最终失败，然后无数次尝试并遭遇失败后很久不敢重新开始。\n很多时候，我们没能实施行动，也没能实现计划，但有没有可能这并不是我们的错，而是我们采用并认可的策略出了问题呢？\n只为培养好习惯\n微习惯不会直接帮你戒烟、戒酒或者控制赌瘾，微习惯策略只会帮你培养你认可的好习惯，给你的生活增添积极行为，持续丰富你的生活。消除坏习惯和建立好习惯有着共同的目标——用更好的行为方式取代原有的行为方式。\n如果你有好习惯，你改变自己的主要动力是靠近这些积极的东西；如果你有坏习惯，你改变自己的主要动力是远离这些消极的东西。\n微习惯简介\n如果你想培养一个新习惯，微习惯基本就是它经过大幅缩减的版本——把“每天做100个俯卧撑”缩减成每天1个，把“每天写3000字”缩减成每天写50字，把“始终保持积极思考”缩减成每天想两件好事。\n微习惯体系的基础在于“微步骤”，那些“小到不可思议的一小步”。\n一家银行可能因为规模太大而不至于失败，而微习惯是因为太小而不至于无法完成，因为，你不会有机会体验未完成目标导致的常见消极情绪，比如愧疚和挫败感。\n习惯还与压力有关\n现在试想一下：如果坏习惯让你压力过大，你会怎么做。压力是负反馈循环的绝佳导火索，它会触发一个坏习惯，坏习惯又会触发内疚感、内心的焦虑和更多压力，这些消极因素会再次触发这个坏习惯。\n但是，如果习惯本身就能缓解压力会怎样？拿锻炼来说，你的压力会把你拽到健身房，锻炼会帮你缓解焦虑。\n养成新习惯需要多长时间\n不是21天，也不是30天。“21天”谬论可能源自整形外科医生麦克斯韦尔·马尔茨（Maxwell Maltz）。据说马尔茨医生发现接受截肢手术的患者需要大约21天来适应肢体残缺的事实，因此，他认为21天是人们适应任何生活变化所需要的时间长度。\n不同行为所需要的时间差别很大，从18天到254天不等，甚至在某些案例中，这些时间可能惊人的长。\n二 大脑的工作原理 大脑是我的一切，华生。身体只是附件而已。 ——阿瑟·柯南·道尔，《福尔摩斯探案集》\n大脑是变化缓慢且状态稳定的\n人类大脑有一套对外部世界做出反应的固定体系。有时我们觉得不易改变的大脑令人感到沮丧，但总体来说，好处还是相当多的\n一旦成功养成健康的新习惯，一切都会变得轻松起来。我们无需跟大脑持久战斗就可以自然的执行这些习惯。\n大脑中的两个核心角色\n愚蠢的重复者——基底神经节\n聪明的管理者——前额皮层\n基底神经节是愚蠢的，你抽烟的时候，它不会考虑到肺癌的可能性；你锻炼的时候，它也不会幻想健康身体的好处。但是它可以高效率地重复模式，节省精力，它的工作几乎无需我们消耗额外的意志力或者动力即可完成。\n前额皮层则相当聪明，是个可以理解长远利益和结果的管理者，它拥有抑制基底神经节的能力，同时它还负责处理短期思维和决策。\n前额皮层的功能这么强大，所以会消耗相当的精力，从而使你感到疲劳。这个时候，掌管重复部分的基底神经节就会接管大脑。\n让大脑的其他部分喜欢上前额皮层想要的东西，是建立新习惯的唯一方式\n三 动力 v.s. 意志力 情绪要么顺服你，要么支配你，这要看谁说了算。 ——吉米·罗恩\n当动力处于峰值时，意志力消耗量为0或可以忽略不计，这是因为你无需强迫自己做你本来就愿意做的事情，可以当动力降为0时，强烈的内心抵触意味着我们必须消耗非常大的意志力\n做事缺乏动力，意志力的消耗猛涨，这种方式很难维持一个行为并将其培养成习惯\n“激发动力”策略的诸多问题\n激发动力有效果吗？答案并不是那么确定。偶尔我们可以激发强烈的动力做某件事，比如锻炼身体，比如阅读，比如学习某项技能，但是扪心自问我们无法确保下次是否还有如此强烈的动力。\n动力是一种能带来诸多好处的重要感觉，但是当它出现时，请把它看作一个额外的奖励，一件美好的事物。我们可以享受它带来的好处，但不要尝试去依赖动力。\n动力并不可靠 动力之所以不可靠，是因为它是以人的感受为基础的，而人类的感受容易改变且无法预测已经是几百年来公认的事实了。几乎所有东西都能改变你的感受，所以我们不要把希望放在如此不稳定的东西上。任何事物能成为基础的第一原则就是它必须牢固可靠。\n我们无法做到每次都愿意激发动力 问题在于，动力是很难或者说几乎不可能按需培养的。我们只有在精力充沛、思维模式健康、没有受到其他强烈诱惑的时候，我们才能依靠动力成功。\n你根本不想让自己想让自己想锻炼。很多时候，你积聚动力只是为了让自己有动力激发动力而已。生活中总有那么几次，你不愿意为了激发动力而激发动力。\n“热情递减法则” “热情递减法则”不是一条真正的法则，是作者创造的术语。它比对应的“边际效用递减法则”更好理解。这条经济法则认为，吃第五块披萨的时候愉悦感略低于吃第四块的时候，吃第四块的时候又略低于吃第三块的时候。可能下面这个例子更形象，一瓶三块钱的冰可乐第一口至少值两块五。甚至我觉得感情生活中的新鲜感同样适用这个法则，新鲜感就是一种动力，然而大部分情侣新鲜感也只能维持几个月而已。\n习惯是一个我们选择做一件事而做一件事的行为，行动开始前和结束后不会出现剧烈波动。有热情是好事，但我们应该把这种动力看作一种额外奖励，而不是实施行动的信号。即表现更稳定和自动的基底神经节掌握控制权。\n为什么意志力能打败动力？\n有必要重申一遍，动力是好东西，只是不可靠而已。借助意志力，动力会变得更加可靠；而且如果先采取行动，继续行动的动力会被迅速激发。\n意志力很可靠\n意志力可以被强化\n意志力可以通过计划执行\n意志力的工作原理\n做决定也会消耗意志力 在同一天里做过艰难决定的人在后来面对诱惑时屈服的可能性更高，这体现了自控力的下降。重大决定和意志力似乎需要消耗同样的能量。比如你上午强迫自己学习了几个小时，在吃晚饭时在炸鸡和更为健康的饮食之间会非常偏向前者，前提是学习和控制饮食你都没有养成习惯的前提下。\n意志力损耗的五个最重要的因素 元分析是从指定主题的相关文献中提取出重要结论的过程。\n2010年的一项针对自我损耗的元分析中发现了引起自我损耗的五个最重要的因素：努力程度、感知难度、消极情绪、主观疲劳和血糖水平。\n总结一下上述的本章内容\n我们是用动力或者意志力开启新的行为的（非习惯性）。 动力不可靠，所以不能充当建立习惯的策略。 意志力可靠，但前提是你没有把它耗尽。 引发意志力损耗的五大重要因素：努力程度、感知难度、消极情绪、主观疲劳和血糖水平。 如果我们能克服这五项障碍，我们就应该能走向成功。 四 微习惯策略 塑造你生活的不是你偶尔做的一两件事，而是你一贯坚持做的事。 ——安东尼·罗宾\n以微习惯方式运用意志力\n微习惯是怎样有效消除意志力的五大威胁的\n努力程度 微习惯需要非常少的实际努力，自我损耗极少。\n感知难度 微习惯的本质决定它几乎不会让你在还没做的时候就感受到困难。一旦你开始做且能随心所欲地继续下去，“已经开始”带来的心理影响会让感知难度明显降低。正如从物理学角度来看，物体的惯性在运动开始时最大，一旦物体处于运动状态，因为存在动量，一切都会变得简单。\n很多时候我们无法坚持做某件事的时候都是因为在一开始就感受到了很大的难度，所以有了这样的想法：如果最终做不到，我们宁可不开始。\n消极情绪 即使微习惯占用了一件本应使你快乐的事情的时间，你要做的努力也非常少，所以几乎感受到消极情绪。何况通常情况下，我们都会用有益的行为取代浪费时间的行为，这个过程本身就会带来积极情绪。\n主观疲劳 这个因素很有意思，不是“疲劳”，而是“主观疲劳”，就是说我们在评估自己的疲劳程度时并不是完全客观的。通常更难的任务在开始前就会感受到很大的压迫感。\n采用微习惯策略的结果：主管疲劳无法彻底消除，但是微习惯可以有效缓解主观疲劳。\n血糖水平 葡萄糖是人体首要的能量来源。如果血液中葡萄糖的含量变低，你会感觉疲惫。\n采用微习惯策略，你无需动用前额皮层去做一些重大决定，或者消耗很多意志力，这有助于保持我们的血糖水平。\n微习惯如何拓宽你的舒适区\n你现在有一个心理舒适区（comfort zone），把它想象成一个圆圈。圆圈内是当下的我们，圆圈外则是我们想要达到的目标，也许是身材变好，读完了几本书，学会了某项技能。但是这些目标都要经历一些不太舒适的过程才能实现（因为脱离了基底神经节目前的模式）。\n通常我们采取“只要能成功怎么做都行”，然后开始大量行动，我们全力冲刺到舒适区外边，拼命挣扎想要留在那里，此时我们的潜意识：“有意思，但是这么剧烈的变化让我很不舒服”，当我们的动力和意志力不足以支撑时，我们会被拽回到舒适圈内。\n而微习惯就像是走到圆圈的边缘，轻轻往外走一小步，我们完全可以走一小步后退回到舒适圈，潜意识不会对这么微小的改变的做出太大反应，但是长此以往，舒适圈就会被我们扩大。\n我们偶尔会超额完成目标，可以用基础物理学知识来解释。牛顿第一运动定律的内容包括：\n除非受到外力作用，否则静止的物体总保持静止状态 除非受到外力作用。否则出于运动状态中的物体的速度不会改变。 我们可以得到一个新等式：一小步+想做的事=较高的进一步行动的可能性\n五 微习惯的独特之处 是故胜兵先胜而后求战，败兵先战而后求胜。 ——孙子，《孙子兵法》\n微习惯能与现有习惯一较高下\n培养一个新习惯也是对之前我们养成的一些习惯的挑战，我们需要摒弃一些曾经不好的习惯，以让更好的行为代替它。大脑会抗拒大幅度的改变，所以我们要以极其微小的行为做出一点点改变，潜移默化的影响我们的大脑，让新的行为成为基底神经节的一种模式。\n微习惯没有截止时间\n很多将心理学、行为学或者其他尝试帮助你养成好习惯的书籍都是基于“习惯是21天或者30天养成的”这个理论，而微习惯没有明确的截止时间，它要求尽可能一天都不能落下，长久地坚持。但是不像其他方法要求你每天健身一小时或者读一个小时的书，我们要做的只是很小很小的一部分，例如读2页书就好，只是这样。\n微习惯可以提升自我效能感\n大多数人都成尝试过把一个良好的行为养成习惯，然后由于各种各样的原因没有坚持下来，这会使我们缺乏基本的自我效能感。\n微习惯正是重新开始的完美方法。你不会再被巨大的目标打垮，也不会因为目标未实现带来的内疚感感到焦虑煎熬。这一次，你每天都能成功。这些胜利也许微不足道，但是对于一颗心灰意冷的心来说是至关重要的。\n微习惯帮你培养正念和意志力\n正念是一个非常重要的技能，它指的是我们对于自己的思维和行动有清醒的认知。正念是目标清晰地活着和敷衍活着之间的区别。如果你的微习惯是每天起床后喝一杯水，那么你就会对自己总共喝了多少水有所认知，如果是每天看两页书，那么你就会时常想自己已经看完了多少书。\n前文提过意志力是一项非常宝贵的资源，而微习惯是一项频繁重复小任务的行为，这是锻炼意志力的一个绝佳方法。\n六 彻底改变只需要八步 一个得不到执行的念头只会消亡 ——罗杰·冯·欧克\n第一步：选择适合自己的微习惯和计划\n选择适合自己的微习惯\n可以是一个每天都要做的事情，也可以是一个时间段的弹性计划，比如一周跑步三次\n把习惯变成一个小到不可思议的一步\n比如把每天做20个俯卧撑改成1个\n我给自己制定的微习惯有两项：每天读两页书、每周换好跑步的装备然后走到小区门口三次（天）。\n第二步：挖掘每个微习惯的内在价值\n我们很多时候无法养成习惯的原因在于我们想做一件事，但为要不要做这件事而苦恼。可以反思一下我们在第一步制定的习惯，一般都是我们长期以来潜意识里觉得正确的事情，却一直缺乏动力或者不那么理解这项行为能给我们带来什么。\n用“为什么钻头”来挖掘一下：\n我想每天读两页书。为什么？ 因为读书几乎一直以来都是成功人士的标配。为什么？ 因为读书是人们汲取知识、拓宽视野极好的途径。为什么？ 因为读书可以练就腹有诗书气自华和沉稳的气质，而这两点正是目前的我极其渴求的。 第三步：明确习惯依据，将其纳入日程\n培养习惯的常见依据有两个：时间和行为方式。对于朝九晚五时间比较规律的人群，更推荐根据时间方式作为依据，日程比较灵活的可以使用行为方式作为依据。\n时间：每周一三五的下午3点锻炼 行为方式：吃完晚饭后半个小时开始读书 也有第三种自由度更高的方式，我们在当天任意时间完成都可以，最低限度是睡觉前。\n第四步：建立回报机制，以奖励提升成就感\n一个有趣的现象：一个申请假释的犯人，假释听证会的最佳时间是在假释官吃完东西、结束休息之后，因为研究发现假释官在吃饱睡好之后做出的判决对被告更有利（大概是因为他们更愿意倾听）。\n拿锻炼来举例，锻炼可以让你获得健康的体魄，良好的身材。但是你刚开始锻炼的时候，锻炼结束后回到家里你收获到的回报是什么？汗水？于此同时，你的大脑却现在就想吃炸鸡，因为糖会刺激味蕾并激活大脑的回报中心，所以炸鸡是一种感官（首要）回报，而锻炼带来的是抽象（次要）回报，比如拥有好身材在沙滩上漫步、对付出的努力感到满意。次级回报需要更长的时间才能在大脑站稳脚跟。\n在一开始锻炼产生的内啡肽和期望产生的回报差距过大时，我们可以给自己一些奖励策略，比如挑选一个想买的东西加入购物车，在跑步一个月后把它买下来、如果能坚持到两个月就给自己买一块专业的运动手表（比如我给自己买的高驰pace2）。\n这像是教小孩骑自行车，一开始我们需要向孩子保证会扶着自行车，可是在某个时候我们把手松开后，孩子不需要扶持也能继续骑车了\n第五步：记录与追踪情况\n遗忘是人类的天性。在一项习惯的前期阶段，遗忘也是一个阻力，我们偶尔会忘记我们给自己制定的计划。所以采取一些策略来提醒我们还是很有必要的。\n方式不重要，重要的是可以有效地提醒我们。如果你有看日历的习惯，就把要做的事情写到便签上放到日历旁边；如果你每天都使用电脑，可以把便签粘到显示器下面。手机闹钟、一些带有提醒功能的APP等等都可以。\n第六步：微量开始，超额完成\n我们在完成微习惯时消耗的是意志力，但是我们在达成目标后继续努力时动力就会开始起作用了。\n当你一旦开始，就会希望多完成一些。到那个时候，继续做和停下来一样容易。\n第七步：服从计划安排，拜托高期待值\n在第一步中，我们已经把习惯的难度定的非常低了，所以超额完成是很平常的事情，正如之前说到的，请把它作为一个奖励，不要把超额完成的部分作为你今后每天的目标，这是一个很危险的行为。读完两页书就是成功，句号！\n举个例子：你连续半个月每天都读了30页书，而不是2页书，于是理所当然的把微习惯改为了每天读30页书，但是一旦某天状态不佳或者因为一些其他因素导致没有完成30页书的目标，你建立起来的自信会受挫。不要嘴上说着读2页书就好，心里却把30页书作为目标。不要忘记你是如何做到读30页书的（是从每天2页书开始的）。\n坚持做一件小事，比偶尔做一件大事能从根本上改变更多。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n第八步：习惯养成的标志\n代表行为已经成为习惯的信号：\n没有抵触情绪：该行为似乎做起来容易，不做反而更难 身份：你打心底认同该行为，可以信心十足地说“我是个跑者” 行动时无需考虑：不需要做任何决定，自然而然地去做。 你不再担心：你知道你会一直做这件事。 常态化：习惯是非情绪化的。你开始你的微习惯时没有任何情绪波动，而不会因为你正在做这件事而“激动不已” 它很无聊：好的习惯并不会让人很兴奋，它只是对你有好处而已。你会因为它们对生活更有激情，但不是对行为本身。 七 微习惯策略的八大规则 1. 绝对不要自欺欺人\n比如觉得某项微习惯太小，觉得偶尔一天不做也没什么；或者给自己制定的微习惯是每天一个俯卧撑，却在心里偷偷要求自己完成更多。\n2. 满意每一个进步\n对小小的进步感到满意和标准低不是一回事。李小龙有一句名言可以很好地总结这一点：“要满意，但别满足”。\n微习惯策略的核心是一个很简单的大脑错觉，同时也是一种重视开始的生活哲理，一种认为行动优于动力的生活哲理，一种相信将每一小步积累起来便能让量变转为质变的生活哲理。\n3. 经常回报自己，尤其在完成微习惯之后\n哪怕在完成微习惯之后对自己说“你很棒”这一点小小的激励，最终都会建立一个正反馈循环。\n4. 保持头脑清醒\n可能坚持几个月的微习惯后你能看到比较大的变化，进而过度兴奋，但别让这种兴奋成为你实施行动的原动力。变得依赖动力或情绪正是导致很多习惯没有养成的原因。\n在完成目标的过程中，无聊才是常态。使用冷静的头脑分析你的行为。\n5. 感到强烈抵触时，后退并缩小目标\n常识告诉我们，突破才能获得进步，然而这只适用于短期目标，比如项目的deadline，你需要逼一逼自己才能完成。但是对于养成一个习惯来说，保证我们可以长时间的坚持才是最重要的。\n如果你给自己制定的计划让你感到很痛苦，你需要考虑是这项行为本身的问题还是目标设立的太大了。养成习惯过程中有抵触情绪是正常的，但采用微习惯策略时假如你能感受到明显的抵触情绪，那一定要缩小目标。\n6. 提醒自己这件事很轻松\n在微习惯策略中，你对实施行动的抵触行为很多时候都是因为考虑的太宽泛，比如健身，这是一个听起来就比较有压力的行为。但是你想想你今天要做的仅仅是做一个俯卧撑，自然而然会感受到轻松。\n7. 绝不要小看微步骤\n每一个大的工程都是由无数个小步骤做成的。持续做一件很小的小事，坚持一段时间，反正又花不了你多长时间，大部分微习惯两分钟之内就能完成，你会慢慢看到效果的。\n8. 用多余精力超额完成任务，而不是制定更大的目标\n大目标在纸面上看着漂亮，但只有行动才算数。\n目标渺小、结果丰满。你是想要这样的结果，还是反过来？\n","permalink":"https://www.lvbibir.cn/en/posts/read/wei-xi-guan/","summary":"前言 书名《微习惯》，作者斯提芬·盖斯[美]，江西人民出版社，译者桂君 微习惯是一种非常微小的积极行为，你需要每天强迫自己完成它。微习惯太小，小到不可能失败。正是因为这个特性，它不会给你造成任何负担，而且具有超强的“欺骗性”，它因此成了极具优势的习惯养成策略。 微习惯策略的科学原理表明","title":"《微习惯》"},{"content":"前言 安装过程中会替换相当一部分系统内置的软件包，不建议用于生产环境\ncephadm依赖python3.6，而此版本的openeuler内置版本为3.7，且不支持platform-python\n参考：openeuler的gitee社区issue\nceph：v16.2（pacific）\n操作系统：icloudos_v1.0_aarch64（openEuler-20.03-LTS-aarch64）\n内核版本：4.19.90-2003.4.0.0037.aarch64\n集群角色：\nip 主机名 角色 192.168.47.133 ceph-aarch64-node1 cephadm，mgr，mon，osd 192.168.47.135 ceph-aarch64-node2 osd 192.168.47.130 ceph-aarch64-node3 osd 基础环境配置(所有节点) 关闭 node_exporter systemctl stop node_exporter systemctl disable node_exporter 修改主机名 hostnamectl set-hostname ceph-aarch64-node1 hostnamectl set-hostname ceph-aarch64-node2 hostnamectl set-hostname ceph-aarch64-node3 vi /etc/hosts # 添加 192.168.47.133 ceph-aarch64-node1 192.168.47.135 ceph-aarch64-node2 192.168.47.130 ceph-aarch64-node3 添加 yum 源 wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/docker-ce.repo 添加 epel 源 yum install epel-release # 修改 $releasever sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-modular.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-playground.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-testing-modular.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-testing.repo 修改 /etc/os-release sed -i \u0026#39;s/ID=\u0026#34;isoft\u0026#34;/ID=\u0026#34;centos\u0026#34;/g\u0026#39; /etc/os-release sed -i \u0026#39;s/VERSION_ID=\u0026#34;1.0\u0026#34;/VERSION_ID=\u0026#34;8.0\u0026#34;/g\u0026#39; /etc/os-release 安装 python3.6 yum install python3-pip-wheel python3-setuptools-wheel wget http://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/aarch64/os/Packages/python3-libs-3.6.8-41.el8.aarch64.rpm wget http://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/aarch64/os/Packages/libffi-3.1-22.el8.aarch64.rpm rpm -ivh libffi-3.1-22.el8.aarch64.rpm --force cp /usr/lib64/libpython3.so /usr/lib64/libpython3.so-3.7.4 rpm -ivh python3-libs-3.6.8-41.el8.aarch64.rpm --force --nodeps mv /lib64/libpython3.so /lib64/python3.so-3.6.8 ln -s /usr/lib64/libpython3.so /lib64/libpython3.so yum install platform-python yum install python3-pip-9.0.3-20.el8.noarch vim /usr/bin/yum # 将 #!/usr/bin/python3 改成 #!/usr/bin/python3.7 yum install python3-prettytable-0.7.2-14.el8 yum install python3-gobject-base-3.28.3-2.el8.aarch64 yum install firewalld-0.9.3-7.el8 安装 docker yum install docker-ce systemctl start docker systemctl status docker systemctl enable docker 安装 cephadm \u0026amp; ceph-common curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release pacific yum install cephadm yum install ceph-common-16.2.9-0.el8 ceph集群配置 集群初始化 cephadm bootstrap --mon-ip 192.168.47.133 访问：https://192.168.47.133:8443/\n第一次访问 dashboard 需要修改初始账号密码\n添加主机 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-aarch64-node2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-aarch64-node3 ceph orch host add ceph-aarch64-node2 192.168.47.135 --labels _admin ceph orch host add ceph-aarch64-node3 192.168.47.130 --labels _admin 添加磁盘 # 单盘添加 ceph orch daemon add osd ceph-aarch64-node1:/dev/vdb # 查看所有可用设备 ceph orch device ls # 自动添加所有可用设备 ceph orch apply osd --all-available-devices 其他 清除ceph集群 # 暂停集群，避免部署新的 ceph 守护进程 ceph orch pause # 验证集群 fsid ceph fsid # 清除集群所有主机的 ceph 守护进程 cephadm rm-cluster --force --zap-osds --fsid \u0026lt;fsid\u0026gt; no active mgr cephadm ls cephadm run --name mgr.ceph-aarch64-node3.ipgtzj --fsid 17136806-0735-11ed-9c4f-52546f3387f3 ceph orch apply mgr label:_admin osd误删除 https://blog.csdn.net/cjfcxf010101/article/details/100411984\n删除 osd 后引起的 cephadm_failed_daemon 错误 https://www.cnblogs.com/st2021/p/15026526.html\n禁用自动添加osd ceph orch apply osd --all-available-devices --unmanaged=true ","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v16-cpehadm-openeuler-aarch64/","summary":"前言 安装过程中会替换相当一部分系统内置的软件包，不建议用于生产环境 cephadm依赖python3.6，而此版本的openeuler内置版本为3.7，且不支持platform-python 参考：openeuler的gitee社区issue ceph：v16.2（pacific） 操作","title":"cephadm 安装 ceph-v16 (pacific) (openeuler) (aarch64)"},{"content":"起因 在使用 cephadm 安装 ceph v16.2 时升级了 python，系统默认版本是 3.7.4 ，升级后版本是 3.8.5，glibc 作为依赖同时进行了升级，系统默认版本是 2.28 ，升级后版本是 2.31，幸好记录及时，截图留存了软件包升级信息，如下\n在没有十分把握的情况下不要用 yum install -y，使用 yum install 先判断好依赖安装带来的影响\n升级过程未出任何问题，便没在意，可是后续 openssh 由于 glibc 的升级导致连接失败，一番 baidu 加 google 未解决 openssh 连接问题，于是便着手开始降级 glibc 至系统默认版本，从系统镜像中找到 glibc 相关的三个软件包\n由于是版本降级，脑子一热便采用 rpm -Uvh --nodeps glibc* 方式强制安装，至此，系统崩溃\n系统几乎所有命令都无法使用，报错如下\n出现这个问题的原因大致是因为强制安装并未完全成功，lib64 一些相关的库文件软链接丢失\n[root@localhost ~]# ls -l /lib64/libc.so.6 lrwxrwxrwx 1 root root 12 7月 14 14:43 /lib64/libc.so.6 -\u0026gt; libc-2.28.so # 恢复前这里是 libc-2.31.so 在强制安装 glibc-2.28 时， libc-2.31.so 已经被替换成了 libc-2.28.so ，由于安装失败 libc.so.6 链接到的还是 libc-2.31.so，自然会报错 no such file\n恢复 系统绝大部分命令都是依赖 libc.so.6 的，我们可以通过 export LD_PRELOAD=\u0026quot;库文件路径\u0026quot; 设置优先使用的库\nexport LD_PRELOAD=/lib64/libc-2.28.so 此时 ls 、cd、mv 等基础命令以及最重要的 ln 链接命令已经可以使用了，接下来就是恢复软链接\nrm -f /lib64/libc.so.6 ln -s /lib64/libc-2.28.so /lib64/libc.so.6 但是 yum 命令依赖的几个库软链接还没有恢复，按照报错提示跟上述步骤一样，先删除掉依赖的库文件，再重新软链接过去\n之后就是重新 yum localinstall 安装一下未安装成功的 glic ，之前强制安装时已经将高版本的 glibc 清理掉了，这里重新安装很顺利\n也许之前使用 yum localinstall 安装可能就不会出现这个问题了，rpm \u0026ndash;nodeps 也要少用~\nyum localinstall glibc* 软件包安装过程中没有报错，经测试系统一切正常，openssh 也可以正常连接了\n以上，系统恢复正常\n","permalink":"https://www.lvbibir.cn/en/posts/tech/glibc-wu-sheng-ji/","summary":"起因 在使用 cephadm 安装 ceph v16.2 时升级了 python，系统默认版本是 3.7.4 ，升级后版本是 3.8.5，glibc 作为依赖同时进行了升级，系统默认版本是 2.28 ，升级后版本是 2.31，幸好记录及时，截图留存了软件包升级信息，如下 在没有十分把握的情况下不要用 yum install -y，使用 yum install 先判断好依赖安装带来的影响 升级过","title":"glibc 误升级后修复"},{"content":"前言 测试环境：\nx86_64（amd ryzen 7 4800u）：vmware workstation V16.1.2\naarch64（kunpeng 920）： kvm-2.12\n注意测试的网络环境中不要存在其他的dhcp服务\n注意测试虚拟机内存尽量大于4G，否则会报错 no space left 或者测试机直接黑屏\n注意 ks.cfg 尽量在当前环境先手动安装一台模板机，使用模板机生成的 ks 文件来进行修改，否则可能会有一些清理磁盘分区的破坏性操作，基本只需要将安装方式从 cdrom 修改成 install 和 url --url=http://......\n服务端配置 基础环境 系统版本：iSoft-ServerOS-V6.0-rc1\nip地址：1.1.1.21\n网卡选择nat模式，注意关闭一下 workstation 自带的 dhcp，也可使用自定义的 lan区段\n关闭防火墙及selinux iptables -F systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config 安装相关的软件包 这里由于 HW 行动的原因，外网 yum 源暂不可用，使用本地 yum 源安装相关软件包\nmount -o loop /root/iSoft-Taiji-Server-OS-6.0-x86_64-rc1-202112311623.iso /mnt mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/isoft* /etc/yum.repos.d/bak/ cat \u0026gt; /etc/yum.repos.d/local.repo \u0026lt;\u0026lt;EOF [local] name=local baseurl=file:///mnt gpgcheck=0 enabled=1 EOF dnf clean all dnf makecache cenots8安装syslinux时需要加 \u0026ndash;nonlinux后缀，centos7则不需要\ndnf install dhcp-server tftp-server httpd syslinux-nonlinux http服务配置 mkdir /var/www/html/ks/ chmod 755 -R /var/www/html/ systemctl start httpd systemctl enable httpd 能访问到 httpd 即可\ntftp服务配置 systemctl start tftp systemctl enable tftp dhcp服务配置 x86_64 架构和 aarch64 架构的 dhcp 的配置略有不同，按照下文分别配置\nsystemctl enable dhcpd x86_64 服务端配置 dhcp 服务配置 vim /etc/dhcp/dhcpd.conf\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; next-server 1.1.1.21; # 本机ip（tftpserver的ip） filename \u0026#34;pxelinux.0\u0026#34;; } systemctl restart dhcpd isoft_4.2_x86 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_4.2/isos/x86_64/ # 挂载镜像文件 mount -o loop /root/iSoft-Server-OS-4.2-x86_64-201907051149.iso /var/www/html/isoft_4.2/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-isoft-4.2-x86.cfg chmod -R 755 /var/www/html ks.cfg文件内容\n#version=DEVEL # System authorization information auth --enableshadow --passalgo=sha512 # Use CDROM installation media install url --url=http://1.1.1.21/isoft_4.2/isos/x86_64/ # Use graphical install graphical # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda # Keyboard layouts keyboard --vckeymap=cn --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=dhcp --device=ens32 --onboot=off --ipv6=auto --no-activate network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$9yXT2.jd8oofY89W$q1nVQ4rRfAE937KeG5bHCAP3iI3GgyVJJF/MN5Ipe9omdXIEjelaTQSPplr9E9aFOGG17F3GkzIzNnifvjdO20 # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc # X Window System configuration information xconfig --startxonboot # System bootloader configuration bootloader --append=\u0026#34; crashkernel=auto\u0026#34; --location=mbr --boot-drive=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel %packages @^gnome-desktop-environment @base @core @desktop-debugging @dial-up @directory-client @fonts @gnome-desktop @guest-agents @guest-desktop-agents @input-methods @internet-browser @java-platform @multimedia @network-file-system-client @networkmanager-submodules @print-client @x11 chrony kexec-tools %end %addon com_redhat_kdump --enable --reserve-mb=\u0026#39;auto\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/icloud_1.0/isos/x86_64/Packages/syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm /root/ rpm2cpio syslinux-4.05-15.el7.isoft.x86_64.rpm | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-4.05-15.el7.isoft.x86_64.rpm | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/ks/ks-isoft-6.0-x86.cfg isoft_6.0-rc1_x86 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_6.0/isos/x86_64/ # 挂载镜像文件 mount -o loop /root/iSoft-Taiji-Server-OS-6.0-x86_64-rc1-202112311623.iso /var/www/html/isoft_6.0/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-isoft-6.0-x86.cfg chmod -R 755 /var/www/html ks.cfg文件内容\n# Use graphical install graphical install url --url=http://1.1.1.21/isoft_6.0/isos/x86_64/ %packages @^graphical-server-environment %end # Keyboard layouts keyboard --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=ens33 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # System timezone timezone Asia/Shanghai --isUtc # Root password rootpw --iscrypted $6$w6X5WYQDyMeAizfs$TFKls9Kuj4Jv6PNKcMZ2BmB1Z/dvRCRkGD9uzm0n8te2UwDgdPCPGkUxCPvExKGenCMINTMcjSH55bCWYDiHx. %addon com_redhat_kdump --disable --reserve-mb=\u0026#39;128\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/isoft_6.0/isos/x86_64/Packages/syslinux-nonlinux-6.04-7.oe1.isoft.noarch /root/ rpm2cpio syslinux-nonlinux-6.04-7.oe1.isoft.noarch | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-nonlinux-6.04-7.oe1.isoft.noarch | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/ks/ks-isoft-6.0-x86.cfg icloud_1.0_x86 http服务配置 mkdir -p /var/www/html/icloud_1.0/isos/x86_64/ # 挂载镜像 mount -o loop /root/i-CloudOS-1.0-x86_64-202108131137.iso /var/www/html/icloud_1.0/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-icloud-1.0-x86.cfg chmod -R 755 /var/www/html ks-icloud-1.0-x86.cfg 文件内容\n#version=RHEL8 ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/icloud_1.0/isos/x86_64/ # Keyboard layouts keyboard --vckeymap=us --xlayouts=\u0026#39;\u0026#39; # System language lang zh_CN.UTF-8 # Root password rootpw --iscrypted 123.com # Run the Setup Agent on first boot firstboot --enable # Do not configure the X Window System skipx # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^vmserver-compute-node %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/icloud_1.0/isos/x86_64/Packages/syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm /root/ rpm2cpio syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default # 下面这三个文件centos7可以不要，centos8对于这三个文件有一定依赖性 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault menu.c32 timeout 30 menu title i-CloudOS 1.0 label linux menu label ^Install i-CloudOS 1.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/icloud_1.0/isos/x86_64/ks-icloud-1.0-x86.cfg openeuler_20.03-LTS-SP1_x86 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/ # 挂载镜像文件 mount -o loop /root/iSoft-Taiji-Server-OS-6.0-x86_64-rc1-202112311623.iso /var/www/html/isoft_6.0/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-openeuler-20.03-LTS-x86.cfg chmod -R 755 /var/www/html /var/www/html/ks/ks-openeuler-20.03-LTS-x86.cfg 文件内容\n# Use graphical install graphical install url --url=http://1.1.1.21/openeuler_20.03-LTS-SP1/isos/x86_64/ %packages @^minimal-environment %end # Keyboard layouts keyboard --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=ens33 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # System timezone timezone Asia/Shanghai --isUtc # Root password rootpw --iscrypted $6$w6X5WYQDyMeAizfs$TFKls9Kuj4Jv6PNKcMZ2BmB1Z/dvRCRkGD9uzm0n8te2UwDgdPCPGkUxCPvExKGenCMINTMcjSH55bCWYDiHx. %addon com_redhat_kdump --disable --reserve-mb=\u0026#39;128\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/Packages/syslinux-nonlinux-6.04-5.oe1.noarch.rpm /root/ rpm2cpio syslinux-nonlinux-6.04-5.oe1.noarch.rpm | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-nonlinux-6.04-5.oe1.noarch.rpm | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/ks/ks-isoft-6.0-x86.cfg aarch64 服务端配置 dhcp服务配置 vim /etc/dhcp/dhcpd.conf\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; next-server 1.1.1.21; # 本机ip（tftpserver的ip） filename \u0026#34;grubaa64.efi\u0026#34;; } systemctl restart dhcpd isoft_6.0_aarch64 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_6.0/isos/aarch64/ # 挂载镜像文件 mount -o loop /root/iSoft-Taiji-Server-OS-6.0-aarch64-202201240952.iso /var/www/html/isoft_6.0/isos/aarch64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-isoft-6.0-aarch64.cfg chmod -R 755 /var/www/html ks-isoft-6.0-aarch64.cfg 文件内容\n#version=DEVEL ignoredisk --only-use=vda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/isoft_6.0/isos/aarch64 # Keyboard layouts keyboard --vckeymap=cn --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=enp3s0 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$x94MGsfCoFdE/G4O$MEakgOwtq0O5i4pRIVzXntKQuMJVh9CJ3anhZKl8YZhZDtSXhzuMk5mpDr3wu..rDareWgy5tjsepCaGiPK3g/ # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^mate-desktop-environment %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* cp /var/www/html/isoft_6.0/isos/aarch64/EFI/BOOT/grub.cfg /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/aarch64/EFI/BOOT/grubaa64.efi /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/aarch64/images/pxeboot/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/aarch64/images/pxeboot/initrd.img /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/grub.cfg\nset default=\u0026#34;1\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=6 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iSoft-Taiji-Server-OS-6.0\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iSoft-Taiji-Server-OS 6.0 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { set root=(tftp,1.1.1.21) linux /vmlinuz ro inst.geoloc=0 console=ttyAMA0 console=tty0 rd.iscsi.waitnet=0 inst.repo=http://1.1.1.21/isoft_6.0/isos/aarch64/ inst.ks=http://1.1.1.21/ks/ks-isoft-6.0-aarch64.cfg initrd /initrd.img } } icloud_1.0_aarch64 这里 iso 没有直接挂载到 apache 目录，是因为该 iso 文件 Packages 目录中有个别软件包没有读取权限，直接挂载无法修改权限\nhttp服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/icloud_1.0/isos/aarch64/ # 挂载镜像文件 mount -o loop /root/iCloudOS-1.0-aarch64-2021-0805-1423-test-1.iso /mnt/ cp -r /mnt/* /var/www/html/icloud_1.0/isos/aarch64/ # 上传 ks.cfg 应答文件 vim /var/www/html/ks/ks-icloud-1.0-aarch64.cfg chmod -R 755 /var/www/html ks-icloud-1.0-aarch64.cfg 文件内容\n#version=RHEL8 ignoredisk --only-use=vda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/icloud_1.0/isos/aarch64/ # Keyboard layouts keyboard --vckeymap=us --xlayouts=\u0026#39;\u0026#39; # System language lang zh_CN.UTF-8 # Root password rootpw --iscrypted 123.com # Run the Setup Agent on first boot firstboot --enable # Do not configure the X Window System skipx # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^vmserver-compute-node %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* cp /var/www/html/icloud_1.0/isos/aarch64/EFI/BOOT/grub.cfg /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/aarch64/EFI/BOOT/grubaa64.efi /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/aarch64/images/pxeboot/vmlinuz /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/aarch64/images/pxeboot/initrd.img /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/grub.cfg\nset default=\u0026#34;1\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=6 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iCloudOS-1.0-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iCloudOS 1.0 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { set root=(tftp,1.1.1.21) linux /vmlinuz ro inst.geoloc=0 console=ttyAMA0 console=tty0 rd.iscsi.waitnet=0 inst.repo=http://1.1.1.21/icloud_1.0/isos/aarch64 inst.ks=http://1.1.1.21/ks/ks-icloud-1.0-aarch64.cfg initrd /initrd.img } openeuler_20.03-LTS_aarch64 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/openeuler_20.03-LTS/isos/aarch64/ # 挂载镜像文件 mount -o loop /root/openEuler-20.03-LTS-aarch64-dvd.iso /var/www/html/openeuler_20.03-LTS/isos/aarch64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-openeuler-20.03-LTS-aarch64.cfg chmod -R 755 /var/www/html ks-openeuler-20.03-LTS-aarch64.cfg 文件内容\n#version=DEVEL ignoredisk --only-use=vda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/openeuler_20.03-LTS/isos/aarch64 # Keyboard layouts keyboard --vckeymap=cn --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=enp3s0 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$x94MGsfCoFdE/G4O$MEakgOwtq0O5i4pRIVzXntKQuMJVh9CJ3anhZKl8YZhZDtSXhzuMk5mpDr3wu..rDareWgy5tjsepCaGiPK3g/ # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^minimal-environment %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 rm -rf /var/lib/tftpboot/* cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/EFI/BOOT/grub.cfg /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/EFI/BOOT/grubaa64.efi /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/images/pxeboot/vmlinuz /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/images/pxeboot/initrd.img /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/grub.cfg\nset default=\u0026#34;1\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=60 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;openEuler-20.03-LTS-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install openEuler 20.03 LTS\u0026#39; --class red --class gnu-linux --class gnu --class os { set root=(tftp,1.1.1.21) linux /vmlinuz ro inst.geoloc=0 console=ttyAMA0 console=tty0 rd.iscsi.waitnet=0 inst.repo=http://1.1.1.21/openeuler_20.03-LTS/isos/aarch64/ inst.ks=http://1.1.1.21/ks/ks-openeuler-20.03-LTS-aarch64.cfg initrd /initrd.img } } 参考 https://docs.openeuler.org/zh/docs/20.03_LTS_SP1/docs/Installation/%E4%BD%BF%E7%94%A8kickstart%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%89%E8%A3%85.html\nhttps://blog.csdn.net/weixin_45651006/article/details/103067283\nhttps://blog.csdn.net/qq_44839276/article/details/106980334\n","permalink":"https://www.lvbibir.cn/en/posts/tech/pxe-install-scripts/","summary":"前言 测试环境： x86_64（amd ryzen 7 4800u）：vmware workstation V16.1.2 aarch64（kunpeng 920）： kvm-2.12 注意测试的网络环境中不要存在其他的dhcp服务 注意测试虚拟机内存尽量大于4G，否则会报错 no space left 或者测试机直接黑屏 注意 ks.cfg 尽量在当前环境先手动安装一台模板机，使用模板机生成的","title":"pxe 安装配置大全"},{"content":"前言 书名《人间失格》，北京燕山出版社，译者高艳\n语句摘录 人是不可能一边笑还一边紧紧攥着拳头的，只有猴子才会这样\n女人如果突然哭起来，只要让她们吃些好吃的东西，她们就会立刻好转\n越是对人感到恐惧的人，反倒越希望亲眼看到狰狞恐怖的怪物；越是胆小怯懦、神经兮兮的人，越是期盼暴风雨来的更猛烈一些\n世上所有人的说话方式，都喜欢这样绕圈子，不明说，也不说破，带着想逃避责任的心理，复杂又微妙\n我对死倒是不在乎，但如果因受伤变成残疾人，我是接受不了的\n“你喝太多酒了。”\n“不喝了！从明天起，我滴酒不沾了！”\n“真的？”\n“真的，我一定戒。假如我戒了，良子肯嫁给我吗？” 说要娶她的事，其实是一句玩笑话。\n“当然了。”\n“好，那我们就一言为定。我肯定戒酒”\n可第二天，我又照样从中午起便捏起酒盅来。傍晚时分，我摇摇晃晃走出酒馆，站在由子家的铺子前。\n“良子，对不起，我又喝酒了。”\n“哎呀，真讨厌，故意装成一副喝醉的样子。”\n我被她的话吓了一跳，酒一下子醒了许多。\n“不，是真的。我真喝酒了，不是故意装成喝醉的样子。”\n“别捉弄我，你真坏。” 她对我丝毫没有疑心。\n“你一看不就明白了？我今天又从中午开始喝酒了。原谅我！”\n“你演戏演得真像。”\n“不是演戏，你这个傻丫头！当心我亲你哦。”\n“亲呀！”\n“不，我没有资格亲你。要你嫁给我的事，就此作罢吧。你看我的脸，通红通红的是吧？我确实喝了。”\n“那是因为夕阳照在脸上的缘故，你骗我也没用的。因为我们昨天说定了，你不可能去喝酒的，你承诺过我，你却说自己喝酒了，肯定是在骗人、骗人、骗人！”\n","permalink":"https://www.lvbibir.cn/en/posts/read/ren-jian-shi-ge/","summary":"前言 书名《人间失格》，北京燕山出版社，译者高艳 语句摘录 人是不可能一边笑还一边紧紧攥着拳头的，只有猴子才会这样 女人如果突然哭起来，只要让她们吃些好吃的东西，她们就会立刻好转 越是对人感到恐惧的人，反倒越希望亲眼看到狰狞恐怖的怪物；越是胆小怯懦、神经兮兮的人，越是期盼暴风雨来的更猛烈一","title":"《人间失格》"},{"content":"2308_5_25km 经历了初阳-\u0026gt;冬天太冷-\u0026gt;二阳-\u0026gt;换工作以及最关键的开始变 懒 , 总算开始恢复正常的跑步节奏了, 大半年的时间体能下降很严重, 体重也涨了起来, 这次一定要熬过冬天\n怎么恢复跑感觉比从零开始还要难啊 o(╥﹏╥)o\n2212-2307_15_66.81km 2211_8_51.04km 2210_16_100.45km 2209_13_123.96km 月跑量123.96km，完成13次\n220917-第一次半程马拉松 2208_15_124.02km 月跑量124.02km，完成15次\n220827-第一次15公里 2207_15_78.64km 月跑量78.64km，完成15次\n220728-第一次10公里 220706-第一次5公里 目前总跑量37公里，第一次不休息完成了5公里\n存在问题：步频较慢，心率太高\n后续计划：尝试提高步频，继续坚持5公里\n2206_10_26km 月跑量26km，完成10次\n","permalink":"https://www.lvbibir.cn/en/posts/life/running/","summary":"2308_5_25km 经历了初阳-\u0026gt;冬天太冷-\u0026gt;二阳-\u0026gt;换工作以及最关键的开始变 懒 , 总算开始恢复正常的跑步节奏了, 大半年的时间体能下降很严重, 体重也涨了起来, 这次一定要熬过冬天 怎么恢复跑感觉比从零开始还要难啊 o(╥﹏╥)o 2212-2307_15_66.81km 2211_8_51.04km 2210_16_100.45km 2209_13_123.96km 月跑量123.96km，完成13次 220917-第一","title":"跑步日常"},{"content":"前言 cve 官网或者工信部会发布一些 cve 漏洞，可以看到该漏洞在某次 commit 提交代码后修复的。\n可以通过检索 kernel.org 中所有内核版本的 ChangeLog 文件中是否包含该 commit 来判断漏洞影响的内核版本（仅针对 linux 的 kernel 相关的漏洞）\n脚本 #!/bin/bash # author: lvbibir # date: 2022-06-23 # 检索 kernel.org 下的所有 ChangeLog 文件，是否包含某项特定的 commit 号 commit=\u0026#39;520778042ccca019f3ffa136dd0ca565c486cedd\u0026#39; version=4 number=0 curl -ks https://cdn.kernel.org/pub/linux/kernel/v$version\\.x/ \u0026gt; list_$version cat list_$version | grep Change | grep -v sign | awk -F\\\u0026#34; \u0026#39;{print $2}\u0026#39; \u0026gt; list_$version\\_cut total=`wc -l list_$version\\_cut | awk \u0026#39;{print $1}\u0026#39;` while read line; do let \u0026#39;number+=1\u0026#39; url=\u0026#34;https://cdn.kernel.org/pub/linux/kernel/v$version.x/$line\u0026#34; echo -e \u0026#34;\\033[31m---------------------正在检索$url----------------第$number 个文件，共$total 个文件\\033[0m\u0026#34; curl -ks $url | grep $commit if [ $? -eq 0 ]; then echo $url \u0026gt;\u0026gt; ./result_$version fi done \u0026lt; ./list_$version\\_cut echo -e \u0026#34;\\033[32m脚本执行完成，结果已保存至当前目录的 result_$version \\033[0m\u0026#34; ","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-search-url-files/","summary":"前言 cve 官网或者工信部会发布一些 cve 漏洞，可以看到该漏洞在某次 commit 提交代码后修复的。 可以通过检索 kernel.org 中所有内核版本的 ChangeLog 文件中是否包含该 commit 来判断漏洞影响的内核版本（仅针对 linux 的 kernel 相关的漏洞） 脚本 #!/bin/bash # author: lvbibir # date: 2022-06-23 # 检索 kernel.org 下的所有 ChangeLog 文件，是否包含某项特定的 commit 号 commit=\u0026#39;520778042ccca019f3ffa136dd0ca565c486cedd\u0026#39; version=4 number=0 curl -ks https://cdn.kernel.org/pub/linux/kernel/v$version\\.x/ \u0026gt; list_$version cat list_$version | grep Change | grep -v sign | awk","title":"shell | 检索某url中所有文件的内容"},{"content":"git命令 submodule 当clone一个含有子模块的git仓库时可以使用如下命令安装所有子模块\ngit submodule init git submodule update git配置 查看 git 设置\n# 当前仓库 git config --list # 全局配置 git config --global --list 设置代理 设置全局代理，使用 http 代理\ngit config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 取消 github.com 代理\ngit config --global --unset http.https://github.com.proxy git config --global --unset https.https://github.com.proxy 设置全局代理，使用 socks5 代理\ngit config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 取消全局代理\ngit config --global --unset http.proxy git config --global --unset https.proxy 只对 github.com 使用代理\ngit config --global http.https://github.com.proxy http://127.0.0.1:7890 git config --global https.https://github.com.proxy http://127.0.0.1:7890 CRLF 和 LF # 提交时转换为LF，检出时转换为CRLF git config --global core.autocrlf true # 提交时转换为LF，检出时不转换 git config --global core.autocrlf input # 提交检出均不转换 git config --global core.autocrlf false # 拒绝提交包含混合换行符的文件 git config --global core.safecrlf true # 允许提交包含混合换行符的文件 git config --global core.safecrlf false # 提交包含混合换行符的文件时给出警告 git config --global core.safecrlf warn 常见问题 git clone 报错 fatal: early EOF\nfatal: fetch-pack: invalid index-pack output\n解决\ngit config --global http.sslVerify \u0026#34;false\u0026#34; git config --global core.compression -1 ","permalink":"https://www.lvbibir.cn/en/posts/tech/git/","summary":"git命令 submodule 当clone一个含有子模块的git仓库时可以使用如下命令安装所有子模块 git submodule init git submodule update git配置 查看 git 设置 # 当前仓库 git config --list # 全局配置 git config --global --list 设置代理 设置全局代理，使用 http 代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 取消 github.com 代理 git config --global --unset http.https://github.com.proxy git config --global --unset https.https://github.com.proxy 设置全局代理，使用 socks5 代理 git config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 取消全","title":"git"},{"content":"shell 脚本通常有 sh filename、bash filename、./filename、source filename 这四种执行方式\nsource filename 可以使用 . filename 代替，在当前的 bash 环境下读取并执行脚本文件中的命令，且脚本文件的变量，在脚本执行完成后会保存下来 ./filename 和 sh filename 或者 bash filename 是等效的，都是开启一个子shell来运行脚本文件，脚本中设置的变量执行完毕后不会保存 除./filename 外，source filename 、. filename 、sh filename 、bash filename 都是不需要执行权限的\n变量和权限问题示例\n# 设置临时变量，仅在当前 bash 环境生效 [root@lvbibir ~]# name=lvbibir [root@lvbibir ~]# echo $name lvbibir [root@lvbibir ~]# [root@lvbibir ~]# cat test.sh #!/bin/bash echo $name # source 或者 . 可以获取到父 bash 环境的变量 [root@lvbibir ~]# source test.sh lvbibir [root@lvbibir ~]# . test.sh lvbibir # sh、bash、./三种方式都使用了子 bash 环境，所以无法获取父 bash 环境的变量 # ./ 方式需要脚本有执行权限 [root@lvbibir ~]# sh test.sh [root@lvbibir ~]# bash test.sh [root@lvbibir ~]# ./test.sh -bash: ./test.sh: Permission denied [root@lvbibir ~]# chmod a+x test.sh [root@lvbibir ~]# ./test.sh 同理，使用 source 或者 . 也可以在 bash 环境中获取到脚本中设置的变量\n[root@lvbibir ~]# cat \u0026gt; test.sh \u0026lt;\u0026lt; EOF \u0026gt; #!/bin/bash \u0026gt; number=22 \u0026gt; \u0026gt; EOF [root@lvbibir ~]# echo $number # sh bash ./ 三种方式无法获取脚本中的变量 [root@lvbibir ~]# [root@lvbibir ~]# sh test.sh [root@lvbibir ~]# echo $number [root@lvbibir ~]# bash test.sh [root@lvbibir ~]# echo $number [root@lvbibir ~]# ./test.sh [root@lvbibir ~]# echo $number # source 方式可以获取脚本中的变量 [root@lvbibir ~]# source test.sh [root@lvbibir ~]# echo $number 22 [root@lvbibir ~]# 其他问题 关于是否在子 bash 环境运行的区别出了变量问题还会存在一些其他影响，如下测试\n已知目前存在一个 mysqld 进程，其 pid 为 29426 ，写一个监控pid的脚本\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | awk \u0026#39;{print $4}\u0026#39;) echo $pid 两种方式分别运行一下\n[root@lvbibir ~]# sh test.sh mysqld 27038 27039 29426 [root@lvbibir ~]# bash test.sh mysqld 27047 27048 29426 [root@lvbibir ~]# ./test.sh mysqld 27056 27057 29426 [root@lvbibir ~]# [root@lvbibir ~]# source test.sh mysqld 29426 [root@lvbibir ~]# . test.sh mysqld 29426 [root@lvbibir ~]# 问题出现了，由于某种原因导致子 bash 环境中执行的脚本监控到多个 pid ，给脚本添加个 sleep 来看下\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | awk \u0026#39;{print $4}\u0026#39;) echo $pid sleep 30 [root@lvbibir ~]# ./test.sh mysqld 27396 27397 29426 新开一个终端，查看进程\n第一个pid是在子shell中执行监控脚本的进程号 第二个pid不太清楚哪里来的，也grep不到这个进程号，应该是脚本执行一瞬间就释放掉了 第三个pid是mysql实际运行中的进程号 实际中脚本的pid和mysqld的pid顺序不太一样，取决于pid的大小\n在脚本再添加个 grep 过滤掉脚本本身的进程来规避这个问题\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | grep -v bash | awk \u0026#39;{print $4}\u0026#39;) echo $pid [root@lvbibir ~]# ./test.sh mysqld 29426 参考 https://blog.csdn.net/houxiaoni01/article/details/105161356\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-different-execution-mode/","summary":"shell 脚本通常有 sh filename、bash filename、./filename、source filename 这四种执行方式 source filename 可以使用 . filename 代替，在当前的 bash 环境下读取并执行脚本文件中的命令，且脚本文件的变量，在脚本执行完成后会保存下来 ./filename 和 sh filename 或者 bash filename 是等效的，都是开启一个子shell来运行脚本文","title":"shell | 不同执行方式的区别"},{"content":"前言 shell脚本是没有debug模式的，不过可以通过 set 指令实现简单的debug功能\nshell脚本中默认每条指令都会从上到下依次执行，但是当某行指令报错时，我们大多数情况下是不希望继续执行后续指令的\n这时可以使用shell脚本中 set 指令的四个参数：-e、-u、-x、-o pipefail\n命令报错即返回值（$?）不为0\nset -e set -e 选项可以在脚本出现异常的时候立即退出，后续命令不再执行，相当于打上了一个断点\nif 判断条件里出现异常也会直接退出，如果不希望退出可以在判断语句后面加上 || true 来阻止退出\nbefore 脚本内容\nfoo是一个不存在的命令，用于模拟命令报错\n#!/bin/bash foo echo \u0026#34;hello\u0026#34; 执行结果\n./test.sh: line 3: foo: command not found hello after 脚本内容\n#!/bin/bash set -e foo echo \u0026#34;hello\u0026#34; 执行结果\n./test.sh: line 5: foo: command not found 阻止立即退出的例子 #!/bin/bash set -e foo || true echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found hello set -o pipefail 默认情况下 bash 只会检查管道（pipelie）操作的最后一个命令的返回值，即最后一个命令返回值为 0 则判断整条管道语句是正确的\n如下\nset -o pipefail 的作用就是管道中只要有一个命令失败，则整个管道视为失败\nbefore #!/bin/bash set -e foo | echo \u0026#34;a\u0026#34; echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found a hello after #!/bin/bash set -eo pipefail foo | echo \u0026#34;a\u0026#34; echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found a set -u set -u 的作用是将所有未定义的变量视为错误，默认情况下 bash 会将未定义的变量视为空\nbefore #!/bin/bash set -eo pipefail echo $a echo \u0026#34;hello\u0026#34; hello after #!/bin/bash set -euo pipefail echo $a echo \u0026#34;hello\u0026#34; ./test.sh: line 5: a: unbound variable set -x set -x 可以让 bash 把每个命令在执行前先打印出来，好处显而易见，可以快速方便的找到出问题的脚本位置，坏处就是 bash 的 log 会格外的乱\n另外，它在打印的时候会先把变量解析出来\n纵然 log 可能会乱一些，但也比debug的时候掉头发强\n#!/bin/bash set -euox pipefail a=2 echo $a echo \u0026#34;hello\u0026#34; + a=2 + echo 2 # 这里已经将变量 a 解析为 2 了 2 + echo hello hello 参考 https://zhuanlan.zhihu.com/p/107135290\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-enable-debug-mode/","summary":"前言 shell脚本是没有debug模式的，不过可以通过 set 指令实现简单的debug功能 shell脚本中默认每条指令都会从上到下依次执行，但是当某行指令报错时，我们大多数情况下是不希望继续执行后续指令的 这时可以使用shell脚本中 set 指令的四个参数：-e、-u、-x、-o pipefail 命令报错即返","title":"shell | 开启debug模式"},{"content":"前言 在工作中需要连接公司内网（有线，不可联网），访问外网时需要连接无线\n同时接入这两个网络时，内网访问正常，外网无法访问。\n此时可以通过调整网络优先级及配置路由实现内外网同时访问\n一般来说，内网的网段数量较少，我们可以配置使默认路由走外网，走内网时通过配置的静态路由\ncentos8 在 linux 系统中网络优先级是通过 metric 控制的，值越小，优先级越高，通过route -n 查看路由\n可以通过修改配置文件实现，在网卡配置文件中添加或者修改 IPV4_ROUTE_METRIC=100 参数实现，之后重启网络服务\n# network systemctl restart network # NetworkManager nmcli c reload nmcli c down enp3s0 nmcli c up enp3s0 route -n 添加路由 临时添加静态路由命令如下（重启服务器或者重启网络服务后消失）\nroute add -net 192.168.45.0 netmask 255.255.255.0 dev enp4s0 metric 3 永久添加静态路由\n参照 /etc/init.d/network 中对 /etc/sysconfig/static-routes 是如何处理的\n/etc/sysconfig/static-routes 文件不存在的话，创建一个即可\n# Add non interface-specific static-routes. if [ -f /etc/sysconfig/static-routes ]; then if [ -x /sbin/route ]; then grep \u0026#34;^any\u0026#34; /etc/sysconfig/static-routes | while read ignore args ; do /sbin/route add -$args done else net_log $\u0026#34;Legacy static-route support not available: /sbin/route not found\u0026#34; fi fi 则，如果添加一条静态路由的路由如下\nroute add -net 192.168.45.0 netmask 255.255.255.0 dev enp4s0 metric 3 那么，在 /etc/sysconfig/static-routes 中对应的则应该写为\nany -net 192.168.45.0 netmask 255.255.255.0 dev enp4s0 metric 3 win10 调整网络优先级 查看默认路由\nroute print 0.0.0.0 这两个路由分别是内网和外网的默认路由，绝大部分情况网络都是走的默认路由，但这里有两条默认路由，默认路由的优先级是按照跃点数的多少决定的，跃点数越少，优先级越高\n将外网无线的跃点数调小\nroute print可以看到跃点数修改成功了，此时外网无线的跃点数更小，优先级更高\n配置路由 配置路由需要以管理员权限运行powershell或者cmd\n配置路由后，内网访问也没有问题了\nroute add 172.16.2.0 mask 255.255.255.0 172.30.4.254 metric 3 route add 172.16.3.0 mask 255.255.255.0 172.30.4.254 metric 3 route add 172.16.4.0 mask 255.255.255.0 172.30.4.254 metric 3 这里配置的路由重启系统后会消失，加 -p选项设置为永久路由\nroute add -p 172.16.2.0 mask 255.255.255.0 172.30.4.254 metric 3 ","permalink":"https://www.lvbibir.cn/en/posts/tech/network-priority/","summary":"前言 在工作中需要连接公司内网（有线，不可联网），访问外网时需要连接无线 同时接入这两个网络时，内网访问正常，外网无法访问。 此时可以通过调整网络优先级及配置路由实现内外网同时访问 一般来说，内网的网段数量较少，我们可以配置使默认路由走外网，走内网时通过配置的静态路由 centos8 在 linux 系统中网络优先","title":"windows \u0026 linux 多网卡时设置默认路由以及添加静态路由"},{"content":"0. 前言 基于 centos-7.9 mysql-5.7.42\nmysql 安装参考 mysql 系列文章\n参考:\n【MySQL】主从复制实现原理详解 1. 主从复制原理 主从复制涉及到 3 个线程, 4 个文件\n1.1 线程 master:\nlog dump : 当 slave 连接 master 时, 主节点会为其创建一个 log dump 线程, 用于发送和读取 binlog 的内容. 在读取 binlog 中的操作时, log dump 线程会对主节点上的 binlog 加锁, 当读取完成, 在发送给从节点之前, 锁会被释放.\n主节点会为自己的每一个从节点创建一个 log dump 线程 .\nslave:\nIO : 接受 master 发送的 binlog 文件位置的副本. 然后将数据的更新记录到 relaylog 中. SQL : 负责读取 relaylog 中的内容, 解析成具体的操作并执行, 最终保证主从数据的一致性 1.2 文件 master:\nbinlog : 二进制文件, 记录库中的信息 slave:\nrelaylog : 中继日志, 用来同步 master 的 binlog relaylog.info : 记录文件复制的进度. master.info : 存放 master 信息, 以及上次读取到的 master 同步过来的 binlog 的位置 1.3 详细步骤 slave 执行 change master to 命令( master 的连接信息+复制的起点), 将以上信息记录到 master.info 文件 slave 执行 start slave 命令,开启 IO 线程和 SQL 线程 slave 的 IO 线程读取 master.info 文件中的信息, 获取到 IP, PORT, User, Pass, binlog 的位置信息 slave 的 IO 线程请求连接 master, master 提供一个 log dump 线程, 负责和 IO 线程交互 IO 线程根据 binlog 的位置信息(mysql-bin.000004 , 444), 请求 master 新的 binlog master 通过 log dump 线程将最新的 binlog 传输给 slave 的 IO 线程 IO 线程接收到新的 binlog 日志, 存储到 TCP/IP 缓存, 立即返回 ACK 给 master , 并更新 master.info IO 线程将 TCP/IP 缓存中的数据, 转储到磁盘 relaylog 中 SQL 线程读取 relaylog.info 中的信息, 获取到上次已经应用过的 relaylog 的位置信息 SQL 线程会按照上次的位置点回放最新的 relaylog, 再次更新 relaylog.info 信息 slave 会自动 purge 应用过的 relaylog 进行定期清理 2. 主从复制模式 mysql 默认一般为异步同步数据\n2.1 全同步 当 mster 执行完一个事务, 然后所有的 slave 都复制了该事务并成功执行完才返回成功信息给客户端. 因为需要等待所有 slave 执行完该事务才能返回成功信息, 所以全同步复制的性能必然会收到严重的影响.\n2.2 半同步 介于异步复制和全同步复制之间, master 在执行完客户端提交的事务后不是立刻返回给客户端, 而是等待至少一个 slave 接收到并写到 relaylog 中才返回成功信息给客户端(只能保证 master 的 binlog 至少传输到了一个 slave 上, 但并不能保证 slave 将此事务执行更新到db中), 否则需要等待直到超时时间然后切换成异步模式再提交. 相对于异步复制, 半同步复制提高了数据的安全性, 一定程度的保证了数据能成功备份到 slave, 同时它也造成了一定程度的延迟, 但是比全同步模式延迟要低, 这个延迟最少是一个 TCP/IP 往返的时间. 所以, 半同步复制最好在低延时的网络中使用.\n2.3 异步 master 不会主动推送 binlog 到 slave, master 在执行完客户端提交的事务后会立即将结果返给给客户端, 并不关心 slave 是否已经接收并处理, 这样就会有一个问题, master 如果崩溃掉了, 此时 master 上已经提交的事务可能并没有传到 slave 上, 如果此时, 强行将 slave 提升为 master, 可能导致新 master 节点上的数据不完整.\n3. 主从复制方式 MySQL 主从复制有三种方式: 基于SQL语句的复制(statement-based replication, SBR), 基于行的复制(row-based replication, RBR), 混合模式复制(mixed-based replication, MBR). 对应的bin-log文件的格式也有三种: STATEMENT, ROW, MIXED\n可通过如下命令查看 binlog 格式\nSHOW VARIABLES LIKE \u0026#34;binlog_format\u0026#34;; 3.1 SBR 就是记录sql语句在bin-log中, Mysql 5.1.4 及之前的版本都是使用的这种复制格式. 优点是只需要记录会修改数据的sql语句到bin-log中, 减少了bin-log日质量, 节约I/O, 提高性能. 缺点是在某些情况下, 会导致主从节点中数据不一致(比如sleep(), now()等).\n3.2 RBR mysql master将SQL语句分解为基于Row更改的语句并记录在bin-log中, 也就是只记录哪条数据被修改了, 修改成什么样. 优点是不会出现某些特定情况下的存储过程、或者函数、或者trigger的调用或者触发无法被正确复制的问题. 缺点是会产生大量的日志, 尤其是修改table的时候会让日志暴增,同时增加bin-log同步时间. 也不能通过bin-log解析获取执行过的sql语句, 只能看到发生的data变更.\n3.3 MBR MySQL NDB cluster 7.3 和7.4 使用的MBR. 是以上两种模式的混合, 对于一般的复制使用STATEMENT模式保存到bin-log, 对于STATEMENT模式无法复制的操作则使用ROW模式来保存, MySQL会根据执行的SQL语句选择日志保存方式.\n4. GTID 复制 在原来基于日志的复制中, slave 需要告知 master 要从哪个偏移量进行增量同步, 如果指定错误会造成数据的遗漏, 从而造成数据的不一致.\n而基于 GTID 的复制中, slave 会告知 master 已经执行的事务的 GTID 的值, 然后 master 会将所有未执行的事务的 GTID 的列表返回给 slave. 并且可以保证同一个事务只在指定的 slave 执行一次. 通过全局的事务 ID 确定 slave 要执行的事务的方式代替了以前需要用 binlog 和 pos 点确定 slave 要执行的事务的方式.\nGTID 是由 server_uuid 和事物 id 组成, 格式为: GTID=server_uuid:transaction_id. server_uuid 是在数据库启动过程中自动生成, 每台机器的 server-uuid 不一样. uuid 存放在数据目录的 auto.cnf 文件中，而 transaction_id 就是事务提交时系统顺序分配的一个不会重复的序列号\nmaster 更新数据时, 会在事务前产生 GTID, 一起记录到 binlog 日志中. slave 的 IO 线程将变更的 binlog写入到本地的 relaylog 中. SQL 线程从 relaylog 中获取 GTID, 然后对比本地 binlog 是否有记录(所以 slave 必须要开启 binlog, 并且将 log_slave_updates 设置为 ON). 如果有记录，说明该 GTID 的事务已经执行, slave 会忽略. 如果没有记录, slave 就会从 relaylog 中执行该 GTID 的事务, 并记录到binlog. 在解析过程中会判断是否有主键, 如果没有就用二级索引, 如果有就用全部扫描.\n5. 并行复制 master 大多数情况下都是多线程多客户端去写, 而 slave 只有一个 SQL 线程进行写, 无法避免地会出现主从复制的延迟问题, 并行复制可以指定线程数量, 从而提高 slave 写的速度.\n在 mysql 5.6 版本之后引入了并行复制的概念\n通过上图我们可以发现其实所谓的并行复制, 就是在中间添加了一个分发的环节, 也就是说原来的 SQL 线程变成了现在的 coordinator 组件, 当 relaylog 日志更新后, coordinator 负责读取日志信息以及分发事务, 真正的执行过程是放在了 worker 线程上, 由多个线程并行的去执行.\n# 查看并行的slave的线程的个数，默认是0.表示单线程 show global variables like \u0026#39;slave_parallel_workers\u0026#39;; # 根据实际情况保证开启多少线程 set global slave_parallel_workers = 4; # 设置并发复制的方式，默认是一个线程处理一个库，值为database show global variables like \u0026#39;%slave_parallel_type%\u0026#39;; # 停止slave stop slave; # 设置属性值 set global slave_parallel_type=\u0026#39;logical_check\u0026#39;; # 开启slave start slave # 查看线程数 show full processlist; ","permalink":"https://www.lvbibir.cn/en/posts/tech/mysql-2-master-slave/","summary":"0. 前言 基于 centos-7.9 mysql-5.7.42 mysql 安装参考 mysql 系列文章 参考: 【MySQL】主从复制实现原理详解 1. 主从复制原理 主从复制涉及到 3 个线程, 4 个文件 1.1 线程 master: log dump : 当 slave 连接 master 时, 主节点会为其创建一个 log dump 线程, 用于发送和读取 binlog 的内容. 在读取 binlog 中的操作时, log dump 线程会对主节点上的 binlog 加锁, 当读取完成, 在发送给从节点之","title":"mysql (二) 主从复制原理 GTID 并行复制"},{"content":"0. 前言 基于 centos-7.9 mysql-5.7.42\n1. 基础环境 配置 hostname\nhostnamectl set-hostname master bash 关闭防火墙及 selinux\niptables -F systemctl disable firewalld systemctl stop firewalld sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 配置 yum 源\nmkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo # curl http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel.repo yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion ntpdate timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com 卸载自带的 mariadb\nyum remove -y $(rpm -qa | grep mariadb) 2. 配置 yum 源 提供清华源和官方源两种方式, 任选其一, 前者速度稍快一些\n2.1 清华源 这里使用的是清华的源\ncat \u0026gt; /etc/yum.repos.d/mysql-community.repo \u0026lt;\u0026lt; EOF [mysql-connectors-community] name=MySQL Connectors Community baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-connectors-community-el7-\\$basearch/ enabled=1 gpgcheck=1 gpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql [mysql-tools-community] name=MySQL Tools Community baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7-\\$basearch/ enabled=1 gpgcheck=1 gpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql [mysql-5.7-community] name=MySQL 5.7 Community Server baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-5.7-community-el7-\\$basearch/ enabled=1 gpgcheck=1 gpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql EOF 2.2 官方源 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm yum localinstall mysql57-community-release-el7-8.noarch.rpm 3. 安装 mysql 执行安装\nrpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022 yum install mysql-community-server 启动服务\nsystemctl enable --now mysqld 获取默认密码\n# grep \u0026#34;password\u0026#34; /var/log/mysqld.log 2023-05-07T02:31:54.375626Z 1 [Note] A temporary password is generated for root@localhost: QZFuIayXk0:l 如果没有返回，找不到root密码，解决方案如下\n# 删除原来安装过的mysql残留的数据 rm -rf /var/lib/mysql # 重启 mysqld 服务, 会重新初始化数据 systemctl restart mysqld # 再去找临时密码 grep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log 登录并修改密码\nmysql -uroot -p # 输入默认密码 ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;\u0026lt;pass\u0026gt;\u0026#39;; # 修改密码, 需要有大小写和特殊符号 exit; ","permalink":"https://www.lvbibir.cn/en/posts/tech/mysql-1-deploy/","summary":"0. 前言 基于 centos-7.9 mysql-5.7.42 1. 基础环境 配置 hostname hostnamectl set-hostname master bash 关闭防火墙及 selinux iptables -F systemctl disable firewalld systemctl stop firewalld sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 配置 yum 源 mkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo # curl http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel.repo yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion ntpdate timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com 卸载自带的 mariadb yum remove -y $(rpm -qa | grep mariadb) 2. 配置 yum 源 提供清华源和官方源两种方式, 任选其一, 前者速度稍快一些 2.1 清华源 这里使用的","title":"mysql (一) 部署"},{"content":"0. 前言 基于 centos-7.9 mysql-5.7.42\n","permalink":"https://www.lvbibir.cn/en/posts/tech/mysql-0-notes/","summary":"0. 前言 基于 centos-7.9 mysql-5.7.42","title":"mysql 杂记"},{"content":"前言 基础环境\n系统：Centos 7.9.2009 minimal 配置：4 cpus / 24G mem / 50G disk 网卡：1.1.1.4/24 我这里采用的是 all-in-one 的配置，即所有操作都在一台主机上，如资源充足可以将 jenkins和gitlab 与后续项目容器分开部署\n1. 系统配置 防火墙、selinux、yum\nsed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion unzip mkdir /mydata 2. docker 先安装docker-compose\nwget https://github.com/docker/compose/releases/download/v2.16.0/docker-compose-linux-x86_64 -O /usr/local/bin/docker-compose chmod a+x /usr/local/bin/docker-compose ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose docker-compose version 安装docker\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo yum -y install docker-ce mkdir -p /etc/docker # 镜像加速器 tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34;] } EOF 允许docker守护进程的tcp访问，为了后续jenkins构建时调用，以生成docker镜像\n[root@localhost ~]# vim /usr/lib/systemd/system/docker.service # 修改如下内容 # ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock systemctl daemon-reload systemctl restart docker \u0026amp;\u0026amp; systemctl enable docker 查看端口，确保修改正确\n[root@localhost ~]# ss -tnlp | grep 2375 LISTEN 0 128 [::]:2375 [::]:* users:((\u0026#34;dockerd\u0026#34;,pid=1124,fd=4)) 安装一系列后续需要的镜像，镜像文件比较大，这步比较耗时\ndocker pull jenkins/jenkins:lts docker pull gitlab/gitlab-ce:latest docker pull mysql:5.7 docker pull redis:7 docker pull nginx:1.22 docker pull rabbitmq:3.9-management docker pull elasticsearch:7.17.3 docker pull logstash:7.17.3 docker pull kibana:7.17.3 docker pull mongo:4 docker pull nacos/nacos-server:v2.1.0 3. jenkins 3.1 启动容器 docker run -d --restart=always \\ -p 8080:8080 -p 50000:5000 \\ --name jenkins -u root \\ -v /mydata/jenkins_home:/var/jenkins_home \\ jenkins/jenkins:lts # 获取初始管理员密码 [root@localhost ~]# cat /mydata/jenkins_home/secrets/initialAdminPassword bd5b64c7c8c8467985a0faa6fbe1848f 3.2 跳过在线验证 启动成功访问 http://1.1.1.4:8080 ，等出现密码界面后输入密码应该会进入一个离线页面，如下\n❗ 这个界面不要关，新开一个窗口访问 http://1.1.1.4:8080/pluginManager/advanced\n将 update site 的 url 修改为 http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json，这步是为了加速插件安装\n接下来跳过jenkins的在线验证，在终端再执行\ndocker exec -it jenkins /bin/sh -c \u0026#34;echo 127.0.0.1 www.google.com \u0026gt;\u0026gt; /etc/hosts\u0026#34; docker exec -it jenkins cat /etc/hosts 然后回到第一个离线页面刷新一下，应该可以看到离线状态消除了，这里是因为jenkins在 /mydata/jenkins_home/updates/default.json 中定义了通过访问 google 来判断 jenkins 节点是否是在线状态\n之后选择安装推荐的插件，进入插件安装界面，这个过程耗时会比较长，如果有插件安装失败可以重试\n之后创建管理员用户，一路确定后到主页\n3.3 插件配置 dashboard -\u0026gt; 系统管理 -\u0026gt; 插件管理中安装ssh插件和Role-based Authorization Strategy插件，安装完成后重启jenkins\n新增 ssh 凭据\n新增 ssh 配置，配置好之后右下角测试一下，连接正常后保存\n新增 maven 配置\n3.4 权限配置 我们可以使用Jenkins的角色管理插件来管理Jenkins的用户，比如我们可以给管理员赋予所有权限，运维人员赋予执行任务的相关权限，其他人员只赋予查看权限。\n在系统管理-\u0026gt;全局安全配置中启用基于角色的权限管理：\n关闭代理，保存\n分配管理员、运维和other三个角色，分别配置对应权限\n将用户和角色绑定\n4. gitlab 4.1 启动容器 docker run --detach --restart=always\\ -p 10443:443 -p 1080:80 -p 1022:22 \\ --name gitlab \\ --restart always \\ --volume /mydata/gitlab/config:/etc/gitlab \\ --volume /mydata/gitlab/logs:/var/log/gitlab \\ --volume /mydata/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest # 获取密码 docker exec -it gitlab grep \u0026#39;Password:\u0026#39; /etc/gitlab/initial_root_password 访问http://1.1.1.4:1080/，默认用户为root\n4.2 配置 配置中文，修改完后刷新网页即可\n修改默认密码\n4.3 上传项目 新建空白项目\n新建 mall-swarm 项目\nclone github上的原项目，我是windows系统，所以这里用的是git-bash\ngit clone https://github.com/macrozheng/mall-swarm.git cd mall-swarm # 重命名github远端仓库 git remote rename origin github # 添加gitlab仓库 git remote add gitlab http://1.1.1.4:1080/root/mall-swarm.git git remote -v 修改一下 docker.host 变量\n新建 commit 并提交到 gitlab 仓库，初次提交需要输入 gitlab 的用户名密码\ngit add . git commit -m \u0026#34;change docker.host -\u0026gt; 1.1.1.4\u0026#34; git push gitlab master 默认配置不合理，修改 docker-compose-env.yml 中 nginx 的配置文件挂载\n- /data/nginx/nginx.conf:/etc/nginx/nginx.conf #配置文件挂载 上传到gitlab\ngit add . git commit -m \u0026#34;update nginx volume config in document/docker/docker-compose.env.yml\u0026#34; git push gitlab master 5. 依赖服务部署 需要上传到服务器的配置文件准备，如下图所示，为了方便可以将整个document目录传到服务器\n5.1 前期配置 Elasticsearch\n设置内核参数，否则会因为内存不足无法启动\nsysctl -w vm.max_map_count=262144 sysctl -p 创建数据目录并设置权限，否则会报权限错误\nmkdir -p /mydata/elasticsearch/data/ chmod 777 /mydata/elasticsearch/data/ Nginx\n创建目录，上传配置文件 mkdir -p /mydata/nginx/conf/ cp /mydata/document/docker/nginx.conf /mydata/nginx/conf/ Logstash\n创建目录上传配置文件 mkdir /mydata/logstash cp /mydata/document/elk/logstash.conf /mydata/logstash/ 5.2 启动服务 docker-compose -f /mydata/document/docker/docker-compose-env.yml up -d docker-compose 会自动创建一个 docker_default 网络，所有容器都在这个网络下\n启动完成后 rabbitmq 由于权限问题未能正常启动，给 log 目录设置权限，再执行 docker-compose 启动异常的容器\nchmod 777 /mydata/rabbitmq/log/ docker-compose -f /mydata/document/docker/docker-compose-env.yml up -d 确保所有容器正常启动\ndocker ps | grep -v \u0026#34;Up\u0026#34; 5.3 服务配置 mysql\n需要创建 mall 数据库并授权给 reader 用户\n将 sql 文件拷贝到容器\ndocker cp /mydata/document/sql/mall.sql mysql:/ 进入mysql容器执行如下操作\n# 进入mysql容器 docker exec -it mysql /bin/bash # 连接到mysql服务 mysql -uroot -proot --default-character-set=utf8 # 创建远程访问用户 grant all privileges on *.* to \u0026#39;reader\u0026#39; @\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; # 创建mall数据库 create database mall character set utf8; # 使用mall数据库 use mall; # 导入mall.sql脚本 source /mall.sql; # 退出数据库 exit # 退出容器 ctrl + d Elasticsearch\n需要安装中文分词器 IKAnalyzer 下载地址\n注意版本需要与 elasticsearch 的版本一致\n上传到服务器并解压到 plugins 目录\nmkdir /mydata/elasticsearch/plugins/analysis-ik unzip /mydata/elasticsearch-analysis-ik-7.17.3.zip -d /mydata/elasticsearch/plugins/analysis-ik/ 重启容器\ndocker restart elasticsearch Logstash\n安装 json_lines 插件并重启\ndocker exec -it logstash /bin/bash logstash-plugin install logstash-codec-json_lines docker restart logstash rabbitmq\n需要创建一个mall用户并设置虚拟host为/mall\n访问管理页面: http://1.1.1.4:15672/ 默认账户密码: guest / guest\n创建管理员用户: mall / mall\n创建一个新的虚拟host为 /mall\n点击mall用户进入用户配置界面\n给mall账户配置虚拟host /mall 的权限\nnacos\n由于我们使用Nacos作为配置中心，统一管理配置，所以我们需要将项目config目录下的所有配置都添加到Nacos中 Nacos访问地址：http://1.1.1.4:8848/nacos/ 账号密码：nacos / nacos\n需要上传的配置\n上传配置\n全部上传完成\n6. jenkins手动发布项目 6.1 脚本配置 Jenkins自动化部署是需要依赖Linux执行脚本的\n添加执行权限\nchmod a+x /mydata/document/sh/*.sh 之前使用的是Docker Compose启动所有依赖服务，会默认创建一个网络，所有的依赖服务都会在此网络之中，不同网络内的服务无法互相访问。所以需要指定sh脚本中服务运行的的网络，否则启动的应用服务会无法连接到依赖服务。\n修改脚本内容，为每个脚本添加--network docker_default \\\nsed -i \u0026#39;/^docker run/ a\\--network docker_default \\\\\u0026#39; /mydata/document/sh/*.sh 确认修改是否成功\n6.2 jenkins配置 6.2.1 mall-admin工程配置 由于各个模块执行任务的创建都大同小异，下面将详细讲解mall-admin模块任务的创建，其他模块将简略讲解。\n源码管理\n创建一个构建，构建mall-swarm项目中的依赖模块，否则当构建可运行的服务模块时会因为无法找到这些模块而构建失败\n# 只install mall-common,mall-mbg两个模块 clean install -pl mall-common,mall-mbg -am 创建一个构建，单独构建并打包mall-admin模块\nclean package ${WORKSPACE}/mall-admin/pom.xml 再创建一个构建，通过SSH去执行sh脚本，这里执行的是mall-admin的运行脚本：\n6.2.2 其他模块工程配置 以 mall-gateway 为例\n输入任务名称，直接复制 mall-admin 工程配置\n修改第二步构建中的 pom 文件位置和第三步构建中的 sh 文件位置\n6.3 开始构建 单击开始构建即可开始构建任务，可以实时看到任务的控制台输出\n由于作为注册中心和配置中心的Nacos已经启动了，其他模块基本没有启动顺序的限制，但是最好还是按照下面的顺序启动。\n推荐启动顺序：\nmall-auth mall-gateway mall-monitor mall-admin mall-portal mall-search ","permalink":"https://www.lvbibir.cn/en/posts/tech/cicd-mall-swarm/","summary":"前言 基础环境 系统：Centos 7.9.2009 minimal 配置：4 cpus / 24G mem / 50G disk 网卡：1.1.1.4/24 我这里采用的是 all-in-one 的配置，即所有操作都在一台主机上，如资源充足可以将 jenkins和gitlab 与后续项目容器分开部署 1. 系统配置 防火墙、selinux、yum sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ curl http://mirrors.aliyun.com/repo/Centos-7.repo","title":"cicd | jenkins部署mall-swarm项目"},{"content":"系统版本：isoft-serveros-v4.2（centos7）\n源码下载链接：\nhttps://dlcdn.apache.org//apr/apr-1.7.0.tar.bz2\nhttps://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2\nhttps://dlcdn.apache.org//httpd/httpd-2.4.52.tar.bz2\n安装依赖\nyum install -y wget gcc rpm-build yum install -y autoconf zlib-devel libselinux-devel libuuid-devel apr-devel apr-util-devel pcre-devel openldap-devel lua-devel libxml2-devel openssl-devel yum install -y libtool doxygen yum install -y postgresql-devel mysql-devel sqlite-devel unixODBC-devel nss-devel libdb4-devel依赖需要使用epel源安装，这里使用阿里的epel源\n# 添加阿里yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 手动修改repo文件中的系统版本，因为本系统检测到的版本号是4 sed -i \u0026#39;s/$releasever/7/g\u0026#39; /etc/yum.repos.d/CentOS-Base.repo # 安装epel源 yum install -y epel-release # 安装libdb4-devel yum install -y libdb4-devel 编译准备\n[root@localhost ~]# mkdir -p /root/rpmbuild/{SPECS,SOURCES} [root@localhost ~]# cd /root/rpmbuild/SOURCES/ [root@localhost SOURCES]# wget --no-check-certificate https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 [root@localhost SOURCES]# tar jxf apr-1.7.0.tar.bz2 [root@localhost SOURCES]# tar jxf apr-util-1.6.1.tar.bz2 [root@localhost SOURCES]# tar jxf httpd-2.4.52.tar.bz2 [root@localhost SOURCES]# cp apr-1.7.0/apr.spec ../SPECS/ [root@localhost SOURCES]# cp apr-util-1.6.1/apr-util.spec ../SPECS/ [root@localhost SOURCES]# cp httpd-2.4.52/httpd.spec ../SPECS/ 开始编译\n[root@localhost SOURCES]# cd ../SPECS/ # 修改spec文件 [root@localhost SPECS]# vim apr.spec Release: 1%dist [root@localhost SPECS]# vim apr-util.spec Release: 1%dist [root@localhost SPECS]# vim httpd.spec Release: 1%dist [root@localhost SPECS]# rpmbuild -ba apr.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/apr-* [root@localhost SPECS]# rpmbuild -ba apr-util.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/apr-util-* [root@localhost SPECS]# rpmbuild -ba httpd.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/httpd-* [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/mod_* # 打包所有的软件包 [root@localhost ~]# tar zcf httpd-2.4.25.tar.gz x86_64/ 这里修改%dist是为了修改编译后生成的软件包的名字，dist具体代表什么可以在/etc/rpm/macros.dist文件中看到\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-build-httpd/","summary":"系统版本：isoft-serveros-v4.2（centos7） 源码下载链接： https://dlcdn.apache.org//apr/apr-1.7.0.tar.bz2 https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 https://dlcdn.apache.org//httpd/httpd-2.4.52.tar.bz2 安装依赖 yum install -y wget gcc rpm-build yum install -y autoconf zlib-devel libselinux-devel libuuid-devel apr-devel apr-util-devel pcre-devel openldap-devel lua-devel libxml2-devel openssl-devel yum install -y libtool doxygen yum install -y postgresql-devel mysql-devel sqlite-devel unixODBC-devel nss-devel libdb4-devel依赖需要使用epel源安装，这里使用阿里的epel源 # 添加阿里yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 手动修改repo","title":"httpd源码打包编译成rpm包"},{"content":"环境 iSoftserver-v4.2(Centos-7)\nopenssl version：1.0.2k\n编译 从github上看到的编译脚本，本地修改后：\n#!/bin/bash set -e set -v mkdir ~/openssl \u0026amp;\u0026amp; cd ~/openssl yum -y install \\ curl \\ which \\ make \\ gcc \\ perl \\ perl-WWW-Curl \\ rpm-build # Get openssl tarball cp /root/openssl-1.1.1m.tar.gz ./ # SPEC file cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; ~/openssl/openssl.spec Summary: OpenSSL 1.1.1m for Centos Name: openssl Version: %{?version}%{!?version:1.1.1m} Release: 1%{?dist} Obsoletes: %{name} \u0026lt;= %{version} Provides: %{name} = %{version} URL: https://www.openssl.org/ License: GPLv2+ Source: https://www.openssl.org/source/%{name}-%{version}.tar.gz BuildRequires: make gcc perl perl-WWW-Curl BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %global openssldir /usr/openssl %description OpenSSL RPM for version 1.1.1m on Centos %package devel Summary: Development files for programs which will use the openssl library Group: Development/Libraries Requires: %{name} = %{version}-%{release} %description devel OpenSSL RPM for version 1.1.1m on Centos (development package) %prep %setup -q %build ./config --prefix=%{openssldir} --openssldir=%{openssldir} make %install [ \u0026#34;%{buildroot}\u0026#34; != \u0026#34;/\u0026#34; ] \u0026amp;\u0026amp; %{__rm} -rf %{buildroot} %make_install mkdir -p %{buildroot}%{_bindir} mkdir -p %{buildroot}%{_libdir} ln -sf %{openssldir}/lib/libssl.so.1.1 %{buildroot}%{_libdir} ln -sf %{openssldir}/lib/libcrypto.so.1.1 %{buildroot}%{_libdir} ln -sf %{openssldir}/bin/openssl %{buildroot}%{_bindir} %clean [ \u0026#34;%{buildroot}\u0026#34; != \u0026#34;/\u0026#34; ] \u0026amp;\u0026amp; %{__rm} -rf %{buildroot} %files %{openssldir} %defattr(-,root,root) /usr/bin/openssl /usr/lib64/libcrypto.so.1.1 /usr/lib64/libssl.so.1.1 %files devel %{openssldir}/include/* %defattr(-,root,root) %post -p /sbin/ldconfig %postun -p /sbin/ldconfig EOF mkdir -p /root/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} cp ~/openssl/openssl.spec /root/rpmbuild/SPECS/openssl.spec mv openssl-1.1.1m.tar.gz /root/rpmbuild/SOURCES cd /root/rpmbuild/SPECS \u0026amp;\u0026amp; \\ rpmbuild \\ -D \u0026#34;version 1.1.1m\u0026#34; \\ -ba openssl.spec # Before Uninstall Openssl : rpm -qa openssl # Uninstall Current Openssl Vesion : yum -y remove openssl # For install: rpm -ivvh /root/rpmbuild/RPMS/x86_64/openssl-1.1.1m-1.el7.x86_64.rpm --nodeps # Verify install: rpm -qa openssl # openssl version 运行脚本\nchmod 755 install_openssl-1.1.1m.sh ./isntall_openssl-1.1.1m.sh tree rpmbuild/*RPMS 升级 rpm -e openssl --nodeps rpm -ivh openssl-1.1.1m-1.el7.isoft.x86_64.rpm --nodeps openssl version ","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-build-openssl/","summary":"环境 iSoftserver-v4.2(Centos-7) openssl version：1.0.2k 编译 从github上看到的编译脚本，本地修改后： #!/bin/bash set -e set -v mkdir ~/openssl \u0026amp;\u0026amp; cd ~/openssl yum -y install \\ curl \\ which \\ make \\ gcc \\ perl \\ perl-WWW-Curl \\ rpm-build # Get openssl tarball cp /root/openssl-1.1.1m.tar.gz ./ # SPEC file cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; ~/openssl/openssl.spec Summary: OpenSSL 1.1.1m for Centos Name: openssl Version: %{?version}%{!?version:1.1.1m} Release: 1%{?dist} Obsoletes: %{name} \u0026lt;= %{version} Provides: %{name} = %{version} URL: https://www.openssl.org/ License: GPLv2+ Source: https://www.openssl.org/source/%{name}-%{version}.tar.gz BuildRequires: make gcc perl perl-WWW-Curl BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %global openssldir /usr/openssl %description OpenSSL RPM for version 1.1.1m on Centos %package devel Summary: Development files for programs which will use the openssl library","title":"openssl源码打包编译成rpm包"},{"content":"前言 在使用 kolla-ansible 部署多节点 openstack 时，所有节点的外网网卡名称和管理网卡名称需要一样，其中两台是型号相同的物理机，网卡名无需变动，第三台是虚拟机，默认是 ens* 形式的网卡，需要改成 enp*s*f* 的格式\n修改配置文件 vim /etc/sysconfig/network-scripts/ifcfg-ens32 配置网络规则命名文件 vim /etc/udev/rules.d/70-persistent-ipoib.rules # 添加如下行，mac 地址自行修改 SUBSYSTEM==\u0026#34;net\u0026#34;, ACTION==\u0026#34;add\u0026#34;, DRIVERS==\u0026#34;?*\u0026#34;, ATTR{address}==\u0026#34;00:0c:29:bc:1e:01\u0026#34;, ATTR{type}==\u0026#34;1\u0026#34;, KERNEL==\u0026#34;eth*\u0026#34;, NAME=\u0026#34;enp11s0f0\u0026#34; 配置 grub 并重启 vim /etc/default/grub # 修改如下行 GRUB_CMDLINE_LINUX=\u0026#34;crashkernel=auto rd.lvm.lv=centos/root net.ifnames=0 rd.lvm.lv=centos/swap rhgb quiet\u0026#34; grub2-mkconfig -o /boot/grub2/grub.cfg 之后直接reboot重启系统\n参考 https://www.xmodulo.com/change-network-interface-name-centos7.html\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-change-network-card-name/","summary":"前言 在使用 kolla-ansible 部署多节点 openstack 时，所有节点的外网网卡名称和管理网卡名称需要一样，其中两台是型号相同的物理机，网卡名无需变动，第三台是虚拟机，默认是 ens* 形式的网卡，需要改成 enp*s*f* 的格式 修改配置文件 vim /etc/sysconfig/network-scripts/ifcfg-ens32 配置网络规则命名文件 vim /etc/udev/rules.d/70-persistent-ipoib.rules # 添加如下行，mac 地址自行修改 SUBSYSTEM==\u0026#34;net\u0026#34;, ACTION==\u0026#34;add\u0026#34;, DRIVERS==\u0026#34;?*\u0026#34;, ATTR{address}==\u0026#34;00:0c:29:bc:1e:01\u0026#34;, ATTR{type}==\u0026#34;1\u0026#34;, KERNEL==\u0026#34;eth*\u0026#34;, NAME=\u0026#34;enp11s0f0\u0026#34; 配置 grub 并重启 vim /etc/default/grub # 修改如","title":"centos7 修改网卡名称"},{"content":"pg_num 用此命令创建存储池时：\nceph osd pool create {pool-name} pg_num 确定pg_num取值是强制性的，因为不能自动计算。常用的较为通用的取值：\n少于5个osd，pg_num设置为128 osd数量在 5 到 10 个时，pg_num设置为512 osd数量在 10 到 50 个时，pg_num = 4096 osd数量大于50是，需要理解ceph的权衡算法，自己计算pg_num取值 自行计算pg_num取值时可使用ceph配套的pg_num取值工具 pgcalc（https://old.ceph.com/pgcalc/） 参考 https://www.cnblogs.com/varden/p/13921172.html\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-pg-num-config-create-pool/","summary":"pg_num 用此命令创建存储池时： ceph osd pool create {pool-name} pg_num 确定pg_num取值是强制性的，因为不能自动计算。常用的较为通用的取值： 少于5个osd，pg_num设置为128 osd数量在 5 到 10 个时，pg_num设置为512 osd数量在 10 到 50 个时，pg_num = 4096 osd数量大于50是，需要理解ceph的权衡","title":"ceph创建pool时pg_num的配置"},{"content":"python方式 批量导出，运行后所有tar包都在当前目录下\n# encoding: utf-8 import re import os import subprocess if __name__ == \u0026#34;__main__\u0026#34;: p = subprocess.Popen(\u0026#39;docker images\u0026#39;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines(): # 此处的正则表达式是为了匹配镜像名以kolla为开头的镜像 # 实际使用中根据需要自行调整 m = re.match(r\u0026#39;(^kolla[^\\s]*\\s*)\\s([^\\s]*\\s)\u0026#39;, line) if not m: continue # 镜像名 iname = m.group(1).strip() # tag itag = m.group(2).strip() # tar包的名字 if iname.find(\u0026#39;/\u0026#39;): tarname = iname.split(\u0026#39;/\u0026#39;)[0] + \u0026#39;_\u0026#39; + iname.split(\u0026#39;/\u0026#39;)[-1] + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; else: tarname = iname + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; print tarname ifull = iname + \u0026#39;:\u0026#39; + itag #save cmd = \u0026#39;docker save -o \u0026#39; + tarname + \u0026#39; \u0026#39; + ifull print os.system(cmd) retval = p.wait() 批量导入，同理导入当前目录下的所有的tar包\nimport os images = os.listdir(os.getcwd()) for imagename in images: if imagename.endswith(\u0026#39;.tar\u0026#39;): print(imagename) os.system(\u0026#39;docker load -i %s\u0026#39;%imagename) bash方式 导出 #!/bin/bash docker images \u0026gt; images.txt awk \u0026#39;{print $1}\u0026#39; images.txt \u0026gt; images_cut.txt sed -i \u0026#39;1d\u0026#39; images_cut.txt while read LINE do docker save $LINE \u0026gt; ${LINE//\\//_}.train.tar echo ok done \u0026lt; images_cut.txt echo finish 导入 #!/bin/bash while read LINE do docker load -i $LINE echo ok done \u0026lt; tarname.txt echo finish 参考 https://www.cnblogs.com/ksir16/p/8865525.html\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-import-export-image/","summary":"python方式 批量导出，运行后所有tar包都在当前目录下 # encoding: utf-8 import re import os import subprocess if __name__ == \u0026#34;__main__\u0026#34;: p = subprocess.Popen(\u0026#39;docker images\u0026#39;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines(): # 此处的正则表达式是为了匹配镜像名以kolla为开头的镜像 # 实际使用中根据需要自行调整 m = re.match(r\u0026#39;(^kolla[^\\s]*\\s*)\\s([^\\s]*\\s)\u0026#39;, line) if not m: continue # 镜像名 iname = m.group(1).strip() # tag itag = m.group(2).strip() # tar包的名字 if iname.find(\u0026#39;/\u0026#39;): tarname = iname.split(\u0026#39;/\u0026#39;)[0] + \u0026#39;_\u0026#39; + iname.split(\u0026#39;/\u0026#39;)[-1] + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; else:","title":"docker | 脚本方式批量导出/导入镜像"},{"content":"查看内核版本\n[dpl@test1 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo) 下载内核\nhttps://elrepo.org/linux/kernel/el7/x86_64/RPMS/ 下载自己所需的内核 更新版本：5.10.81\n内核版本介绍：\nlt longterm的缩写 长期维护版 ml mainline的缩写 最新稳定版 使用 wget 命令下载内核 RPM 包\n[dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-5.10.81-1.el7.x86_64.rpm [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 安装内核\nyum localinstall -y kernel-ml-5.10.81-1.el7.x86_64.rpm kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 查看所有可用内核启动项\n[dpl@test1 ~] awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg 0 : CentOS Linux (5.10.81-1.el7.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.21.1.el7.x86_64) 7 (Core) 2 : CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) 3 : CentOS Linux (0-rescue-9a4efd5deb094f5d8a9a259066ff4f3d) 7 (Core) 记下 5.11.9 内核前面的序号，修改启动项需要\n修改默认启动项\n默认启动项由/etc/default/grub中的 GRUB_DEFAULT 控制，如果 GRUB_DEFAULT=saved，则该参数将存在 /boot/grub2/grubenv\n输入grub2-editenv list命令查看默认启动项\n[root@localhost ~]# grub2-editenv list saved_entry=CentOS Linux (3.10.0-1060.el7.x86_64) 7 (Core) 输入 grub2-set-default 命令修改默认启动项，0表示5.11.9内核的序号\n[dpl@test1 ~]# grub2-set-default 0 再次查看默认启动项，发现默认启动项已经改成了0\n10.81-1.el7.elrepo.x86_64 [dpl@test1 ~]# uname -r 5.10.81-1.el7.elrepo.x86_64 参考：https://blog.csdn.net/cqchengdan/article/details/106031823\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos-update-kernel-to-5.10/","summary":"查看内核版本 [dpl@test1 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo) 下载内核 https://elrepo.org/linux/kernel/el7/x86_64/RPMS/ 下载自己所需的内核 更新版本：5.10.81 内核版本介绍： lt longterm的缩写 长期维护版 ml mainline的缩写 最新稳定版 使用 wget 命令下载内核 RPM 包 [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-5.10.81-1.el7.x86_64.rpm [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 安装内核 yum localinstall -y kernel-ml-5.10.81-1.el7.x86_64.rpm kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 查看所有可用内核启动项 [dpl@test1 ~] awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg 0 : CentOS","title":"centos7.5 升级内核至 5.10"},{"content":"前言 基于CVE-1999-0526漏洞的披露，对系统X服务的6000端口进行关闭\n有三种方式：\n修改系统/usr/bin/X内容，增加nolisten参数 开启系统防火墙，关闭6000端口的对外访问 禁用桌面(runlevel-5)，开机进入字符界面(runlevel-3) 修改/usr/bin/X脚本 关闭 rm -f /usr/bin/X vim /usr/bin/X ################### 添加如下内容 #!/bin/bash exec /usr/bin/Xorg \u0026#34;$@\u0026#34; -nolisten tcp exit 0 #################### chmod 777 /usr/bin/X kill -9 进程号 # ps -elf |grep X 显示的进程号 恢复 rm -f /usr/bin/X ln -s /usr/bin/Xorg /usr/bin/X kill -9 进程号 # pe -elf | grep Xorg 显示的进程号 在测试过程中出现过杀死X服务进程后没有自启的情况，可尝试使用 init 3 \u0026amp;\u0026amp; init 5 尝试重新启动X服务\n修改防火墙方式 # 开启除6000端口以外的所有端口(6000端口无法访问) systemctl start firewalld firewall-cmd --permanent --zone=public --add-port=1-65535/udp firewall-cmd --permanent --zone=public --add-port=1-5999/tcp firewall-cmd --permanent --zone=public --add-port=6001-65535/tcp firewall-cmd --reload firewall-cmd --list-all # 恢复（6000端口可以访问） firewall-cmd --permanent --zone=public --add-port=6000/tcp firewall-cmd --reload firewall-cmd --list-all 参考 https://bugzilla.redhat.com/show_bug.cgi?id=1647621\n","permalink":"https://www.lvbibir.cn/en/posts/tech/cve-1999-0526/","summary":"前言 基于CVE-1999-0526漏洞的披露，对系统X服务的6000端口进行关闭 有三种方式： 修改系统/usr/bin/X内容，增加nolisten参数 开启系统防火墙，关闭6000端口的对外访问 禁用桌面(runlevel-5)，开机进入字符界面(runlevel-3) 修改/usr/","title":"CVE-1999-0526"},{"content":"pam 模块\npam：Pluggable Authentication Modules 可插拔的认证模块，linux 中的认证方式，“可插拔的”说明可以按需对认证内容进行变更。与nsswitch一样，也是一个通用框架。只不过是提供认证功能的。\n查看密码失败次数\npam_tally2 -u root # 或者 faillock --user root 重置密码失败次数\npam_tally2 -r -u root # 或者 faillock --user root --reset 具体取决于在规则文件中使用的是 pam_faillock.so模块还是 pam_tally2.so 模块\n例：\ncat /etc/pam.d/system-auth ","permalink":"https://www.lvbibir.cn/en/posts/tech/centos-too-many-password-attempts/","summary":"pam 模块 pam：Pluggable Authentication Modules 可插拔的认证模块，linux 中的认证方式，“可插拔的”说明可以按需对认证内容进行变更。与nsswitch一样，也是一个通用框架。只不过是提供认证功能的。 查看密码失败次数 pam_tally2 -u root # 或者 faillock --user root 重置密码失败次数 pam_tally2 -r -u root # 或者 faillock --user root --reset 具体取决于在规则文件","title":"密码尝试次数过多"},{"content":"基本环境 物理环境：Vmware Workstaion 系统版本：Centos-7.9-Minimal 两个osd节点添加一块虚拟磁盘，建议不小于20G ip hostname services 192.168.150.101 ceph-admin(ceph-deploy) mds1、mon1、mon_mgr、ntp-server 192.168.150.102 ceph-node1 osd1 192.168.150.103 ceph-node2 osd2 前期配置 以下操作所有节点都需执行\n修改主机名\nhostnamectl set-hostname ceph-admin bash hostnamectl set-hostname ceph-node1 bash hostnamectl set-hostname ceph-node2 bash 修改hosts文件\nvim /etc/hosts 192.168.150.101 ceph-admin 192.168.150.102 ceph-node1 192.168.150.103 ceph-node2 关闭防火墙和selinux、修改yum源及安装一些常用工具\n这里提供了一个简单的系统初始化脚本用来做上述操作，适用于Centos7\nchmod 777 init.sh ./init.sh #!/bin/bash echo \u0026#34;========start=============\u0026#34; sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld echo \u0026#34;====dowload wget=========\u0026#34; yum install -y wget echo \u0026#34;====backup repo===========\u0026#34; mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ echo \u0026#34;====dowload aliyum-repo====\u0026#34; wget http://mirrors.aliyun.com/repo/Centos-7.repo -O /etc/yum.repos.d/Centos-Base.repo wget http://mirrors.aliyun.com/repo/epel-7.repo -O /etc/yum.repos.d/epel.repo echo \u0026#34;====upgrade yum============\u0026#34; yum clean all yum makecache fast echo \u0026#34;====dowload tools=========\u0026#34; yum install -y net-tools vim bash-completion echo \u0026#34;=========finish============\u0026#34; 每个节点安装和配置NTP（官方推荐的是集群的所有节点全部安装并配置 NTP，需要保证各节点的系统时间一致。没有自己部署ntp服务器，就在线同步NTP）\nyum install chrony -y systemctl start chronyd systemctl enable chronyd ceph-admin\nvim /etc/chrony.conf systemctl restart chronyd chronyc sources 这里使用阿里云的ntp服务器\nceph-node1、ceph-node2\nvim /etc/chrony.conf systemctl restart chronyd chronyc sources 这里指定ceph-admin节点的ip即可\n添加ceph源\nyum -y install epel-release rpm --import http://mirrors.163.com/ceph/keys/release.asc rpm -Uvh --replacepkgs http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-nautilus/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-nautilus/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-nautilus/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc 磁盘准备 以下操作在osd节点（ceph-node1、ceph-node2）执行\n# 检查磁盘 [root@ceph-node1 ~]# fdisk -l /dev/sdb Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes # 格式化磁盘 [root@ceph-node1 ~]# parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100% [root@ceph-node1 ~]# mkfs.xfs /dev/sdb -f meta-data=/dev/sdb isize=512 agcount=4, agsize=1310720 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=5242880, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 查看磁盘格式 [root@ceph-node1 ~]# blkid -o value -s TYPE /dev/sdb xfs 安装ceph集群 配置ssh免密\n[root@ceph-admin ~]# ssh-keygen # 一路回车 [root@ceph-admin ~]# ssh-copy-id root@ceph-node1 [root@ceph-admin ~]# ssh-copy-id root@ceph-node2 安装ceph-deploy\n[root@ceph-admin ~]# yum install -y python2-pip [root@ceph-admin ~]# yum install -y ceph-deploy 创建文件夹用户存放集群文件\n[root@ceph-admin ~]# mkdir /root/my-ceph [root@ceph-admin ~]# cd /root/my-ceph/ 创建集群（后面填写monit节点的主机名，这里monit节点和管理节点是同一台机器，即ceph-admin）\n[root@ceph-admin my-ceph]# ceph-deploy new ceph-admin [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph-admin [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] func : \u0026lt;function new at 0x7f2217df3de8\u0026gt; [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f221756e4d0\u0026gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : [\u0026#39;ceph-admin\u0026#39;] [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-admin][DEBUG ] connected to host: ceph-admin [ceph-admin][DEBUG ] detect platform information from remote host [ceph-admin][DEBUG ] detect machine type [ceph-admin][DEBUG ] find the location of an executable [ceph-admin][INFO ] Running command: /usr/sbin/ip link show [ceph-admin][INFO ] Running command: /usr/sbin/ip addr show [ceph-admin][DEBUG ] IP addresses found: [u\u0026#39;192.168.150.101\u0026#39;] [ceph_deploy.new][DEBUG ] Resolving host ceph-admin [ceph_deploy.new][DEBUG ] Monitor ceph-admin at 192.168.150.101 [ceph_deploy.new][DEBUG ] Monitor initial members are [\u0026#39;ceph-admin\u0026#39;] [ceph_deploy.new][DEBUG ] Monitor addrs are [\u0026#39;192.168.150.101\u0026#39;] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... 修改集群配置文件\n注意：mon_host必须和public network 网络是同网段内\n[root@ceph-admin my-ceph]# vim ceph.conf # 添加如下两行内容 ...... public_network = 192.168.150.0/24 osd_pool_default_size = 2 开始安装\n[root@ceph-admin my-ceph]# ceph-deploy install --release nautilus ceph-admin ceph-node1 ceph-node2 # 出现以下提示说明安装成功 [ceph-node2][DEBUG ] Complete! [ceph-node2][INFO ] Running command: ceph --version [ceph-node2][DEBUG ] ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) nautilus (stable) 初始化monit监控节点，并收集所有密钥\n[root@ceph-admin my-ceph]# ceph-deploy mon create-initial [root@ceph-admin my-ceph]# ceph-deploy gatherkeys ceph-admin 检查OSD节点上所有可用的磁盘\n[root@ceph-admin my-ceph]# ceph-deploy disk list ceph-node1 ceph-node2 删除所有osd节点上的分区、准备osd及激活osd\n主机上有多块磁盘要作为osd时：ceph-deploy osd create ceph-node21 --data /dev/sdb --data /dev/sdc\n[root@ceph-admin my-ceph]# ceph-deploy osd create ceph-node1 --data /dev/sdb [root@ceph-admin my-ceph]# ceph-deploy osd create ceph-node2 --data /dev/sdb 在两个osd节点上通过命令已显示磁盘已成功mount\n[root@ceph-node1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 20G 0 disk └─ceph--2bb0ec8d--547c--42c2--9858--08ccfd043bd4-osd--block--33e8dba4--6dfc--4753--b9ba--0d0c54166f0c 253:2 0 20G 0 lvm sr0 查看osd\n[root@ceph-admin my-ceph]# ceph-deploy disk list ceph-node1 ceph-node2 ...... ...... [ceph-node1][INFO ] Disk /dev/mapper/ceph--2bb0ec8d--547c--42c2--9858--08ccfd043bd4-osd--block--33e8dba4--6dfc--4753--b9ba--0d0c54166f0c: 21.5 GB, 21470642176 bytes, 41934848 sectors ...... ...... [ceph-node2][INFO ] Disk /dev/mapper/ceph--f9a95e6c--fc7b--46b4--a835--dd997c0d6335-osd--block--db903124--4c01--40d7--8a58--b26e17c1db29: 21.5 GB, 21470642176 bytes, 41934848 sectors 同步集群文件，这样就可以在所有节点执行ceph命令了\n[root@ceph-admin my-ceph]# ceph-deploy admin ceph-admin ceph-node1 ceph-node2 在其他节点查看osd的目录树\n[root@ceph-node1 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.03897 root default -3 0.01949 host ceph-node1 0 hdd 0.01949 osd.0 up 1.00000 1.00000 -5 0.01949 host ceph-node2 1 hdd 0.01949 osd.1 up 1.00000 1.00000 配置mgr\n[root@ceph-admin my-ceph]# ceph-deploy mgr create ceph-admin 查看集群状态和集群service状态\n此时是HEALTH_WARN状态，是由于启用了不安全模式\n[root@ceph-admin my-ceph]# ceph health HEALTH_WARN mon is allowing insecure global_id reclaim [root@ceph-admin my-ceph]# ceph -s cluster: id: fd816347-598c-4ed6-b356-591a618a0bdc health: HEALTH_WARN mon is allowing insecure global_id reclaim services: mon: 1 daemons, quorum ceph-admin (age 3h) mgr: mon_mgr(active, since 17s) osd: 2 osds: 2 up (since 3m), 2 in (since 3m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2.0 GiB used, 38 GiB / 40 GiB avail pgs: 禁用不安全模式\n[root@ceph-admin my-ceph]# ceph config set mon auth_allow_insecure_global_id_reclaim false [root@ceph-admin my-ceph]# ceph health HEALTH_OK 开启dashboard [root@ceph-admin my-ceph]# yum install -y ceph-mgr-dashboard [root@ceph-admin my-ceph]# ceph mgr module enable dashboard # 创建自签证书 [root@ceph-admin my-ceph]# ceph dashboard create-self-signed-cert # 创建密码文件 [root@ceph-admin my-ceph]# echo abc123 \u0026gt; ./dashboard_user_pw # 创建dashboard的登录用户 [root@ceph-admin my-ceph]# ceph dashboard ac-user-create admin -i ./dashboard_user_pw administrator {\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;lastUpdate\u0026#34;: 1646037503, \u0026#34;name\u0026#34;: null, \u0026#34;roles\u0026#34;: [\u0026#34;administrator\u0026#34;], \u0026#34;password\u0026#34;: \u0026#34;$2b$12$jGsvau8jFMb4pDwLU/t8KO1sKvmBMcNUYycbXusmgkvTQzlzrMyKi\u0026#34;, \u0026#34;email\u0026#34;: null} [root@ceph-admin my-ceph]# ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;https://ceph-admin:8443/\u0026#34; } 测试访问\n上图中测试环境是win10+chrome，同事反应mac+chrome会出现无法访问的情况，原因是我们使用的自签证书，浏览器并不信任此证书，可以通过以下两种方式解决\n关闭dashboard的ssl访问\n下载证书配置浏览器信任证书\n关闭dashboard的ssl访问 [root@ceph-admin my-ceph]# ceph config set mgr mgr/dashboard/ssl false [root@ceph-admin my-ceph]# ceph mgr module disable dashboard [root@ceph-admin my-ceph]# ceph mgr module enable dashboard [root@ceph-admin my-ceph]# ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;http://ceph-admin:8080/\u0026#34; } 如果出现Module 'dashboard' has failed: IOError(\u0026quot;Port 8443 not free on '::'\u0026quot;,)这种报错，需要重启下mgr：systemctl restart ceph-mgr@ceph-admin\n测试访问\n开启rgw管理功能 默认object gateway功能没有开启\n创建rgw实例\nceph-deploy rgw create ceph-admin 默认运行端口是7480\n创建rgw用户\n[root@ceph-admin my-ceph]# radosgw-admin user create --uid=rgw --display-name=rgw --system 提供dashboard证书\n[root@ceph-admin my-ceph]# echo UI2T50HNZUCVVYYZNDHP \u0026gt; rgw_user_access_key [root@ceph-admin my-ceph]# echo 11rg0WbXuh2Svexck3vJKs19u1UQINixDWIpN5Dq \u0026gt; rgw_user_secret_key [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-access-key -i rgw_user_access_key Option RGW_API_ACCESS_KEY updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-secret-key -i rgw_user_secret_key Option RGW_API_SECRET_KEY updated 禁用ssl\n[root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-ssl-verify False Option RGW_API_SSL_VERIFY updated 启用rgw\n[root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-host 192.168.150.101 Option RGW_API_HOST updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-port 7480 Option RGW_API_PORT updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-scheme http Option RGW_API_SCHEME updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-user-id rgw Option RGW_API_USER_ID updated [root@ceph-admin my-ceph]# systemctl restart ceph-radosgw.target 验证\n目前object gateway功能已成功开启\n其他 清除ceph集群 清除安装包\n[root@ceph-admin ~]# ceph-deploy purge ceph-admin ceph-node1 ceph-node2 清除配置信息\n[root@ceph-admin ~]# ceph-deploy purgedata ceph-admin ceph-node1 ceph-node2 [root@ceph-admin ~]# ceph-deploy forgetkeys 每个节点删除残留的配置文件\nrm -rf /var/lib/ceph/osd/* rm -rf /var/lib/ceph/mon/* rm -rf /var/lib/ceph/mds/* rm -rf /var/lib/ceph/bootstrap-mds/* rm -rf /var/lib/ceph/bootstrap-osd/* rm -rf /var/lib/ceph/bootstrap-mon/* rm -rf /var/lib/ceph/tmp/* rm -rf /etc/ceph/* rm -rf /var/run/ceph/* 清理磁盘设备(/dev/mapper/ceph*)\nls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % dashboard无法访问的问题 在关闭dashboard的https后，出现了一个很奇怪的问题，使用chrome浏览器无法访问dashboard了，edge或者使用chrome无痕模式可以正常访问，期间尝试了各种方法包括重新配置dashboard和清理chrome浏览器的缓存和cookie等方式都没有解决问题，结果第二天起来打开环境一看自己好了（淦）\n问题情况见下图\n日志报错：\n同步ceph配置文件 ceph-deploy --overwrite-conf config push ceph-node{1,2,3,4} 添加mon节点和mgr节点 ceph-deploy mon create ceph-node{1,2,3,4} ceph-deploy mgr create ceph-node{1,2,3,4} 记得修改配置文件\n之后同步配置文件\nceph-deploy --overwrite-conf config push ceph-node{1,2,3,4} 参考 https://www.cnblogs.com/kevingrace/p/9141432.html\nhttps://www.cnblogs.com/weijie0717/p/8378485.html\nhttps://www.cnblogs.com/weijie0717/p/8383938.html\nhttps://blog.csdn.net/qq_40017427/article/details/106235456\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v12-nautilus-cephdeploy/","summary":"基本环境 物理环境：Vmware Workstaion 系统版本：Centos-7.9-Minimal 两个osd节点添加一块虚拟磁盘，建议不小于20G ip hostname services 192.168.150.101 ceph-admin(ceph-deploy) mds1、mon1、mon_mgr、ntp-server 192.168.150.102 ceph-node1 osd1 192.168.150.103 ceph-node2 osd2 前期配置 以下操作所有节点都需执行 修改主机名 hostnamectl set-hostname ceph-admin bash hostnamectl set-hostname ceph-node1 bash hostnamectl set-hostname ceph-node2 bash 修改hos","title":"centos7 部署 ceph-v12 (nautilus) + dashboard"},{"content":"前言 在openEuler20.03 (LTS-SP1)系统上进行一些测试，发现某个东西会自动修改ssh配置文件导致系统无法通过密码登录，最后排查是由于安装了cloud-init导致的。\n以下是大致的排查思路\n出现这个问题前做的操作是安装了一些项目组同事指定的包，问题就应该出在这些包上\nyum install -y telnet rsync ntpdate zip unzip libaio dos2unix sos vim vim-enhanced net-tools man ftp lrzsz psmisc gzip network-scripts cloud-init cloud-utils-growpart tar libnsl authselect-compat 大致看了下，除了cloud-Init和cloud-utils-growpart这两个包其他包基本不可能去修改ssh的配置\n直接检索这两个包的所有文件中的配置，是否与PasswordAuthentication有关\n[root@localhost ~]# grep -nr PasswordAuthentication `rpm -ql cloud-utils-growpart` [root@localhost ~]# grep -nr PasswordAuthentication `rpm -ql cloud-init` 找到了修改这个参数代码的具体实现\n查看该文件\n[root@localhost ~]# vim +98 /usr/lib/python3.7/site-packages/cloudinit/config/cc_set_passwords.py 具体的判断操作和修改操作\n修改操作就不去深究了，主要看下判断操作，可以看到判断操作是使用了 util.is_true() ，该util模块也在该文件中引用了\n再去找这个util模块的具体实现\npython引用的模块路径如下，否则会抛出错误\n文件的同级路径下 sys.path 路径下 并没有在同级目录下\n[root@localhost ~]# ll /usr/lib/python3.7/site-packages/cloudinit/config/ | grep cloudinit sys.path 路径不知道可以用python终端输出下\n在/usr/lib/python3.7/site-packages路径下找到了cloudinit模块的util子模块\n查看util.is_true和util.is_false具体的函数实现\n逻辑很简单，判断 val 参数是否为bool值，否则对val参数的值进行处理后再查看是否在check_set中\n再回头看之前的/usr/lib/python3.7/site-packages/cloudinit/config/cc_set_passwords.py文件是怎样对util.is_true和util.is_false传参的\n可以看到是由handle_ssh_pwauth()函数传进来的\n再继续找哪个文件调用了这个函数\n还是这个文件，第230行\n这里参数pw_auth传的值是cfg.get(\u0026lsquo;ssh_pwauth\u0026rsquo;)\ncfg.get()这个函数get的东西是/etc/cloud/cloud.cfg配置文件下的ssh_pwauth的值\n到这里，就可以回头再看整个逻辑了\n调用handle_ssh_pwauth()函数，传了一个参数 pw_auth=0 调用util.is_true()和util.is_false函数，传了同一个参数 val=0 上述两个函数执行完后cfg_val的值最终为no 调用update_ssh_config({cfg_name: cfg_val})函数，cfg_name=PasswordAuthentication，cfg_val=no 即将sshd的配置文件的PasswordAuthentication值改为no ","permalink":"https://www.lvbibir.cn/en/posts/tech/cloud-init-change-ssh-config/","summary":"前言 在openEuler20.03 (LTS-SP1)系统上进行一些测试，发现某个东西会自动修改ssh配置文件导致系统无法通过密码登录，最后排查是由于安装了cloud-init导致的。 以下是大致的排查思路 出现这个问题前做的操作是安装了一些项目组同事指定的包，问题就应该出在这些包上 yum","title":"cloud-init自动修改ssh配置文件"},{"content":"清单 地址 中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple 清华 https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣 http://pypi.douban.com/simple 华中理工大学 http://pypi.hustunique.com/simple 山东理工大学 http://pypi.sdutlinux.org/simple 阿里云 https://mirrors.aliyun.com/pypi/simple/ linux环境 mkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF windows环境 打开 cmd 使用 dos命令 set 找到 userprofile 路径，在该路径下创建 pip文件夹，在 pip文件夹下创建 pip.ini\npip.ini具体配置\n[global] timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn ","permalink":"https://www.lvbibir.cn/en/posts/tech/python3-change-pip-repo/","summary":"清单 地址 中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple 清华 https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣 http://pypi.douban.com/simple 华中理工大学 http://pypi.hustunique.com/simple 山东理工大学 http://pypi.sdutlinux.org/simple 阿里云 https://mirrors.aliyun.com/pypi/simple/ linux环境 mkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF windows环境 打开 cmd 使用 dos命令 set 找到 userprofile 路径，在该路径下创建 pip文件夹，在 pip文件夹下创建 pip.ini pip.ini具体配置 [global] timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn","title":"python3修改pip源"},{"content":"前言 要修改rpm包中的文件，对于自己编译的rpm包，只需要在源码中修改好然后重新编译即可。而对于并不是自己编译的rpm包，且不熟悉编译环境的情况下，可以使用rpm-build和rpm-rebuild工具反编译来修改rpm中的文件\n这里使用ceph-mgr软件包进行演示\n安装rpm-build\u0026amp;rpmrebuild rpmrebuild官网：http://rpmrebuild.sourceforge.net\nrpmrebuild下载地址：https://sourceforge.net/projects/rpmrebuild/files/rpmrebuild/2.15/rpmrebuild-2.15.tar.gz/download\n解压rpmrebuild\n[root@localhost ~]# mkdir -p /data/rpmbuild [root@localhost ~]# tar zxf rpmrebuild-2.15.tar.gz -C /data/rpmbuild/ [root@localhost ~]# ll /opt/rpmrebuild/ rpm-build直接使用yum安装即可\n[root@localhost ~]# yum install -y rpm-build 反编译\u0026amp;修改\u0026amp;重新编译 安装准备重新打包的rpm\n[root@localhost ~]# rpm -ivh ceph-mgr-12.2.13-0.el7.x86_64.rpm 查看rpm的安装名称\n[root@localhost ~]# rpm -qa |grep mgr ceph-mgr-12.2.13-0.el7.x86_64 配置rpm编译目录\nvim ~/.rpmmacros %_topdir /data/rpmbuild 创建目录\nmkdir /data/rpmbuild/BUILDROOT mkdir /data/rpmbuild/SPECS 执行脚本\n[root@localhost ~]# cd /data/rpmbuild/ [root@localhost rpmbuild]# ./rpmrebuild.sh -s SPECS/abc.spec ceph-mgr [root@localhost rpmbuild]# cd 解压原版RPM包\n[root@localhost ~]# rpm2cpio ceph-mgr-12.2.13-0.el7.x86_64.rpm |cpio -idv 这里软件包解压后是两个目录\n根据需求替换修改解压后的文件，这里我替换两个文件/root/usr/lib64/ceph/mgr/dashboard/static/Ceph_Logo_Standard_RGB_White_120411_fa.png和/root/usr/lib64/ceph/mgr/dashboard/static/logo-mini.png，并给原先的文件做一个备份\n[root@localhost static]# mv logo-mini.png logo-mini.png.bak [root@localhost static]# mv Ceph_Logo_Standard_RGB_White_120411_fa.png Ceph_Logo_Standard_RGB_White_120411_fa.png.bak [root@localhost static]# cp kubernetes-logo.svg logo-mini.png [root@localhost static]# cp kubernetes-logo.svg Ceph_Logo_Standard_RGB_White_120411_fa.png 修改abc.spec文件\n找到原文件所在的行，添加备份文件\n[root@localhost ~]# vim /data/rpmbuild/SPECS/abc.spec 这里创建的bbb目录是临时使用，编译过程肯定会报错，因为路径不对，根据报错修改路径\n[root@localhost ~]# mkdir -p /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# mv ./usr/ /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# mv ./var/ /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# rpmbuild -ba /data/rpmbuild/SPECS/abc.spec 这里可以看到他请求的路径\n修改目录名\n[root@localhost ~]# mv /data/rpmbuild/BUILDROOT/bbb/ /data/rpmbuild/BUILDROOT/ceph-mgr-12.2.13-0.el7.x86_64 再次编译\n[root@localhost ~]# rpmbuild -ba /data/rpmbuild/SPECS/abc.spec 生成的rpm位置在/data/rpmbuild/RPMS/\n查看原rpm包的文件\n[root@localhost ~]# cd /usr/lib64/ceph/mgr/dashboard/static [root@localhost static]# ll total 16 drwxr-xr-x 5 root root 117 Dec 6 03:11 AdminLTE-2.3.7 -rw-r--r-- 1 root root 4801 Jan 30 2020 Ceph_Logo_Standard_RGB_White_120411_fa.png -rw-r--r-- 1 root root 1150 Jan 30 2020 favicon.ico drwxr-xr-x 7 root root 94 Dec 6 03:11 libs -rw-r--r-- 1 root root 1811 Jan 30 2020 logo-mini.png 安装新rpm包，查看文件\n[root@localhost ~]# cd /data/rpmbuild/RPMS/x86_64 [root@localhost x86_64]# rpm -e --nodeps ceph-mgr [root@localhost x86_64]# rpm -ivh ceph-mgr-12.2.13-0.el7.x86_64.rpm [root@localhost x86_64]# cd /usr/lib64/ceph/mgr/dashboard/static [root@localhost static]# ll total 24 drwxr-xr-x 5 root root 117 Dec 6 03:53 AdminLTE-2.3.7 -rw-r--r-- 1 root root 1877 Dec 6 03:44 Ceph_Logo_Standard_RGB_White_120411_fa.png -rw-r--r-- 1 root root 4801 Dec 6 03:41 Ceph_Logo_Standard_RGB_White_120411_fa.png.bak -rw-r--r-- 1 root root 1150 Dec 6 03:41 favicon.ico drwxr-xr-x 7 root root 94 Dec 6 03:53 libs -rw-r--r-- 1 root root 1877 Dec 6 03:44 logo-mini.png -rw-r--r-- 1 root root 1811 Dec 6 03:41 logo-mini.png.bak 至此，rpm包中的文件修改以及重新打包的所有步骤都已完成\n参考 https://www.cnblogs.com/felixzh/p/10564895.html\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-change-file/","summary":"前言 要修改rpm包中的文件，对于自己编译的rpm包，只需要在源码中修改好然后重新编译即可。而对于并不是自己编译的rpm包，且不熟悉编译环境的情况下，可以使用rpm-build和rpm-rebuild工具反编译来修改rpm中的文件 这里使用ceph-mgr软件包进行演示 安装rpm-b","title":"通过反编译修改rpm包内的文件"},{"content":"前期准备 1. 安装包准备 Ambari2.7.5. HDP3.1.5. libtirpc-devel: 链接：https://pan.baidu.com/s/1eteZ2jGkSq4Pz5YFfHyJgQ 提取码：6hq3\n2. 服务器配置 主机名 cpu 内存 硬盘 系统版本 ip地址 node001 4c 10g 50g isoft-serveros-4.2 192.168.150.106 node002 2c 4g 20g isoft-serveros-4.2 192.168.150.107 3. 修改系统版本文件(allnode) sed -i \u0026#39;s/4/7/g\u0026#39; /etc/redhat-release sed -i \u0026#39;s/4/7/g\u0026#39; /etc/os-release 4. 配置主机名(allnode) 2台服务器的 hosts 都需要做如下修改\n修改主机名 hostnamectl set-hostname node001 bash 修改 hosts 文件 vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.150.106 node001 192.168.150.107 node002 5. 关闭防火墙及selinux(allnode) 2台服务器上分别执行以下操作，关闭防火墙并配置开机不自动启动\nsystemctl stop firewalld systemctl disable firewalld setenforce 0 为了重启后依然关闭，配置如下文件\nvim /etc/sysconfig/selinux 修改 SELINUX=disabled 6. 配置ssh互信(allnode) 方法一\n在每台服务器上执行如下操作，一直回车即可\nssh-keygen -t rsa ssh-copy-id -i /root/.ssh/id_rsa.pub node001 ssh-copy-id -i /root/.ssh/id_rsa.pub node002 方法二\n在每台服务器上执行如下操作，一直回车即可\nssh-keygen -t rsa 在服务器1上将公钥（名为 id_rsa.pub 文件）追加到认证文件（名为 authorized_keys 文件）中:\ncat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 去其他服务器节点将~/.ssh/id_rsa.pub中的内容拷贝到服务器1的~/.ssh/authorized_keys中,查看文件中的内容\ncat ~/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/eA09X4s5RIYvuYNxVvtOo6unY1mgipsFyoz/hy/Gwk0onfZvBi/Sl3TVRZO5aqcHccAGlLF7OPTKH1qUuKVtnUOQik0TouL5VKsOBDMHHRT9D5UwqaIE8tYDC8V6uwieFgscZcBjhrsJ/Iramo9ce7N9RTO3otRMRQxOs+Wd1F/ZOmpRtMGU2N4RH4i2quRU6m2lt/eJKpNupSHKoztTQRsEanilHVASnikAXH8JpG70iO7RXR/hLz+/Of3ISUrOMSO4/ZIIu4xnYN3jvsXOdK/qIhP/PI2s+uF22IvVE6xZYVadQFa4zAuhQmCBWkE7vMyI1UJkxP7OQYj72LUH root@node001 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnz8wHoytR2Xlnl04rQq4I2vgUVWbkKjv30pj+Toz4719ah4cY9pvZj0JsfhVzaaCsR14BLFVLkqKUhCWK3K6muT4iHb+N0WirpbwfJkztmQeco7Ha9xrPQ8v/I4xZujFoMVA0tkb/32zRTxOkPv9AUgB8V6Lin6LnB/AcnhnmoIs5PdbAdh/kBGpQGKIZkbyCUOYz9/PZuGJoJBblqfWiqzxYYLN9+cYMkmPnB1HdDewAepIsIC18U3ujE+1Su2UlmISPvvr1zG4XR4ZZoKQsOOJq3XRMGVkDvmFhl03JHZpd6BW0796CeYVZ41UomWXTOduQql+tYWUbegzGLmRZ root@node002 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8AFoGJHp2M45xLYNzXUHLWzRwHsgRPHjeErStq0tEy9bQv4OkN41j0FrxVAYJiGHdHGturriVgUEtL59RjcrJH6bAvhP54nM5YiQlNnWnSUR27Zuaodz4nhYUFq/Co5eDN6lTfL8pgYiEdpBOvE5t1w3bisdblP7YGQ2lF1zzCEGfQ79QbntEbyGNoR9sGHm11x9fOH+fape8TjQJrEAO4d1tAhMqVygQKwqwAPKeqhEum6BaLli83TsXzd7gyz9H7AAc1m04NaLB26xfynW6MVuk1j94awXKlGXjrbNTC/Kg6M8bd5PT/k3DOkx4b+nEs8xZ5x1j4D2OaO1X6rZx root@node003 设置认证文件的权限：\nchmod 600 ~/.ssh/authorized_keys 将~/.ssh/authorized_keys同步到其他节点\nscp ~/.ssh/authorized_keys node002:~/.ssh/authorized_keys 注意：这里第一次使用同步还需要密码，之后就不需要了\n验证免密是否配置成功\nssh 到不同服务器\nssh node002 7. 配置ntp时钟同步 选择一台服务器作为 NTP Server，这里选择 node001\n将如下配置vim /etc/ntp.conf\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 127.127.1.0 fudge 127.127.1.0 stratum 10 node002节点做如下配置\nvim /etc/ntp.conf 将\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 192.168.150.106 在每台服务器上启动ntpd服务，并配置服务开机自启动\nsystemctl restart ntpd systemctl enable ntpd 9. 设置swap(allnode) echo vm.swappiness = 1 \u0026gt;\u0026gt; /etc/sysctl.conf sysctl vm.swappiness=1 sysctl -p 10. 关闭透明大页面(allnode) 由于透明超大页面已知会导致意外的节点重新启动并导致RAC出现性能问题，因此Oracle强烈建议禁用\necho never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled 11. 安装http服务(node001) 安装apache的httpd服务主要用于搭建OS. Ambari和hdp的yum源。在集群服务器中选择一台服务器来安装httpd服务，命令如下：\nyum -y install httpd systemctl start httpd systemctl enable httpd.service 验证，在浏览器输入http://192.168.150.106看到如下截图则说明启动成功。\n13. 安装Java(allnode) 下载地址：https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\ntar -zxvf jdk-8u271-linux-x64.tar.gz mkdir /usr/local/java mv jdk1.8.0_271/* /usr/local/java 配置环境变量\nvim /root/.bashrc 添加如下配置\nexport JAVA_HOME=/usr/local/java export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export JRE_HOME=$JAVA_HOME/jre 激活配置\nsource /root/.bashrc java -version 14. 安装maven3.6(node001) 下载解压\ntar -zxvf apache-maven-3.6.3-bin.tar.gz mkdir -p /opt/src/maven mv apache-maven-3.6.3/* /opt/src/maven 配置maven环境变量\nvim /root/.bashrc # set maven home export PATH=$PATH:/opt/src/maven/bin 激活\nsource /root/.bashrc 安装Ambari\u0026amp;HDP 1. 配置本地源 解压\ntar -zxvf ambari-2.7.5.0-centos7.tar.gz -C /var/www/html/ tar -zxvf HDP-3.1.5.0-centos7-rpm.tar.gz -C /var/www/html/ tar -zxvf HDP-GPL-3.1.5.0-centos7-gpl.tar.gz -C /var/www/html/ tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /var/www/html/ ll /var/www/html/ 总用量 0 drwxr-xr-x. 3 root root 21 11月 23 22:31 ambari drwxr-xr-x. 3 1001 users 21 12月 18 2019 HDP drwxr-xr-x. 3 1001 users 21 12月 18 2019 HDP-GPL drwxr-xr-x. 3 1001 users 21 8月 13 2018 HDP-UTILS 设置设置用户组和授权\nchown -R root:root /var/www/html/HDP chown -R root:root /var/www/html/HDP-GPL chown -R root:root /var/www/html/HDP-UTILS chmod -R 755 /var/www/html/HDP chmod -R 755 /var/www/html/HDP-GPL chmod -R 755 /var/www/html/HDP-UTILS 创建 libtirpc-devel 本地源\nmkdir /var/www/html/libtirpc mv /root/libtirpc-* /var/www/html/libtirpc/ cd /var/www/html/libtirpc createrepo . 制作本地源\n配置 ambari.repo\nvim /etc/yum.repos.d/ambari.repo [Ambari-2.7.5.0] name=Ambari-2.7.5.0 baseurl=http://192.168.150.106/ambari/centos7/2.7.5.0-72/ gpgcheck=0 enabled=1 priority=1 配置 HDP 和 HDP-TILS\nvim /etc/yum.repos.d/HDP.repo [HDP-3.1.5.0] name=HDP Version - HDP-3.1.5.0 baseurl=http://192.168.150.106/HDP/centos7/3.1.5.0-152/ gpgcheck=0 enabled=1 priority=1 [HDP-UTILS-1.1.0.22] name=HDP-UTILS Version - HDP-UTILS-1.1.0.22 baseurl=http://192.168.150.106/HDP-UTILS/centos7/1.1.0.22/ gpgcheck=0 enabled=1 priority=1 [HDP-GPL-3.1.5.0] name=HDP-GPL Version - HDP-GPL-3.1.5.0 baseurl=http://192.168.150.106/HDP-GPL/centos7/3.1.5.0-152 gpgcheck=0 enabled=1 priority=1 配置 libtirpc.repo\nvim /etc/yum.repos.d/libtirpc.repo [libtirpc_repo] name=libtirpc-0.2.4-0.16 baseurl=http://192.168.150.106/libtirpc/ gpgcheck=0 enabled=1 priority=1 拷贝到其他节点\nscp /etc/yum.repos.d/* node002:/etc/yum.repos.d/ 查看源\nyum clean all yum repolist 2. 安装mariadb(node001) 安装 MariaDB 服务器\nyum install mariadb-server -y 启动并设置开机启动\nsystemctl enable mariadb systemctl start mariadb 初始化\n/usr/bin/mysql_secure_installation [...] Enter current password for root (enter for none): OK, successfully used password, moving on... [...] Set root password? [Y/n] Y New password:123456 Re-enter new password:123456 [...] Remove anonymous users? [Y/n] Y [...] Disallow root login remotely? [Y/n] N [...] Remove test database and access to it [Y/n] Y [...] Reload privilege tables now? [Y/n] Y [...] All done! If you\u0026#39;ve completed all of the above steps, your MariaDB 18 installation should now be secure. Thanks for using MariaDB! 为 MariaDB 安装 MySQL JDBC 驱动程序\ntar zxf mysql-connector-java-5.1.40.tar.gz mv mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/share/java/mysql-connector-java.jar 创建需要的数据库\n如果需要 ranger，编辑以下⽂件： vim /etc/my.cnf 并添加以下⾏：\nlog_bin_trust_function_creators = 1 重启数据库并登录\nsystemctl restart mariadb mysql -u root -p123456 3. 安装和配置ambari-server (node001) 安装 ambari-server\nyum -y install ambari-server 复制 mysql jdbc 驱动到 /var/lib/ambari-server/resources/\ncp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/ 配置 /etc/ambari-server/conf/ambari.properties，添加如下行\nvim /etc/ambari-server/conf/ambari.properties server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar 执行\nambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 初始化 ambari-server\nambari-server setup 1） 提示是否自定义设置。输入：y Customize user account for ambari-server daemon [y/n] (n)? y （2）ambari-server 账号。 Enter user account for ambari-server daemon (root): 如果直接回车就是默认选择root用户 如果输入已经创建的用户就会显示： Enter user account for ambari-server daemon (root):ambari Adjusting ambari-server permissions and ownership... （3）设置JDK。输入：2 Checking JDK... Do you want to change Oracle JDK [y/n] (n)? y [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Custom JDK ============================================================================== Enter choice (1): 2 如果上面选择3自定义JDK,则需要设置JAVA_HOME。输入：/usr/local/java WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: /usr/local/java Validating JDK on Ambari Server...done. Completing setup... （4）安装GPL，选择：y Checking GPL software agreement... GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html Enable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? y （5）数据库配置。选择：y Configuring database... Enter advanced database configuration [y/n] (n)? y （6）选择数据库类型。输入：3 Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL/ MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere ============================================================================== Enter choice (3): 3 （7）设置数据库的具体配置信息，根据实际情况输入，如果和括号内相同，则可以直接回车。如果想重命名，就输入。 Hostname (localhost):node001 Port (3306): 3306 Database name (ambari): ambari Username (ambari): ambari Enter Database Password (bigdata):ambari123 Re-Enter password: ambari123 （8）将Ambari数据库脚本导入到数据库 WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql 这个sql后面会用到，导入数据库 Proceed with configuring remote database connection properties [y/n] (y)? y 登录 mariadb 创建 ambari 安装所需要的库\n设置的账号后面配置 ambari-server 的时候会用到\nmysql -uroot -p123456 CREATE DATABASE ambari; use ambari; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;node001\u0026#39;; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql show tables; use mysql; select host,user from user where user=\u0026#39;ambari\u0026#39;; CREATE DATABASE hive; use hive; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;node001\u0026#39;; CREATE DATABASE oozie; use oozie; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;node001\u0026#39;; FLUSH PRIVILEGES; 4. 安装ambari-agent(allnode) pssh -h /node.list -i \u0026#39;yum -y install ambari-agent\u0026#39; pssh -h /node.list -i \u0026#39;systemctl start ambari-agent\u0026#39; 5. 安装libtirpc-devel(allnode) pssh -h /node.list -i \u0026#39;yum -y install libtirpc-devel\u0026#39; 6. 启动ambari服务 ambari-server start 部署集群 1. 登录界面 http://192.168.150.106:8080\n默认管理员账户登录， 账户：admin 密码：admin\n2. 选择版本，配置yum源 1）选择 Launch Install Wizard 2）配置集群名称 3）选择版本并修改本地源地址\n选HDP-3.1(Default Version Definition); 选Use Local Repository; 选redhat7:\nHDP-3.1：http://node001/HDP/centos7/3.1.5.0-152/ HDP-3.1-GPL: http://node001/HDP-GPL/centos7/3.1.5.0-152/ HDP-UTILS-1.1.0.22: http://node001/HDP-UTILS/centos7/1.1.0.22/\n3. 配置节点和密钥 下载主节点的 /root/.ssh/id_rsa，并上传！点击下一步，进入确认主机界面\n也可直接 cat /root/.ssh/id_rsa 粘贴即可\n验证通过\n4. 勾选需要安装的服务 由于资源有限，这里并没有选择所有服务\n5. 分配服务 master 6. 分配服务 slaves 设置相关服务的密码 Grafana Admin: 123456 Hive Database: hive Activity Explorer’s Admin: admin\n7. 连接数据库 8. 编辑配置，默认即可 9. 开始部署 10. 安装成功 右上角两个警告是磁盘使用率警告，虚机分配的磁盘较小\n其他 1. 添加其他系统支持 HDP默认不支持安装到 isoft-serverosv4.2，需手动添加支持\nvim /usr/lib/ambari-server/lib/ambari_commons/resources/os_family.json 添加如下两行，注意缩进和逗号\n2. YARN Registry DNS 服务启动失败 lsof -i:53 kill -9 3. 设置初始检测的系统版本 vim /etc/ambari-server/conf/ambari.properties server.os_family=redhat7 server.os_type=redhat7 参考 https://blog.csdn.net/qq_36048223/article/details/116113987\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ambari-2.7.5-and-hdp-3.1.5/","summary":"前期准备 1. 安装包准备 Ambari2.7.5. HDP3.1.5. libtirpc-devel: 链接：https://pan.baidu.com/s/1eteZ2jGkSq4Pz5YFfHyJgQ 提取码：6hq3 2. 服务器配置 主机名 cpu 内存 硬盘 系统版本 ip地址 node001 4c 10g 50g isoft-serveros-4.2 192.168.150.106 node002 2c 4g 20g isoft-serveros-4.2 192.168.150.107 3. 修改系统版本文件(allnode) sed -i \u0026#39;s/4/7/g\u0026#39; /etc/redhat-release sed -i \u0026#39;s/4/7/g\u0026#39; /etc/os-release 4. 配置主机名(al","title":"部署 Ambari 2.7.5 + HDP3.1.5"},{"content":"👉友链为随机顺序 lvbibir\u0026#39;s Blog 我的 wordpress 站点 cuikx\u0026#39;s blog cuikx\u0026#39;s blog Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 陈桂林博客 成功最有效的方法就是向有经验的人学习！ 黄忠德的博客 DevOps,SRE,Python,Golang程序员,开源爱好者 阿虚同学的储物间 收集了很多实用网站 👉友链格式 名称: lvbibir's Blog\n网址: https://www.lvbibir.cn\n图标: https://www.lvbibir.cn/img/avatar.gif\n描述: life is a fucking movie\n👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告\n","permalink":"https://www.lvbibir.cn/en/links/","summary":"👉友链为随机顺序 lvbibir\u0026#39;s Blog 我的 wordpress 站点 cuikx\u0026#39;s blog cuikx\u0026#39;s blog Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 陈桂林博客 成功最有效的方法就是向有经验的人学习！ 黄忠德的博客 DevOps,SRE,Python,Golang程序员,开源爱好者 阿虚同学的储物间 收集了很多实用网站 👉友链格式 名称: lvbibir's Blog 网址: https://www.lvbibir.cn 图标: https://www.lvbibir.cn/img/avatar.gif 描述: life is a","title":"🤝 友链"},{"content":"\n🏡 关于本站 本博客主要记录一些学习生活，和一些个人觉得值得记录的问题及其解决办法。如果本博客能有哪些内容帮助到了你，那也是极好的。\n👦🏻 博主是谁 网络时代的素质教育漏网之鱼 | 晚睡协会常任理事 | 国家级抬杠运动员 | 中国驰名窝里横 | 国宝级老污龟 | 超水平怼人大师 | 一秒入戏准影帝\n精通CSS、JavaScript、PHP、C、C＋＋、C#、java、Ruby、Perl、Lisp、python等单词的拼写；\n熟悉windows、Linux、Mac、Android、IOS等系统的开关机；\n🏹 兴趣爱好 🏃‍♂️ 跑步 | 🎮️ 游戏 | 🎧 音乐 | 📺 动漫 | 🛌 摆烂\n📈 博客变更记录 2022年9月8日 本博客正式加入 十年之约\n今夕何夕，人生能有几个十年\n2022年7月16日 迁移之前发布在 csdn 的文章，将图片外链全部转为七牛图床\n2022年7月4日 hugo 站点试运行，域名：https://www.lvbibir.cn\n2021年8月15日 将阿里云轻量服务器自带的 wordpress 应用改为 docker 应用，wordpress 站点改为 docker-compose 部署\n2021年7月13日 wordpress 博客站点开始运行，域名：https://lvbibir.cn\n","permalink":"https://www.lvbibir.cn/en/about/","summary":"🏡 关于本站 本博客主要记录一些学习生活，和一些个人觉得值得记录的问题及其解决办法。如果本博客能有哪些内容帮助到了你，那也是极好的。 👦🏻 博主是谁 网络时代的素质教育漏网之鱼 | 晚睡协会常任理事 | 国家级抬杠运动员 | 中国驰名窝里横 | 国宝级老污龟 | 超水平怼人大师 | 一秒入戏准影帝 精通CSS、Ja","title":"🙋🏻‍♂️ 关于"},{"content":"kolla ansible简介 kolla 的使命是为 openstack 云平台提供生产级别的、开箱即用的交付能力。kolla 的基本思想是一切皆容器，将所有服务基于 Docker 运行，并且保证一个容器只跑一个服务（进程），做到最小粒度的运行 docker。\nkolla 要实现 openetack 部署总体上分为两步，第一步是制作 docker 镜像，第二步是编排部署。因此，kolla 项目又被分为两个小项目：kolla、kolla-ansible 。\nkolla-ansible项目 https://github.com/openstack/kolla-ansible\nkolla项目 https://tarballs.opendev.org/openstack/kolla/\ndockerhub镜像地址 https://hub.docker.com/u/kolla/\n部署 openstack 集群 安装环境准备 官方部署文档： https://docs.openstack.org/kolla-ansible/train/user/quickstart.html\n本次部署train版all-in-one单节点，使用一台centos7.8 minimal节点进行部署，该节点同时作为控制节点、计算节点、网络节点和cinder存储节点使用，同时也是kolla ansible的部署节点。\nkolla安装节点要求：\n2 network interfaces 8GB main memory 40GB disk space\n如果是vmware workstation环境，勾选处理器选项的虚拟化引擎相关功能，否则后面需要配置nova_compute_virt_type=qemu参数，这里选择勾选，跳过以下步骤。\ncat /etc/kolla/globals.yml nova_compute_virt_type: \u0026#34;qemu\u0026#34; #或者部署完成后手动调整 [root@kolla ~]# cat /etc/kolla/nova-compute/nova.conf |grep virt_type #virt_type = kvm virt_type = qemu [root@kolla ~]# docker restart nova_compute kolla的安装要求目标机器至少两块网卡，本次安装使用2块网卡对应管理网络和外部网络两个网络平面，在vmware workstation虚拟机新增一块网卡ens34：\nens32，NAT模式，管理网络，正常配置静态IP即可。租户网络与该网络复用，租户vm网络不单独创建网卡 ens34，桥接模式，外部网络，无需配置IP地址，这个其实是让neutron的br-ex 绑定使用，虚拟机通过这块网卡访问外网。\nens34网卡配置参考： https://docs.openstack.org/install-guide/environment-networking-controller.html\ncat \u0026gt; /etc/sysconfig/network-scripts/ifcfg-ens34 \u0026lt;\u0026lt;EOF NAME=ens34 DEVICE=ens34 TYPE=Ethernet ONBOOT=\u0026#34;yes\u0026#34; BOOTPROTO=\u0026#34;none\u0026#34; EOF #重新加载ens34网卡设备 nmcli con reload \u0026amp;\u0026amp; nmcli con up ens34 如果启用cinder还需要额外添加磁盘，这里以添加一块/dev/sdb磁盘为例，创建为物理卷并加入卷组。\npvcreate /dev/sdb vgcreate cinder-volumes /dev/sdb 注意卷组名称为cinder-volumes，默认与后面的globals.yml中定义一致。\n[root@kolla ~]# cat /etc/kolla/globals.yml | grep cinder_volume_group #cinder_volume_group: \u0026#34;cinder-volumes\u0026#34; 部署 kolla ansible 配置主机名,kolla预检查时rabbitmq可能需要能够进行主机名解析\nhostnamectl set-hostname kolla 安装依赖\nyum install -y python-devel libffi-devel gcc openssl-devel libselinux-python python2-pip python-pbr epel-release ansible 配置阿里云pip源，否则pip安装时会很慢\nmkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF 安装 kolla-ansible\nkolla版本与openstack版本对应关系：https://releases.openstack.org/teams/kolla.html\npip install setuptools==22.0.5 pip install pip==20.3.4 pip install wheel pip install kolla-ansible==9.1.0 --ignore-installed PyYAML 复制 kolla-ansible配置文件到当前环境\nmkdir -p /etc/kolla chown $USER:$USER /etc/kolla cp -r /usr/share/kolla-ansible/etc_examples/kolla/* /etc/kolla cp /usr/share/kolla-ansible/ansible/inventory/* . 修改ansible配置文件\ncat \u0026lt;\u0026lt; EOF | sed -i \u0026#39;/^\\[defaults\\]$/ r /dev/stdin\u0026#39; /etc/ansible/ansible.cfg host_key_checking=False pipelining=True forks=100 EOF 默认有all-in-one和multinode两个inventory文件，这里使用all-in-one，来规划集群角色，配置默认即可\n[root@kolla ~]# cat all-in-one | more 检查inventory配置是否正确，执行：\nansible -i all-in-one all -m ping 生成openstack组件用到的密码，该操作会填充/etc/kolla/passwords.yml，该文件中默认参数为空。\nkolla-genpwd 修改keystone_admin_password，可以修改为自定义的密码方便后续horizon登录，这里改为kolla。\n$ sed -i \u0026#39;s#keystone_admin_password:.*#keystone_admin_password: kolla#g\u0026#39; /etc/kolla/passwords.yml $ cat /etc/kolla/passwords.yml | grep keystone_admin_password keystone_admin_password: kolla 修改全局配置文件globals.yml，该文件用来控制安装哪些组件，以及如何配置组件，由于全部是注释，这里直接追加进去，也可以逐个找到对应项进行修改。\ncp /etc/kolla/globals.yml{,.bak} cat \u0026gt;\u0026gt; /etc/kolla/globals.yml \u0026lt;\u0026lt;EOF # Kolla options kolla_base_distro: \u0026#34;centos\u0026#34; kolla_install_type: \u0026#34;binary\u0026#34; openstack_release: \u0026#34;train\u0026#34; kolla_internal_vip_address: \u0026#34;192.168.150.155\u0026#34; # Docker options # docker_registry: \u0026#34;registry.cn-beijing.aliyuncs.com\u0026#34; # docker_namespace: \u0026#34;kollaimage\u0026#34; # Neutron - Networking Options network_interface: \u0026#34;ens32\u0026#34; neutron_external_interface: \u0026#34;ens34\u0026#34; neutron_plugin_agent: \u0026#34;openvswitch\u0026#34; enable_neutron_provider_networks: \u0026#34;yes\u0026#34; # OpenStack services enable_cinder: \u0026#34;yes\u0026#34; enable_cinder_backend_lvm: \u0026#34;yes\u0026#34; EOF 参数说明：\nkolla_base_distro: kolla镜像基于不同linux发型版构建，主机使用centos这里对应使用centos类型的docker镜像即可。 kolla_install_type: kolla镜像基于binary二进制和source源码两种类型构建，实际部署使用binary即可。 openstack_release: openstack版本可自定义，会从dockerhub拉取对应版本的镜像 kolla_internal_vip_address: 单节点部署kolla也会启用haproxy和keepalived，方便后续扩容为高可用集群，该地址是ens32网卡网络中的一个可用IP。 docker_registry: 默认从dockerhub拉取镜像，也可以本地搭建仓库，提前推送镜像上去。 docker_namespace: 阿里云kolla镜像仓库所在的命名空间，dockerhub官网默认是kolla。 network_interface: 管理网络的网卡 neutron_external_interface: 外部网络的网卡 neutron_plugin_agent: 默认启用openvswitch enable_neutron_provider_networks: 启用外部网络 enable_cinder: 启用cinder enable_cinder_backend_lvm: 指定cinder后端存储为lvm\n部署 openstack 组件 部署openstack\n# 预配置，安装docker、docker sdk、关闭防火墙、配置时间同步等 kolla-ansible -i ./all-in-one bootstrap-servers # 部署前环境检查，可能会报docker版本的错，可以忽略 kolla-ansible -i ./all-in-one prechecks # 拉取镜像，也可省略该步骤，默认会自动拉取 kolla-ansible -i ./all-in-one pull # 执行实际部署，拉取镜像，运行对应组件容器 kolla-ansible -i ./all-in-one deploy # 生成openrc文件 kolla-ansible post-deploy 以上部署没有报错中断说明部署成功，所有openstack组件以容器方式运行，查看容器\n[root@kolla ~]# docker ps -a 确认没有Exited等异常状态的容器\n[root@kolla ~]# docker ps -a | grep -v Up 本次部署运行了38个容器\n[root@localhost kolla-env]# docker ps -a | wc -l 39 查看拉取的镜像\n[root@kolla ~]# docker images | wc -l 39 [root@kolla ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE kolla/centos-binary-heat-api train b97df3444b35 10 months ago 1.11GB kolla/centos-binary-heat-engine train e19de6feec32 10 months ago 1.11GB ...... 查看cinder使用的卷，自动创建了lvm\n[root@kolla ~]# lsblk | grep cinder ├─cinder--volumes-cinder--volumes--pool_tmeta 253:3 0 20M 0 lvm │ └─cinder--volumes-cinder--volumes--pool 253:5 0 19G 0 lvm └─cinder--volumes-cinder--volumes--pool_tdata 253:4 0 19G 0 lvm └─cinder--volumes-cinder--volumes--pool 253:5 0 19G 0 lvm [root@kolla ~]# lvs | grep cinder cinder-volumes-pool cinder-volumes twi-a-tz-- 19.00g 0.00 10.55 另外需要注意，不要在该节点安装libvirt等工具，这些工具安装后可能会启用libvirtd和iscsid.sock等服务，kolla已经在容器中运行了这些服务，这些服务会调用节点上的sock文件，如果节点上也启用这些服务去抢占这些文件，会导致容器异常。默认kolla在预配置时也会主动禁用节点上的相关服务。\n安装 openStack 客户端 可以直接安装到服务器上或者使用docker安装容器\n推荐使用docker容器方式运行客户端\n使用docker容器作为客户端\ndocker run -d --name client \\ --restart always \\ -v /etc/kolla/admin-openrc.sh:/admin-openrc.sh:ro \\ -v /usr/share/kolla-ansible/init-runonce:/init-runonce:rw \\ kolla/centos-binary-openstack-base:train sleep infinity docker exec -it client bash source /admin-openrc.sh openstack service list yum安装openstack客户端\n#启用openstack存储库 yum install -y centos-release-openstack-train #安装openstack客户端 yum install -y python-openstackclient #启用selinux,安装openstack-selinux软件包以自动管理OpenStack服务的安全策略 yum install -y openstack-selinux #报错处理 pip uninstall urllib3 yum install -y python2-urllib3 运行 cirros 实例 kolla ansible提供了一个快速创建cirros demo实例的脚本/usr/share/kolla-ansible/init-runonce。\n脚本需要cirros镜像，如果网络较慢可以使用浏览器下载放在/opt/cache/files目录下：\nwget https://github.com/cirros-dev/cirros/releases/download/0.4.0/cirros-0.4.0-x86_64-disk.img mkdir -p /opt/cache/files/ mv cirros-0.4.0-x86_64-disk.img /opt/cache/files/ 定义init-runonce示例脚本外部网络配置：\n#定义init-runonce示例脚本外部网络配置 vim /usr/share/kolla-ansible/init-runonce EXT_NET_CIDR=${EXT_NET_CIDR:-\u0026#39;192.168.35/24\u0026#39;} EXT_NET_RANGE=${EXT_NET_RANGE:-\u0026#39;start=192.168.35.150,end=192.168.35.188\u0026#39;} EXT_NET_GATEWAY=${EXT_NET_GATEWAY:-\u0026#39;192.168.35.1\u0026#39;} #执行脚本，上传镜像到glance，创建内部网络、外部网络、flavor、ssh key，并运行一个实例 source /etc/kolla/admin-openrc.sh /usr/share/kolla-ansible/init-runonce 参数说明：\nEXT_NET_CIDR 指定外部网络，由于使用桥接模式，直接桥接到了电脑的无线网卡，所以这里网络就是无线网卡的网段。 EXT_NET_RANGE 指定从外部网络取出一个地址范围，作为外部网络的地址池 EXT_NET_GATEWAY 外部网络网关，这里与wifi网络使用的网关一致\n根据最终提示运行实例\nopenstack server create \\ --image cirros \\ --flavor m1.tiny \\ --key-name mykey \\ --network demo-net \\ demo1 访问 openstack horizon 访问openstack horizon需要使用vip地址，节点上可以看到由keepalived容器生成的vip\n[root@kolla ~]# ip a |grep ens32 2: ens32: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 192.168.150.101/24 brd 192.168.150.255 scope global noprefixroute dynamic ens32 inet 192.168.150.155/32 scope global ens32 浏览器直接访问该地址即可登录到horizon\nhttp://192.168.150.155\n我这里的用户名密码为admin/kolla，信息可以从admin-openrc.sh中获取\n[root@kolla ~]# cat /etc/kolla/admin-openrc.sh # Clear any old environment that may conflict. for key in $( set | awk \u0026#39;{FS=\u0026#34;=\u0026#34;} /^OS_/ {print $1}\u0026#39; ); do unset $key ; done export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=kolla export OS_AUTH_URL=http://192.168.150.155:35357/v3 export OS_INTERFACE=internal export OS_ENDPOINT_TYPE=internalURL export OS_IDENTITY_API_VERSION=3 export OS_REGION_NAME=RegionOne export OS_AUTH_PLUGIN=password 默认登录后如下\n在horizion查看创建的网络和实例\n登录实例控制台，验证实例与外网的连通性，cirros用户密码在初次登录时有提示：\n为实例绑定浮动IP地址，方便从外部ssh远程连接到实例\n点击+随机分配一个浮动IP\n在实例界面可以看到绑定的浮动ip\n在kolla节点上或者在集群外部使用SecureCRT等ssh工具连接到实例。cirros镜像默认用户密码为cirros/gocubsgo，该镜像信息官网有介绍： https://docs.openstack.org/image-guide/obtain-images.html#cirros-test\n[root@kolla ~]# ssh cirros@192.168.35.183 cirros@192.168.35.183\u0026#39;s password: 运行 centos 实例 centos官方维护有相关cloud image，如果不需要进行定制，可以直接下载来运行实例。\n参考：https://docs.openstack.org/image-guide/obtain-images.html\nCentOS官方维护的镜像下载地址： http://cloud.centos.org/centos/7/images/\n也可以使用命令直接下载镜像，但是下载可能较慢，建议下载好在进行上传。以centos7.8为例：\nwget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2c 下载完成后上传镜像到openstack，直接在horizon上传即可。也可以使用命令上传。\n注意：默认该镜像运行的实例只能使用ssh key以centos用户身份登录，如果需要使用root远程ssh连接到实例需要在上传前为镜像配置root免密并开启ssh访问。\n参考：https://blog.csdn.net/networken/article/details/106713658\n另外我们的命令客户端在容器中，所有这里有些不方便，首先要将镜像复制到容器中，然后使用openstack命令上传。\n这里复制到client容器的根目录下。\n[root@kolla ~]# docker cp CentOS-7-x86_64-GenericCloud-2003.qcow2c client:/ [root@kolla ~]# docker exec -it client bash ()[root@f11a103c5ade /]# ()[root@f11a103c5ade /]# source /admin-openrc.sh ()[root@f11a103c5ade /]# ls | grep CentOS CentOS-7-x86_64-GenericCloud-2003.qcow2c 执行以下openstack命令上传镜像\nopenstack image create \u0026#34;CentOS78-image\u0026#34; \\ --file CentOS-7-x86_64-GenericCloud-2003.qcow2c \\ --disk-format qcow2 --container-format bare \\ --public 创建实例\nopenstack server create \\ --image CentOS78-image \\ --flavor m1.small \\ --key-name mykey \\ --network demo-net \\ demo-centos 创建完成后为实例绑定浮动IP。\n如果实例创建失败可以查看相关组件报错日志\n[root@kolla ~]# tail -100f /var/log/kolla/nova/nova-compute.log 如果没有提前定制镜像修改root密码，只能使用centos用户及sshkey登录，由于是在容器中运行的demo示例，ssh私钥也保存在容器的默认目录下，在容器中连接实例浮动IP测试\n[root@kolla ~]# docker exec -it client bash ()[root@b86f87f7f101 ~]# ssh -i /root/.ssh/id_rsa centos@192.168.35.186 Last login: Fri Oct 29 08:10:42 2021 from 192.168.35.188 [centos@demo-centos ~]$ sudo -i [root@demo-centos ~]# 运行 ubuntu 实例 下载镜像\nwget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img docker cp bionic-server-cloudimg-amd64.img client:/ 上传镜像\nopenstack image create \u0026#34;Ubuntu1804\u0026#34; \\ --file bionic-server-cloudimg-amd64.img \\ --disk-format qcow2 --container-format bare \\ --public 创建实例\nopenstack server create \\ --image Ubuntu1804 \\ --flavor m1.small \\ --key-name mykey \\ --network demo-net \\ demo-ubuntu 绑定浮动ip\nubuntu镜像默认用户为ubuntu，首次登陆使用sshkey方式\n调整集群配置 新增 magnum \u0026amp; ironic 组件 magnum 和 ironic 默认状态下是没有安装的，在 /etc/kolla/globals.yml 可以看到默认配置\n#enable_magnum: \u0026#34;no\u0026#34; #enable_ironic: \u0026#34;no\u0026#34; 在 /etc/kolla/globals.yml 之前的配置下面新增如下，参数的具体含义查看 官方文档\n# ironic enable_ironic: true ironic_dnsmasq_interface: \u0026#34;enp11s0f1\u0026#34; ironic_dnsmasq_dhcp_range: \u0026#34;192.168.45.200,192.168.45.210\u0026#34; ironic_dnsmasq_default_gateway: 192.168.45.1 ironic_cleaning_network: \u0026#34;public1\u0026#34; ironic_dnsmasq_boot_file: pxelinux.0 # magnum enable_magnum: true ironic 组件还需要一些其他操作\nmkdir -p /etc/kolla/config/ironic/ curl https://tarballs.openstack.org/ironic-python-agent/dib/files/ipa-centos7-master.kernel -o /etc/kolla/config/ironic/ironic-agent.kernel curl https://tarballs.openstack.org/ironic-python-agent/dib/files/ipa-centos7-master.initramfs -o /etc/kolla/config/ironic/ironic-agent.initramfs 在现有集群中新增组件\nkolla-ansible -i all-in-one deploy --tags horizon,magnum,ironic 修改组件配置 集群部署完成后需要开启新的组件或者扩容，可以修改/etc/kolla/global.yml调整参数。 或者在/etc/kolla/config目录下创建自定义配置文件，例如\n# mkdir -p /etc/kolla/config/nova # vim /etc/kolla/config/nova/nova.conf [DEFAULT] block_device_allocate_retries = 300 block_device_allocate_retries_interval = 3 重新配置openstack，kolla会自动重建配置变动的容器组件。\nkolla-ansible -i all-in-one reconfigure -t nova kolla配置和日志文件 各个组件配置文件目录： /etc/kolla/ 各个组件日志文件目录：/var/log/kolla/ 清理kolla ansilbe集群 kolla-ansible destroy --include-images --yes-i-really-really-mean-it # 或者 [root@kolla ~]# cd /usr/share/kolla-ansible/tools/ [root@all tools]# ./cleanup-containers [root@all tools]# ./cleanup-host #重置cinder卷，谨慎操作 vgremove cinder-volumes 重新部署 kolla ansible 集群 ## 清除操作 先关闭所有运行的实例，再进行下面操作 [root@kolla ~]# cd /usr/share/kolla-ansible/tools/ [root@all tools]# ./cleanup-containers vgremove cinder-volumes ## 重建操作 pvcreate /dev/sdb vgcreate cinder-volumes /dev/sdb kolla-ansible -i ./all-in-one deploy kolla-ansible post-deploy 可能遇到的问题 虚拟ip分配失败 这种情况多半是由于虚拟ip没有分配到，并不是端口问题\n解决方法1 在全局的配置中添加/修改这个id值，必须是0-255之间的数字，并且确保在整个二层网络中是唯一的\nvim /etc/kolla/globals.yml keepalived_virtual_router_id: \u0026#34;199\u0026#34; https://www.bianchengquan.com/article/506138.html\n解决方法2 https://www.nuomiphp.com/serverfault/en/5fff3e4524544316281a16b0.html\n参考 官方文档\nhttps://blog.csdn.net/networken/article/details/106728002\nhttps://blog.csdn.net/qq_33316576/article/details/107457111\nhttps://blog.csdn.net/networken/article/details/106745167\n","permalink":"https://www.lvbibir.cn/en/posts/tech/openstack-kolla-ansible-allinone-train/","summary":"kolla ansible简介 kolla 的使命是为 openstack 云平台提供生产级别的、开箱即用的交付能力。kolla 的基本思想是一切皆容器，将所有服务基于 Docker 运行，并且保证一个容器只跑一个服务（进程），做到最小粒度的运行 docker。 kolla 要实现 openetack 部署总体上分为两步，第一步是制作 docker 镜像，第二步是编排部署。因此，ko","title":"kolla-ansible 部署 Train版 openstack（all-in-one）"},{"content":"Kubernetes概述 kubernetes是什么\nkubernetes 是 Google 在 2014年开源的一个容器集群管理平台，kubernetes简称 k8s k8s用于容器化应用程序的部署，扩展和管理。 k8s提供了容器的编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能 kubernetes目标是让部署容器化应用简单高效 Kubernetes特性\n自我修复\n在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端请求，确保线上服务不中断。\n伸缩性\n使用命令、UI或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行服务。\n自动部署和回滚\nK8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不受影响业务。\n服务发现和负载均衡\nK8S为多个容器提供一个统一访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。\n机密和配置管理\n管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。\n存储编排\n挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网络存储（如NFS、GlusterFS、Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。\n批处理\n提供一次性任务，定时任务；满足批量数据处理和分析的场景。\nKubeadm概述\nkubeadm是Kubernetes项目自带的及集群构建工具，负责执行构建一个最小化的可用集群以及将其启动等的必要基本步骤，kubeadm是Kubernetes集群全生命周期的管理工具，可用于实现集群的部署、升级、降级及拆除。kubeadm部署Kubernetes集群是将大部分资源以pod的方式运行，例如（kube-proxy、kube-controller-manager、kube-scheduler、kube-apiserver、flannel)都是以pod方式运行。\nKubeadm仅关心如何初始化并启动集群，余下的其他操作，例如安装Kubernetes Dashboard、监控系统、日志系统等必要的附加组件则不在其考虑范围之内，需要管理员自行部署。\nKubeadm集成了Kubeadm init和kubeadm join等工具程序，其中kubeadm init用于集群的快速初始化，其核心功能是部署Master节点的各个组件，而kubeadm join则用于将节点快速加入到指定集群中，它们是创建Kubernetes集群最佳实践的“快速路径”。另外，kubeadm token可于集群构建后管理用于加入集群时使用的认证令牌（token)，而kubeadm reset命令的功能则是删除集群构建过程中生成的文件以重置回初始状态。\n1. 环境准备 基于centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n部署Kubernetes集群需要满足每个节点至少满足2核CPU、2G内存和30GB硬盘且都可以访问外网\n角色 IP k8s-node1 1.1.1.1 k8s-node2 1.1.1.2 k8s-node3 1.1.1.3 1.1 基础配置 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭selinux sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭swap swapoff -a # 临时 vim /etc/fstab # 永久, 注释掉swap分区相关行 # 设置主机名 hostnamectl set-hostname \u0026lt;hostname\u0026gt; # 添加hosts cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 1.1.1.1 k8s-node1 1.1.1.2 k8s-node2 1.1.1.3 k8s-node3 EOF # 将桥接的IPv4流量传递到iptables的链 cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 生效 # 时间同步 timedatectl set-timezone Asia/Shanghai yum install ntpdate -y ntpdate time.windows.com 1.2 安装Docker Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo yum list docker-ce --show-duplicates yum install docker-ce-20.10.23-3.el7.x86_64 配置镜像下载加速器，同时修改docker的cgroupdriver为systemd\nmkdir /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34; ], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF systemctl daemon-reload systemctl enable docker \u0026amp;\u0026amp; systemctl start docker docker info 1.3 kubeadm/kubelet/kubectl 添加阿里云YUM软件源\ncat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 这里指定版本号部署\nyum install -y kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3 systemctl enable kubelet systemctl start kubelet 2. 部署Kubernetes Master https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node\n在1.1.1.1（Master）执行。\nkubeadm init \\ --apiserver-advertise-address=1.1.1.1 \\ --kubernetes-version v1.22.3 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all \\ --image-repository registry.aliyuncs.com/google_containers \u0026ndash;apiserver-advertise-address 集群通告地址 \u0026ndash;kubernetes-version K8s版本，与上面安装的一致 \u0026ndash;service-cidr 集群内部虚拟网络，Pod统一访问入口 \u0026ndash;pod-network-cidr Pod网络，与下面部署的CNI网络组件yaml中保持一致 \u0026ndash;ignore-preflight-errors=all，跳过一些错误 \u0026ndash;image-repository 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址 或者使用配置文件引导：\ncat \u0026gt; kubeadm.conf \u0026lt;\u0026lt; EOF apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.22.3 imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12 EOF kubeadm init --config kubeadm.conf --ignore-preflight-errors=all 拷贝kubectl使用的连接k8s认证文件到默认路径：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看k8s集群状态\nkubectl get cs NAME STATUS MESSAGE ERROR scheduler Unhealthy Get \u0026#34;http://127.0.0.1:10251/healthz\u0026#34;: dial tcp 127.0.0.1:10251: connect: connection refused controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} vim /etc/kubernetes/manifests/kube-scheduler.yaml # 注释掉 --port=0 ，scheduler会自动重启，稍等一小会状态变为正常 kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} 3. 加入Kubernetes Node https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/\n在192.168.150.102/103（Node）执行。\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\nkubeadm join 1.1.1.1:6443 --token esce21.q6hetwm8si29qxwn \\ --discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5 默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\nkubeadm token create kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; 63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 kubeadm join 1.1.1.1:6443 --token nuja6n.o3jrhsffiqs9swnu --discovery-token-ca-cert-hash sha256:63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 或者直接命令快捷生成: kubeadm token create --print-join-command\n4. 部署容器网络(cni) Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。\nCalico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。\n此外，Calico项目还实现了Kubernetes网络策略，提供ACL功能。\nquickstart\n版本对照表，在此页面可以看到calico每个版本支持的kubernetes的版本\n安装calico\nwget --no-check-certificate https://docs.tigera.io/archive/v3.23/manifests/calico.yaml 修改Pod网络和网卡识别参数，Pod网络与前面kubeadm init指定的一样\n[root@k8s-node1 ~]# vim calico.yaml # 修改位置：DaemonSet.spec.template.spec.containers.env # 新增如下四行 - name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.244.0.0/16\u0026#34; - name: IP_AUTODETECTION_METHOD value: interface=bond*,ens* #网卡名根据实际情况修改 kubectl apply -f calico.yaml kubectl get pods -n kube-system # 所有Pod起来后，节点状态应该都是Ready状态了 [root@k8s-node1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 153m v1.22.3 k8s-node2 Ready \u0026lt;none\u0026gt; 151m v1.22.3 k8s-node3 Ready \u0026lt;none\u0026gt; 151m v1.22.3 5. metric-server cadvisor负责提供数据，已集成到k8s中\nMetrics-server负责数据汇总，需额外安装\n下载yaml\nwget --no-check-certificate https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.0/components.yaml mv components.yaml metrics-server.yaml 修改yaml\ncontainers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP # 第一处修改 - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # 第二处修改 image: registry.aliyuncs.com/google_containers/metrics-server:v0.6.0 # 第三处修改 imagePullPolicy: IfNotPresent \u0026ndash;kubelet-insecure-tls\n不验证kubelet自签的证书\n\u0026ndash;kubelet-preferred-address-types=InternalIP\nMetrics-server连接cadvisor默认通过主机名即node的名称进行连接，而Metric-server作为pod运行在集群中默认是无法解析的，所以这里修改成通过节点ip连接\n部署metrics-server\n[root@k8s-node1 ~]# kubectl apply -f metrics-server.yaml [root@k8s-node1 ~]# kubectl get pods -n kube-system -l k8s-app=metrics-server NAME READY STATUS RESTARTS AGE metrics-server-7f66b69ff6-bkfqg 1/1 Running 0 59s [root@k8s-node1 ~]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-node1 226m 11% 2004Mi 54% k8s-node2 97m 4% 1047Mi 28% k8s-node3 98m 4% 1096Mi 29% 6. 测试kubernetes集群 验证Pod工作 验证Pod网络通信 验证DNS解析 在Kubernetes集群中创建一个pod，验证是否正常运行：\nkubectl create deployment nginx --image=nginx kubectl expose deployment nginx --type=NodePort --port=80 --target-port=80 [root@k8s-node1 ~]# kubectl get pod,deploy,svc NAME READY STATUS RESTARTS AGE pod/nginx-6799fc88d8-57bqd 1/1 Running 0 10m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1/1 1 1 10m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 170m service/nginx NodePort 10.102.188.108 \u0026lt;none\u0026gt; 80:30954/TCP 2m31s 访问地址：http://1.1.1.1:30954，端口是固定的，ip可以是集群内任一节点的ip\n7. 部署Dashboard wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml 默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：\nvi recommended.yaml ... kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard type: NodePort ... kubectl apply -f recommended.yaml kubectl get pods -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-6b4884c9d5-gl8nr 1/1 Running 0 13m kubernetes-dashboard-7f99b75bf4-89cds 1/1 Running 0 13m 访问地址：https://NodeIP:30001\n创建service account并绑定默认cluster-admin管理员集群角色：\n# 创建用户 kubectl create serviceaccount dashboard-admin -n kube-system # 用户授权 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取用户Token kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 使用输出的token登录Dashboard。\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-deploy-v1.22.3/","summary":"Kubernetes概述 kubernetes是什么 kubernetes 是 Google 在 2014年开源的一个容器集群管理平台，kubernetes简称 k8s k8s用于容器化应用程序的部署，扩展和管理。 k8s提供了容器的编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能 kubernetes目标是让部署容器化应","title":"kubernetes | kubeadm 搭建 K8s集群v1.22.3"},{"content":"openssh-8.7p1 编译环境 编译平台：\tvmware workstation\n系统版本：\t普华服务器操作系统v4.2\n系统内核：\t3.10.0-327.el7.isoft.x86_64\n软件版本：\nopenssh-8.7p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 编译步骤 yum安装依赖工具\nyum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.7p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.7p1.tar.gz cp openssh-8.7p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 操作验证\ncd /root/rpmbuild/RPMS/x86_64/ vim run.sh #!/bin/bash set -e cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd chmod 755 run.sh ./run.sh ssh -V 打包归档\n[root@localhost ~]# cd /root/rpmbuild/RPMS/x86_64/ [root@localhost x86_64]# ls openssh-8.7p1-1.el7.isoft.x86_64.rpm openssh-askpass-8.7p1-1.el7.isoft.x86_64.rpm openssh-askpass-gnome-8.7p1-1.el7.isoft.x86_64.rpm openssh-clients-8.7p1-1.el7.isoft.x86_64.rpm openssh-debuginfo-8.7p1-1.el7.isoft.x86_64.rpm openssh-server-8.7p1-1.el7.isoft.x86_64.rpm run.sh [root@localhost x86_64]# vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd [root@localhost x86_64]# tar zcvf openssh-8.7p1.rpm.x86_64.tar.gz ./* [root@localhost x86_64]# mv openssh-8.7p1.rpm.x86_64.tar.gz /root 使用 tar zxf openssh-8.7p1.rpm.x86_64.tar.gz ./run.sh openssh-9.0p1 编译环境 编译平台：\tvmware workstation\n系统版本：\t普华服务器操作系统v3.0\n系统内核：\n2.6.32-279.el6.isoft.x86_64 2.6.32-504.el6.isoft.x86_64 软件版本：\nopenssh-9.0p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 这两个内核版本步骤基本一样，区别在于 279 内核需要升级 openssl\n编译步骤 添加阿里云yum源和本地yum源\n# 阿里yum源 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-6.10.repo # 本地yum源 mount /dev/sr0 /mnt/ cat \u0026gt; /etc/yum.repos.d/local.repo \u0026lt;\u0026lt;EOF [local] name=local baseurl=file:///mnt gpgcheck=0 enabled=1 EOF yum安装依赖工具\nyum clean all yum makecache yum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://mirrors.aliyun.com/pub/OpenBSD/OpenSSH/portable/openssh-9.0p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz --no-check-certificate tar -zxf openssh-9.0p1.tar.gz cp openssh-9.0p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 添加缺少的文件\ncd /root/rpmbuild/SOURCES/openssh-9.0p1/contrib/redhat cp sshd.init sshd.init.old cp sshd.pam sshd.pam.old 重新打包，否则会报错找不到 sshd.pam.old 和 sshd.init.old\ncd /root/rpmbuild/SOURCES/ tar zcf openssh-9.0p1.tar.gz openssh-9.0p1 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 注意，从这步开始两个内核版本的后续操作不太相同\n2.6.32-279.el6.isoft.x86_64 准备目录\nmkdir -pv /root/openssh-9.0p1-rpms/openssl-1.0.1e-rpms/ cp /root/rpmbuild/RPMS/x86_64/* /root/openssh-9.0p1-rpms/ 下载 openssl-1.0.1e 离线包\n这步由于之前安装编译的依赖的时候已经安装过，可以用全新的系统重新下载 openssl-1.0.1e 的依赖\nyum install -y yum-plugin-downloadonly yum install openssl openssl-devel --downloadonly --downloaddir=/root/openssh-9.0p1-rpms/openssl-1.0.1e-rpms/ 编写升级脚本\ncat \u0026gt; /root/openssh-9.0p1-rpms/run.sh \u0026lt;\u0026lt;EOF #!/bin/bash set -e cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -e --nodeps libsepol-2.0.41-4.el6.isoft.x86_64 rpm -Uvh ./openssl-1.0.1e-rpms/*.rpm rpm -Uvh ./*.rpm cp /etc/pam.d/sshd_bak /etc/pam.d/sshd cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key service sshd restart ssh -V EOF chmod 755 /root/openssh-9.0p1-rpms/run.sh 打包\ntar zcf /root/openssh-9.0p1-rpms.tar.gz /root/openssh-9.0p1-rpms 2.6.32-504.el6.isoft.x86_64 准备目录\nmkdir /root/openssh-9.0p1-rpms/ cp /root/rpmbuild/RPMS/x86_64/* /root/openssh-9.0p1-rpms/ 编写升级脚本\ncat \u0026gt; /root/openssh-9.0p1-rpms/run.sh \u0026lt;\u0026lt;EOF #!/bin/bash set -e ssh -V /bin/cp /etc/pam.d/sshd /etc/pam.d/sshd_bak /bin/cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm /bin/cp /etc/pam.d/sshd_bak /etc/pam.d/sshd /bin/cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key service sshd restart ssh -V EOF chmod 755 /root/openssh-9.0p1-rpms/run.sh 打包\ntar zcf /root/openssh-9.0p1-rpms.tar.gz /root/openssh-9.0p1-rpms 使用 tar zxf openssh-9.0p1-rpms.tar.gz cd openssh-9.0p1-rpms sh run.sh openssh-8.6p1-aarch64 编译环境 系统版本：普华服务器操作系统openeuler版\n系统内核：4.19.90-2003.4.0.0036.oe1.aarch64\n软件版本：\nopenssh-8.6p1.tar.gz\nx11-ssh-askpass-1.2.4.1.tar.gz\n编译步骤 dnf安装依赖工具\ndnf install gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.6p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.6p1.tar.gz cp openssh-8.6p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 修改下面两行 %attr(4711,root,root) %{_libexecdir}/openssh/ssh-sk-helper %attr(0644,root,root) %{_mandir}/man8/ssh-sk-helper.8.gz 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 操作验证\ncd /root/rpmbuild/RPMS/aarch64 vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd chmod 755 run.sh ./run.sh ssh -V OpenSSH_8.6p1, OpenSSL 1.1.1d 10 Sep 2019 从版本看，ssh已经升级成功。但是每次重启服务都会提示sshd的unit文件发生改变，需要执行systemctl daemon-reload。执行完reload后重启sshd依旧报错Warning: The unit file, source configuration file or drop-ins of sshd.service changed on disk. Run \u0026lsquo;systemctl daemon-reload\u0026rsquo; to reload units. 先不管这个问题，测试下sshd服务是否正常。\n用终端连接试试\n一切正常，如果出现PAM unable to dlopen(/usr/lib64/security/pam_stack.so): /usr/lib64/security/pam_stack.so: cannot open shared object file: No such file or directory类似报错，需要还原原先的/etc/pam.d/sshd文件\n继续看之前那个报错，一般这种错误为服务的配置文件或者unit文件发生改变，需要执行daemon-reload重新加载一下，逐个排查\n查看配置文件 查看unit文件 没有找到sshd.service的unit文件，find查找一下 第一个文件是老版本ssh的残留的自启的unit链接文件，已经失效了。第三个和第四个文件都是第二个文件的链接文件。 不知为何我们自己编译的ssh安装后unit文件会放到这个位置，后续再研究，尝试自己写一份unit文件，试试能不能恢复sshd。\n备份unit文件\n[root@localhost ~]# cp /run/systemd/generator.late/sshd.service /root/sshd.service-20210702 查看unit文件中的控制参数和pid文件位置等 自建一个unit文件，放到/usr/lib/systemd/system目录\n[root@localhost ~]# vim /usr/lib/systemd/system/sshd.service [UNIT] Description=OpenSSH server daemon After=network.target sshd-keygen.target Wants=sshd-keygen.target [Service] Type=forking ExecStart=/etc/rc.d/init.d/sshd start ExecReload=/etc/rc.d/init.d/sshd restart ExecStop=/etc/rc.d/init.d/sshd stop PrivateTmp=True [Install] WantedBy=multi-user.target [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart sshd [root@localhost ~]# systemctl status sshd [root@localhost ~]# ssh -V OpenSSH_8.6p1, OpenSSL 1.1.1d 10 Sep 2019 打包归档\n[root@localhost ~]# cp /usr/lib/systemd/system/sshd.service /root/rpmbuild/RPMS/aarch64/ [root@localhost ~]# cd /root/rpmbuild/RPMS/aarch64/ [root@localhost aarch64]# ls openssh-8.6p1-1.isoft.isoft.aarch64.rpm openssh-debugsource-8.6p1-1.isoft.isoft.aarch64.rpm openssh-askpass-8.6p1-1.isoft.isoft.aarch64.rpm openssh-server-8.6p1-1.isoft.isoft.aarch64.rpm openssh-askpass-gnome-8.6p1-1.isoft.isoft.aarch64.rpm run.sh openssh-clients-8.6p1-1.isoft.isoft.aarch64.rpm sshd.service openssh-debuginfo-8.6p1-1.isoft.isoft.aarch64.rpm [root@localhost aarch64]# vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config cp ./sshd.service /usr/lib/systemd/system/sshd.service rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd systemctl enable sshd [root@localhost aarch64]# tar zcvf openssh-8.6p1-rpm-aarch64.tar.gz ./* [root@localhost aarch64]# mv openssh-8.6p1-rpm-aarch64.tar.gz /root 参考 systemd和sysv的服务管理\nsystemd-sysv-generator 中文手册\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-build-openssh/","summary":"openssh-8.7p1 编译环境 编译平台： vmware workstation 系统版本： 普华服务器操作系统v4.2 系统内核： 3.10.0-327.el7.isoft.x86_64 软件版本： openssh-8.7p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 编译步骤 yum安装依赖工具 yum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录 mkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置 cd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.7p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.7p1.tar.gz cp openssh-8.7p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define","title":"openssh源码打包编译成rpm包"},{"content":"代码如下\n#!/bin/bash #参数定义 date=`date +\u0026#34;%Y-%m-%d-%H:%M:%S\u0026#34;` centosVersion=$(awk \u0026#39;{print $(NF-1)}\u0026#39; /etc/redhat-release) VERSION=`date +%F` #日志相关 LOGPATH=\u0026#34;/tmp/awr\u0026#34; [ -e $LOGPATH ] || mkdir -p $LOGPATH RESULTFILE=\u0026#34;$LOGPATH/HostCheck-`hostname`-`date +%Y%m%d`.txt\u0026#34; #调用函数库 [ -f /etc/init.d/functions ] \u0026amp;\u0026amp; source /etc/init.d/functions export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile #root用户执行脚本 [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34;请用root用户执行此脚本！\u0026#34; \u0026amp;\u0026amp; exit 1 function version(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;[${date}] \u0026gt;\u0026gt;\u0026gt; `hostname -s` 主机巡检\u0026#34; } function getSystemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m****************************************************系统检查****************************************************\\033[0m\u0026#34; if [ -e /etc/sysconfig/i18n ];then default_LANG=\u0026#34;$(grep \u0026#34;LANG=\u0026#34; /etc/sysconfig/i18n | grep -v \u0026#34;^#\u0026#34; | awk -F \u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39;)\u0026#34; else default_LANG=$LANG fi export LANG=\u0026#34;en_US.UTF-8\u0026#34; Release=$(cat /etc/redhat-release 2\u0026gt;/dev/null) Kernel=$(uname -r) OS=$(uname -o) Hostname=$(uname -n) SELinux=$(/usr/sbin/sestatus | grep \u0026#34;SELinux status: \u0026#34; | awk \u0026#39;{print $3}\u0026#39;) LastReboot=$(who -b | awk \u0026#39;{print $3,$4}\u0026#39;) uptime=$(uptime | sed \u0026#39;s/.*up \\([^,]*\\), .*/\\1/\u0026#39;) echo \u0026#34; 系统：$OS\u0026#34; echo \u0026#34; 发行版本：$Release\u0026#34; echo \u0026#34; 内核：$Kernel\u0026#34; echo \u0026#34; 主机名：$Hostname\u0026#34; echo \u0026#34; SELinux：$SELinux\u0026#34; echo \u0026#34;语言/编码：$default_LANG\u0026#34; echo \u0026#34; 当前时间：$(date +\u0026#39;%F %T\u0026#39;)\u0026#34; echo \u0026#34; 最后启动：$LastReboot\u0026#34; echo \u0026#34; 运行时间：$uptime\u0026#34; export LANG=\u0026#34;$default_LANG\u0026#34; } function getCpuStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m****************************************************CPU检查*****************************************************\\033[0m\u0026#34; Physical_CPUs=$(grep \u0026#34;physical id\u0026#34; /proc/cpuinfo| sort | uniq | wc -l) Virt_CPUs=$(grep \u0026#34;processor\u0026#34; /proc/cpuinfo | wc -l) CPU_Kernels=$(grep \u0026#34;cores\u0026#34; /proc/cpuinfo|uniq| awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39;) CPU_Type=$(grep \u0026#34;model name\u0026#34; /proc/cpuinfo | awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39; | sort | uniq) CPU_Arch=$(uname -m) echo \u0026#34;物理CPU个数:$Physical_CPUs\u0026#34; echo \u0026#34;逻辑CPU个数:$Virt_CPUs\u0026#34; echo \u0026#34;每CPU核心数:$CPU_Kernels\u0026#34; echo \u0026#34; CPU型号:$CPU_Type\u0026#34; echo \u0026#34; CPU架构:$CPU_Arch\u0026#34; } function getMemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************内存检查*****************************************************\\033[0m\u0026#34; if [[ $centosVersion \u0026lt; 7 ]];then free -mo else free -h fi #报表信息 MemTotal=$(grep MemTotal /proc/meminfo| awk \u0026#39;{print $2}\u0026#39;) #KB MemFree=$(grep MemFree /proc/meminfo| awk \u0026#39;{print $2}\u0026#39;) #KB let MemUsed=MemTotal-MemFree MemPercent=$(awk \u0026#34;BEGIN {if($MemTotal==0){printf 100}else{printf \\\u0026#34;%.2f\\\u0026#34;,$MemUsed*100/$MemTotal}}\u0026#34;) } function getDiskStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************磁盘检查******************************************************\\033[0m\u0026#34; df -hiP | sed \u0026#39;s/Mounted on/Mounted/\u0026#39;\u0026gt; /tmp/inode df -hTP | sed \u0026#39;s/Mounted on/Mounted/\u0026#39;\u0026gt; /tmp/disk join /tmp/disk /tmp/inode | awk \u0026#39;{print $1,$2,\u0026#34;|\u0026#34;,$3,$4,$5,$6,\u0026#34;|\u0026#34;,$8,$9,$10,$11,\u0026#34;|\u0026#34;,$12}\u0026#39;| column -t #报表信息 diskdata=$(df -TP | sed \u0026#39;1d\u0026#39; | awk \u0026#39;$2!=\u0026#34;tmpfs\u0026#34;{print}\u0026#39;) #KB disktotal=$(echo \u0026#34;$diskdata\u0026#34; | awk \u0026#39;{total+=$3}END{print total}\u0026#39;) #KB diskused=$(echo \u0026#34;$diskdata\u0026#34; | awk \u0026#39;{total+=$4}END{print total}\u0026#39;) #KB diskfree=$((disktotal-diskused)) #KB diskusedpercent=$(echo $disktotal $diskused | awk \u0026#39;{if($1==0){printf 100}else{printf \u0026#34;%.2f\u0026#34;,$2*100/$1}}\u0026#39;) inodedata=$(df -iTP | sed \u0026#39;1d\u0026#39; | awk \u0026#39;$2!=\u0026#34;tmpfs\u0026#34;{print}\u0026#39;) inodetotal=$(echo \u0026#34;$inodedata\u0026#34; | awk \u0026#39;{total+=$3}END{print total}\u0026#39;) inodeused=$(echo \u0026#34;$inodedata\u0026#34; | awk \u0026#39;{total+=$4}END{print total}\u0026#39;) inodefree=$((inodetotal-inodeused)) inodeusedpercent=$(echo $inodetotal $inodeused | awk \u0026#39;{if($1==0){printf 100}else{printf \u0026#34;%.2f\u0026#34;,$2*100/$1}}\u0026#39;) } function get_resource(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************资源消耗统计**************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m*************带宽资源消耗统计*************\\033[0m\u0026#34; #用数组存放网卡名 nic=(`ifconfig | grep ^[a-z] | grep -vE \u0026#39;lo|docker0\u0026#39;| awk -F: \u0026#39;{print $1}\u0026#39;`) time=`date \u0026#34;+%Y-%m-%d %k:%M\u0026#34;` num=0 for ((i=0;i\u0026lt;${#nic[@]};i++)) do #循环五次，避免看到的是偶然的数据 while (( $num\u0026lt;5 )) do rx_before=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $2}\u0026#39;) tx_before=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $10}\u0026#39;) sleep 2 #用sed先获取第7列,再用awk获取第2列，再cut切割,从第7个到最后，即只切割网卡流量数字部分 rx_after=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $2}\u0026#39;) tx_after=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $10}\u0026#39;) #注意下面截取的相差2秒的两个时刻的累计和发送的bytes(即累计传送和接收的位) rx_result=$[(rx_after-rx_before)/1024/1024/2*8] tx_result=$[(tx_after-tx_before)/1024/1024/2*8] echo \u0026#34;$time Now_In_Speed: $rx_result Mbps Now_OUt_Speed: $tx_result Mbps\u0026#34; \u0026gt;\u0026gt; /tmp/network.txt let \u0026#34;num++\u0026#34; done #注意下面grep后面的$time变量要用双引号括起来 rx_result=$(cat /tmp/network.txt|grep \u0026#34;$time\u0026#34;|awk \u0026#39;{In+=$4}END{print In}\u0026#39;) tx_result=$(cat /tmp/network.txt|grep \u0026#34;$time\u0026#34;|awk \u0026#39;{Out+=$7}END{print Out}\u0026#39;) In_Speed=$(echo \u0026#34;scale=2;$rx_result/5\u0026#34;|bc) Out_Speed=$(echo \u0026#34;scale=2;$tx_result/5\u0026#34;|bc) echo -e \u0026#34;\\033[32m In_Speed_average: $In_Speed Mbps Out_Speed_average: $Out_Speed Mbps! \\033[0m\u0026#34; done echo -e \u0026#34;\\033[36m*************CPU资源消耗统计*************\\033[0m\u0026#34; #使用vmstat 1 5命令统计5秒内的使用情况，再计算每秒使用情况 total=`vmstat 1 5|awk \u0026#39;{x+=$13;y+=$14}END{print x+y}\u0026#39;` cpu_average=$(echo \u0026#34;scale=2;$total/5\u0026#34;|bc) #判断CPU使用率（浮点数与整数比较） if [ `echo \u0026#34;${cpu_average} \u0026gt; 70\u0026#34; | bc` -eq 1 ];then echo -e \u0026#34;\\033[31m Total CPU is already use: ${cpu_average}%,请及时处理！\\033[0m\u0026#34; else echo -e \u0026#34;\\033[32m Total CPU is already use: ${cpu_average}%! \\033[0m\u0026#34; fi echo -e \u0026#34;\\033[36m*************磁盘资源消耗统计*************\\033[0m\u0026#34; #磁盘使用情况(注意：需要用sed先进行格式化才能进行累加处理) disk_used=$(df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{used+=$3} END{print used}\u0026#39;) disk_totalSpace=$(df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{totalSpace+=$2} END{print totalSpace}\u0026#39;) disk_all=$(echo \u0026#34;scale=4;$disk_used/$disk_totalSpace\u0026#34; | bc) disk_percent1=$(echo $disk_all | cut -c 2-3) disk_percent2=$(echo $disk_all | cut -c 4-5) disk_warning=`df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{if ($5\u0026gt;85) print $6 \u0026#34;目录使用率：\u0026#34; $5;} \u0026#39;` echo -e \u0026#34;\\033[32m Total disk has used: $disk_percent1.$disk_percent2% \\033[0m\u0026#34; #echo -e \u0026#34;\\t\\t..\u0026#34; 表示换行 if [ -n \u0026#34;$disk_warning\u0026#34; ];then echo -e \u0026#34;\\033[31m${disk_warning} \\n [Error]以上目录使用率超过85%，请及时处理！\\033[0m\u0026#34; fi echo -e \u0026#34;\\033[36m*************内存资源消耗统计*************\\033[0m\u0026#34; #获得系统总内存 memery_all=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $2}\u0026#39;) #获得占用内存（操作系统 角度） system_memery_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) #获得buffer、cache占用内存，当内存不够时会及时回收，所以这两部分可用于可用内存的计算 buffer_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $6}\u0026#39;) cache_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $7}\u0026#39;) #获得被使用内存，所以这部分可用于可用内存的计算，注意计算方法 actual_used_all=$[memery_all-(free+buffer_used+cache_used)] #获得实际占用的内存 actual_used_all=`expr $memery_all - $free + $buffer_used + $cache_used ` memery_percent=$(echo \u0026#34;scale=4;$system_memery_used / $memery_all\u0026#34; | bc) memery_percent2=$(echo \u0026#34;scale=4; $actual_used_all / $memery_all\u0026#34; | bc) percent_part1=$(echo $memery_percent | cut -c 2-3) percent_part2=$(echo $memery_percent | cut -c 4-5) percent_part11=$(echo $memery_percent2 | cut -c 2-3) percent_part22=$(echo $memery_percent2 | cut -c 4-5) #获得占用内存（操作系统角度） echo -e \u0026#34;\\033[32m system memery is already use: $percent_part1.$percent_part2% \\033[0m\u0026#34; #获得实际内存占用率 echo -e \u0026#34;\\033[32m actual memery is already use: $percent_part11.$percent_part22% \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m buffer is already used : $buffer_used M \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m cache is already used : $cache_used M \\033[0m\u0026#34; } function getServiceStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************服务检查*******************************************************\\033[0m\u0026#34; echo \u0026#34;\u0026#34; if [[ $centosVersion \u0026gt; 7 ]];then conf=$(systemctl list-unit-files --type=service --state=enabled --no-pager | grep \u0026#34;enabled\u0026#34;) process=$(systemctl list-units --type=service --state=running --no-pager | grep \u0026#34;.service\u0026#34;) else conf=$(/sbin/chkconfig | grep -E \u0026#34;:on|:启用\u0026#34;) process=$(/sbin/service --status-all 2\u0026gt;/dev/null | grep -E \u0026#34;is running|正在运行\u0026#34;) fi echo -e \u0026#34;\\033[36m******************服务配置******************\\033[0m\u0026#34; echo \u0026#34;$conf\u0026#34; | column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m**************正在运行的服务****************\\033[0m\u0026#34; echo \u0026#34;$process\u0026#34; } function getAutoStartStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************自启动检查*******************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m****************自启动命令*****************\\033[0m\u0026#34; conf=$(grep -v \u0026#34;^#\u0026#34; /etc/rc.d/rc.local| sed \u0026#39;/^$/d\u0026#39;) echo \u0026#34;$conf\u0026#34; } function getLoginStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************登录检查********************************************************\\033[0m\u0026#34; last | head } function getNetworkStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************网络检查********************************************************\\033[0m\u0026#34; if [[ $centosVersion \u0026lt; 7 ]];then /sbin/ifconfig -a | grep -v packets | grep -v collisions | grep -v i net6 else #ip a for i in $(ip link | grep BROADCAST | awk -F: \u0026#39;{print $2}\u0026#39;);do ip add show $i | grep -E \u0026#34;BROADCAST|global\u0026#34;| awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; ;echo \u0026#34;\u0026#34; ;done fi GATEWAY=$(ip route | grep default | awk \u0026#39;{print $3}\u0026#39;) DNS=$(grep nameserver /etc/resolv.conf| grep -v \u0026#34;#\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;\u0026#34; echo \u0026#34;网关：$GATEWAY \u0026#34; echo \u0026#34;DNS：$DNS\u0026#34; #报表信息 IP=$(ip -f inet addr | grep -v 127.0.0.1 | grep inet | awk \u0026#39;{print $NF,$2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) MAC=$(ip link | grep -v \u0026#34;LOOPBACK\\|loopback\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sed \u0026#39;N;s/\\n//\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;\u0026#34; ping -c 4 www.baidu.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ];then echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[32m网络连接：正常！\\033[0m\u0026#34; else echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[31m网络连接：异常！\\033[0m\u0026#34; fi } function getListenStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************监听检查********************************************************\\033[0m\u0026#34; TCPListen=$(ss -ntul | column -t) echo \u0026#34;$TCPListen\u0026#34; } function getCronStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**********************************************计划任务检查******************************************************\\033[0m\u0026#34; Crontab=0 for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(grep \u0026#34;$shell\u0026#34; /etc/passwd| awk -F: \u0026#39;{print $1}\u0026#39;);do crontab -l -u $user \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 status=$? if [ $status -eq 0 ];then echo -e \u0026#34;\\033[36m************$user用户的定时任务**************\\033[0m\u0026#34; crontab -l -u $user let Crontab=Crontab+$(crontab -l -u $user | wc -l) echo \u0026#34;\u0026#34; fi done done #计划任务 #find /etc/cron* -type f | xargs -i ls -l {} | column -t #let Crontab=Crontab+$(find /etc/cron* -type f | wc -l) } function getHowLongAgo(){ # 计算一个时间戳离现在有多久了 datetime=\u0026#34;$*\u0026#34; [ -z \u0026#34;$datetime\u0026#34; ] \u0026amp;\u0026amp; echo `stat /etc/passwd|awk \u0026#34;NR==6\u0026#34;` Timestamp=$(date +%s -d \u0026#34;$datetime\u0026#34;) Now_Timestamp=$(date +%s) Difference_Timestamp=$(($Now_Timestamp-$Timestamp)) days=0;hours=0;minutes=0; sec_in_day=$((60*60*24)); sec_in_hour=$((60*60)); sec_in_minute=60 while (( $(($Difference_Timestamp-$sec_in_day)) \u0026gt; 1 )) do let Difference_Timestamp=Difference_Timestamp-sec_in_day let days++ done while (( $(($Difference_Timestamp-$sec_in_hour)) \u0026gt; 1 )) do let Difference_Timestamp=Difference_Timestamp-sec_in_hour let hours++ done echo \u0026#34;$days 天 $hours 小时前\u0026#34; } function getUserLastLogin(){ # 获取用户最近一次登录的时间，含年份 # 很遗憾last命令不支持显示年份，只有\u0026#34;last -t YYYYMMDDHHMMSS\u0026#34;表示某个时间之间的登录，我 # 们只能用最笨的方法了，对比今天之前和今年元旦之前（或者去年之前和前年之前……）某个用户 # 登录次数，如果登录统计次数有变化，则说明最近一次登录是今年。 username=$1 : ${username:=\u0026#34;`whoami`\u0026#34;} thisYear=$(date +%Y) oldesYear=$(last | tail -n1 | awk \u0026#39;{print $NF}\u0026#39;) while(( $thisYear \u0026gt;= $oldesYear));do loginBeforeToday=$(last $username | grep $username | wc -l) loginBeforeNewYearsDayOfThisYear=$(last $username -t $thisYear\u0026#34;0101000000\u0026#34; | grep $username | wc -l) if [ $loginBeforeToday -eq 0 ];then echo \u0026#34;从未登录过\u0026#34; break elif [ $loginBeforeToday -gt $loginBeforeNewYearsDayOfThisYear ];then lastDateTime=$(last -i $username | head -n1 | awk \u0026#39;{for(i=4;i\u0026lt;(NF-2);i++)printf\u0026#34;%s \u0026#34;,$i}\u0026#39;)\u0026#34; $thisYear\u0026#34; lastDateTime=$(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34; -d \u0026#34;$lastDateTime\u0026#34;) echo \u0026#34;$lastDateTime\u0026#34; break else thisYear=$((thisYear-1)) fi done } function getUserStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************用户检查*******************************************************\\033[0m\u0026#34; #/etc/passwd 最后修改时间 pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; Modify=$(stat /etc/passwd | grep Modify | tr \u0026#39;.\u0026#39; \u0026#39; \u0026#39; | awk \u0026#39;{print $2,$3}\u0026#39;) echo \u0026#34;/etc/passwd: $Modify ($(getHowLongAgo $Modify))\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************特权用户******************\\033[0m\u0026#34; RootUser=\u0026#34;\u0026#34; for user in $(echo \u0026#34;$pwdfile\u0026#34; | awk -F: \u0026#39;{print $1}\u0026#39;);do if [ $(id -u $user) -eq 0 ];then echo \u0026#34;$user\u0026#34; RootUser=\u0026#34;$RootUser,$user\u0026#34; fi done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************用户列表******************\\033[0m\u0026#34; USERs=0 echo \u0026#34;$( echo \u0026#34;用户名 UID GID HOME SHELL 最后一次登录\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for username in $(grep \u0026#34;$shell\u0026#34; /etc/passwd| awk -F: \u0026#39;{print $1}\u0026#39;);do userLastLogin=\u0026#34;$(getUserLastLogin $username)\u0026#34; echo \u0026#34;$pwdfile\u0026#34; | grep -w \u0026#34;$username\u0026#34; |grep -w \u0026#34;$shell\u0026#34;| awk -F: -v lastlogin=\u0026#34;$(echo \u0026#34;$userLastLogin\u0026#34; | tr \u0026#39; \u0026#39; \u0026#39;_\u0026#39;)\u0026#34; \u0026#39;{print $1,$3,$4,$6,$7,lastlogin}\u0026#39; done let USERs=USERs+$(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34;| wc -l) done )\u0026#34; | column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************空密码用户****************\\033[0m\u0026#34; USEREmptyPassword=\u0026#34;\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34; | cut -d: -f1);do r=$(awk -F: \u0026#39;$2==\u0026#34;!!\u0026#34;{print $1}\u0026#39; /etc/shadow | grep -w $user) if [ ! -z $r ];then echo $r USEREmptyPassword=\u0026#34;$USEREmptyPassword,\u0026#34;$r fi done done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*****************相同ID用户*****************\\033[0m\u0026#34; USERTheSameUID=\u0026#34;\u0026#34; UIDs=$(cut -d: -f3 /etc/passwd | sort | uniq -c | awk \u0026#39;$1\u0026gt;1{print $2}\u0026#39;) for uid in $UIDs;do echo -n \u0026#34;$uid\u0026#34;; USERTheSameUID=\u0026#34;$uid\u0026#34; r=$(awk -F: \u0026#39;ORS=\u0026#34;\u0026#34;;$3==\u0026#39;\u0026#34;$uid\u0026#34;\u0026#39;{print \u0026#34;:\u0026#34;,$1}\u0026#39; /etc/passwd) echo \u0026#34;$r\u0026#34; echo \u0026#34;\u0026#34; USERTheSameUID=\u0026#34;$USERTheSameUID $r,\u0026#34; done } function getPasswordStatus { echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************密码检查*******************************************************\\033[0m\u0026#34; pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************密码过期检查****************\\033[0m\u0026#34; result=\u0026#34;\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34; | cut -d: -f1);do get_expiry_date=$(/usr/bin/chage -l $user | grep \u0026#39;Password expires\u0026#39; | cut -d: -f2) if [[ $get_expiry_date = \u0026#39; never\u0026#39; || $get_expiry_date = \u0026#39;never\u0026#39; ]];then printf \u0026#34;%-15s 永不过期\\n\u0026#34; $user result=\u0026#34;$result,$user:never\u0026#34; else password_expiry_date=$(date -d \u0026#34;$get_expiry_date\u0026#34; \u0026#34;+%s\u0026#34;) current_date=$(date \u0026#34;+%s\u0026#34;) diff=$(($password_expiry_date-$current_date)) let DAYS=$(($diff/(60*60*24))) printf \u0026#34;%-15s %s天后过期\\n\u0026#34; $user $DAYS result=\u0026#34;$result,$user:$DAYS days\u0026#34; fi done done report_PasswordExpiry=$(echo $result | sed \u0026#39;s/^,//\u0026#39;) echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************密码策略检查****************\\033[0m\u0026#34; grep -v \u0026#34;#\u0026#34; /etc/login.defs | grep -E \u0026#34;PASS_MAX_DAYS|PASS_MIN_DAYS|PASS_MIN_LEN|PASS_WARN_AGE\u0026#34; } function getSudoersStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**********************************************Sudoers检查*******************************************************\\033[0m\u0026#34; conf=$(grep -v \u0026#34;^#\u0026#34; /etc/sudoers| grep -v \u0026#34;^Defaults\u0026#34; | sed \u0026#39;/^$/d\u0026#39;) echo \u0026#34;$conf\u0026#34; echo \u0026#34;\u0026#34; } function getInstalledStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************软件检查*******************************************************\\033[0m\u0026#34; rpm -qa --last | head | column -t } function getProcessStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************进程检查*******************************************************\\033[0m\u0026#34; if [ $(ps -ef | grep defunct | grep -v grep | wc -l) -ge 1 ];then echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m***************僵尸进程***************\\033[0m\u0026#34; ps -ef | head -n1 ps -ef | grep defunct | grep -v grep fi echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m************CPU占用TOP 10进程*************\\033[0m\u0026#34; echo -e \u0026#34;用户 进程ID %CPU 命令 $(ps aux | awk \u0026#39;{print $1, $2, $3, $11}\u0026#39; | sort -k3rn | head -n 10 )\u0026#34;| column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m************内存占用TOP 10进程*************\\033[0m\u0026#34; echo -e \u0026#34;用户 进程ID %MEM 虚拟内存 常驻内存 命令 $(ps aux | awk \u0026#39;{print $1, $2, $4, $5, $6, $11}\u0026#39; | sort -k3rn | head -n 10 )\u0026#34;| column -t #echo \u0026#34;\u0026#34; #echo -e \u0026#34;\\033[36m************SWAP占用TOP 10进程*************\\033[0m\u0026#34; #awk: fatal: cannot open file `/proc/18713/smaps\u0026#39; for reading (No such file or directory) #for i in `cd /proc;ls |grep \u0026#34;^[0-9]\u0026#34;|awk \u0026#39; $0 \u0026gt;100\u0026#39;`;do awk \u0026#39;{if (-f /proc/$i/smaps) print \u0026#34;$i file is not exist\u0026#34;; else print \u0026#34;$i\u0026#34;}\u0026#39;;done # for i in `cd /proc;ls |grep \u0026#34;^[0-9]\u0026#34;|awk \u0026#39; $0 \u0026gt;100\u0026#39;` ;do awk \u0026#39;/Swap:/{a=a+$2}END{print \u0026#39;\u0026#34;$i\u0026#34;\u0026#39;,a/1024\u0026#34;M\u0026#34;}\u0026#39; /proc/$i/smaps ;done |sort -k2nr \u0026gt; /tmp/swap.txt #echo -e \u0026#34;进程ID SWAP使用 $(cat /tmp/swap.txt|grep -v awk | head -n 10)\u0026#34;| column -t } function getSyslogStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************syslog检查*******************************************************\\033[0m\u0026#34; echo \u0026#34;SYSLOG服务状态：$(getState rsyslog)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m***************rsyslog配置******************\\033[0m\u0026#34; cat /etc/rsyslog.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | grep -v \u0026#34;^\\\\$\u0026#34; | sed \u0026#39;/^$/d\u0026#39; | column -t } function getFirewallStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************防火墙检查*******************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m****************防火墙状态******************\\033[0m\u0026#34; if [[ $centosVersion = 7 ]];then systemctl status firewalld \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 status=$? if [ $status -eq 0 ];then s=\u0026#34;active\u0026#34; elif [ $status -eq 3 ];then s=\u0026#34;inactive\u0026#34; elif [ $status -eq 4 ];then s=\u0026#34;permission denied\u0026#34; else s=\u0026#34;unknown\u0026#34; fi else s=\u0026#34;$(getState iptables)\u0026#34; fi echo \u0026#34;firewalld: $s\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************防火墙配置******************\\033[0m\u0026#34; cat /etc/sysconfig/firewalld 2\u0026gt;/dev/null } function getSNMPStatus(){ #SNMP服务状态，配置等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************SNMP检查*********************************************************\\033[0m\u0026#34; status=\u0026#34;$(getState snmpd)\u0026#34; echo \u0026#34;SNMP服务状态：$status\u0026#34; echo \u0026#34;\u0026#34; if [ -e /etc/snmp/snmpd.conf ];then echo \u0026#34;/etc/snmp/snmpd.conf\u0026#34; echo \u0026#34;--------------------\u0026#34; cat /etc/snmp/snmpd.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; fi } function getState(){ if [[ $centosVersion \u0026lt; 7 ]];then if [ -e \u0026#34;/etc/init.d/$1\u0026#34; ];then if [ `/etc/init.d/$1 status 2\u0026gt;/dev/null | grep -E \u0026#34;is running|正在运行\u0026#34; | wc -l` -ge 1 ];then r=\u0026#34;active\u0026#34; else r=\u0026#34;inactive\u0026#34; fi else r=\u0026#34;unknown\u0026#34; fi else #CentOS 7+ r=\u0026#34;$(systemctl is-active $1 2\u0026gt;\u0026amp;1)\u0026#34; fi echo \u0026#34;$r\u0026#34; } function getSSHStatus(){ #SSHD服务状态，配置,受信任主机等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************SSH检查*********************************************************\\033[0m\u0026#34; #检查受信任主机 pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; echo \u0026#34;SSH服务状态：$(getState sshd)\u0026#34; Protocol_Version=$(cat /etc/ssh/sshd_config | grep Protocol | awk \u0026#39;{print $2}\u0026#39;) echo \u0026#34;SSH协议版本：$Protocol_Version\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************信任主机******************\\033[0m\u0026#34; authorized=0 for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep /bin/bash | awk -F: \u0026#39;{print $1}\u0026#39;);do authorize_file=$(echo \u0026#34;$pwdfile\u0026#34; | grep -w $user | awk -F: \u0026#39;{printf $6\u0026#34;/.ssh/authorized_keys\u0026#34;}\u0026#39;) authorized_host=$(cat $authorize_file 2\u0026gt;/dev/null | awk \u0026#39;{print $3}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) if [ ! -z $authorized_host ];then echo \u0026#34;$user 授权 \\\u0026#34;$authorized_host\\\u0026#34; 无密码访问\u0026#34; fi let authorized=authorized+$(cat $authorize_file 2\u0026gt;/dev/null | awk \u0026#39;{print $3}\u0026#39;|wc -l) done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*******是否允许ROOT远程登录***************\\033[0m\u0026#34; config=$(cat /etc/ssh/sshd_config | grep PermitRootLogin) firstChar=${config:0:1} if [ $firstChar == \u0026#34;#\u0026#34; ];then PermitRootLogin=\u0026#34;yes\u0026#34; else PermitRootLogin=$(echo $config | awk \u0026#39;{print $2}\u0026#39;) fi echo \u0026#34;PermitRootLogin $PermitRootLogin\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*************ssh服务配置******************\\033[0m\u0026#34; cat /etc/ssh/sshd_config | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; } function getNTPStatus(){ #NTP服务状态，当前时间，配置等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************NTP检查**********************************************************\\033[0m\u0026#34; if [ -e /etc/ntp.conf ];then echo \u0026#34;NTP服务状态：$(getState ntpd)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*************NTP服务配置******************\\033[0m\u0026#34; cat /etc/ntp.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; fi } function check(){ version getSystemStatus get_resource getCpuStatus getMemStatus getDiskStatus getNetworkStatus getListenStatus getProcessStatus getServiceStatus getAutoStartStatus getLoginStatus getCronStatus getUserStatus getPasswordStatus getSudoersStatus getFirewallStatus getSSHStatus getSyslogStatus getSNMPStatus getNTPStatus getInstalledStatus } #执行检查并保存检查结果 check \u0026gt; $RESULTFILE echo -e \u0026#34;\\033[44;37m 主机巡检结果存放在：$RESULTFILE \\033[0m\u0026#34; #上传检查结果的文件 #curl -F \u0026#34;filename=@$RESULTFILE\u0026#34; \u0026#34;$uploadHostDailyCheckApi\u0026#34; 2\u0026gt;/dev/null cat $RESULTFILE ","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-server-inspection/","summary":"代码如下 #!/bin/bash #参数定义 date=`date +\u0026#34;%Y-%m-%d-%H:%M:%S\u0026#34;` centosVersion=$(awk \u0026#39;{print $(NF-1)}\u0026#39; /etc/redhat-release) VERSION=`date +%F` #日志相关 LOGPATH=\u0026#34;/tmp/awr\u0026#34; [ -e $LOGPATH ] || mkdir -p $LOGPATH RESULTFILE=\u0026#34;$LOGPATH/HostCheck-`hostname`-`date +%Y%m%d`.txt\u0026#34; #调用函数库 [ -f /etc/init.d/functions ] \u0026amp;\u0026amp; source /etc/init.d/functions export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile #root用户执行脚本 [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34;请用root用户执行此脚本！\u0026#34; \u0026amp;\u0026amp; exit 1 function version(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;[${date}] \u0026gt;\u0026gt;\u0026gt; `hostname -s` 主机巡检\u0026#34; } function getSystemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***","title":"shell | 服务器巡检脚本"},{"content":"前言 有需求需要在 openeuler 的操作系统上测试一个 C 程序，做了一个简化版的程序，程序很简单，循环读取一个文件并打印文件内容，在程序执行过程中使用 echo 手动向文件中追加内容，程序要能读取到，效果如下：\n测试程序代码如下：\n#include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0) { buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;read:%s\\n\u0026#34;,buffer); } else { printf(\u0026#34;noread\\n\u0026#34;); } sleep(2); } return 0; } 在 Rhel-7.5 上测试一切正常，开始在 openeuler 上进行测试，结果发现后续追加的内容没有输出：\n故障排查 考虑到影响程序执行结果的几个因素：程序本身，内核版本，gcc版本，glibc版本。\n程序本身应该是没问题的，内核版本一般对C语言程序的影响也不会很大，还是优先看gcc版本和glibc版本。\n按照思路进行了一些测试，测试结果：\n可行： centos7.5（gcc-4.8.5，kernel-3.10，glibc\u0026lt;=2.28） centos7.5（gcc-7.3.0，kernel-3.10，glibc\u0026lt;=2.28） centos7.5（gcc-7.3.0，kernel-5.12，glibc\u0026lt;=2.28） 不可行： isoft-server-6.0（gcc-7.3.0，4.19.90，glibc\u0026gt;=2.28） centos8（gcc-8.4.0，kernel-4.18.0，glibc\u0026gt;=2.28） openeuler-20.03-LTS-SP1（gcc-7.3.0，kernel-4.19.90，glibc\u0026gt;=2.28） 按照测试结果，似乎 gcc 版本和内核版本对程序没什么影响，大概率应该是 glibc 版本导致的。由于程序很简单，只是以 rb 方式 fopen 打开文件循环读取文件内容，求证(google)起来也比较轻松，很快就找到了问题在哪：glibc 2.28修复了 fread 的行为\n这个 glibc 的 bug 是05年提的，到18年才修复，也是担心 break 之前大量的代码。https://sourceware.org/bugzilla/show_bug.cgi?id=1190\n现在再修改一下代码：\n#include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0) { buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;read:%s\\n\u0026#34;,buffer); } else { if (feof (f)) { printf(\u0026#34;Read error, clear error flag to retry...\\n\u0026#34;); clearerr (f); } } sleep(2); } return 0; } 添加了一块清除标记的片段，在 glibc\u0026gt;=2.28 的系统上程序也可以正常运行了\n","permalink":"https://www.lvbibir.cn/en/posts/tech/record-of-program-test/","summary":"前言 有需求需要在 openeuler 的操作系统上测试一个 C 程序，做了一个简化版的程序，程序很简单，循环读取一个文件并打印文件内容，在程序执行过程中使用 echo 手动向文件中追加内容，程序要能读取到，效果如下： 测试程序代码如下： #include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0)","title":"记一次程序测试"},{"content":"\r","permalink":"https://www.lvbibir.cn/en/talk/","summary":"","title":"💬 说说"},{"content":"前言 介绍在CentOS7上部署BBR的详细过程\nBBR简介：（Bottleneck Bandwidth and RTT）是一种新的拥塞控制算法，由Google开发。有了BBR，Linux服务器可以显着提高吞吐量并减少连接延迟\n1. 查看当前内核版本 uname -r 显示当前内核为3.10.0，因此我们需要更新内核\n2. 使用 ELRepo RPM 仓库升级内核 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org //无返回内容 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 使用ELRepo repo更新安装5.12.3内核\nyum --enablerepo=elrepo-kernel install kernel-ml -y\n更新完成后，执行如下命令，确认更新结果\nrpm -qa | grep kernel\nkernel-ml-5.12.3-1.el7.elrepo.x86_64 //为更新后文件版本\n3. 通过设置默认引导为 grub2 ，来启用5.12.3内核 egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \\'\n根据显示结果得知5.12.3内核处于行首，对应行号为 0 执行以下命令将其设置为默认引导项\ngrub2-set-default 0\n4. 重启系统并确认内核版本 shutdown -r now or reboot\n当服务器重新联机时，请进行root登录并重新运行uname命令以确认您当前内核版本\nuname -r\n至此完成内核更新与默认引导设置\n5. 启用BBR 执行命令查看当前拥塞控制算法\nsysctl -n net.ipv4.tcp_congestion_control\n启用 BBR 算法，需要对 sysctl.conf 配置文件进行修改，依次执行以下每行命令\necho \u0026#39;net.core.default_qdisc=fq\u0026#39; | tee -a /etc/sysctl.conf echo \u0026#39;net.ipv4.tcp_congestion_control=bbr\u0026#39; | tee -a /etc/sysctl.conf sysctl -p 进行BBR的启用验证\nsysctl net.ipv4.tcp_available_congestion_control sysctl -n net.ipv4.tcp_congestion_control 最后检查BBR模块是否已经加载\nlsmod | grep bbr\n至此，BBR的部署已全部完成。\n参考 https://blog.csdn.net/desertworm/article/details/116759380\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-open-bbr/","summary":"前言 介绍在CentOS7上部署BBR的详细过程 BBR简介：（Bottleneck Bandwidth and RTT）是一种新的拥塞控制算法，由Google开发。有了BBR，Linux服务器可以显着提高吞吐量并减少连接延迟 1. 查看当前内核版本 uname -r 显示当前内核为3.10.0，因此我们需要更新内核 2. 使用 ELRepo RPM 仓库","title":"centos7开启bbr算法"},{"content":"前端时间在国家信息中心的一个项目上需要在 H3C 服务器上安装操作系统然后配置一套 spring boot 项目，结果在装操作系统过程中就遇到了问题：安装完操作系统后无法自动引导，只能通过重启服务器按 F7 进入引导选项，选择对应的逻辑盘才能正常引导\n服务器有7块物理磁盘，前两块是 600 GB 的机械盘，后五块是 1T 的机械盘，前两块 600GB 的盘做了 raid1 ，剩下的5块盘，选择 n+2 做 raid6 。\n规划是这样的，操作系统安装在 raid6 上，raid1 那块逻辑磁盘等系统安装完后再进行挂载，用作业务的数据备份。\n安装完之后却发现有很多台系统引导不起来，必须手动引导，只有一台可以重启后直接进入系统。为了快速解决问题，还是第一时间联系了 H3C 的售后开工单解决，结果不言而喻，业务水平堪忧，并没有解决。不过也给我提供了一些思路。\n整理一下思路：\n出现问题之后更换安装介质重新安装了两次，问题都是一样的 系统安装这块操作肯定没问题，那问题就出在硬件上面了 开始寻找硬件上面的问题，服务器都是全新的，只是做了 raid 。询问了下做raid的同事，看可以正常引导的服务器和非正常引导的服务器之间 raid 配置有何不同\n问题估计找到了：正常服务器是先创建的 raid6 ，剩下的都是先创建的raid1。\n解决方案：\n系统需要重装：删除原先已经创建好的 raid，先创建系统使用的 raid6. 系统无需重装：删除掉 raid1 ，保存后重新创建 raid1。这时，raid6 的顺位会比raid1高，系统就可以正常启动了 最终我们这边采取的是第二种方案\n","permalink":"https://www.lvbibir.cn/en/posts/tech/h3c-server-can-not-boot-system/","summary":"前端时间在国家信息中心的一个项目上需要在 H3C 服务器上安装操作系统然后配置一套 spring boot 项目，结果在装操作系统过程中就遇到了问题：安装完操作系统后无法自动引导，只能通过重启服务器按 F7 进入引导选项，选择对应的逻辑盘才能正常引导 服务器有7块物理磁盘，前两块是 600 GB 的机械盘，后五块是 1T 的机械盘，前","title":"H3C服务器装完系统无法引导"},{"content":"1、进入bios修改启动模式，将 UEFI 改为 Legacy bios\n2、 重启服务器，ctrl + r 进入 lsi 阵列卡管理\n3、选择对应阵列卡\n4、配置逻辑盘\n5、配置完逻辑盘后可以选择从某一块逻辑盘启动\nCtrl-P 进入到ctrl mgmt. -\u0026gt; TAB切换到boot device\n回车后可以看到当前的逻辑盘，上下选择要引导的逻辑盘即可。\nApply保存退出完成。\n","permalink":"https://www.lvbibir.cn/en/posts/tech/h3c-server-config-raid/","summary":"1、进入bios修改启动模式，将 UEFI 改为 Legacy bios 2、 重启服务器，ctrl + r 进入 lsi 阵列卡管理 3、选择对应阵列卡 4、配置逻辑盘 5、配置完逻辑盘后可以选择从某一块逻辑盘启动 Ctrl-P 进入到ctrl mgmt. -\u0026gt; TAB切换到boot device 回车后可以看到当前的逻辑盘，上下选择要引导的逻辑盘即可。 Apply保存退出完","title":"H3C服务器配置raid"},{"content":"CentOS6及以前 在CentOS6及以前的版本中，free命令输出是这样的：\n[root@wordpress ~]# free -m total used free shared buffers cached Mem: 1002 769 233 0 62 421 -/+ buffers/cache: 286 716 Swap: 1153 0 1153 第一行：\n​\t系统内存主要分为五部分：total(系统内存总量)，used(程序已使用内存)，free(空闲内存)，buffers(buffer cache)，cached(Page cache)。\n​\t系统总内存total = used + free； buffers和cached被算在used里，因此第一行系统已使用内存used = buffers + cached + 第二行系统已使用内存used\n​\t由于buffers和cached在系统需要时可以被回收使用，因此系统可用内存 = free + buffers + cached；\n​\tshared为程序共享的内存空间，往往为0。\n第二行：\n正因为buffers和cached中的一部分内存容量在系统需要时可以被回收使用，因此buffer和cached中有部分内存其实可以算作可用内存，因此：\n系统已使用内存，即第二行的used = total - 第二行free\n系统可用内存，即第二行的free = 第一行的free + buffers + cached\n第三行：\nswap内存交换空间使用情况\nCentOS7及以后 CentOS7及以后free命令的输出如下：\n[root@wordpress ~]# free -m total used free shared buff/cache available Mem: 1839 866 74 97 897 695 Swap: 0 0 0 buffer和cached被合成一组，加入了一个available，关于此available，文档上的说明如下：\nMemAvailable: An estimate of how much memory is available for starting new applications, without swapping.\n即系统可用内存，之前说过由于buffer和cache可以在需要时被释放回收，系统可用内存即 free + buffer + cache，在CentOS7之后这种说法并不准确，因为并不是所有的buffer/cache空间都可以被回收。\n即available = free + buffer/cache - 不可被回收内存(共享内存段、tmpfs、ramfs等)。\n因此在CentOS7之后，用户不需要去计算buffer/cache，即可以看到还有多少内存可用，更加简单直观。\nbuffer/cache相关介绍 什么是buffer/cache？ buffer 和 cache 是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义。在 Linux 的内存管理中，这里的 buffer 指 Linux 内存的： Buffer cache 。这里的 cache 指 Linux 内存中的： Page cache 。翻译成中文可以叫做缓冲区缓存和页面缓存。在历史上，它们一个（ buffer ）被用来当成对 io 设备写的缓存，而另一个（ cache ）被用来当作对 io 设备的读缓存，这里的 io 设备，主要指的是块设备文件和文件系统上的普通文件。但是现在，它们的意义已经不一样了。在当前的内核中， page cache 顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以 page 进行分配管理的，都可以使用 page cache 作为其缓存来管理使用。当然，不是所有的内存都是以页（ page ）进行管理的，也有很多是针对块（ block ）进行管理的，这部分内存使用如果要用到 cache 功能，则都集中到 buffer cache 中来使用。（从这个角度出发，是不是 buffer cache 改名叫做 block cache 更好？）然而，也不是所有块（ block ）都有固定长度，系统上块的长度主要是根据所使用的块设备决定的，而页长度在 X86 上无论是 32 位还是 64 位都是 4k 。\n明白了这两套缓存系统的区别，就可以理解它们究竟都可以用来做什么了。\n什么是 page cache Page cache 主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有 read ／ write 操作的时候。如果你仔细想想的话，作为可以映射文件到内存的系统调用： mmap 是不是很自然的也应该用到 page cache ？在当前的系统实现里， page cache 也被作为其它文件类型的缓存设备来用，所以事实上 page cache 也负责了大部分的块设备文件的缓存工作。\n什么是 buffer cache Buffer cache 则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。这意味着某些对块的操作会使用 buffer cache 进行缓存，比如我们在格式化文件系统的时候。一般情况下两个缓存系统是一起配合使用的，比如当我们对一个文件进行写操作的时候， page cache 的内容会被改变，而 buffer cache 则可以用来将 page 标记为不同的缓冲区，并记录是哪一个缓冲区被修改了。这样，内核在后续执行脏数据的回写（ writeback ）时，就不用将整个 page 写回，而只需要写回修改的部分即可。\n如何回收 cache ？ Linux 内核会在内存将要耗尽的时候，触发内存回收的工作，以便释放出内存给急需内存的进程使用。一般情况下，这个操作中主要的内存释放都来自于对 buffer ／ cache 的释放。尤其是被使用更多的 cache 空间。既然它主要用来做缓存，只是在内存够用的时候加快进程对文件的读写速度，那么在内存压力较大的情况下，当然有必要清空释放 cache ，作为 free 空间分给相关进程使用。所以一般情况下，我们认为 buffer/cache 空间可以被释放，这个理解是正确的。\n但是这种清缓存的工作也并不是没有成本。理解 cache 是干什么的就可以明白清缓存必须保证 cache 中的数据跟对应文件中的数据一致，才能对 cache 进行释放。所以伴随着 cache 清除的行为的，一般都是系统 IO 飙高。因为内核要对比 cache 中的数据和对应硬盘文件上的数据是否一致，如果不一致需要写回，之后才能回收。\n在系统中除了内存将被耗尽的时候可以清缓存以外，我们还可以使用下面这个文件来人工触发缓存清除的操作\n[root@tencent64 ~]# cat /proc/sys/vm/drop_caches\n方法是：\necho 3 \u0026gt; /proc/sys/vm/drop_caches 当然，这个文件可以设置的值分别为 1 、 2 、 3 。它们所表示的含义为：\n表示清除 pagecache echo 1 \u0026gt; /proc/sys/vm/drop_caches 表示清除回收 slab 分配器中的对象（包括目录项缓存和 inode 缓存）。 slab 分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的 pagecache echo 2 \u0026gt; /proc/sys/vm/drop_caches 表示清除 pagecache 和 slab 分配器中的缓存对象。 echo 3 \u0026gt; /proc/sys/vm/drop_caches 参考 https://blog.csdn.net/qq_41781322/article/details/87187957\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-command-free/","summary":"CentOS6及以前 在CentOS6及以前的版本中，free命令输出是这样的： [root@wordpress ~]# free -m total used free shared buffers cached Mem: 1002 769 233 0 62 421 -/+ buffers/cache: 286 716 Swap: 1153 0 1153 第一行： ​ 系统内存主要分为五部分：total(系统内存总量)，used(程序已使用内存)，free(空闲内存)，buffers(buffer cache)","title":"linux | free命令详解"},{"content":"pxe环境 dhcp+tftp+http\npxe-server：isoft-serveros-v4.2（3.10.0-957.el7.isoft.x86_64）\n引导的iso：isoft-serveros-aarch64-oe1-v5.1（4.19.90-2003.4.0.0036.oe1.aarch64）\n物理服务器：浪潮 Inspur\ndhcpd.conf配置 [root@localhost isoft-5.1-arm]# vim /etc/dhcp/dhcpd.conf default-lease-time 43200; max-lease-time 345600; option space PXE; option arch code 93 = unsigned integer 16; option routers 192.168.1.1; option subnet-mask 255.255.255.0; option broadcast-address 192.168.1.255; option time-offset -18000; ddns-update-style none; allow client-updates; allow booting; allow bootp; next-server 192.168.1.1; if option arch = 00:07 or arch = 00:09 { filename \u0026#34;x86/bootx64.efi\u0026#34;; } else { filename \u0026#34;arm/grubaa64.efi\u0026#34;; } shared-network works { subnet 192.168.1.0 netmask 255.255.255.0 { range dynamic-bootp 192.168.1.221 192.168.1.253; } } grub.cfg配置 [root@localhost tftpboot]# vim arm/grub.cfg set default=\u0026#34;0\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=60 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iSoftServerOS-5.1-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iSoftServerOS 5.1 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { # linux /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=iSoftServerOS-5.1-aarch64 ro inst.geoloc=0 selinux=0 # initrd /images/pxeboot/initrd.img linux /arm51/vmlinuz ip=dhcp method=http://192.168.1.1/isoft-5.1-arm ks=http://192.168.1.1/isoft-5.1-arm/anaconda-ks.cfg initrd /arm51/initrd.img } #menuentry \u0026#39;Install iSoftServerOS 5.1 for ZF with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { # linux /arm51-zf/vmlinuz ip=dhcp method=http://192.168.1.1/isoft-5.1-zfarm # initrd /arm51-zf/initrd.img #} ks.cfg配置 [root@localhost isoft-5.1-arm]# vim anaconda-ks.cfg lang zh_CN.UTF-8 # Network information network --bootproto=dhcp --device=eno1 --ipv6=auto --no-activate network --bootproto=dhcp --device=eno2 --ipv6=auto network --bootproto=dhcp --device=eno3 --ipv6=auto network --bootproto=dhcp --device=eno4 --ipv6=auto network --bootproto=dhcp --device=enp22s0f0 --ipv6=auto network --bootproto=dhcp --device=enp22s0f1 --ipv6=auto network --bootproto=dhcp --device=enp22s0f2 --ipv6=auto network --bootproto=dhcp --device=enp22s0f3 --ipv6=auto network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$afv9h6qEnQTq3WSl$GHtOmvLkHrBin8vTWLbRaa2r.Ur9mUQR7XypWRoEWZYCwwJ2MnuMPxpNiNLSG1vSa5qBODHJcqIUUWkHm0IVl. # SELinux configuration selinux --disabled # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc user --groups=wheel --name=testuser --password=$6$9SyzoTjQU2syj2Bk$SQ4WZAV/go3KeX6rJN3cieNpY4l7aU2wHxad75yWlbKBh.ithhrU/jfA09JUq7cb10D0QTCwtClmItfg/N47t. --iscrypted --gecos=\u0026#34;testuser\u0026#34; # Disk partitioning information part /boot/efi --fstype=\u0026#34;efi\u0026#34; --ondisk=sda --size=200 --fsoptions=\u0026#34;umask=0077,shortname=winnt\u0026#34; part pv.521 --fstype=\u0026#34;lvmpv\u0026#34; --ondisk=sda --size=913974 part /boot --fstype=\u0026#34;ext4\u0026#34; --ondisk=sda --size=1024 volgroup isoftserveros --pesize=4096 pv.521 logvol /home --fstype=\u0026#34;xfs\u0026#34; --size=756272 --name=home --vgname=isoftserveros logvol swap --fstype=\u0026#34;swap\u0026#34; --size=4096 --name=swap --vgname=isoftserveros logvol / --fstype=\u0026#34;xfs\u0026#34; --size=153600 --name=root --vgname=isoftserveros %packages @^mate-desktop-environment @additional-devel @development @file-server @headless-management @legacy-unix @network-server @network-tools @scientific @security-tools @system-tools @virtual-tools %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot ","permalink":"https://www.lvbibir.cn/en/posts/tech/pxe-inspur-isoft5.1-aarch64/","summary":"pxe环境 dhcp+tftp+http pxe-server：isoft-serveros-v4.2（3.10.0-957.el7.isoft.x86_64） 引导的iso：isoft-serveros-aarch64-oe1-v5.1（4.19.90-2003.4.0.0036.oe1.aarch64） 物理","title":"pxe 安装 isoft-5.1(aarch64)"},{"content":"前言 前段时间着手开始搭建自己的wordpress博客，刚开始图方便直接买了阿里云的轻量应用服务器，它是一套预先搭建好的lamp架构，并已经做了一些初始化配置，直接访问ip就可以进行wordpress的安装和配置了。\n这套wordpress的一个非常好的优点就是可以在阿里云的控制台一键配置https证书，当然仅限在阿里云购买的ssl证书\n后续还是决定将wordpress整体迁移到docker中，全部服务都用docker跑。这样只要数据做好持久化，使用docker的灵活性会好很多，做全站备份和迁移也很方便。\n备份\u0026amp;迁移 wordpress迁移起来还是比较方便的，需要备份的内容大概有这些：插件、主题、uploads、数据库\n备份插件：UpdraftPlus，这是一款个人使用过一款比较优秀的备份/迁移插件，免费版的功能基本满足大部分人需求，支持手动备份和定时备份、备份和恢复都支持部分备份，比如只备份数据库，只恢复数据库的某一张表。\n免费版的并不支持wordpress迁移，但我们可以通过导入导出备份文件的方式实现站点迁移，前提是做好测试。\n备份步骤：\n在备份插件中手动备份一次 下载备份文件 迁移步骤：\n准备好系统环境和docker环境（docker-compose） 启动docker容器 http访问wordpress地址初始化安装 安装备份插件和ssl插件（really simple ssl） 上传备份文件并进行恢复操作（不恢复wp-options表） 为nginx反代服务器配置ssl证书，开启https访问 在really simple ssl中为wordpress启用https 恢复wp-options表 手动备份\u0026amp;下载备份文件 备份完之后可以直接从web端下载，但是建议从web端下载一份，通过ssh或者ftp等方式再下载一份，避免备份文件出现问题\n备份的文件在wordpress目录/wp-content/updraft目录中\n通过scp下载到本地\n准备系统环境 安装好docker和docker-compose即可，docker的安装和使用教程在本博客中docker分类有\ndocker-compose一键启动wordpress环境 这里我提供了一键部署的docker-compose文件和各服务进行了优化的配置文件，可以直接拿来用下载链接\n注意：\n使用前建议修改数据库相关信息\n建议不要随意改动ip\n所有的数据文件和配置文件默认都在当前的目录下\n如果前面不加nginx反代，记得把注释掉的端口映射改成自己想要的\n所有的配置文件都在nginx目录下，已经预先定义好，可以自行进行修改\n内置的wordpress目录权限用户和组是 33:tape\nversion: \u0026#39;3.1\u0026#39; services: proxy: image: superng6/nginx:debian-stable-1.18.0 container_name: nginx-proxy restart: always networks: wordpress_net: ipv4_address: 172.19.0.6 ports: - 80:80 - 443:443 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/proxy/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/proxy/default.conf:/etc/nginx/conf.d/default.conf - $PWD/ssl:/etc/nginx/ssl - $PWD/logs/proxy:/var/log/nginx depends_on: - web web: image: superng6/nginx:debian-stable-1.18.0 container_name: wordpress-nginx restart: always networks: wordpress_net: ipv4_address: 172.19.0.5 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/nginx/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/nginx/default.conf:/etc/nginx/conf.d/default.conf - $PWD/conf/fastcgi.conf:/etc/nginx/fastcgi.conf - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/wordpress:/var/www/html - $PWD/logs/nginx:/var/log/nginx depends_on: - wordpress wordpress: image: wordpress:5-fpm container_name: wordpress-php restart: always networks: wordpress_net: ipv4_address: 172.19.0.4 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/wordpress:/var/www/html - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/conf/uploads.ini:/usr/local/etc/php/php.ini depends_on: - redis - db redis: image: redis:5 container_name: wordpress-redis restart: always networks: wordpress_net: ipv4_address: 172.19.0.3 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/redis-data:/data depends_on: - db db: image: mysql:5.7 container_name: wordpress-mysql restart: always networks: wordpress_net: ipv4_address: 172.19.0.2 environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;1\u0026#39; volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/mysql-data:/var/lib/mysql - $PWD/conf/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf networks: wordpress_net: driver: bridge ipam: config: - subnet: 172.19.0.0/16 进入到 wordpress-blog 目录下使用 docker-compose up -d启动docker容器\n配置nginx反向代理 配置80和443端口的反代\n把域名、证书路径以及后端服务器等信息换成自己的\n免费ssl证书的申请我在 阿里云wordpress配置免费ssl证书 中介绍过，直接下载nginx版的证书放到wordpress-blog/ssl/目录下即可\n[root@lvbibir ~]# vim wordpress-blog/conf/proxy/default.conf server { listen 80; listen [::]:80; server_name lvbibir.cn; # return 301 https://$host$request_uri; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Real-Port $remote_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $host; proxy_set_header X-NginX-Proxy true; } } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;gzip\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } [root@lvbibir ~]# docker exec -i nginx-proxy nginx -s reload 安装wordpress 现在已经可以通过http访问nginx反代的80端口访问wordpress了\n安装信息跟之前站点设置一样即可\n恢复备份 安装好之后启用插件，把备份文件上传到备份目录\n记得修改权限\n[root@lvbibir ~]# chown -R 33:tape wordpress-blog/wordpress/wp-content/ 恢复备份\n注：如果站点之前开启了https，在这步不要恢复wp-options表，不然会导致后台访问不了\n点击恢复即可\n配置ssl 启用 really simple ssl 插件，因为之前在nginx反代配置了ssl证书，虽然我们没有通过https访问，但是这个插件已经检测到了证书，可以一键为wordpress配置ssl\n这里我们已经可以通过https访问我们的wordpress了\n站点路径该插件也会自动修改，之前不恢复wp-options表的原因就在这，在我们没有配置好ssl之前，直接覆盖wordpress的各项设置会导致站点访问不了，重定向循环等各种各样的问题。\n恢复 wp-options 表 开启了ssl之后，通过备份插件再恢复一次，可以只恢复一张wp-options表，也可以再全量恢复下数据库，至此，站点迁移工作基本完成了。\n后续优化 开启https强制跳转 开启https强制跳转后，所有使用http访问我们站点的请求都会转到https，提高站点安全性\n[root@lvbibir ~]# vim /etc/nginx/nginx.conf server { listen 80; listen [::]:80; server_name lvbibir.cn; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;gzip\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } [root@lvbibir ~]# docker exec -i nginx-proxy nginx -s reload 开启redis缓存 wordpress搭配redis加速网站访问速度\n搭配jsdelivr-CDN实现全站cdn WordPress+jsDelivr开启伪全站CDN\n参考 从能用到好用-快速搭建高性能WordPress指南\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-to-docker/","summary":"前言 前段时间着手开始搭建自己的wordpress博客，刚开始图方便直接买了阿里云的轻量应用服务器，它是一套预先搭建好的lamp架构，并已经做了一些初始化配置，直接访问ip就可以进行wordpress的安装和配置了。 这套wordpress的一个非常好的优点就是可以在阿里云的控制台一","title":"wordpress迁移到docker"},{"content":"1. 实验环境 3台 centos6.5，1台 win10，openvpn-2.4.7，easy-rsa-3.0.5\n2. 拓扑结构 Win10 安装 openvpn-gui，三台 centos6.5 为 vmware虚拟机，分为 client、vpnserver、proxy\n三台 centos6.5 的 eth0 网卡均为内网(lan区段)地址 1.1.1.0/24 网段，proxy 额外添加一块 eth1 网卡设置 nat 模式模拟外网 ip\n3. 实验目的 win10访问proxy的外网ip对应端口连接到vpnserver，分配到内网ip后可以访问到client\n4. 实验思路 proxy配置ipv4转发，将访问到本机eth1网卡相对应的端口上的流量转发给vpnserver的vpn服务端口\nvpnserver为win10分配ip实现访问内网\n5. 实施步骤 5.1 初始化环境 虚拟机安装过程 略\n配置ip 节点 ip client： 1.1.1.1/24 vpnserver： 1.1.1.2/24 proxy： 1.1.1.3/24 192.168.150.114/24 win10： 192.168.150.1/24 环境初始化（client和vpnserver关闭iptables和selinux，proxy仅关闭selinux） [root@vpnserver ~]# sed -i \u0026lsquo;/SELINUX/s/enforcing/disabled/\u0026rsquo; /etc/selinux/config [root@vpnserver ~]# setenforce 0\n5.2 安装vpnserver及easy-rsa vpnserver安装openvpn 由于centos6的所有官方源已失效，使用https://www.xiaofeng.org/article/2019/10/centos6buildinstallopenvpnrpm-17.html中的方法将源码编译成rpm包。\nopenvpn版本：2.4.7\n下载easy-rsa 下载地址：https://github.com/OpenVPN/easy-rsa/tree/v3.0.5\n5.3 创建openvpn目录，配置vars变量 解压easy-rsa目录 [root@vpnserver ~]# mkdir openvpn [root@vpnserver ~]# unzip easy-rsa-3.0.5.zip [root@vpnserver ~]# mv easy-rsa-3.0.5 easy-rsa [root@vpnserver ~]# mkdir -p /etc/openvpn [root@vpnserver ~]# cp -a easy-rsa /etc/openvpn\n配置/etc/openvpn目录 [root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# cp vars.example vars [root@vpnserver easyrsa3]# vim vars 添加如下变量\nset_var EASYRSA_REQ_COUNTRY \u0026#34;CN\u0026#34; set_var EASYRSA_REQ_PROVINCE \u0026#34;Beijing\u0026#34; set_var EASYRSA_REQ_CITY \u0026#34;Beijing\u0026#34; set_var EASYRSA_REQ_ORG \u0026#34;lvbibir\u0026#34; set_var EASYRSA_REQ_EMAIL \u0026#34;lvbibir@163.com\u0026#34; set_var EASYRSA_REQ_OU \u0026#34;My OpenVPN\u0026#34; 5.4 创建服务端证书及key 创建服务端证书及key 初始化\n[root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa init-pki\n创建根证书\n[root@vpnserver easyrsa3]# ./easyrsa build-ca\n注意：在上述部分需要输入PEM密码 PEM pass phrase，输入两次，此密码必须记住，不然以后不能为证书签名。还需要输入common name 通用名，这个你自己随便设置个独一无二的\n创建服务器端证书\n[root@vpnserver easyrsa3]# ./easyrsa gen-req server nopass\n该过程中需要输入common name，随意但是不要跟之前的根证书的一样\n签约服务端证书\n[root@vpnserver easyrsa3]# ./easyrsa sign server server\n需要手动输入yes去人，还需要提供创建ca证书时的密码\n创建Diffie-Hellman，确保key穿越不安全网络的命令\n[root@vpnserver easyrsa3]# ./easyrsa gen-dh\n5.5 创建客户端证书及key 创建客户端证书 初始化\n[root@vpnserver ~]# mkdir client [root@vpnserver ~]# cd client/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa init-pki\n需输入yes确认\n创建客户端key及生成证书\n[root@vpnserver easyrsa3]# ./easyrsa gen-req zhijie.liu\n名字自己自定义，该密码是用户使用该key登录时输入的密码，可以加nopass参数在客户端登录时无需输入密码\n导入req证书\n[root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa import-req /root/client/easy-rsa/easyrsa3/pki/reqs/zhijie.liu.req zhijie.liu\n签约证书\n[root@vpnserver easyrsa3]# ./easyrsa sign client zhijie.liu\n这里生成client，名字要与之前导入名字一致\n签约证书期间需要输入yes确认，期间需要输入CA的密码\n5.6 归置服务器和客户端的证书 把服务器端必要文件放到/etc/openvpn下（ca证书、服务端证书、密钥） [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/private/server.key /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/server.crt /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/dh.pem /etc/openvpn/\n把客户端必要文件放到/root/client目录下（客户端的证书、密钥） [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /root/client [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/zhijie.liu.crt /root/client/ [root@vpnserver ~]# cp /root/client/easy-rsa/easyrsa3/pki/private/zhijie.liu.key /root/client\n5.7 vpn服务端server.conf配置文件修改 为服务器端编写配置文件 安装好配置文件后他会提供一个server配置的文件案例，将该文件放到/etc/openvpn下\n[root@vpnserver ~]# rpm -ql openvpn | grep server.conf\n[root@vpnserver ~]# cp /usr/share/doc/openvpn-2.4.7/sample/sample-config-files/server.conf /etc/openvpn/\n修改配置文件 [root@vpnserver ~]# vim /etc/openvpn/server.conf\n[root@vpnserver ~]# grep \u0026#39;^[^#|;]\u0026#39; /etc/openvpn/server.conf local 0.0.0.0 #监听地址 port 1194 #监听端口 proto tcp #监听协议 dev tun #采用路由隧道模式 ca /etc/openvpn/ca.crt #ca证书路径 cert /etc/openvpn/server.crt #服务器证书 key /etc/openvpn/server.key # This file should be kept secret 服务器秘钥 dh /etc/openvpn/dh.pem #密钥交换协议文件 server 10.8.0.0 255.255.255.0 #给客户端分配地址池，注意：不能和VPN服务器内网网段有相同 ifconfig-pool-persist ipp.txt push \u0026#34;route 1.1.1.0 255.255.255.0\u0026#34;\t#推送内网地址 client-to-client #客户端之间互相通信 keepalive 10 120 #存活时间，10秒ping一次,120 如未收到响应则视为断线 comp-lzo #传输数据压缩 max-clients 100 #最多允许 100 客户端连接 user openvpn #用户 group openvpn #用户组 persist-key persist-tun status /var/log/openvpn/openvpn-status.log log /var/log/openvpn/openvpn.log verb 3 5.8 后续设置（用户、iptables和路由转发） 后续设置 [root@vpnserver ~]# mkdir /var/log/openvpn/ [root@vpnserver ~]# useradd openvpn -s /sbin/nologin [root@vpnserver ~]# chown -R openvpn.openvpn /var/log/openvpn/ [root@vpnserver ~]# chown -R openvpn.openvpn /etc/openvpn/*\niptables设置nat规则和打开路由转发 [root@vpnserver ~]# iptables -A INPUT -p tcp \u0026ndash;dport 1194 -j ACCEPT [root@vpnserver ~]# iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE [root@vpnserver ~]# iptables -vnL -t nat\nChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * * 10.8.0.0/24 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination [root@vpnserver ~]# vim /etc/sysctl.conf\nnet.ipv4.ip_forward = 1 [root@vpnserver ~]# sysctl -p\n开启openvpn服务 [root@vpnserver ~]# openvpn \u0026ndash;daemon \u0026ndash;config /etc/openvpn/server.conf [root@vpnserver ~]# netstat -anput | grep 1194\nproxy开启端口转发/映射 [root@along ~]# vim /etc/sysctl.conf //打开路由转发\nnet.ipv4.ip_forward = 1 [root@proxy ~]# sysctl -p\n[root@proxy ~]# iptables -t nat -A PREROUTING -d 192.168.150.114 -p tcp \u0026ndash;dport 1194 -j DNAT \u0026ndash;to-destination 1.1.1.2:1194 [root@proxy ~]# iptables -t nat -A POSTROUTING -d 1.1.1.2 -p tcp \u0026ndash;dport 1194 -j SNAT \u0026ndash;to 1.1.1.3 [root@proxy ~]# iptables -A FORWARD -o eth0 -d 1.1.1.2 -p tcp \u0026ndash;dport 1194 -j ACCEPT [root@proxy ~]# iptables -A FORWARD -i eth0 -s 1.1.1.2 -p tcp \u0026ndash;sport 1194 -j ACCEPT\n[root@proxy ~]# iptables -A INPUT -p tcp \u0026ndash;dport 1194 -j ACCEPT\n[root@proxy ~]# service iptables save [root@proxy ~]# service iptables reload [root@proxy ~]# iptables -L -n\n6.客户段连接测试 下载openvpn客户端 略\n6.1 配置client端配置文件 [root@vpnserver ~]# rpm -ql openvpn | grep client.ovpn\n/usr/share/doc/openvpn-2.4.7/sample/sample-plugins/keying-material-exporter-demo/client.ovpn\n[root@vpnserver ~]# cp /usr/share/doc/openvpn-2.4.7/sample/sample-plugins/keying-material-exporter-demo/client.ovpn /root/client [root@vpnserver ~]# vim /root/client/client.ovpn\nclient dev tun proto tcp remote 192.168.150.114 1194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert client.crt key client.key comp-lzo verb 3\n6.2 拷贝客户端证书及配置文件 vpnserver没装vmtools所以先将所有文件放到proxy上然后通过远程工具下载\n[root@vpnserver openvpn]# scp /root/client/ca.crt root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/zhijie.liu.crt root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/zhijie.liu.key root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/client.ovpn root@1.1.1.3:/root/\n将这四个文件放到win10的C:\\Users\\lvbibir\\OpenVPN\\config目录下\n6.3 ping测试 ping client的内网ip1.1.1.1\n参考：\ncentos6源码编译openvpn并打包成rpm\nhttps://www.xiaofeng.org/article/2019/10/centos6buildinstallopenvpnrpm-17.html\nopenvpn源码下载地址\nhttps://openvpn.net/community-downloads/\ncentos6搭建openvpn\nhttp://www.likecs.com/show-6021.html\ncentos6做端口映射/端口转发\nhttps://blog.csdn.net/weixin_30872499/article/details/96654741?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0\u0026spm=1001.2101.3001.4242\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos6-deploy-openvpn/","summary":"1. 实验环境 3台 centos6.5，1台 win10，openvpn-2.4.7，easy-rsa-3.0.5 2. 拓扑结构 Win10 安装 openvpn-gui，三台 centos6.5 为 vmware虚拟机，分为 client、vpnserver、proxy 三台 centos6.5 的 eth0 网卡均为内网(lan区段)地址 1.1.1.0/24 网段，proxy","title":"centos6 部署 openvpn"},{"content":"介绍 项目地址\n这个项目准备打造一个安全基线检查平台，期望能够以最简单的方式在需要进行检查的服务器上运行。能够达到这么一种效果：基线检查脚本(以后称之为agent)可以单独在目标服务器上运行，并展示出相应不符合基线的地方，并且可以将检查时搜集到的信息以json串的形式上传到后端处理服务器上，后端服务器可以进行统计并进行可视化展示。\nAgent用到的技术：\nShell脚本 Powershell脚本 后端服务器用到的技术：\npython django bootstrap html 存储所用：\nsqlite3 前端页面部署 环境 系统 centos7.8(最小化安装) 前端：192.168.150.101 client端：192.168.150.102 安装python3.6 源码包下载地址\nyum install gcc gcc-c++ zlib-devel sqlite-devel mariadb-server mariadb-devel openssl-devel tcl-devel tk-devel tree libffi-devel -y tar -xf Python-3.6.10.tgz ./configure --enable-optimizations make make install python3 -V 安装pip3+django 源码包下载地址\ntar zxvf pip-21.0.1.tar.gz cd pip-21.0.1/ python3 setup.py build python3 setup.py install pip3 install django==2.2.15 git clone项目到本地 yum install -y git git clone https://github.com/chroblert/assetmanage.git 部署server端项目 cd assetManage # 使用python3安装依赖包 python3 -m pip install -r requirements.txt python3 manage.py makemigrations python3 manage.py migrate python3 manage.py runserver 0.0.0.0:8888 # 假定该服务器的IP未112.112.112.112 访问测试：http://192.168.150.101:8888/\n客户端进行检查 将项目目录中的Agent目录copy到需要进行基线检查的客户端 scp -r assetmanage/Agent/ 192.168.150.102:/root/ cd Agent/ chmod a+x ./*.sh 修改 linux_baseline_check.sh 文件的最后一行，配置前端django项目的ip和端口 运行脚本即可，终端会有检查结果的输出，前端页面相应也会有数据 ","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-deploy-benchmark/","summary":"介绍 项目地址 这个项目准备打造一个安全基线检查平台，期望能够以最简单的方式在需要进行检查的服务器上运行。能够达到这么一种效果：基线检查脚本(以后称之为agent)可以单独在目标服务器上运行，并展示出相应不符合基线的地方，并且可以将检查时搜集到的信息以json串的形式上传到后端处理服","title":"centos7基线检查（benchmark）平台部署"},{"content":" 环境：centos7.8 在centos中可以在如下文件中查看一个NIC的配置 ： /etc/sysconfig/network-scripts/ifcfg-N\nHWADDR=, 其中 以AA:BB:CC:DD:EE:FF形式的以太网设备的硬件地址.在有多个网卡设备的机器上，这个字段是非常有用的，它保证设备接口被分配了正确的设备名 ，而不考虑每个网卡模块被配置的加载顺序.这个字段不能和MACADDR一起使用.\nMACADDR=, 其中 以AA:BB:CC:DD:EE:FF形式的以太网设备的硬件地址.在有多个网卡设备的机器上.这个字段用于给一个接口分配一个MAC地址，覆盖物理分配的MAC地址 . 这个字段不能和HWADDR一起使用.\n简单总结一下：\nMACADDR是系统的网卡物理地址，因为在接收数据包时需要根据这个值来做包过滤。 HWADDR是网卡的硬件物理地址，只有厂家才能修改 可以用MACADDR来覆盖HWADDR，但这两个参数不能同时使用 ifconfig和nmcli等网络命令中显示的物理地址其实是MACADDR的值，虽然显示的名称写的是HWADDR(ether)。 修改网卡的mac地址\n#sudo vim /etc/sysconfig/network-scripts/ifcfg-ens32 注释其中的\u0026#34;HWADDR=xx:xx:xx:xx:xx:xx\u0026#34; 添加或者修改\u0026#34;MACADDR=xx:xx:xx:xx:xx:xx\u0026#34; 如果没有删除或者注释掉HWADDR，当HWADDR与MACADDR地地不同时，启动不了网络服务的提示：　“Bringing up interface eth0: Device eth0 has different MAC address than expected,ignoring.” 故正确的操作是将HWADDR删除或注释掉，改成MACADDR 查看系统初始的mac地址即HWADDR 把配置文件中的MACADDR注释或者删除掉，不用配置HWADDR，重启网络服务后用命令查看到的mac地址就是网卡的HWADDR\n参考 https://blog.csdn.net/rikeyone/article/details/108406865\nhttps://zhidao.baidu.com/question/505133906.html\nhttps://blog.csdn.net/caize340724/article/details/100958968?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.control\u0026spm=1001.2101.3001.4242\n","permalink":"https://www.lvbibir.cn/en/posts/tech/hwaddr-macaddr-different/","summary":"环境：centos7.8 在centos中可以在如下文件中查看一个NIC的配置 ： /etc/sysconfig/network-scripts/ifcfg-N HWADDR=, 其中 以AA:BB:CC:DD:EE:FF形式的以太网设备的硬件地址.在有多个网卡设备的机器上，这个字段是非常有用的，它保证设备接口被分配了正确的设备名 ，而不考虑每个网卡模块被配置的加载顺序.这个字段","title":"hwaddr和macaddr的区别"},{"content":"七牛云配置 1. 注册七牛云，新建存储空间 七牛云新用户有10G的免费空间，作为个人博客来说基本足够了\n2. 为存储空间配置加速域名 3. 配置https证书 购买免费证书 补全域名信息 域名验证 根据在域名提供商处新建解析\ndns配置好之后等待CA机构审核后颁发证书就可以了\n开启https PicGo配置 下载安装 下载链接：https://github.com/Molunerfinn/PicGo/releases/\n建议下载稳定版\n配置七牛云图床 ak和sk在七牛云→个人中心→密钥管理中查看\n在picgo端配置各项信息，注意网址要改成 https\ntypora测试图片上传 下载地址：https://www.typora.io/\n在文件→偏好设置→图像中配置图片上传，选择安装好的PicGo的应用程序\n点击验证图片上传\n到七牛云存储空间看是否有这两个文件\ntypora可以实现自动的图片上传，并将本地连接自动转换为外链地址\n可能的报错 一般报错原因都可在picgo的日志文件找到，路径：C:\\Users\\username\\AppData\\Roaming\\picgo\nfailed to fetch 日志报错如下\n问题在于端口冲突，如果你打开了多个picgo程序，就会端口冲突，picgo自动帮你把36677端口改为366771端口，导致错误。\n重新验证\n","permalink":"https://www.lvbibir.cn/en/posts/blog/typora-picgo-qiniu-upload-image/","summary":"七牛云配置 1. 注册七牛云，新建存储空间 七牛云新用户有10G的免费空间，作为个人博客来说基本足够了 2. 为存储空间配置加速域名 3. 配置https证书 购买免费证书 补全域名信息 域名验证 根据在域名提供商处新建解析 dns配置好之后等待CA机构审核后颁发证书就可以了 开启https PicGo配置 下载安","title":"markdown图片存储方案 | typora+picgo+七牛云"},{"content":"现象 博客加载不出来我在七牛云的图片资源 使用浏览器直接访问图片url却是可以成功的 我将之前csdn的博客迁移到了wordpress，图片外链地址就是csdn的，都可以正常加载。 使用浏览器直接访问图片url却是可以成功的\n我将之前csdn的博客迁移到了wordpress，图片外链地址就是csdn的，都可以正常加载。\n排查 1、由于浏览器直接访问七牛云图床的url地址是可以访问的，证明地址并没错，有没有可能是referer防盗链的配置问题\n查看防盗链配置，并没有开\n2、wordpress可以加载出来csdn的外链图片，期间也试了其他图床都是没问题的。\n3、看看七牛的图片外链和csdn的有何区别\n注意到七牛的图片外链是http，当时嫌麻烦并没有配置https，看来问题是出在这了\n因为我的网站配置了ssl证书，可能由于安全问题浏览器不予加载http项目，用http访问站点测试下图片是否可以加载\n访问成功了！\n解决 给图床服务器安装ssl证书，开启https访问，参考：typora-picgo-qiniu-upload-image\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-load-image-failed/","summary":"现象 博客加载不出来我在七牛云的图片资源 使用浏览器直接访问图片url却是可以成功的 我将之前csdn的博客迁移到了wordpress，图片外链地址就是csdn的，都可以正常加载。 使用浏览器直接访问图片url却是可以成功的 我将之前csdn的博客迁移到了wordpress，图片外链地址就","title":"wordpress加载图片失败"},{"content":"默认主题下在后台设置里修改即可\ndux主题修改方式：在后台管理→dux主题编辑器→网站底部信息中添加\n\u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;京ICP备2021023168号-1\u0026lt;/a\u0026gt; 通用修改方式\n在主题目录的footer.php 文件中的\u0026lt;footer\u0026gt;\u0026lt;/footer\u0026gt; 下添加代码\n\u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;你的备案号\u0026lt;/a\u0026gt; ","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-add-icp/","summary":"默认主题下在后台设置里修改即可 dux主题修改方式：在后台管理→dux主题编辑器→网站底部信息中添加 \u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;京ICP备2021023168号-1\u0026lt;/a\u0026gt; 通用修改方式 在主题目录的footer.php 文件中的\u0026lt;fo","title":"wordpress添加icp备案号"},{"content":"配置ssl证书 1、登录阿里云，选择产品中的ssl证书\n如果域名是阿里的他会自动创建dns解析，如果是其他厂商需要按照图片配置，等待几分钟进行验证\n点击审核，等待签发\n签发后根据需求下载所需证书\n我的wordpress是直接买的阿里轻量应用服务器，打开轻量应用服务器的控制台配置域名\n选择刚申请好的ssl证书\n在wordpress后台修改地址\n大功告成\n配置https强制跳转 一般站点需要在httpd.conf中的\u0026lt;VirtualHost *:80\u0026gt; \u0026lt;/VirtualHost\u0026gt;中配置重定向\nwordpress不同，需要在伪静态文件（.htaccess）中配置重定向，无需在httpd.conf中配置\n修改伪静态文件（.htaccess） 伪静态文件一般在网页根目录，是一个隐藏文件\n在#END Wordpress前添加如下重定向代码，记得把域名修改成自己的\nRewriteEngine On RewriteCond %{HTTPS} !on RewriteRule ^(.*)$ https://lvbibir.cn/%{REQUEST_URI} [L,R=301] 图中两段重定向代码略有不同\n第一段代码重定向触发器：当访问的端口不是443时进行重定向重定向规则：重定向到：https://{原域名}/{原url资源} 第二段代码重定向触发器：当访问的协议不是 TLS/SLL（https）时进行重定向重定向规则：重定向到：https://lvbibir.cn/{原url资源} 第一段代码使用端口判断，第二段代码通过访问方式判断，建议使用访问方式判断，这样服务改了端口也可以正常跳转 第一段代码重定向的原先的域名，第二段代码可以把ip地址重定向到指定域名 测试 curl -I http://lvbibir.cn 使用http访问站点的80端口成功通过301跳转到了https\n参考 https://help.aliyun.com/document_detail/98727.html?spm=5176.smartservice_service_chat.0.0.1508709aJMmZwg\nhttps://blog.csdn.net/weixin_39037804/article/details/102801202\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-ssl/","summary":"配置ssl证书 1、登录阿里云，选择产品中的ssl证书 如果域名是阿里的他会自动创建dns解析，如果是其他厂商需要按照图片配置，等待几分钟进行验证 点击审核，等待签发 签发后根据需求下载所需证书 我的wordpress是直接买的阿里轻量应用服务器，打开轻量应用服务器的控制台配置域名 选择刚申","title":"wordpress配置免费ssl证书和https强制跳转"},{"content":"Docker Machine简介 Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker环境。 Docker Machine支持在常规Linux操作系统、虚拟化平台、openstack、公有云等不同环境下安装配置dockerhost。 Docker Machine 项目基于 Go 语言实现，目前在 Github 上的维护地址：https://github.com/docker/machine/\nDocker Machine实践 环境准备 三台centos7，两台新系统，一台装有docker ip： machine：192.168.1.101 host1:192.168.1.127 host2:192.168.1.180 保证三台centos7可以连接到外网 下载并安装machine base=https://github.com/docker/machine/releases/download/v0.14.0 \u0026amp;\u0026amp; curl -L $base/docker-machine-$(uname -s)-$(uname -m) \u0026gt;/tmp/docker-machine \u0026amp;\u0026amp; sudo install /tmp/docker-machine /usr/local/bin/docker-machine\t下载并安装doker-machine，路径在/usr/local/bin下\n创建machine machine指的是docker daemon主机，其实就是在host上安装和部署docker。\n创建流程： 安装docker软件包 ssh免密登陆远程主机 复制证书 配置docker daemon 启动docker 创建machine要求免密登录远程主机 ssh-keygen ssh-copy-id 目标ip [root@server5 ~]# ssh-keygen [root@server5 ~]# ssh-copy-id 192.168.1.127 [root@server5 ~]# ssh-copy-id 192.168.1.180 测试：\nssh 192.168.1.127 ssh 192.168.1.180 创建主机（离线安装需要在目标主机提前安装好docker软件包） docker-machine create --driver generic --generic-ip-address=192.168.1.127 host1 参考 docker三剑客之machine\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-machine/","summary":"Docker Machine简介 Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker环境。 Docker Machine支持在常规Linux操作系统、虚拟化平台、openstack、公有云等不同环境下安装配置dockerhost。 Docker Machine 项目基于 Go 语言实现，目前在 Github 上的","title":"docker | 三剑客之machine"},{"content":"docker swarm 概述 https://blog.csdn.net/anumbrella/article/details/80369913\ndocker swarm 使用 环境搭建 准备3台Ubuntu系统主机(即用于搭建集群的3个Docker机器)，每台机器上都需要安装Docker并且可以连接网络，同时要求Docker版本必须是1.12及以上，因为老版本不支持Docker Swarm 集群管理节点Docker机器的IP地址必须固定，集群中所有节点都能够访问该管理节点。 集群节点之间必须使用相应的协议并保证其以下端口可用： 1. 用于集群管理通信的TCP端口2377； 2. TCP 和UDP 端口7946，用于节点间的通信； 3. UDP 端口 4789，用于覆盖网络流量 为了进行本实例演示，此处按照要求安装了3台使用centos7.4系统的机器，这三台机器的主机名称分别为manager1(作为管理节点)，worker1(作为工作节点)，worker2(作为工作节点),其IP地址分别如下：\n主机名 IP地址 manager 192.168.0.101 worker-1 192.168.0.102 worker-2 192.168.0.103 创建 Docker Swarm集群 在 manager 上创建 swarm 集群 [root@node-1 ~]# docker swarm init --advertise-addr 192.168.0.101 使用 docker node ls 查看集群节点信息 [root@manager ~]# docker node ls 向 Docker Swarm集群添加工作节点 在 worker1 和 worker2 中执行米慧玲，加入 swarm 集群 docker swarm join --token SWMTKN-1-2zhqxsklcroivbpjzzntn5snsim79o5z7xzj4hzexk9phsz68q-d0seaxjgxpjebk8fdqt6d6yz5 192.168.0.101:2377 2. 在管理节点上，使用 docker node ls 查看集群节点信息\n[root@manager ~]# docker node ls 向 Docker Swarm集群部署服务 在向 docker swarm 集群中部署服务时，既可以使用 docker hub 上自带的镜像来启动服务，也可以自己通过 dockerfile 的镜像来启动服务，如果使用自己的 dockerfile 构建的镜像来启动服务，那么必须先将镜像推送到 docker hub 中心仓库 这里，我们使用 docker hub 上自带的 alpine 镜像为例来部署集群服务\n[root@manager ~]# docker service create --replicas 1 --name helloworld alpine ping docker.com 查看Docker Swarm 集群中的服务 当服务部署完成后，在管理节点上可以通过 docker service ls 查看当前集群中的服务列表信息 [root@manager ~]# docker service ls 使用 docker service inspect 查看部署的服务具体详情 [root@manager ~]# docker service inspect helloworl 使用 docker service ps 查看指定服务在集群节点上的分配和运行情况 [root@manager ~]# docker service ps helloworld 更改 Docker Swarm 集群服务样本数量 在集群中部署的服务，如果只运行一个副本，就无法体现出集群的优势，并且一旦该机器或副本崩溃，该服务将无法访问，所以通常一个服务会启动多个副本\n[root@manager ~]# docker service scale helloworld=5 更改完成后，就可以谈过 docker service ps 查看这五个服务副本在3个节点上的具体分布和运行情况 [root@manager ~]# docker service ps helloworld 删除服务 对于不需要的服务，我们可以进行删除\n[root@manager ~]# docker service rm helloworld 访问服务 在管理节点上，执行 docker network ls 查看网络列表 [root@manager ~]# docker network ls 在管理节点上，创建 overlay 网络 [root@manager ~]# docker network create -d overlay ov_net 在管理节点上，再次部署服务 [root@manager ~]# docker service create --network ov_net --name my-web --publish 8080:80 --replicas 2 nginx 访问 nginx 服务 参考 docker swarm删除节点（解散集群） 截取已创建好的 swarm 集群的 token\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-swarm/","summary":"docker swarm 概述 https://blog.csdn.net/anumbrella/article/details/80369913 docker swarm 使用 环境搭建 准备3台Ubuntu系统主机(即用于搭建集群的3个Docker机器)，每台机器上都需要安装Docker并且可以连接网络，同时要求Docker版本必须是1.12及以上，因为老版本不支持Docker Swarm 集群管理节点Docker机器的IP地址必须固定，集群中所有","title":"docker | 三剑客之swarm"},{"content":"info 查看docker的各项信息 查看docke的各项操作，包括docker版本、容器数量、镜像数量、仓库地址、镜像存放位置等 容器操作 run 启动容器 docker run ：创建一个新的容器并运行一个命令 语法\ndocker run [OPTIONS] IMAGE [COMMAND] [ARG\u0026hellip;]\nOPTIONS说明：\n-a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 1. 只指定容器端口（宿主机端口随机映射） docker run -p 80 -it ubuntu /bin/bash 2. 主机端口：容器端口 docker run -p 8080:80 -it ubuntu /bin/bash 3. IP：容器端口 docker run -p 0.0.0.0:80 -it ubuntu /bin/bash 4. IP：端口：容器端口 dokcer run -p 0.0.0.0:8080:80 -it ubuntu /bin/bash -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=\u0026#34;nginx-lb\u0026#34;: 为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \u0026#34;mars\u0026#34;: 指定容器的hostname； -e username=\u0026#34;ritchie\u0026#34;: 设置环境变量； -env-file=[]: 从指定文件读入环境变量； --cpuset=\u0026#34;0-2\u0026#34; or --cpuset=\u0026#34;0,1,2\u0026#34;: 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； --net=\u0026#34;bridge\u0026#34;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --volume , -v: 绑定一个卷 -v [hostpath]:containerpath:[ro|wo|rw] 如果没有设置 hostpath 则挂载到 /var/lib/docker/volumes/\u0026lt;containerid\u0026gt;/ 目录下 如果没有设置权限，默认 rw(读写) 实例\n使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。\ndocker run --name mynginx -d nginx:latest 使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。\ndocker run -P -d nginx:latest 使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。\ndocker run -p 80:80 -v /data:/data -d nginx:latest 绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。\ndocker run -p 127.0.0.1:80:8080/tcp ubuntu bash 使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。\nrunoob@runoob:~$ docker run -it nginx:latest /bin/bash root@b8573233d675:/# ps 查看容器 docker ps : 列出容器 语法\ndocker ps [OPTIONS]\nOPTIONS说明：\n-a: 显示所有的容器，包括未运行的。 -f: 根据条件过滤显示的内容。 --format: 指定返回值的模板文件。 -l: 显示最近创建的容器。 -n: 列出最近创建的n个容器。 --no-trunc: 不截断输出。 -q: 静默模式，只显示容器编号。 -s: 显示总的文件大小。 实例\n列出所有在运行的容器信息。\nrunoob@runoob:~$ docker ps CONTAINER ID IMAGE COMMAND ... PORTS NAMES 09b93464c2f7 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; ... 80/tcp, 443/tcp myrunoob 96f7f14e99ab mysql:5.6 \u0026quot;docker-entrypoint.sh\u0026quot; ... 0.0.0.0:3306-\u0026gt;3306/tcp mymysql 列出最近创建的5个容器信息。\nrunoob@runoob:~$ docker ps -n 5 CONTAINER ID IMAGE COMMAND CREATED 09b93464c2f7 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; 2 days ago ... b8573233d675 nginx:latest \u0026quot;/bin/bash\u0026quot; 2 days ago ... b1a0703e41e7 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; 2 days ago ... f46fb1dec520 5c6e1090e771 \u0026quot;/bin/sh -c 'set -x \\t\u0026quot; 2 days ago ... a63b4a5597de 860c279d2fec \u0026quot;bash\u0026quot; 2 days ago ... 列出所有创建的容器ID。\nrunoob@runoob:~$ docker ps -a -q 09b93464c2f7 b8573233d675 b1a0703e41e7 f46fb1dec520 a63b4a5597de 6a4aa42e947b de7bb36e7968 43a432b73776 664a8ab1a585 ba52eb632bbd inspect 查看详细信息 docker inspect : 获取容器/镜像的元数据。 语法\ndocker inspect [OPTIONS] NAME|ID [NAME|ID\u0026hellip;]\nOPTIONS说明：\n-f :指定返回值的模板文件。 -s :显示总的文件大小。 --type json:为指定类型返回JSON。 --format :以指定格式返回数据 实例\n获取所有容器的ip地址\ndocker inspect --format=\u0026#39;{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; $(docker ps -aq) 获取某个容器的挂载信息\ndocker inspect --format=\u0026#39;{{json .Mounts}}\u0026#39; demo-volume-2 | python -m json.tool start/stop/restart 开启/关闭/重启容器 docker start :启动一个或多个已经被停止的容器\ndocker stop :停止一个运行中的容器\ndocker restart :重启容器\n语法\ndocker start [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\ndocker stop [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\ndocker restart [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\n实例\n启动已被停止的容器myrunoob\ndocker start myrunoob 停止运行中的容器myrunoob\ndocker stop myrunoob 重启容器myrunoob\ndocker restart myrunoob rm 删除容器 docker rm ：删除一个或多少容器 语法\ndocker rm [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\nOPTIONS说明：\n-f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷 实例\n强制删除容器db01、db02\ndocker rm -f db01 db02 移除容器nginx01对容器db01的连接，连接名db\ndocker rm -l db 删除容器nginx01,并删除容器挂载的数据卷\ndocker rm -v nginx01 attach 进入一个开启的容器中 docker attach :连接到正在运行中的容器。\n语法\ndocker attach [OPTIONS] CONTAINER\n要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。\n官方文档中说attach后可以通过CTRL-C来detach，但实际上经过我的测试，如果container当前在运行bash，CTRL-C自然是当前行的输入，没有退出；如果container当前正在前台运行进程，如输出nginx的access.log日志，CTRL-C不仅会导致退出容器，而且还stop了。这不是我们想要的，detach的意思按理应该是脱离容器终端，但容器依然运行。好在attach是可以带上\u0026ndash;sig-proxy=false来确保CTRL-D或CTRL-C不会关闭容器。\n实例\n容器mynginx将访问日志指到标准输出，连接到容器查看访问信息。\nrunoob@runoob:~$ docker attach --sig-proxy=false mynginx 192.168.239.1 - - [10/Jul/2016:16:54:26 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 304 0 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; logs 查看容器日志 docker logs : 获取容器的日志 语法\ndocker logs [OPTIONS] CONTAINER\nOPTIONS说明：\n-f : 跟踪日志输出。类似 tail 命令的 -f 选项 --since :显示某个开始时间的所有日志 -t : 显示时间戳 --tail :仅列出最新N条容器日志 实例\n跟踪查看容器mynginx的日志输出。\nrunoob@runoob:~$ docker logs -f mynginx 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 2016/07/10 16:53:33 [error] 5#5: *1 open() \u0026quot;/usr/share/nginx/html/favicon.ico\u0026quot; failed (2: No such file or directory), client: 192.168.239.1, server: localhost, request: \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot;, host: \u0026quot;192.168.239.130\u0026quot;, referrer: \u0026quot;http://192.168.239.130/\u0026quot; 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot; 404 571 \u0026quot;http://192.168.239.130/\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 192.168.239.1 - - [10/Jul/2016:16:53:59 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 304 0 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; ... 查看容器mynginx从2016年7月1日后的最新10条日志。\ndocker logs --since=\u0026quot;2016-07-01\u0026quot; --tail=10 mynginx top 查看容器内的进程 docker top :查看容器中运行的进程信息，支持 ps 命令参数。 语法\ndocker top [OPTIONS] CONTAINER [ps OPTIONS]\n容器运行时不一定有/bin/bash终端来交互执行top命令，而且容器还不一定有top命令，可以使用docker top来实现查看container中正在运行的进程。 实例\n查看容器mymysql的进程信息。\nrunoob@runoob:~/mysql$ docker top mymysql UID PID PPID C STIME TTY TIME CMD 999 40347 40331 18 00:58 ? 00:00:02 mysqld 查看所有运行容器的进程信息。\nfor i in `docker ps |grep Up|awk '{print $1}'`;do echo \\ \u0026amp;\u0026amp;docker top $i; done exec 在容器中启动新的进程 docker exec ：在运行的容器中执行命令 语法\ndocker exec [OPTIONS] CONTAINER COMMAND [ARG\u0026hellip;]\nOPTIONS说明：\n-d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开 -t :分配一个伪终端 实例\n在容器 mynginx 中以交互模式执行容器内 /root/runoob.sh 脚本:\nrunoob@runoob:~$ docker exec -it mynginx /bin/sh /root/runoob.sh http://www.runoob.com/ 在容器 mynginx 中开启一个交互模式的终端:\nrunoob@runoob:~$ docker exec -i -t mynginx /bin/bash root@b1a0703e41e7:/# 也可以通过 docker ps -a 命令查看已经在运行的容器，然后使用容器 ID 进入容器。\n查看已经在运行的容器 ID：\ndocker ps -a ... 9df70f9a0714 openjdk \u0026quot;/usercode/script.sh…\u0026quot; ... 第一列的 9df70f9a0714 就是容器 ID。\n通过 exec 命令对指定的容器执行 bash:\ndocker exec -it 9df70f9a0714 /bin/bash kill 停止容器 docker kill :杀掉一个运行中的容器。 语法\ndocker kill [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\nOPTIONS说明：\n-s :向容器发送一个信号 实例\n杀掉运行中的容器mynginx\nrunoob@runoob:~$ docker kill -s KILL mynginx mynginx 镜像操作 images 查看镜像 docker images : 列出本地镜像。 语法\ndocker images [OPTIONS] [REPOSITORY[:TAG]]\nOPTIONS说明：\n-a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； --digests :显示镜像的摘要信息； -f :显示满足条件的镜像； --format :指定返回值的模板文件； --no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 实例\n查看本地镜像列表。\nrunoob@runoob:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 5 minutes ago 329 MB runoob/ubuntu v4 1c06aa18edee 2 days ago 142.1 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5c6e1090e771 2 days ago 165.9 MB httpd latest ed38aaffef30 11 days ago 195.1 MB alpine latest 4e38e38c8ce0 2 weeks ago 4.799 MB mongo 3.2 282fd552add6 3 weeks ago 336.1 MB redis latest 4465e4bcad80 3 weeks ago 185.7 MB php 5.6-fpm 025041cd3aa5 3 weeks ago 456.3 MB python 3.5 045767ddf24a 3 weeks ago 684.1 MB ... 列出本地镜像中REPOSITORY为ubuntu的镜像列表。\nroot@runoob:~# docker images ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 14.04 90d5884b1ee0 9 weeks ago 188 MB ubuntu 15.10 4e3b13c8a266 3 months ago 136.3 MB inspect 查看详细信息 docker inspect : 获取容器/镜像的元数据。 语法\ndocker inspect [OPTIONS] NAME|ID [NAME|ID\u0026hellip;]\nOPTIONS说明：\n-f :指定返回值的模板文件。 -s :显示总的文件大小。 --type :为指定类型返回JSON。 --format :以指定格式返回数据 实例\n获取镜像mysql:5.6的元信息。\nrunoob@runoob:~$ docker inspect mysql:5.6 [ { \u0026quot;Id\u0026quot;: \u0026quot;sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec\u0026quot;, \u0026quot;RepoTags\u0026quot;: [ \u0026quot;mysql:5.6\u0026quot; ], \u0026quot;RepoDigests\u0026quot;: [], \u0026quot;Parent\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Created\u0026quot;: \u0026quot;2016-05-24T04:01:41.168371815Z\u0026quot;, \u0026quot;Container\u0026quot;: \u0026quot;e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54\u0026quot;, \u0026quot;ContainerConfig\u0026quot;: { \u0026quot;Hostname\u0026quot;: \u0026quot;b0cf605c7757\u0026quot;, \u0026quot;Domainname\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;AttachStdin\u0026quot;: false, \u0026quot;AttachStdout\u0026quot;: false, \u0026quot;AttachStderr\u0026quot;: false, \u0026quot;ExposedPorts\u0026quot;: { \u0026quot;3306/tcp\u0026quot;: {} }, ... rmi 删除镜像 docker rmi : 删除本地一个或多少镜像。 语法\ndocker rmi [OPTIONS] IMAGE [IMAGE\u0026hellip;]\nOPTIONS说明：\n-f :强制删除； --no-prune :不移除该镜像的过程镜像，默认移除； 注：IMAGE可以使用[仓库：标签]的格式，也可以使用镜像ID，可以同时删除多个镜像 1、使用[仓库：标签]的格式：删除一个标签。当一个镜像文件有多个标签时，删除完所有的标签，镜像文件也随之删除 2、使用镜像ID的格式：先将该镜像文件的所有标签删除，再删除镜像文件\n删除所有镜像\ndocker rmi $(docker images -q) 删除某个仓库的所有镜像\ndocker rmi $(docker images -q ubuntu) 实例\n强制删除本地镜像 runoob/ubuntu:v4。\nroot@runoob:~# docker rmi -f runoob/ubuntu:v4 Untagged: runoob/ubuntu:v4 Deleted: sha256:1c06aa18edee44230f93a90a7d88139235de12cd4c089d41eed8419b503072be Deleted: sha256:85feb446e89a28d58ee7d80ea5ce367eebb7cec70f0ec18aa4faa874cbd97c73 search 查找镜像 语法\ndocker search [OPTIONS] TERM\nOPTIONS说明：\n--automated :只列出 automated build类型的镜像； --no-trunc :显示完整的镜像描述； -s :列出收藏数不小于指定值的镜像。 实例\n从Docker Hub查找所有镜像名包含java，并且收藏数大于10的镜像\nrunoob@runoob:~$ docker search -s 10 java NAME DESCRIPTION STARS OFFICIAL AUTOMATED java Java is a concurrent, class-based... 1037 [OK] anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC ... 115 [OK] develar/java 46 [OK] isuper/java-oracle This repository contains all java... 38 [OK] lwieske/java-8 Oracle Java 8 Container - Full + ... 27 [OK] nimmis/java-centos This is docker images of CentOS 7... 13 [OK] pull 拉取镜像 docker pull [OPTIONS] NAME[:TAG|@DIGEST]\nOPTIONS说明：\n-a :拉取所有 tagged 镜像 --disable-content-trust :忽略镜像的校验,默认开启 实例\n从Docker Hub下载java最新版镜像。\ndocker pull java 从Docker Hub下载REPOSITORY为java的所有镜像。\ndocker pull -a java push 推送镜像 docker push : 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库 语法\ndocker push [OPTIONS] NAME[:TAG]\nOPTIONS说明：\n--disable-content-trust :忽略镜像的校验,默认开启 实例\n上传本地镜像myapache:v1到镜像仓库中。\ndocker push myapache:v1 commit 通过容器构建镜像 docker commit :从容器创建一个新的镜像。 语法\ndocker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\nOPTIONS说明：\n-a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 实例\n将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。\nrunoob@runoob:~$ docker commit -a \u0026quot;runoob.com\u0026quot; -m \u0026quot;my apache\u0026quot; a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057 runoob@runoob:~$ docker images mymysql:v1 REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 15 seconds ago 329 MB build 通过Dockerfile构建镜像 docker build 命令用于使用 Dockerfile 创建镜像。 语法\ndocker build [OPTIONS] PATH | URL | -\nOPTIONS说明：\n--build-arg=[] :设置镜像创建时的变量； --cpu-shares :设置 cpu 使用权重； --cpu-period :限制 CPU CFS周期； --cpu-quota :限制 CPU CFS配额； --cpuset-cpus :指定使用的CPU id； --cpuset-mems :指定使用的内存 id； --disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； --force-rm :设置镜像过程中删除中间容器； --isolation :使用容器隔离技术； --label=[] :设置镜像使用的元数据； -m :设置内存最大值； --memory-swap :设置Swap的最大值为内存+swap，\u0026quot;-1\u0026quot;表示不限swap； --no-cache :创建镜像的过程不使用缓存； --pull :尝试去更新镜像的新版本； --quiet, -q :安静模式，成功后只输出镜像 ID； --rm :设置镜像成功后删除中间容器； --shm-size :设置/dev/shm的大小，默认值是64M； --ulimit :Ulimit配置。 --tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 --network: 默认 default。在构建期间设置RUN指令的网络模式 实例\n使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1。\ndocker build -t runoob/ubuntu:v1 . 使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像。\ndocker build github.com/creack/docker-firefox 也可以通过 -f Dockerfile 文件的位置：\n$ docker build -f /path/to/a/Dockerfile . 在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回：\n$ docker build -t test/myapp . Sending build context to Docker daemon 2.048 kB Error response from daemon: Unknown instruction: RUNCMD ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-command/","summary":"info 查看docker的各项信息 查看docke的各项操作，包括docker版本、容器数量、镜像数量、仓库地址、镜像存放位置等 容器操作 run 启动容器 docker run ：创建一个新的容器并运行一个命令 语法 docker run [OPTIONS] IMAGE [COMMAND] [ARG\u0026hellip;] OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器I","title":"docker | 命令手册"},{"content":"环境准备 主机版本为Centos7.4，docker版本为docker-ce-18.09.7-3.el7.x86_64 node1：192.168.0.111 node2：192.168.0.107\n两台安装docker的环境 保证两台主机上的docker的Client API与Server APi版本一致 修改daemon.json配置文件，添加label，用于区别两台docker主机 node1：\n[root@localhost ~]# vim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;], \u0026#34;labels\u0026#34;: [\u0026#34;-label nodeName=node1\u0026#34;] #添加label } 查看效果\n[root@localhost ~]# systemctl restart docker [root@localhost ~]# docker info node2; 修改Client与守护进程通信的方式（修改为tcp的方式） 修改通信方式共有三种方式：\n修改daemon.json文件，添加host键值对 添加：\u0026ldquo;hosts\u0026rdquo;: [\u0026ldquo;tcp://0.0.0.0:2375\u0026rdquo;] 开放本机ip的2375端口，可以让其他docker主机的client进行连接 修改/lib/systemd/system/docker.service文件，添加-H启动参数 修改：ExecStart=/usr/bin/docker -H tcp://0.0.0.0:2375 使用dokcerd启动docker，添加-H参数 dockerd -H tcp://0.0.0.0:2375 Centos7中/etc/docker/daemon.json会被docker.service的配置文件覆盖，直接添加daemon.json不起作用 所以我使用的是第二种方式\nnode1：\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/docker -H tcp://0.0.0.0:2375 [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 5775 1 3 23:17 ? 00:00:00 /usr/bin/dockerd -H tcp://0.0.0.0:2375 root 5779 5775 0 23:17 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 5879 3919 0 23:17 pts/1 00:00:00 grep --color=auto docker 远程访问 node2：\n[root@localhost ~]# curl http://192.168.0.111:2375/info [root@localhost ~]# docker -H tcp://192.168.0.111:2375 info 如果频繁使用-H选项未免太过于麻烦，可以修改DOCKER_HOST这个环境变量的值，node2就可以像使用本地的docker一样来远程连接node1的守护进程\n[root@localhost ~]# export DOCKER_HOST=\u0026#34;tcp://192.168.0.111:2375\u0026#34; [root@localhost ~]# docker info 当无需再远程连接node1的守护进程时，将DOCKER_HOST环境变量置空即可\n[root@localhost ~]# export DOCKER_HOST=\u0026#34;\u0026#34; [root@localhost ~]# docker info node1： 因为node1设置了修改Client与守护进程的通信方式，所以本地无法再通过默认的socket进行连接，必须使用-H选项通过tcp来进行连接，也可以通过DOCKER_HOST来修改\n[root@localhost ~]# docker info Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? [root@localhost ~]# docker -H 0.0.0.0:2375 info 如果本机依旧希望使用默认的socket进行连接，可以在/lib/systemd/system/docker.service中再添加一个-H选项\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 6462 1 2 23:40 ? 00:00:00 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock root 6467 6462 0 23:40 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 6567 3919 0 23:40 pts/1 00:00:00 grep --color=auto docker [root@localhost ~]# docker info ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-remote-call-daemon/","summary":"环境准备 主机版本为Centos7.4，docker版本为docker-ce-18.09.7-3.el7.x86_64 node1：192.168.0.111 node2：192.168.0.107 两台安装docker的环境 保证两台主机上的docker的Client API与Server","title":"docker | 守护进程的远程调用"},{"content":"实现跨主机的docker容器之间的通讯：\n使用网桥实现跨主机的连接 docker原生的网络：overlay、macvlan 第三方网络：flaanel、weave、calic 网桥 open vswitch weave macvlan macvlan是Linux操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以为一张物理网卡设置多个mac地址，相当于物理网卡施展了影分身之术，由一个变多个，同时要求物理网卡打开混杂模式。针对每个mac地址，都可以设置IP地址，本来是一块物理网卡连接到交换机，现在是多块虚拟网卡连接到交换机。\n当容器需要直连入物理网络时，可以使用Macvlan。Macvlan本身不创建网络，本质上首先使宿主机物理网卡工作在‘混杂模式’，这样物理网卡的MAC地址将会失效，所有二层网络中的流量物理网卡都能收到。接下来就是在这张物理网卡上创建虚拟网卡，并为虚拟网卡指定MAC地址，实现一卡多用，在物理网络看来，每张虚拟网卡都是一个单独的接口。使用Macvlan有几点需要注意：\n容器直接连接物理网络，由物理网络负责分配IP地址，可能的结果是物理网络IP地址被耗尽，另一个后果是网络性能问题，物理网络中接入的主机变多，广播包占比快速升高而引起的网络性能下降问题。 前边说过了，宿主机上的某张网上需要工作在‘混乱模式’下。 从长远来看bridge网络与overlay网络是更好的选择，原因就是虚拟网络应该与物理网络隔离而不是共享。 优缺点：\n优点是性能非常好 缺点是地址需要手动分配 Macvlan网络有两种模式：bridge模式与802.1q trunk bridge模式。\nbridge模式，Macvlan网络流量直接使用宿主机物理网卡。 802.1q trunk bridge模式，Macvlan网络流量使用Docker动态创建的802.1q子接口，对于路由与过虑，这种模式能够提供更细粒度的控制 环境准备：\n两台centos7 docker版本：18.03 ip：192.168.0.53（node-1） 192.168.0.54（node-2） node-1 node-2 注意：node-1使用的物理网卡是ens33，node-2使用的是ens32 [root@node-1 ~]# ip link show ens33 [root@node-1 ~]# ip link set ens32 promisc on #开启混杂模式，保证多个ip可以通过 [root@node-1 ~]# docker network create -d macvlan --subnet 10.0.0.0/24 --gateway=10.0.0.1 -o parent=ens33 mac_net1 [root@node-1 ~]# docker network ls node-1 docker run -itd --name bbox-1 --ip 10.0.0.11 --network mac_net1 busybox node-2 docker run -itd --name bbox-2 --ip 10.0.0.12 --network mac_net1 busybox node-1 [root@node-1 ~]# docker exec bbox-1 ping 10.0.0.12 [root@node-1 ~]# docker exec bbox-1 ping bbox-2 可以ping通ip，但是无法ping通主机名，因为它没有dns解析 [root@node-1 ~]# brctl show 因为macvlan不依赖于bridge网络，所以查看不到新的桥接网络 [root@node-1 ~]# docker exec bbox-1 ip link 查看到eth0连接到了if2 [root@node-1 ~]# ip link show ens33 可以查看到ens33的编号是2，即bbox-1容器的eth0网卡连接到了ens33物理网卡 [root@node-1 ~]# docker network create -d macvlan -o parent=ens33 mac_net2 Error response from daemon: network dm-b34ee1020a96 is already using parent interface ens33 再创建macvlan网络时发现已经无法再创建，即一块网卡只能添加一个macvlan的地址\n一块网卡绑定多个macvlan地址 [root@node-1 ~]# modinfo 8021q # 查看内核是否支持802.1q封装 [root@node-1 ~]# modprobe 8021q # 如果上条命令执行后没有结果，使用该命令加载该模块 node-1 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=manual 修改为不需要ip的manual模式\nnode-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32 BOOTPROTO=manual node-1 添加两块虚拟网卡，注意与实际的ens32网卡的网段区分开 ens32使用的是192.168.0.0/24网段，虚拟网卡使用的是192.168.1.0/24和192.168.2.0/24\n[root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens33.10 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.10 BOOTPROTO=none NAME=ens33.10 DEVICE=ens33.10 ONBOOT=yes IPADDR=192.168.1.10 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33.10 /etc/sysconfig/network-scripts/ifcfg-ens33.20 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.20 BOOTPROTO=none NAME=ens33.20 DEVICE=ens33.20 ONBOOT=yes IPADDR=192.168.2.10 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-1 ~]# ifup ens33.10 [root@node-1 ~]# ifup ens33.20 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.10 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.10 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.20 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.20 node-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.10 BOOTPROTO=none NAME=ens32.10 DEVICE=ens32.10 ONBOOT=yes IPADDR=192.168.1.20 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.20 BOOTPROTO=none NAME=ens32.20 DEVICE=ens32.20 ONBOOT=yes IPADDR=192.168.2.20 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-2 ~]# ifup ens32.10 [root@node-2 ~]# ifup ens32.20 node-1 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens33.10 mac_net11 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens33.20 mac_net12 node-2 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens32.10 mac_net11 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens32.20 mac_net12 node-1 [root@node-2 ~]# docker run -itd --name bbox-11 --ip=172.16.11.11 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-12 --ip=172.16.12.11 --network mac_net12 busybox node-2 [root@node-2 ~]# docker run -itd --name bbox-21 --ip=172.16.11.12 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-22 --ip=172.16.12.12 --network mac_net12 busybox node-1 [root@node-1 ~]# docker exec bbox-11 ping 172.16.11.12 PING 172.16.11.12 (172.16.11.12): 56 data bytes 64 bytes from 172.16.11.12: seq=0 ttl=64 time=0.867 ms 64 bytes from 172.16.11.12: seq=1 ttl=64 time=1.074 ms 64 bytes from 172.16.11.12: seq=2 ttl=64 time=1.145 ms 64 bytes from 172.16.11.12: seq=3 ttl=64 time=0.938 ms ^C [root@node-1 ~]# docker exec bbox-12 ping 172.16.12.12 PING 172.16.12.12 (172.16.12.12): 56 data bytes 64 bytes from 172.16.12.12: seq=0 ttl=64 time=0.858 ms 64 bytes from 172.16.12.12: seq=1 ttl=64 time=1.140 ms 64 bytes from 172.16.12.12: seq=2 ttl=64 time=0.818 ms 64 bytes from 172.16.12.12: seq=3 ttl=64 time=1.056 ms ^C 在两台系统进行修改，添加网关，修改防火墙策略\nnode-1中记得将ens32更换为ens33\nifconfig ens32.10 172.16.10.1 netmask 255.255.255.0 ifconfig ens32.20 172.16.20.1 netmask 255.255.255.0 iptables -t nat -A POSTROUTING -o ens32.10 -j MASQUERADE iptables -t nat -A POSTROUTING -o ens32 -j MASQUERADE iptables -A FORWARD -i ens32.10 -o ens32 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32.10 -o ens32 -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -j ACCEPT overlay 一、跨主机网络概述 二、准备overlay环境 为支持容器的跨主机通信，Docker提供了overlay driver。Docker overlay网络需要一个key-value数据库用于保存网络状态信息，包括Network、Endpoint、IP等。Consul、Etcd和ZooKeeper都是Docker支持的key-value软件，这里我们使用Consul\n1. 环境描述\n节点 系统版本 docker版本 角色 IP地址 node-1 centos7.4 docker-18.03.0 consul 192.168.0.101 node-2 centos7.4 docker-18.03.0 host 192.168.0.102 node-3 centos7.4 docker-18.03.0 host 192.168.0.103 2. 创建consul\nnode-1; [root@node-1 ~]# docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 容器启动后可以通过192.168.0.101:8500访问到consul 3. 修改docker配置文件 修改node-2和node-3的docker daemon的配置文件/etc/systemd/system/docker.service\n[root@node-2 ~]# vim /etc/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.0.101:8500 --cluster-advertise=ens32:2376 [root@node-2 ~]# systemctl daemon-reload [root@node-2 ~]# systemctl restart docker -H ：tcp：允许tcp连接daemon -H：unix：默认的socket连接方式，支持远程的同时，本地也可以连接 \u0026ndash;cluster-store 指定consul的地址 \u0026ndash;cluster-advertise 告知consul自己的连接地址 node-2和node-3会自动注册到consul数据库中。 三、创建overlay网络 1、在node-2中创建网络 在node-2中创建overlay网络ov_net1\n[root@node-2 ~]# docker network create -d overlay ov_net1 -d overlay：指定driver为overlay [root@node-2 ~]# docker network ls 2、node-3查看创建的网络 注意到ov_net1的 SCOPE 为 global，而其他网络为 local 。在node-3上查看存在的网络:\n[root@node-3 ~]# docker network ls node-3上也能看到ov_net1，只是因为创建ov_net1时将overlay网络信息存入了consul，node-3从consul读取到了新网络数据。之后ov_net1的任何变化都会同步到node-2和node-3. 3、查看ov_net1详细信息\n[root@node-2 ~]# docker network inspect ov_net1 IPAM 是指 IP Address Management，docker自动为 ov_net1 分配的 IP 空间为 10.0.0.0/24 四、在overlay中运行容器 1、创建容器 bbox-1 在 node-2 上运行一个 busybox 容器并连接到 ov_net1.\n[root@node-2 ~]# docker run -itd --name bbox-1 --network ov_net1 busybox 2、查看 bbox-1 网络配置\n[root@node-2 ~]# docker exec bbox-1 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 bbox-1 有两个网络接口，eth0 和 eth1 eth0 IP 为 10.0.0.2，连接的是overlay网络 ov_net1 eth1 IP 为 172.18.0.2 容器的默认路由是走 eth1，其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力 [root@node-2 ~]# docker network ls [root@node-2 ~]# docker network inspect docker_gwbridge 从 docker network inspect docker_gwbridge 输出可确认 docker_gwbridge 的 IP 地址范围是 172.18.0.0/16，当前连接的容器就是 bbox-1（172.18.0.2） 而且此网络的网关就是网桥 docker_gwbridge 的 IP 172.18.0.1\n[root@node-2 ~]# ifconfig docker_gwbridge docker_gwbridge: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fe80::42:c5ff:fe45:937 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 02:42:c5:45:09:37 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 这样容器 bbox-1 就可以通过docker_gwbridge 访问外网\n[root@node-2 ~]# docker exec bbox-1 ping -c 4 www.baidu.com PING www.baidu.com (182.61.200.6): 56 data bytes 64 bytes from 182.61.200.6: seq=0 ttl=53 time=6.721 ms 64 bytes from 182.61.200.6: seq=1 ttl=53 time=7.954 ms 64 bytes from 182.61.200.6: seq=2 ttl=53 time=11.723 ms 64 bytes from 182.61.200.6: seq=3 ttl=53 time=15.105 ms --- www.baidu.com ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 6.721/10.375/15.105 ms 五、overlay网络连通性 1、node-3 中 运行 bbox-2\n[root@node-3 ~]# docker run -itd --name bbox-2 --network ov_net1 busybox 2、查看 bbox-2 路由情况\n[root@node-3 ~]# docker exec bbox-2 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.3 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 3、互通测试\n[root@node-3 ~]# docker exec bbox-2 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=2.628 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=1.004 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=1.277 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=1.505 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 1.004/1.603/2.628 ms 可见 overlay 网络中的容器可以直接通信，同时docker也实现了DNS服务 4、实现原理 docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0， veth pair 一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。 br0 除了连接所有的 veth pair，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示： [root@node-2 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024217edc413 no docker_gwbridge 8000.0242c5450937 no vethc59120e virbr0 8000.525400b76fd4 yes virbr0-nic [root@node-3 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242ef3c7df7 no docker_gwbridge 8000.0242c81afaee no vethf4562a9 virbr0 8000.525400c28478 yes virbr0-nic 要查看 overlay 网络的 namespace 可以在 node-2 和 node-3 上执行 ip netns（请确保在此之前执行过 ln -s /var/run/docker/netns /var/run/netns），可以看到两个 node 上有一个相同的 namespace \u0026ldquo;1-dd91de7599\u0026rdquo;\n[root@node-2 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-2 ~]# ip netns 6889f61efc4b (id: 1) 1-dd91de7599 (id: 0) [root@node-3 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-3 ~]# ip netns 8e4722847745 (id: 1) 1-dd91de7599 (id: 0) \u0026ldquo;1-dd91de7599\u0026rdquo; 这就是 ov_net1 的 namespace，查看 namespace 中的 br0 上的设备\n[root@node-2 ~]# ip netns exec 1-dd91de7599 brctl show bridge name bridge id STP enabled interfaces br0 8000.0e7576c7c035 no veth0 vxlan0 六、overlay网络隔离 不同的 overlay 网络是相互隔离的。我们创建第二个 overlay 网络 ov_net2 并运行容器 bbox-3 1、创建网络 ov_net2\n[root@node-2 ~]# docker network create -d overlay ov_net2 2、启动容器 bbox-3\n[root@node-2 ~]# docker run -itd --name bbox-3 --network ov_net2 busybox 3、查看 bbox-3 网络 bbox-3 分配到的 IP 是 10.0.1.2，尝试 ping bbox-1（10.0.0.2）\n[root@node-2 ~]# docker exec -it bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes --- 10.0.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes --- 172.18.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss ping 失败，可见不同 overlay 网络之间是隔离的，即使通过 docker_gwbridge 也不能通信 如果要实现 bbox-3 和 bbox-1 通信，可以将 bbox-3 也连接到 ov_net1 这时 bbox-3 同时连接到了 ov_net1 和 ov_net2 上\n[root@node-2 ~]# docker network connect ov_net1 bbox-3 [root@node-2 ~]# docker exec bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth2 scope link src 10.0.0.4 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=0.184 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.158 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=0.162 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=0.093 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.093/0.149/0.184 ms docker 默认为 overlay 网络分配 24 位掩码的子网（10.0.X.0/24），所有主机共享这个 subnet，容器启动时会顺序从此空间分配 IP。当然我们也可以通过 \u0026ndash;subnet 指定 IP 空间。\ndocker network create -d overlay --subnet 10.22.1.0/24 ov_net ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-rong-qi-kua-zhu-ji-lian-jie/","summary":"实现跨主机的docker容器之间的通讯： 使用网桥实现跨主机的连接 docker原生的网络：overlay、macvlan 第三方网络：flaanel、weave、calic 网桥 open vswitch weave macvlan macvlan是Linux操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以","title":"docker | 容器的跨主机连接"},{"content":"什么是数据卷 docker的理念之一就是将应用与其运行的环境打包。通常docker容器的生命周期都是与在容器中运行的程序相一致的，我们对于数据的要求就是持久化；另一方面docker容器之间也需要一个共享文件的渠道。\n数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS），为一个或者过个容器提供服务 数据卷设计的目的，在于数据的持久化，他完全独立于容器的生存周期，因此，docker不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理 从图片中：\n数据卷独立于docker容器存在，它存在于docker的宿主机中 数据卷可以是目录，也可以是文件 docker容器可以利用数据卷与宿主机共享文件 同一个数据卷可以支持多个容器的访问 数据卷的特点 数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中 数据卷可以在容器之间共享和重用 可以对数据卷里的内容直接进行修改 数据卷的变化不会影响镜像的更新 数据卷会一直存在，即使挂载数据卷的容器已经被删除 数据卷操作 为容器添加数据卷 docker run -it -v HOST_DIRECTORY:CONTAINER_DIRETORY IMAGE [COMMADN] HOST-DIRECTORY：指定主机目录，不存在时即创建 CONTAINER：指定容器目录，不存在时即创建 示例：\n[root@localhost ~]# docker run -it -v /docker/data_volume:/data_volume busybox /bin/sh / # touch /data_volume/test\t#创建测试文件 / # echo \u0026#34;lvbibir\u0026#34; \u0026gt; /data_volume/test / # cat /data_volume/test lvbibir [root@localhost ~]# cat /docker/data_volume/test\t#验证测试文件 lvbibir [root@localhost ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES be3fad8d789e busybox \u0026#34;/bin/sh\u0026#34; 6 minutes ago Up 6 minutes elastic_boyd [root@localhost ~]# docker inspect elastic_boyd 为数据卷添加访问权限 docker run -it -v HOST_DIRECTORY:CONTAINER_DIRETORY:r/w IMAGE [COMMADN] 权限可以设置为：\nro：only-read，只读 wo：only-write，只写 rw：write and read，读写 示例：\n[root@localhost ~]# docker run -itd -v /docker/data_volume:/data_volume:ro busybox /bin/sh 3ee3a2b7a97c0a10125d46ee1135bf59af1d97932572d49fdd5c0bb64bf775a5 [root@localhost ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3ee3a2b7a97c busybox \u0026#34;/bin/sh\u0026#34; 4 seconds ago Up 3 seconds confident_hopper [root@localhost ~]# docker inspect confident_hopper 使用dockerfile构建包含数据卷的镜像 dockerfile指令： VOLUME [\u0026ldquo;HOST_DIRECTORY\u0026rdquo;]\ndockerfile中配置数据卷无法指定映射到本地的目录 构建好镜像启动容器时，数据卷会进行初始化，docker会在/var/lib/docker/volumes/下为数据卷创建新的随机名字的目录（不同版本该目录位置可能不同，具体以inspect查看到的为准） 使用同一个镜像构建的多个容器，映射的本地目录也不一样 通过数据卷容器来进行容器间的数据共享 示例：\n[root@localhost ~]# cat Dockerfile #For test data_volume FROM busybox:latest VOLUME [\u0026#34;/data_volume1\u0026#34;,\u0026#34;/data_volume2\u0026#34;] CMD /bin/sh [root@localhost ~]# docker build -t test/data_volume . [root@localhost ~]# docker run -itd --name test_data_volume_1 test/data_volume /bin/sh ee8347a4bd3590e8cb65a28e1ebfc5d01e44f2ce70d33a2fa9bbc19782e34f21 [root@localhost ~]# docker exec test_data_volume_1 ls -l / | grep data_volume drwxr-xr-x 2 root root 6 Aug 14 15:20 data_volume1 drwxr-xr-x 2 root root 6 Aug 14 15:20 data_volume2 [root@localhost ~]# docker inspect test_data_volume_1 [root@localhost ~]# docker run -itd --name test_data_volume_2 test/data_volume /bin/sh b4f654706ea15e657cd61bb92d16fa6c6b8eb9129a68b1c9209ea21967175b24 [root@localhost ~]# docker exec test_data_volume_2 ls -l / | grep data_volume drwxr-xr-x 2 root root 6 Aug 14 15:24 data_volume1 drwxr-xr-x 2 root root 6 Aug 14 15:24 data_volume2 [root@localhost ~]# docker inspect test_data_volume_2 数据卷容器 一个命名的容器挂载了数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 使用数据卷容器而不是用数据卷直接挂载，可以不暴露宿主机的实际目录 删除数据卷容器对于已经挂载了该容器的容器没有影响，因为数据卷容器只是传递了挂载信息，任何对于目录的更改都不需要通过数据卷容器 从图片中：\n数据卷容器挂载了一个本地目录，其他容器通过连接这个数据卷容器来实现数据的共享 数据卷容器操作 挂载数据卷容器 docker run -it --volumes-from [CONTAINER] IMAGE [COMMAND] CONTAINER必须是已经挂载了卷组的容器，dockerfile和-v两个方式都可以 CONTAINER可以未运行，但必须存在 示例： 创建数据卷容器\n[root@localhost ~]# cat Dockerfile #For test data_volume FROM busybox:latest VOLUME [\u0026#34;/data_volume1\u0026#34;,\u0026#34;/data_volume2\u0026#34;] CMD /bin/sh [root@localhost ~]# docker build -t test/data_volume . [root@localhost ~]# docker run -it --name data_volume_container test/data_volume /bin/sh / # touch /data_volume1/test1 / # touch /data_volume2/test2 / # exit 创建一个容器，挂载数据卷容器进行验证\n[root@localhost ~]# docker run -itd --name test_dvc_1 --volumes-from data_volume_container busybox /bin/sh 6c4afa29df7ef226da7f1f0d394a356d53b92e3b20fa6c4632e7197ba393612c [root@localhost ~]# docker exec test_dvc_1 ls /data_volume1/ test1 [root@localhost ~]# docker exec test_dvc_1 ls /data_volume2/ test2 使用这个新容器对挂载的目录进行更改\n[root@localhost ~]# docker exec test_dvc touch /data_volume1/test2 [root@localhost ~]# docker exec test_dvc ls /data_volume1/ test1 test2 再创建一个新容器验证上一个容器对挂载目录的更改是否生效\n[root@localhost ~]# docker run -itd --name test_dvc_2 --volumes-from data_volume_container busybox /bin/sh 276c24ecd6ee62f35abf24855ffc5416b9abe987c1bb693ec57bf27d241383d2 [root@localhost ~]# docker exec test_dvc_2 ls /data_volume1 test1 test2 [root@localhost ~]# docker inspect --format=\u0026#34;{{.Mounts}}\u0026#34; test_dvc_1 [{volume 1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be /var/lib/docker/volumes/1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be/_data /data_volume1 local true } {volume d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764 /var/lib/docker/volumes/d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764/_data /data_volume2 local true }] [root@localhost ~]# docker inspect --format=\u0026#34;{{.Mounts}}\u0026#34; test_dvc_2 [{volume d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764 /var/lib/docker/volumes/d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764/_data /data_volume2 local true } {volume 1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be /var/lib/docker/volumes/1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be/_data /data_volume1 local true }] [root@localhost ~]# docker inspect test_dvc_1 [root@localhost ~]# docker inspect test_dvc_2 删除数据卷容器 删除数据卷容器后，已经挂载了这个数据卷容器的容器不受任何影响 数据卷容器只传递链接信息，挂载的数据并不需要通过数据卷容器来进行传输\n数据卷的备份和还原 数据备份 备份这个数据卷容器挂载的所有目录\ndocker run --volumes-from [container] -v $(pwd):/backup [image] tar cvf /backup/backup.tar [container data volume] -v $(pwd):/backup：挂载一个数据卷用于存放备份文件 tar命令：将数据卷容器挂载的目录进行压缩，备份到/backup目录 数据还原 docker run --volumes-from [container] -v $(pwd):/backup [image] tar xvf /backup/backup.tar [container data volume] ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-data-volume/","summary":"什么是数据卷 docker的理念之一就是将应用与其运行的环境打包。通常docker容器的生命周期都是与在容器中运行的程序相一致的，我们对于数据的要求就是持久化；另一方面docker容器之间也需要一个共享文件的渠道。 数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS），为一个或","title":"docker | 数据卷（data volume）"},{"content":"docker简介 Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在GitHub (https://github.com/docker/docker) 上进行维护。 Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容（LXC）等技术。 在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。\n为什么要使用docker 作为一种新兴的虚拟化方式，Docker 跟传统的虚拟化方式相比具有众多的优势。 首先，Docker 容器的启动可以在秒级实现，这相比传统的虚拟机方式要快得多。 其次，Docker 对系统资源的 利用率很高，一台主机上可以同时运行数千个 Docker 容器。 容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚 拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。 具体说来，Docker 在如下几个方面具有较大的优势。\n更快速的交付和部署 对开发和运维（devop）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。 开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。 更高效的虚拟化 Docker 容器的运行不需要额外的 hypervisor 支持，它是内核级的虚拟化，因此可以实现更高的性能和效率。 更轻松的迁移和扩展 Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 更简单的管理 使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。 对比传统虚拟机 docker的应用场景 简化配置 一次构建，多处运行 提升开发效率 应用隔离 多租户环境 为每个容器启用多个不同的容器 快速的部署 代码流水线管理 代码调试 docker镜像 docker镜像是一套使用联合加载技术实现的层叠的只读文件系统，包含基础镜像和附加镜像层\n为什么docker镜像很小 Linux操作系统分别由两部分组成 1.内核空间(kernel) 2.用户空间(rootfs) 内核空间是kernel,Linux刚启动时会加载bootfs文件系统，之后bootf会被卸载掉， 用户空间的文件系统是rootfs,包含常见的目录，如/dev、/proc、/bin、/etc等等 不同的Linux发行版本(红帽，centos，ubuntu等)主要的区别是rootfs, 多个Linux发行版本的kernel差别不大。 每个不同linux发行版的docker镜像只包含对应的rootfs，所以比完整的系统镜像要小得多\ndocker镜像的存储位置 /var/lib/docker(可以使用docker info来进行查看) 写时复制（copy on write） 当一个新容器启动时，读写层是没有任何数据的，当用户需要读取一些文件时，可以直接从只读层进行读取，只有当用户要修改只读层一些文件时，docker才会将该文件从只读层复制出来放在读写层供使用者修改，只读层中的文件是没有改变的\n仓库（repository）与仓库注册服务器（registry） Repository：本身是一个仓库，这个仓库里面可以放具体的镜像，是指具体的某个镜像的仓库，比如Tomcat下面有很多个版本的镜像，它们共同组成了Tomcat的Repository。\nRegistry：镜像的仓库，比如官方的是Docker Hub，它是开源的，也可以自己部署一个，Registry上有很多的Repository，Redis、Tomcat、MySQL等等Repository组成了Registry。\ndocker的C/S模式 用户在Docker Client中运行Docker的各种命令，这些命令会传送给在docker宿主机上运行的docker守护进程，docker的守护进程来实现docker的各种功能 启动docker服务后，docker的守护进程会一直在后台运行\nRemote API docker命令行接口是docker最常用的与守护进程进行通信的接口，docker的二进制命令文件（例如docker run）此时就是docker的Client，docker也提供了其他的接口：Remote API 用户可以通过编写程序调用Remote API，与docker守护进程进行通信，将自己的程序与docker进行集成\nRESTful风格的API：与大多数程序的API风格类似 STDIN、STDOUT、STDERR：Remote API在某些复杂的情况下，也支持这三种方式来与docker守护进程进行通信 如图就是使用自定义程序调用Remote API与docker守护进程通信的C/S模式 Client与守护进程的连接方式 unix:///var/run/docker.sock是默认的连接方式，可以通过配置修改为其他的socket连接方式\nunix:///var/run/docker.sock tcp://host:port fd://socketfd 用户可以通过dokcer的二进制命令接口或者自定义程序，自定义程序通过Remote API来调用docker守护进程，Client与Server之间通过Socket来进行连接，这种连接意味着Client与Server既可以在同一台机器上运行，也可以在不同机器上运行，Client可以通过远程的方式来连接Server ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-jian-jie/","summary":"docker简介 Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在GitHub (https://github.com/docker/docker) 上进行维护。 Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其RHEL","title":"docker | 简介以及基础概念"},{"content":"概述 独立容器网络：none host none网络最为安全，只有localback接口 host网络只和物理机相连，保证跟物理机相连的网络效率\t跟物理机完全一样（网络协议栈与主机名）\n容器间的网络：bridge docker bridge详解 docker启动时默认会有一个docker0网桥，该网桥就是桥接模式的体现 用户也可以自建bridge网络，建立后dokcer也会创建一个网桥\n跨主机的容器间的网络：macvlan overlay\n第三方网络：flannel weave calic\ndocker0 安装了docker的系统，使用ifconfig可以查看到docker0设备，docker守护进程就是通过docker0为容器提供网络连接的各种服务 docker0实际上是linux虚拟网桥（交换机) 网桥是数据链路层的设备，它通过mac地址来对网络进行划分，并且在不同的网络之间传递数据\nlinux虚拟网桥的特点：\n可以设置ip地址（二层的网桥可以设置三层的ip地址） 相当于拥有一个隐藏的虚拟网卡 docker0的地址划分：\nIP：172.17.0.1（各版本可能不同） 子网掩码：255.255.0.0 MAC：02:42:00:00:00:00 到 02:42:ff:ff:ff:ff（各版本可能不同） 总共提供了65534个地址 每当一个容器启动时，docker守护进程会创建网络连接的两端，一端在容器内创建eth0网卡，另一端在dokcer0网桥中开启一个端口veth*\n查看网桥设备需要预先安装bridge-utils软件包\n[root@localhost ~]# yum install -y bridge-utils [root@localhost ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024247d799bf no virbr0 8000.525400b76fd4 yes virbr0-nic 开启一个容器，查看网络设置：\n[root@localhost ~]# docker run -it --name nwt1 centos /bin/bash [root@0ef32e882bcf /]# ifconfig bash: ifconfig: command not found [root@0ef32e882bcf /]# yum install -y net-tools [root@0ef32e882bcf /]# ifconfig ctrl+p，ctrl+q 让这个人继续后台运行 再查看一下网桥\n[root@localhost ~]# brctl show [root@localhost ~]# ifconfig 自定义docker0 当默认docker0的ip或者网段与主机环境发生冲突时，可以修改docker0的地址和网段来进行自定义\nifconfig docker0 IP netmask NETMASK [root@localhost ~]# ifconfig docker0 192.168.200.1 netmask 255.255.255.0 [root@localhost ~]# ifconfig [root@localhost ~]# systemctl restart docker [root@localhost ~]# docker run -it centos /bin/bash [root@a5c6ebf79340 /]# yum install -y net-tools [root@a5c6ebf79340 /]# ifconfig 自定义虚拟网桥 添加虚拟网桥：\nbrctl addbr br0 ifconfig br0 IP netmask NETMASK 更改docker守护进程的启动配置\n/lib/systemd/system/docker.service中添加-b=br0 [root@localhost ~]# brctl addbr br0 [root@localhost ~]# ifconfig br0 192.168.100.1 netmask 255.255.255.0 [root@localhost ~]# ifconfig [root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -b=br0 [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 4156 1 1 14:06 ? 00:00:00 /usr/bin/dockerd -b=br0 root 4161 4156 0 14:06 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 4263 1558 0 14:06 pts/0 00:00:00 grep --color=auto docker 开启一个容器\n[root@localhost ~]# docker run -it --name nwt3 centos /bin/bash [root@d70269c9557e /]# yum install -y net-tools [root@d70269c9557e /]# ifconfig 同一宿主机间容器的连接 允许单台主机内所有容器互联（默认情况） 拒绝容器间连接 允许特定容器间的连接 允许单台主机内所有容器互联（默认情况） \u0026ndash;icc=true 默认为true，即允许同一宿主机下所有容器之间网络连通\n[root@localhost ~]# docker run -itd --name test1 busybox /bin/sh 7ec641b21b66b6472f4e92cfaa7f9c0674c4322a5265a05e272ae180b0d4687c [root@localhost ~]# docker exec test1 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) [root@localhost ~]# docker run -itd --name test2 busybox /bin/sh fee0ff3e7f82cd1fa06eea11d850251931dff4dff2f0c7ee3e5a9904532beeb6 [root@localhost ~]# docker exec test2 ping 172.17.0.2 -c 4 PING 172.17.0.2 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.133 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.136 ms 64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.264 ms 64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.163 ms --- 172.17.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.133/0.174/0.264 ms 容器的ip是不可靠的连接 可以使用\u0026ndash;link选项来连接两个容器\ndocker run --link=[CONTAINER_NAME]:[ALIAS] [IMAGE] [COMMAND] \u0026ndash;link后面的test3指连接到test3容器，nt是为test3创建了一个别名 新建两个容器进行测试\n[root@localhost ~]# docker run -itd --name test3 busybox /bin/sh 1fd4e373dba17fdf1fa93121e08ea2f1f32d8f4116339c072a72a73574b0926f [root@localhost ~]# docker run -itd --name test4 --link=test3:nt busybox /bin/sh c04b9b759bd4cc9af54000a742df58c8369a7f1bfc8862a8325481f1d61db135 [root@localhost ~]# [root@localhost ~]# docker exec test4 ping nt -c 4 PING nt (172.17.0.4): 56 data bytes 64 bytes from 172.17.0.4: seq=0 ttl=64 time=0.256 ms 64 bytes from 172.17.0.4: seq=1 ttl=64 time=0.196 ms 64 bytes from 172.17.0.4: seq=2 ttl=64 time=0.164 ms 64 bytes from 172.17.0.4: seq=3 ttl=64 time=0.148 ms --- nt ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.148/0.191/0.256 ms \u0026ndash;link选项对容器做了如下改变：\n修改了env环境变量 修改了hosts文件 [root@localhost ~]# docker exec test4 env [root@localhost ~]# docker exec test4 cat /etc/hosts 删除之前使用的test1与test2容器，这两个容器占用的ip释放，重启test3后，使用最新的ip地址\n[root@localhost ~]# docker rm -f test1 test1 [root@localhost ~]# docker rm -f test2 test2 [root@localhost ~]# docker restart test3 test3 [root@localhost ~]# docker exec test3 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) 可以看到随着test3的ip地址发生改变，test4容器中的hosts文件也随之改变\n[root@localhost ~]# docker exec test4 cat /etc/hosts 拒绝容器间连接 修改守护进程的启动选项：\u0026ndash;icc=false\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --icc=false [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker 新建两个容器进行测试，可以看到无法ping通\n[root@localhost ~]# docker run -itd --name test10 busybox /bin/sh 700f026459206531b0fda811a43bc12af2f0815dc695f317a1f52939bfada2a1 [root@localhost ~]# docker run -itd --name test11 busybox /bin/sh 792cc31481739e1b2537597bc54c76737333bf95412dac2209e050f35d276dd4 [root@localhost ~]# docker exec test10 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:06 inet addr:172.17.0.6 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) [root@localhost ~]# docker exec test11 ping 172.16.0.6 ^C 允许特定容器间的连接 修改守护进程选项：\n\u0026ndash;icc=false \u0026ndash;iptables=true\t#允许docker容器配置添加到linux的iptables设置中 \u0026ndash;link 只有设置了\u0026ndash;link的两个容器间才可以互通\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --icc=false --iptables=true [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker 新建两个容器进行验证\n[root@localhost ~]# docker run -itd --name test21 busybox /bin/sh 77f56db227acaa590f729c12a4852d3131f1729851ea8c613a670effbfa512ad [root@localhost ~]# docker run -itd --name test22 --link=test21:nt busybox /bin/sh f4e346387588198cafcfd1d6a2c330a20375b746d05c08bf06e100f9af294a9e [root@localhost ~]# docker exec test22 ping nt -c 4 PING nt (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.201 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.164 ms 64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.195 ms 64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.188 ms --- nt ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.164/0.187/0.201 ms [root@localhost ~]# docker exec test22 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:14 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1124 (1.0 KiB) TX bytes:476 (476.0 B) [root@localhost ~]# docker exec test21 ping 172.17.0.3 -c 4 PING 172.17.0.3 (172.17.0.3): 56 data bytes 64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.181 ms 64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.168 ms 64 bytes from 172.17.0.3: seq=2 ttl=64 time=0.109 ms 64 bytes from 172.17.0.3: seq=3 ttl=64 time=0.226 ms --- 172.17.0.3 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.109/0.171/0.226 ms ","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-network/","summary":"概述 独立容器网络：none host none网络最为安全，只有localback接口 host网络只和物理机相连，保证跟物理机相连的网络效率 跟物理机完全一样（网络协议栈与主机名） 容器间的网络：bridge docker bridge详解 docker启动时默认会有一个docker0网桥，该网桥就是桥接模式","title":"docker | 网络简介"},{"content":"LVM基本特性：（可以通过插件CLVM，实现群集逻辑卷管理） PV物理卷\nLV逻辑卷（逻辑卷管理：会在物理存储上生成抽象层，以便创建逻辑存储卷，方便设备命名）（下面是逻辑卷的分类） Linear\t线性卷(这是默认的lvm形式，即按顺序占用磁盘，一块写完了再写另一块) Stripe\t条带逻辑卷 RAID\traid逻辑卷 Mirror\t镜像卷 Thinly-Provision\t精简配置逻辑卷 Snapshot\t快照卷 Thinly-Provisioned Snapshot\t精简配置快照卷 Cache\t缓存卷\n创建PV时（一同被创建的有） 1：接近设备起始处，放置一个标签，包括uuid，元数据的位置　#(这个标签每个磁盘默认都保持一份) 2：lvm元数据，包含lvm卷组的配置详情 3：剩余空间，用于存储数据\nlvm逻辑卷概念 及　创建lvm的步骤 LVM的组成 PE：（物理拓展，是VG卷组的基本组成单位） PV：（物理卷） VG：（卷组） LV：（逻辑卷） 创建lvm的步骤 1：将磁盘创建为PV（物理卷），其实物理磁盘被条带化为PV，划成了一个一个的PE，默认每个PE大小是4MB 2：创建VG（卷组），其实它是一个空间池，不同PV加入同一VG 3：创建LV（逻辑卷），组成LV的PE可能来自不同的物理磁盘 4：格式化LV，挂载使用 lvm相关命令工具 pv操作命令 pvchange\t更改物理卷的属性 pvck\t检查物理卷元数据 pvcreate\t初始化磁盘或分区以供lvm使用 pvdisplay\t显示物理卷的属性 pvmove\t移动物理Exent pvremove\t删除物理卷 pvresize\t调整lvm2使用的磁盘或分区的大小 pvs\t报告有关物理卷的信息 pvscan\t扫描物理卷的所有磁盘 vg操作命令 vgcfgbackup\t备份卷组描述符区域 vgcfgrestore\t恢复卷组描述符区域 vgchange\t更改卷组的属性 vgck\t检查卷组元数据 vgconvert\t转换卷组元数据格式 vgcreate\t创建卷组 vgdisplay\t显示卷组的属性 vgexport\t使卷组对系统不了解（这是个什么） vgextend\t将物理卷添加到卷组 vgimportclone\t导入并重命名重复的卷组（例如硬件快照） vgmerge\t合并两个卷组 vgmknodes\t重新创建卷组目录和逻辑卷特殊文件 vgreduce\t通过删除一个或多个物理卷来减少卷组（将物理卷踢出VG） vgremove\t删除卷组 vgrename\t重命名卷组 vgs\t报告有关卷组信息 vgscan\t扫描卷组的所有磁盘并重建高速缓存 vgsplit\t将卷组拆分为两个，通过移动整个物理卷将任何逻辑卷从一个卷组移动到另一个卷组 lv操作命令 lvchange\t更改逻辑卷属性 lvconvert\t将逻辑卷从线性转换为镜像或快照 lvcreate\t将现有卷组中创建逻辑卷 lvdisplay\t显示逻辑卷的属性 lvextend\t扩展逻辑卷的大小 lvmconfig\t在加载lvm.conf和任何其他配置文件后显示配置信息 lvmdiskscan\t扫描lvm2可见的所有设备 lvmdump\t创建lvm2信息转储以用于诊断目的 lvreduce\t减少逻辑卷的大小 lvremove\t删除逻辑卷 lvrename\t重命名逻辑卷 lvresize\t调整逻辑卷大小 lvs\t报告有关逻辑卷的信息 lvscan\t扫描所有的逻辑卷 PV管理 制作PV pvcreate /dev/sdb1\n删除pv撤销PV（需先踢出vg） pvremove /dev/sdb1\nVG管理 制作VG vgcreate datavg /dev/sdb1 vgcreate datavg /dev/sdb1 /dev/sdb2 #解释：vgcreate vg名 分区\nvgcreate -s 16M datavg2 /dev/sdb3 #解释：-s 指定pe的大小为16M，默认不指定是4M\n从卷组中移除缺失的磁盘 vgreduce \u0026ndash;removemissing datavg vgreduce \u0026ndash;removemissing datavg \u0026ndash;force\t#强制移除\n扩展VG空间 vgextend datavg /dev/sdb3 pvs\n踢出vg中的某个成员 vgreduce datavg /dev/sdb3 vgs\nLV管理 制作LV lvcreate -n lvdata1 -L 1.5G datavg #解释：-n lv的name，-L 指定lv的大小，datavg 是vg的名字，表示从那个vg\n激活修复后的逻辑卷 lvchange -ay /dev/datavg/lvdata1 lvchange -ay /dev/datavg/lvdata1 -K\t#强制激活\nLVM的快照 用途：注意用途是数据一致性备份，先做一个快照，冻结当前系统，这样快照里面的内容可暂时保持不变，系统本身继续运行，通过重新挂载备份快照卷，实现不中断服务备份。\nlvcreate -s -n kuaizhao01 -L 100M /dev/datavg/lvdata1\n查看，删除使用方法 1：查看物理卷信息 pvs,pvdisplay\n2：查看卷组信息 vgs,vgdisplay\n3：查看逻辑卷信息 lvs,lvdisplay\n4：删除LV lvremove /dev/mapper/VG-mylv\n5：删除VG vgremove VG\n6：删除PV（注意删除顺序是LV，VG，PV） pvremove /dev/sdb\nvg卷组改名 vgrename xxxx-vgid-xxxx-xxxx xinname 细述LVM基本特性及日常管理细述LVM基本特性及日常管理\n拉伸一个逻辑卷LV 1:用vgdisplay查看vg还有多少空余空间 2:扩充逻辑卷 lvextend -L +1G /dev/VG/LV01 lvextend -L +1G /dev/VG/LV01 -r #这个命令表示在扩展的同时也更新文件系统，但是不是所有的发行版本都支持，部分文件系统不支持在线扩展的除外 3:进行扩充操作后，df -h你会发现大小并没有变 4:更新文件系统（争对不同的文件系统，其更新的命令也不一样） e2fsck -f /dev/datavg/lvdata1\t#ext4文件系统，检查lv的文件系统 resize2fs /dev/VG/LV01\t#ext4文件系统命令，该命令后面接lv的设备名就行\nxfs_growfs /nas\t#xfs文件系统，该命令后面直接跟的是挂载点 当更新文件系统后，你就会发现，df -h正常了\n缩小逻辑卷LV（必须离线，umount） 1：卸载\n2：缩小文件系统 resize2fs /dev/VG/LV01 2G\n3：缩小LV lvreduce -L -1G /dev/VG/LV01\n4：查看lvs，挂载使用\n拉伸一个卷组VG 1:新插入一块硬盘，若不是热插拔的磁盘，可以试试这个在系统上强制刷新硬盘接口 for i in /sys/class/scsi_host/*; do echo \u0026ldquo;- - -\u0026rdquo; \u0026gt; $i/scan; done\n2:将/dev/sdd条带化，格式化为PE pvcreate /dev/sdd\n3:将一块新的PV加入到现有的VG中 vgextend VG /dev/sdd\n4:查看大小 vgs\n缩小卷组VG（注意不要有PE在占用） 1：将一个PV从指定卷中移除 vgreduce VG /dev/sdd\n2：查看缩小后的卷组大小\n将磁盘加入和踢出VG 将sdd1踢出datavg组里 vgreduce datavg /dev/sdd1\n将sdb1加入datavg组里 vgextend datavg /dev/sdb1\nlvm灾难恢复场景案例 场景再现： 三块盘做lvm,现在有一块物理坏了，将剩下两块放到其他linux服务器上\n恢复步骤 第一，查看磁盘信息，lvm信息，确认能查到lvm相关信息，找到VG组的名字（pvs,lvs,vgs,fidsk,blkid） 第二：删除lvm信息中损坏的磁盘角色，（强制提出故障磁盘）\u0026ldquo;vgreduce \u0026ndash;removemissing VG_name \u0026quot; 第三：强制激活VG组 \u0026ldquo;vgchange -ay\u0026rdquo; 第四：强制激活LVM \u0026ldquo;lvchange -ay /dev/VG_name\u0026rdquo; 第五：挂载\n","permalink":"https://www.lvbibir.cn/en/posts/tech/lvm-characteristic-management/","summary":"LVM基本特性：（可以通过插件CLVM，实现群集逻辑卷管理） PV物理卷 LV逻辑卷（逻辑卷管理：会在物理存储上生成抽象层，以便创建逻辑存储卷，方便设备命名）（下面是逻辑卷的分类） Linear 线性卷(这是默认的lvm形式，即按顺序占用磁盘，一块写完了再写另一块) Stripe 条带逻辑卷 RAID raid逻辑卷 Mirror 镜","title":"lvm基本特性及日常管理"},{"content":"前言 查看硬件信息，并将信息整合成json数值，然后传给前段进行分析，最后再进行相应的处理。在装系统的时候，或是进行监控时，都是一个标准的自动化运维流程。使用shell直接生成好json数据再进行传输，会变得非常方便。\n环境 [root@sys-idc-pxe01 ~]# yum install jq lsscsi MegaCli 脚本内容 #!/bin/sh #description: get server hardware info #author: lvbibir #date: 20180122 #需要安装jq工具 yum install jq #用于存放该服务器的所有信息，个人喜欢把全局变量写到外面 #写到函数里面，没有加local的变量也是全局变量 INFO=\u0026#34;{}\u0026#34; #定义一个工具函数，用于生成json数值，后面将会频繁用到 function create_json() { #utility function local key=$1 local value=\u0026#34;$2\u0026#34; local json=\u0026#34;\u0026#34; #if value is string if [ -z \u0026#34;$(echo \u0026#34;$value\u0026#34; |egrep \u0026#34;\\[|\\]|\\{|\\}\u0026#34;)\u0026#34; ] then json=$(jq -n {\u0026#34;$key\u0026#34;:\u0026#34;\\\u0026#34;$value\\\u0026#34;\u0026#34;}) #if value is json, object elif [ \u0026#34;$(echo \u0026#34;$value\u0026#34; |jq -r type)\u0026#34; == \u0026#34;object\u0026#34; ] then json=$(jq -n {\u0026#34;$key\u0026#34;:\u0026#34;$value\u0026#34;}) #if value is array elif [ \u0026#34;$(echo \u0026#34;$value\u0026#34; |jq -r type)\u0026#34; == \u0026#34;array\u0026#34; ] then json=$(jq -n \u0026#34;{$key:$value}\u0026#34;) else echo \u0026#34;value type error...\u0026#34; exit 1 return 0 fi echo $json return 0 } #获取CPU信息 function get_cpu() { #获取cpu信息，去掉空格和制表符和空行，以便于for循环 local cpu_model_1=$(dmidecode -s processor-version |grep \u0026#39;@\u0026#39; |tr -d \u0026#34; \u0026#34; |tr -s \u0026#34;\\n\u0026#34; |tr -d \u0026#34;\\t\u0026#34;) local cpu_info=\u0026#34;{}\u0026#34; local i=0 #因为去掉了空格和制表符，以下默认使用换行符分隔 for line in $(echo \u0026#34;$cpu_model_1\u0026#34;) do local cpu_model=\u0026#34;$line\u0026#34; local cpu1=$(create_json \u0026#34;cpu_model\u0026#34; \u0026#34;$cpu_model\u0026#34;) #获取每块cpu的信息，这里只记录了型号 local cpu=$(create_json \u0026#34;cpu_$i\u0026#34; \u0026#34;$cpu1\u0026#34;) local cpu_info=$(jq -n \u0026#34;$cpu_info + $cpu\u0026#34;) i=$[ $i + 1] done #将cpu的信息整合成一个json，key是cpu local info=$(create_json \u0026#34;cpu\u0026#34; \u0026#34;$cpu_info\u0026#34;) #将信息加入到全局变量中 INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_mem() { #generate json {Locator:{sn:sn,size:size}} local mem_info=\u0026#34;{}\u0026#34; #获取每个内存的信息，包括Size:|Locator:|Serial Number: local mem_info_1=$(dmidecode -t memory |egrep \u0026#39;Size:|Locator:|Serial Number:\u0026#39; |grep -v \u0026#39;Bank Locator:\u0026#39; |awk \u0026#39; { if (NR%3==1 \u0026amp;\u0026amp; $NF==\u0026#34;MB\u0026#34;) { size=$2; getline (NR+1); locator=$2; getline (NR+2); sn=$NF; printf(\u0026#34;%s,%s,%s\\n\u0026#34;,locator,size,sn) } }\u0026#39;) #根据上面的信息，将信息过滤并整合成json local i=0 for line in $(echo \u0026#34;$mem_info_1\u0026#34;) do local locator=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39;) local sn=$(echo $line |awk -F , \u0026#39;{print $3}\u0026#39;) local size=$(echo $line |awk -F , \u0026#39;{print $2}\u0026#39;) local mem1=$(create_json \u0026#34;locator\u0026#34; \u0026#34;$locator\u0026#34;) local mem2=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local mem3=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local mem4=$(jq -n \u0026#34;$mem1 + $mem2 + $mem3\u0026#34;) #每条内存的信息，key是内存从0开始的序号 local mem=$(create_json \u0026#34;mem_$i\u0026#34; \u0026#34;$mem4\u0026#34;) #将这些内存的信息组合到一个json中 mem_info=$(jq -n \u0026#34;$mem_info + $mem\u0026#34;) i=$[ $i + 1 ] done #给这些内存的信息设置key，mem local info=$(create_json \u0026#34;mem\u0026#34; \u0026#34;$mem_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_megacli_disk() { #设置megacli工具的路径，此条可以根据情况更改 local raid_tool=\u0026#34;/opt/MegaRAID/MegaCli/MegaCli64\u0026#34; #将硬盘信息获取，保存下来，省去每次都执行的操作 $raid_tool pdlist aall \u0026gt;/tmp/megacli_pdlist.txt local disk_info=\u0026#34;{}\u0026#34; #获取硬盘的必要信息 local disk_info_1=$(cat /tmp/megacli_pdlist.txt |egrep \u0026#39;Enclosure Device ID:|Slot Number:|PD Type:|Raw Size:|Inquiry Data:|Media Type:\u0026#39;|awk \u0026#39; { if(NR%6==1 \u0026amp;\u0026amp; $1$2==\u0026#34;EnclosureDevice\u0026#34;) { E=$NF; getline (NR+1); S=$NF; getline (NR+2); pdtype=$NF; getline (NR+3); size=$3$4; getline (NR+4); sn=$3$4$5$6; getline (NR+5); mediatype=$3$4$5$6; printf(\u0026#34;%s,%s,%s,%s,%s,%s\\n\u0026#34;,E,S,pdtype,size,sn,mediatype) } }\u0026#39;) #将获取到的硬盘信息进行整合，生成json local i=0 for line in $(echo $disk_info_1) do #local key=$(echo $line |awk -F , \u0026#39;{printf(\u0026#34;ES%s_%s\\n\u0026#34;,$1,$2)}\u0026#39;) local E=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39;) local S=$(echo $line |awk -F , \u0026#39;{print $2}\u0026#39;) local pdtype=$(echo $line |awk -F , \u0026#39;{print $3}\u0026#39;) local size=$(echo $line |awk -F , \u0026#39;{print $4}\u0026#39;) local sn=$(echo $line |awk -F , \u0026#39;{print $5}\u0026#39;) local mediatype=$(echo $line |awk -F , \u0026#39;{print $6}\u0026#39;) local disk1=$(create_json \u0026#34;pdtype\u0026#34; \u0026#34;$pdtype\u0026#34;) local disk1_1=$(create_json \u0026#34;enclosure_id\u0026#34; \u0026#34;$E\u0026#34;) local disk1_2=$(create_json \u0026#34;slot_id\u0026#34; \u0026#34;$S\u0026#34;) local disk2=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local disk3=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local disk4=$(create_json \u0026#34;mediatype\u0026#34; \u0026#34;$mediatype\u0026#34;) local disk5=$(jq -n \u0026#34;$disk1 + $disk1_1 + $disk1_2 + $disk2 + $disk3 + $disk4\u0026#34;) local disk=$(create_json \u0026#34;disk_$i\u0026#34; \u0026#34;$disk5\u0026#34;) disk_info=$(jq -n \u0026#34;$disk_info + $disk\u0026#34;) i=$[ $i + 1 ] done #echo $disk_info local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;$disk_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_hba_disk() { #对于hba卡的硬盘，使用smartctl获取硬盘信息 local disk_tool=\u0026#34;smartctl\u0026#34; local disk_info=\u0026#34;{}\u0026#34; #lsscsi 需要使用yum install lsscsi 安装 local disk_info_1=$(lsscsi -g |grep -v \u0026#39;enclosu\u0026#39; |awk \u0026#39;{printf(\u0026#34;%s,%s,%s,%s\\n\u0026#34;,$1,$2,$(NF-1),$NF)}\u0026#39;) local i=0 for line in $(echo $disk_info_1) do local E=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39; |awk -F \u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) local S=$(echo $line |awk -F , \u0026#39;{print $NF}\u0026#39; |egrep -o [0-9]*) local sd=$(echo $line |awk -F , \u0026#39;{print $(NF-1)}\u0026#39;) $disk_tool -i $sd \u0026gt;/tmp/disk_info.txt local pdtype=\u0026#34;SATA\u0026#34; if [ \u0026#34;$(cat /tmp/disk_info.txt |grep \u0026#39;Transport protocol:\u0026#39; |awk \u0026#39;{print $NF}\u0026#39;)\u0026#34; == \u0026#34;SAS\u0026#34; ] then local pdtype=\u0026#34;SAS\u0026#34; fi local size=$(cat /tmp/disk_info.txt |grep \u0026#39;User Capacity:\u0026#39; |awk \u0026#39;{printf(\u0026#34;%s%s\\n\u0026#34;,$(NF-1),$NF)}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) local sn=$(cat /tmp/disk_info.txt |grep \u0026#39;Serial Number:\u0026#39; |awk \u0026#39;{print $NF}\u0026#39;) local mediatype=\u0026#34;disk\u0026#34; local disk1=$(create_json \u0026#34;pdtype\u0026#34; \u0026#34;$pdtype\u0026#34;) local disk1_1=$(create_json \u0026#34;enclosure_id\u0026#34; \u0026#34;$E\u0026#34;) local disk1_2=$(create_json \u0026#34;slot_id\u0026#34; \u0026#34;$S\u0026#34;) local disk2=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local disk3=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local disk4=$(create_json \u0026#34;mediatype\u0026#34; \u0026#34;$mediatype\u0026#34;) local disk5=$(jq -n \u0026#34;$disk1 + $disk1_1 + $disk1_2 + $disk2 + $disk3 + $disk4\u0026#34;) local disk=$(create_json \u0026#34;disk_$i\u0026#34; \u0026#34;$disk5\u0026#34;) disk_info=$(jq -n \u0026#34;$disk_info + $disk\u0026#34;) i=$[ $i + 1 ] done #echo $disk_info local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;$disk_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_disk() { #根据获取到的硬盘控制器类型，来判断使用什么工具采集硬盘信息 if [ \u0026#34;$(echo \u0026#34;$INFO\u0026#34; |jq -r .disk_ctrl.disk_ctrl_0.type)\u0026#34; == \u0026#34;raid\u0026#34; ] then get_megacli_disk elif [ \u0026#34;$(echo \u0026#34;$INFO\u0026#34; |jq -r .disk_ctrl.disk_ctrl_0.type)\u0026#34; == \u0026#34;hba\u0026#34; ] then get_hba_disk else local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;error\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) fi #hp机器比较特殊，这里我没有做hp机器硬盘信息采集，有兴趣的朋友可以自行添加上 #if hp machine return 0 } function get_diskController() { local disk_ctrl=\u0026#34;{}\u0026#34; #if LSI Controller local disk_ctrl_1=\u0026#34;$(lspci -nn |grep LSI)\u0026#34; local i=0 #以换行符分隔 IFS_OLD=$IFS \u0026amp;\u0026amp; IFS=$\u0026#39;\\n\u0026#39; for line in $(echo \u0026#34;$disk_ctrl_1\u0026#34;) do #echo $line local ctrl_id=$(echo \u0026#34;$line\u0026#34; |awk -F \u0026#39;]:\u0026#39; \u0026#39;{print $1}\u0026#39; |awk \u0026#39;{print $NF}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) case \u0026#34;$ctrl_id\u0026#34; in #根据控制器的id或进行判断是raid卡还是hba卡，因为品牌比较多，后续可以在此处进行扩展添加 0104) # 获取Logic以后的字符串，并进行拼接 local ctrl_name=$(echo \u0026#34;${line##*\u0026#34;Logic\u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s\\n\u0026#34;,$1,$2,$3)}\u0026#39;) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$ctrl_id\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;raid\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;$ctrl_name\u0026#34;) ;; 0100|0107) local ctrl_name=$(echo \u0026#34;${line##*\u0026#34;Logic\u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s\\n\u0026#34;,$1,$3,$4)}\u0026#39;) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$ctrl_id\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;hba\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;$ctrl_name\u0026#34;) ;; *) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;----\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;----\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;----\u0026#34;) ;; esac local ctrl_tmp=$(jq -n \u0026#34;$ctrl1 + $ctrl2 + $ctrl3\u0026#34;) local ctrl=$(create_json \u0026#34;disk_ctrl_$i\u0026#34; \u0026#34;$ctrl_tmp\u0026#34;) disk_ctrl=$(jq -n \u0026#34;$disk_ctrl + $ctrl\u0026#34;) i=$[ $i + 1 ] done IFS=$IFS_OLD local info=$(create_json \u0026#34;disk_ctrl\u0026#34; \u0026#34;$disk_ctrl\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_netcard() { local netcard_info=\u0026#34;{}\u0026#34; local netcard_info_1=\u0026#34;$(lspci -nn |grep Ether)\u0026#34; local i=0 #echo \u0026#34;$netcard_info_1\u0026#34; IFS_OLD=$IFS \u0026amp;\u0026amp; IFS=$\u0026#39;\\n\u0026#39; for line in $(echo \u0026#34;$netcard_info_1\u0026#34;) do local net_id=$(echo $line |egrep -o \u0026#39;[0-9a-z]{4}:[0-9a-z]{4}\u0026#39;) local net_id_1=$(echo $net_id |awk -F : \u0026#39;{print $1}\u0026#39;) case \u0026#34;$net_id_1\u0026#34; in 8086) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; 14e4) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; *) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; esac local net1=$(jq -n \u0026#34;$net1 + $net2 + $net3\u0026#34;) #echo $net local net2=$(create_json \u0026#34;net_$i\u0026#34; \u0026#34;$net1\u0026#34;) netcard_info=$(jq -n \u0026#34;$netcard_info + $net2\u0026#34;) i=$[ $i + 1 ] done IFS=$IFS_OLD local info=$(create_json \u0026#34;net\u0026#34; \u0026#34;$netcard_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_server() { local product=$(dmidecode -s system-product-name |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) local manufacturer=$(dmidecode -s system-manufacturer |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) local server1=$(create_json \u0026#34;manufacturer\u0026#34; \u0026#34;$manufacturer\u0026#34;) local server2=$(create_json \u0026#34;product\u0026#34; \u0026#34;$product\u0026#34;) local server3=$(jq -n \u0026#34;$server1 + $server2\u0026#34;) local info=$(create_json \u0026#34;basic_info\u0026#34; \u0026#34;$server3\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } ALL_INFO=\u0026#34;\u0026#34; function get_all() { #因为硬盘信息的获取依赖硬盘控制器的信息，所以get_diskController要放到get_disk前面 get_server get_cpu get_mem get_diskController get_disk get_netcard local sn=$(dmidecode -s system-serial-number |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) ALL_INFO=$(create_json \u0026#34;$sn\u0026#34; \u0026#34;$INFO\u0026#34;) return 0 } function main() { get_all echo $ALL_INFO return 0 } #------------------------------------------------- main ","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-get-server-hardware-information/","summary":"前言 查看硬件信息，并将信息整合成json数值，然后传给前段进行分析，最后再进行相应的处理。在装系统的时候，或是进行监控时，都是一个标准的自动化运维流程。使用shell直接生成好json数据再进行传输，会变得非常方便。 环境 [root@sys-idc-pxe01 ~]# yum install jq lsscsi MegaCli 脚本内容 #!/bin/sh #description: get server hardware info #author: lvbibir #date: 20180122 #需要安装jq工具","title":"shell | 获取服务器硬件信息（整合为json格式）"}]