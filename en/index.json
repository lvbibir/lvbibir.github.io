[{"content":"0 前言 本文内容比较杂乱, 无法保证实时更新, 如果遇到问题, 可以在 github 查看最新的配置:\nhugo 相关配置 docker 相关配置 研究 hugo 建站之初是打算采用 Github Pages 来发布静态博客\n优点 仅需一个 github 账号和简单配置即可将静态博客发布到 github pages 没有维护的时间成本, 可以将精力更多的放到博客内容本身上去 无需备案 无需 ssl 证书 缺点 访问速度较慢 访问速度较慢 访问速度较慢 虽说访问速度较慢可以通过各家的 cdn 加速来解决, 但由于刚开始建立 blog 选择的是 wordpress, 域名, 服务器, 备案, 证书等都已经一应俱全, 且之前的架构采用 docker, 添加一台 nginx 来跑 hugo 的静态网站是很方便的\n1 将博客部署到阿里云 整个步骤最难的地方可能就是 docker-compose 和 nginx 的配置了, 如果之前没有接触过可能会比较吃力, 因此我打包了一份开袋即食的配置文件, 只需要修改一些必要配置, 点此链接下载\n下载完将压缩包上传到自己的服务器, 解压后重命名为 blog (当然你可以用其他名字)\n下面正式开始部署:\n确保服务器公网 ip、安全组权限 (80/443), 域名绑定, ssl 证书等基础配置已经一应俱全 确保服务器安装了 docker 和 docker-compose 修改 blog/conf/nginx-hugo/nginx.conf 和 blog/conf/nginx-proxy/default.conf, 需要修改的地方在文件中已经标注出来了 将你的 ssl 证书放到 blog/ssl/ 目录下 在 blog 目录下执行 docker-compose up -d 即可启动容器 配置 twikoo 的前端代码, 见本文章节 3.2 将 hugo 生成的静态文件上传到 blog/data/hugo/ 目录, 见本文章节 2 至此已经配置完成, 应该可以通过域名访问 hugo 站点了, 后续更新内容只需要重复最后一步, 将 hugo 生成的静态文件上传到服务器即可\n所有的配置、应用数据、日志都保存在 blog 目录下, 你可以在不同的服务器上快速迁移 hugo 环境, 无需担心后续想要迁移新服务器时遇到的各种问题\n2 workflow 在这里简单介绍一下我从写博客 -\u0026gt; 发布到服务器 -\u0026gt; 归档备份的整个流程\n总体流程:\nobsidian 编辑文章, 图片通过 Image Auto Upload Plugin 插件配合 piclist 上传到阿里云 OSS, 具体配置和操作见 docker 部署 piclist 编辑完成后将通过 此脚本 将编辑后的文章更新到 hugo site 目录, 同时也是 git 仓库 使用 hugo server -D 预览变更, 如有问题重复前两个步骤 确认无误后通过 此脚本 生成静态文件, 并将文件远程同步到公网服务器, 完成博客内容变更 最后将 git 仓库的变更提交后同步到 github 远程仓库, 完成归档备份 其实如果使用 vscode 直接编辑 git 仓库中的博客文章可以让整个流程更加简化, 但是 vscode 的 markdown 编辑体验实在是比不上 typora 或者 obsidian, 工欲善其事必先利其器, 有了好的编辑体验才更愿意输出内容\n3 twikoo 3.1 部署 twikoo 官方提供了 丰富的部署方式, 考虑到访问速度, 本文使用的是 docker 方式部署到阿里云服务器\n如果是使用本文章节 1 步骤中的配置文件部署了 twikoo, 这步直接忽略, 配置前端代码即可\ndocker run --name twikoo -e TWIKOO_THROTTLE=1000 -p 8080:8080 -v ${PWD}/data:/app/data -d imaegoo/twikoo 部署完成后看到如下结果即成功\n[root@lvbibir ~]# curl http://localhost:8080 {\u0026#34;code\u0026#34;:100, \u0026#34;message\u0026#34;:\u0026#34;Twikoo 云函数运行正常, 请参考 https://twikoo.js.org/quick-start.html#%E5%89%8D%E7%AB%AF%E9%83%A8%E7%BD%B2 完成前端的配置\u0026#34;, \u0026#34;version\u0026#34;:\u0026#34;1.6.7\u0026#34;} 后续最好套上反向代理, 加上域名和证书\n3.2 前端代码 创建或者修改 layouts\\partials\\comments.html\n\u0026lt;!-- Twikoo --\u0026gt; \u0026lt;div\u0026gt; \u0026lt;div class=\u0026#34;pagination__title\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;pagination__title-h\u0026#34; style=\u0026#34;font-size: 20px;\u0026#34;\u0026gt;💬评论\u0026lt;/span\u0026gt; \u0026lt;hr /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;tcomment\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.staticfile.org/twikoo/{{ .Site.Params.twikoo.version }}/twikoo.all.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; twikoo.init({ envId: \u0026#34;\u0026#34;, //填自己的, 例如：https://example.com el: \u0026#34;#tcomment\u0026#34;, lang: \u0026#39;zh-CN\u0026#39;, path: window.TWIKOO_MAGIC_PATH||window.location.pathname, }); \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; 调用上述 twikoo 代码的位置：layouts/_default/single.html\n\u0026lt;article class=\u0026#34;post-single\u0026#34;\u0026gt; // 其他代码...... {{- if (.Param \u0026#34;comments\u0026#34;) }} {{- partial \u0026#34;comments.html\u0026#34; . }} {{- end }} \u0026lt;/article\u0026gt; 在站点配置文件 config 中加上版本号\nparams: twikoo: version: 1.6.7 3.3 更新 修改 dockerfile.yml 中的镜像 tag 部署新版本容器 docker-compose up -d 在 hugo 配置文件 config.yml 中修改 twikoo 版本 3.4 修改数据 直接修改 blog/data/twikoo/ 目录下的文件后重启容器, ❗慎重修改\n3.5 修改 smms 图床的 api 地址 已于 1.6.12 新版本修复, https://github.com/imaegoo/twikoo/releases/tag/1.6.12\n由于 sm.ms 域名国内无法访问, twikoo 官方还没有出具体的修改方式, 自己修改容器配置文件进行修改\n# 复制配置文件 [root@lvbibir blog]# docker cp twikoo:/app/node_modules/twikoo-func/utils/image.js /root/blog/conf/twikoo/ # 修改配置文件, 原来的配置是 https://sm.ms/api.v2/upload [root@lvbibir blog]# grep smms conf/twikoo/image.js } else if (config.IMAGE_CDN === \u0026#39;smms\u0026#39;) { const uploadResult = await axios.post(\u0026#39;https://smms.app/api/v2/upload\u0026#39;, formData, { # 将配置文件映射进容器内, 重启容器即可 [root@lvbibir blog]# grep twikoo docker-compose.yml twikoo: image: imaegoo/twikoo container_name: twikoo - $PWD/data/twikoo:/app/data - $PWD/conf/twikoo/image.js:/app/node_modules/twikoo-func/utils/image.js 4 Artitalk 官方文档\n需要注意的是如果使用的是国际版的 LeadCloud, 需要绑定自定义域名后才能正常访问\n4.1 leancloud 配置 前往 LeanCloud 国际版, 注册账号 注册完成之后根据 LeanCloud 的提示绑定手机号和邮箱 绑定完成之后点击 创建应用, 应用名称随意, 接着在 结构化数据 中创建 class, 命名为 shuoshuo 在你新建的应用中找到 结构化数据 下的 用户 点击 添加用户, 输入想用的用户名及密码 回到 结构化数据 中, 点击 class 下的 shuoshuo 找到权限, 在 Class 访问权限 中将 add_fields 以及 create 权限设置为指定用户, 输入你刚才输入的用户名会自动匹配为了安全起见, 将 delete 和 update 也设置为跟它们一样的权限 然后新建一个名为 atComment 的 class, 权限什么的使用默认的即可 点击 class 下的 _User 添加列, 列名称为 img, 默认值填上你这个账号想要用的发布说说的头像 url, 这一项不进行配置, 说说头像会显示为默认头像 —— Artitalk 的 logo 在最菜单栏中找到设置 -\u0026gt; 应用 keys, 记下来 AppID 和 AppKey , 一会会用 最后将 _User 中的权限全部调为指定用户, 或者数据创建者, 为了保证不被篡改用户数据以达到强制发布说说 在设置 -\u0026gt;域名绑定中绑定自定义域名 ❗ 关于设置权限的这几步 这几步一定要设置好, 才可以保证不被 “闲人” 破解发布说说的验证\n4.2 hugo 配置 新增 content/talk.md 页面, 内容如下, 注意修改标注的内容, front-matter 的内容自行修改\n--- title: \u0026#34;💬 说说\u0026#34; date: 2021-08-31 hidemeta: true description: \u0026#34;胡言乱语\u0026#34; comments: true reward: false showToc: false TocOpen: false showbreadcrumbs: false --- \u0026lt;body\u0026gt; \u0026lt;!-- 引用 artitalk --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://unpkg.com/artitalk\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 存放说说的容器 --\u0026gt; \u0026lt;div id=\u0026#34;artitalk_main\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; new Artitalk({ appId: \u0026#39;**********\u0026#39;, // Your LeanCloud appId appKey: \u0026#39;************\u0026#39;, // Your LeanCloud appKey serverURL: \u0026#39;*********\u0026#39; // 绑定的自定义域名 }) \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; 这个时候已经可以直接访问了, https://your.domain.com/talk\n输入 leancloud 配置步骤中的第 4 步配置的用户名密码登录后就可以发布说说了\n5 自定义 footer 自定义页脚内容\n添加完下面的页脚内容后要修改 assets\\css\\extended\\blank.css 中的 --footer-height 的大小, 具体数字需要考虑到行数和字体大小\n5.1 自定义徽标 徽标功能源自：https://shields.io/ 考虑到访问速度, 可以在生成完徽标后放到自己的 cdn 上\n在 layouts\\partials\\footer.html 中的 \u0026lt;footer\u0026gt; 添加如下\n\u0026lt;a href=\u0026#34;https://gohugo.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://img.shields.io/static/v1?\u0026amp;style=plastic\u0026amp;color=308fb5\u0026amp;label=Power by\u0026amp;message=hugo\u0026amp;logo=hugo\u0026#34; style=\u0026#34;display: unset;\u0026#34;\u0026gt; \u0026lt;/a\u0026gt; 5.2 网站运行时间 在 layouts\\partials\\footer.html 中的 \u0026lt;footer\u0026gt; 添加如下\n起始时间自行修改\n\u0026lt;span id=\u0026#34;runtime_span\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt;function show_runtime(){window.setTimeout(\u0026#34;show_runtime()\u0026#34;, 1000);X=new Date(\u0026#34;7/13/2021 1:00:00\u0026#34;);Y=new Date();T=(Y.getTime()-X.getTime());M=24*60*60*1000;a=T/M;A=Math.floor(a);b=(a-A)*24;B=Math.floor(b);c=(b-B)*60;C=Math.floor((b-B)*60);D=Math.floor((c-C)*60);runtime_span.innerHTML=\u0026#34;网站已运行\u0026#34;+A+\u0026#34;天\u0026#34;+B+\u0026#34;小时\u0026#34;+C+\u0026#34;分\u0026#34;+D+\u0026#34;秒\u0026#34;}show_runtime();\u0026lt;/script\u0026gt; 5.3 访问人数统计 统计功能源自：http://busuanzi.ibruce.info/\n在 layouts\\partials\\footer.html 中的 \u0026lt;footer\u0026gt; 添加如下\n\u0026lt;script async src=\u0026#34;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;span id=\u0026#34;busuanzi_container\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css\u0026#34;\u0026gt; 总访客数: \u0026lt;i class=\u0026#34;fa fa-user\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;span id=\u0026#34;busuanzi_value_site_uv\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; | 总访问量: \u0026lt;i class=\u0026#34;fa fa-eye\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;span id=\u0026#34;busuanzi_value_site_pv\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; | 本页访问量: \u0026lt;i class=\u0026#34;fa fa-eye\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;span id=\u0026#34;busuanzi_value_page_pv\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt; 6 自定义字体 可以使用一些在线的字体, 可能会比较慢, 推荐下载想要的字体放到自己的服务器或者 cdn 上\n修改 assets\\css\\extended\\fonts.css, 添加 @font-face\n@font-face { font-family: \u0026#34;LXGWWenKaiLite-Bold\u0026#34;; src: url(\u0026#34;https://your.domain.com/fonts/test.woff2\u0026#34;) format(\u0026#34;woff2\u0026#34;); font-display: swap; } 修改 assets\\css\\extended\\blank.css, 推荐将英文字体放在前面, 可以实现英文和中文使用不同字体\n.post-content { font-family: Consolas, \u0026#34;LXGWWenKaiLite-Bold\u0026#34;; //修改 } body { font-family: Consolas, \u0026#34;LXGWWenKaiLite-Bold\u0026#34;; //修改 } 7 修改链接颜色 在 hugo+papermod 默认配置下, 链接颜色是黑色字体带下划线的组合, 个人非常喜欢 typora-vue 的渲染风格 hugo官方文档 给出了通过 render hooks 覆盖默认的 markdown 渲染 link 的方式\n新建 layouts/_default/_markup/render-link.html 文件, 内容如下在官方给出的示例中添加了 style=\u0026quot;color:#42b983, 颜色可以自行修改\n\u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34;{{ with .Title}} title=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ if strings.HasPrefix .Destination \u0026#34;http\u0026#34; }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; style=\u0026#34;color:#42b983\u0026#34;;{{ end }}\u0026gt;{{ .Text | safeHTML }}\u0026lt;/a\u0026gt; 8 shortcode ppt、bilibili、youtube、豆瓣阅读和电影卡片\nmermaid\n图片画廊\n9 其他修改 其他 css 样式修改基本都是通过 f12 控制台一点点摸索改的, 不太规范且比较琐碎就不单独记录了, 其实我根本已经忘记还改了哪些东西\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/hello-hugo/","summary":"0 前言 本文内容比较杂乱, 无法保证实时更新, 如果遇到问题, 可以在 github 查看最新的配置: hugo 相关配置 docker 相关配置 研究 hugo 建站之初是打算采用 Github Pages 来发布静态博客 优点 仅需一个 github 账号和简单配置即可将静态博客发布到 github pages 没有维护的时间成本, 可以将精力更多的放到博客内容本身上去 无需备案 无需 ssl 证书 缺点 访问速度","title":"【置顶】Hello, hugo!"},{"content":" 在 20 岁到 30 岁这十年的过程中, 我们都走过一样的路. 你觉得孤独就对了, 那是让你认识自己的机会. 你觉得不被理解就对了, 那是让你认清朋友的机会. 你觉得黑暗就对了, 那样你才分辨得出什么是你的光芒. 你觉得无助就对了, 那样你才能明白谁是你成长中能扶你一把的人. 你觉得迷茫就对了, 谁的青春不迷茫.\n这个假期见了很多同学和老友, 喝了很多酒. 以前我也想, 等到毕业一年, 三年, 五年再见, 但其实过程中很多人就断了联系. 所由现在能见到的朋友都是见一次少一次, 你甚至不知道下一次再见的时间, 所有少年相约的承诺都只是当下的安慰. 你总有一天会明白: 有些人, 有些事, 一时错过, 就是一世.\n我们心里永远有一个人, 不是他, 也会有别人. 刚开始我们都会折磨自己, 埋怨自己, 恨自己为什么一直放不下这个人. 后来才知道, 不是你放不下谁, 而是你放不下想念对方的那个自己.\n很多事情, 只要能做到心甘情愿, 一切都理所当然.\n每堂课 45 分钟, 如果放到现在, 每一分每一秒我都尽力记住老师说的每句话, 隔壁周围的每张脸吧.\n看上了就追, 相中了就买, 绝不再做后悔的事情. 现在有人问我, 为什么你总是那么激动, 那么草率地做决定? 看到这篇日志, 我才想起来, 原来早在那么多年前我就说服自己要改变. 宁肯做一个草率地决定, 也不要一直后悔地回忆.\n喜欢一个事物光有自己的勇气是不行的, 一定要让别人觉得你喜欢的东西是世界上最好的, 而且要大声地说, 大胆地说, 理直气壮地说.\n无论再矫情再幼稚再做作, 那都是一个真真实实的我们, 没有什么好鄙视, 更不用一个激动就按了删除键. 要知道你删除的并不是幼稚, 而是一段青春好看的风景, 这些风景或许你现在随处可见, 但这些风景未来你世界难寻.\n感情不能假手于人, 中间一旦掺杂了等价交换物, 也许最后记得的只是等价物了.\n任何事情, 不要将希望寄托到别人身上, 无论是情感还是工作, 否则唯一的结果便是措手不及, 安全感只能自己给自己.\n你错过的, 别人才会得到, 正如你得到的都是别人错过的.\n成长中所有遇到的问题, 都是量身定做的. 解决了, 你就成为这类人中的幸存者. 不解决, 你永远不知道自己可能成为谁.\n因为年轻, 所以没有选择, 只能试试.\n不如我们定下一个誓约, 看看十年之后, 我们彼此又在哪里, 听谁的歌, 看谁的字, 身边的人又是谁?\n好多好多美好的事情, 就应该遇见, 而不是追逐, 或者等待.\n我们之所以迷茫, 并不是因为我们不知道自己想要什么. 而是因为, 无数人教育我们应该要怎么样, 却忘了告诉我们, 想要更重要. 所以我们在应该和想之间徘徊, 就是迷茫. 其实, 做自己就好.\n所谓成长, 不是学会, 就是懂得.\n我妈说: 没有人会一直正确, 他们只会越来越正确.\n这辈子我们需要一见钟情很多人, 两情相悦一些人, 然后白头偕老一个人.\n梦里所有的一切都顺理成章, 而清醒之后漏洞百出.\n成长有一瞬间给我的感觉就是, 并不是学会了避开危险, 而是学会了不怕疼痛.\n不如我们立个约定, 见证彼此的下一年. 希望在未来的日子里, 我们不为了生活而忘了梦想, 不为了老练而丢弃冲动, 不为了成熟而失去格调.\n","permalink":"https://www.lvbibir.cn/en/posts/read/shei-de-qing-chun-bu-mi-mang/","summary":"在 20 岁到 30 岁这十年的过程中, 我们都走过一样的路. 你觉得孤独就对了, 那是让你认识自己的机会. 你觉得不被理解就对了, 那是让你认清朋友的机会. 你觉得黑暗就对了, 那样你才分辨得出什么是你的光芒. 你觉得无助就对了, 那样你才能明白谁是你成长中能扶你一把的人. 你觉得迷茫就对了, 谁的青春不迷茫.","title":"《谁的青春不迷茫》"},{"content":"0 前言 本文参考以下链接:\n指尖飞舞: vscode + vim 高效开发 vim 备忘清单 一直憧憬 vim 的全键盘操作, 于是开始折腾将 obsidian 和 vscode 的编辑模式都转到 vim, obsidian 使用自带的 vim 模式加 vimrc 插件, vscode 使用 vim 插件\n为了保持 obsidian, vscode, wsl 及 linux 中的 vim 习惯一致, 我的 vim 使用理念:\n尽量使用 vim 原生自带的功能, 拒绝任何三方插件 尽量使用各平台通用的 vimrc 配置 (除了 vscode 使用 setting.json) 1 vim 通用操作 1.1 示例 vim 中的操作都是通过如下方式进行操作的:\n[数字] \u0026lt;操作符\u0026gt; \u0026lt;动作\u0026gt;/\u0026lt;文本对象\u0026gt; \u0026lt;操作符\u0026gt; [数字] \u0026lt;动作\u0026gt;/\u0026lt;文本对象\u0026gt; \u0026gt;i{ | 将当前 {} 内的内容向右缩进 dfa | 删除直到 a 字符 d/hello | 删除直到 hello ggyG | 复制整个文档 dip | 删除整个段落 ciw | 更改当前 word cit | 更改当前 html 标签的内容 1.2 operator 操作符 d | 删除 y | yank (复制) c | 更改 (删除然后插入) p | 粘贴 = | 格式代码 g~ | 切换案例 gU | 大写 gu | 小写 \u0026gt; | 右缩进 \u0026lt; | 左缩进 ! | 外部程序过滤 1.3 motion 动作 基础动作\nh/j/k/l | 左/下/上/右 ctrl + u/d | 上/下 半页 ctrl + b/f | 上/下 翻页 字 (词)\nb/w | 上一个/下一个 单词开始 ge/e | 上一个/下一个 单词末尾 行\n0/$ | 行首/行尾 ^ | 行首 (非空白) 字符串\nFe/fe | 移动到上一个/下一个 e To/to | 在上一个/下一个 o 之前/之后移动 | / n| | 转到一个 /n 列 文档\ngg/G | 第一行/最后一行 :n/nG | 转到第 n 行 { / } | 上一个/下一个空行 窗口\nH/M/L | 上/中/下 屏幕 zt/zz/zb | 上/中/下 这条线 1.4 文本对象 inner(内部) / around(周围)\np | 段落 w | 单词 W | WORD(被空格包围) s | 句子 [({\u0026lt;\u0026gt;})] | [], (), {} 或者 \u0026lt;\u0026gt; 块 \u0026#39;\u0026#34;\\` | 带引号的字符串 b | 同 () B | 同 {} t | html 标签块 2 vscode 中的 vim 下述功能源于 vscode vim 插件\n2.1 easymotion \u0026lt;leader\u0026gt;\u0026lt;leader\u0026gt;s\u0026lt;str\u0026gt; | 可以快速向后查找 \u0026lt;leader\u0026gt;\u0026lt;leader\u0026gt;f\u0026lt;str\u0026gt; | 可以快速向前查找 2.2 surround cs\u0026#34;\u0026#39; | 将 `\u0026#34;` 替换为 \u0026#39; ds\u0026#34; | 删除包围的 \u0026#34; ys\u0026lt;motion\u0026gt;\u0026#34; | 添加包围的 \u0026#34;, 如 ysiw\u0026#34; 2.3 multi-cursor 多光标 可以使用 gb 代替 vscode 中的 ctrl-d\n2.4 其他操作 gh | 可以模拟鼠标悬浮 gd | 可以切换定义 3 vimrc vimrc 的位置:\nobsidian: 在插件配置中我将 vimrc 的默认文件名从 .obsidian.vimrc 改成了 .vimrc 存放到了 obsidian 仓库的根目录 wsl: 我的 wsl 是 ubuntu, 为了使用 sudo 时 vimrc 配置生效, vimrc 修改通过修改 /etc/vim/vimrc 实现 vscode: vscode 直接使用 setting.json 中 vim 的配置 我的 vimrc 配置示例\n\u0026#34; 插入模式下使用 jj 快速返回到 normal 模式 inoremap jj \u0026lt;Esc\u0026gt; \u0026#34; 使上下移动的时候按照视觉的行数移动, 对于多行的段落很有效 nmap j gj nmap k gk \u0026#34; 快捷行首和行尾 \u0026#34; normal 模式使用 nmap H ^ nmap L $ \u0026#34; 操作模式使用, 用于 yL, dH 等操作 omap H ^ omap L $ \u0026#34; visual 模式使用 vmap H ^ vmap L $ vscode 中的 vim 配置示例\n// vim 相关 \u0026#34;vim.leader\u0026#34;: \u0026#34;\u0026lt;space\u0026gt;\u0026#34;, \u0026#34;vim.incsearch\u0026#34;: true, \u0026#34;vim.easymotion\u0026#34;: true, // 使用系统剪贴板作为 vim 寄存器 \u0026#34;vim.useSystemClipboard\u0026#34;: true, // 由 vim 接管 ctrl + any 快捷键 \u0026#34;vim.useCtrlKeys\u0026#34;: true, // 突出显示与当前搜索匹配的所有文本 \u0026#34;vim.hlsearch\u0026#34;: true, // 下列按键由 vscode 接管而不是 vim \u0026#34;vim.handleKeys\u0026#34;: { \u0026#34;\u0026lt;C-a\u0026gt;\u0026#34;: false, \u0026#34;\u0026lt;C-f\u0026gt;\u0026#34;: false, \u0026#34;\u0026lt;C-k\u0026gt;\u0026#34;: false, \u0026#34;\u0026lt;C-n\u0026gt;\u0026#34;: false, \u0026#34;\u0026lt;C-p\u0026gt;\u0026#34;: false, }, \u0026#34;vim.insertModeKeyBindings\u0026#34;: [ { \u0026#34;before\u0026#34;: [\u0026#34;j\u0026#34;, \u0026#34;j\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;\u0026lt;Esc\u0026gt;\u0026#34;] } ], \u0026#34;vim.normalModeKeyBindingsNonRecursive\u0026#34;: [ { \u0026#34;before\u0026#34;: [\u0026#34;j\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;g\u0026#34;, \u0026#34;j\u0026#34;] }, { \u0026#34;before\u0026#34;: [\u0026#34;k\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;g\u0026#34;, \u0026#34;k\u0026#34;] }, { \u0026#34;before\u0026#34;: [\u0026#34;K\u0026#34;], \u0026#34;commands\u0026#34;: [\u0026#34;lineBreakInsert\u0026#34;], \u0026#34;silent\u0026#34;: true }, { \u0026#34;before\u0026#34;: [\u0026#34;L\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;$\u0026#34;] }, { \u0026#34;before\u0026#34;: [\u0026#34;H\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;^\u0026#34;] } ], \u0026#34;vim.operatorPendingModeKeyBindings\u0026#34;: [ { \u0026#34;before\u0026#34;: [\u0026#34;L\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;$\u0026#34;] }, { \u0026#34;before\u0026#34;: [\u0026#34;H\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;^\u0026#34;] } ], \u0026#34;vim.visualModeKeyBindingsNonRecursive\u0026#34;: [ { \u0026#34;before\u0026#34;: [\u0026#34;L\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;$\u0026#34;] }, { \u0026#34;before\u0026#34;: [\u0026#34;H\u0026#34;], \u0026#34;after\u0026#34;: [\u0026#34;^\u0026#34;] } ], \u0026#34;extensions.experimental.affinity\u0026#34;: { \u0026#34;vscodevim.vim\u0026#34;: 1 }, 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/vim/","summary":"0 前言 本文参考以下链接: 指尖飞舞: vscode + vim 高效开发 vim 备忘清单 一直憧憬 vim 的全键盘操作, 于是开始折腾将 obsidian 和 vscode 的编辑模式都转到 vim, obsidian 使用自带的 vim 模式加 vimrc 插件, vscode 使用 vim 插件 为了保持 obsidian, vscode, wsl 及 linux 中的 vim 习惯一致, 我的 vim 使用理念: 尽量使用 vim 原生自带的功能, 拒绝任何三方插件 尽量使用各平台通用的 vimrc 配置 (","title":"vim | 基础配置和使用"},{"content":"0 前言 用了很多年的搜狗输入法, 苦于越来越多的后台, 又换到微软原生的输入法, 结果又出现了 vscode vim 中使用中文输入法的时候会一直乱跳, 遂又产生了换输入法的想法\n我对输入法的要求很简单: 简洁方便, 后台干净, 自带良好的词库即可, 最后了解到了小狼毫输入法 (rime 的 windows 版本, 又称 rime weasel) 加 雾凇方案, 可定制项非常多, 也可以集成其他方案, 慢慢打磨成自己顺手的输入法\n1 安装 安装前确认区域和语言设置中文的输入法为微软默认的输入法, 安装完成后在区域和语言设置中新增小狼毫输入法并删除微软默认输入法\nrime 由于 windows 上的 rime 更新有点慢, 当前版本的 vim mode 有些问题, 所以这里我采用了 rime nightly build 预览版, 下载 exe 安装包进行安装即可\n安装位置选择 D:\\software\\Rime, 用户文件夹选择 D:\\software\\Rime\\profile\n雾凇方案 直接下载 zip 解压后将所有文件复制到 D:\\software\\Rime\\profile 即可\n之后在任务栏的语言栏右击小狼毫图标选择重新部署, 等待部署完成后按 F4 选择 雾凇方案 即可, 也可以切换简繁体, 中英文标点和全半角等\n2 配置 雾凇方案 - 官方配置指南 rime weasel - wiki\n可以通过直接修改 profile 下的 default.yaml 和 weasel.yaml 实现, 但是后续无法再方便地进行更新, 推荐通过 patch 方式进行修改, 比如 default.yaml 的 patch 文件就是 default.custom.yaml\n修改 D:\\software\\Rime\\profile\\weasel.custom.yaml, 添加如下\npatch: # 配色方案 \u0026#34;style/color_scheme\u0026#34;: \u0026#34;lost_temple\u0026#34; # 字体相关配置 \u0026#34;style/font_face\u0026#34;: \u0026#34;LXGW WenKai, Segoe UI Emoji:30:39, Segoe UI Emoji:23:23, Segoe UI Emoji:2a:2a, Segoe UI Emoji:fe0f:fe0f, Segoe UI Emoji:20e3:20e3, Microsoft YaHei, SF Pro, Segoe UI Emoji, Noto Color Emoji\u0026#34; # 标签字体 \u0026#34;style/label_font_face\u0026#34;: \u0026#34;LXGW WenKai\u0026#34; # 注释字体 \u0026#34;style/comment_font_face\u0026#34;: \u0026#34;LXGW WenKai\u0026#34; # 全局字体字号 \u0026#34;style/font_point\u0026#34;: 16 # 标签字体字号，不设定 fallback 到 font_point \u0026#34;style/label_font_point\u0026#34;: 16 # 注释字体字号，不设定 fallback 到 font_point \u0026#34;style/comment_font_point\u0026#34;: 14 # 行内取消显示预编辑区, 可以解决 vscode 输入中文的光标跳动问题 \u0026#34;style/inline_preedit\u0026#34;: false # 针对不同的应用程序设置输入法的默认状态 # ascii_mode true 表示使用英文 # vim_mode true 表示在使用 \u0026lt;Esc\u0026gt; 或者 ctrl + [ 时自动切换到英文 app_options: WindowsTerminal.exe: ascii_mode: true vim_mode: true Obsidian.exe: ascii_mode: true vim_mode: true Code.exe: ascii_mode: true vim_mode: true MobaXterm.exe: ascii_mode: true vim_mode: true 修改 D:\\software\\Rime\\profile\\default.custom.yaml\npatch: # 方案选单, 我只使用全拼 schema_list: - {schema: rime_ice} # 修改了默认配置中的快捷键和是否折叠 switcher: caption: [方案选单] # 修改快捷键 F4 呼出方案选单 hotkeys: - F4 save_options: - ascii_punct - traditionalization - emoji - full_shape - single_char # 呼出时是否折叠 fold_options: false # 折叠时是否缩写选项 abbreviate_options: true # 折叠时的选项分隔符 option_list_separator: \u0026#39; | \u0026#39; # 添加 , 和 . 翻页 key_binder/bindings/+: - { when: paging, accept: comma, send: Page_Up } - { when: has_menu, accept: period, send: Page_Down } 修改完成后重新部署即可, 后续更改配置基本只需要修改这两个文件, 更新方案直接通过覆盖文件的方式进行全量更新\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windowns-rime-input-method/","summary":"0 前言 用了很多年的搜狗输入法, 苦于越来越多的后台, 又换到微软原生的输入法, 结果又出现了 vscode vim 中使用中文输入法的时候会一直乱跳, 遂又产生了换输入法的想法 我对输入法的要求很简单: 简洁方便, 后台干净, 自带良好的词库即可, 最后了解到了小狼毫输入法 (rime 的 windows 版本, 又称 rime weasel) 加 雾凇方案, 可定制项非常","title":"windows | rime 输入法 \u0026 雾凇方案"},{"content":"#!/bin/bash username=\u0026#34;root\u0026#34; password=\u0026#34;123123\u0026#34; port=\u0026#34;22\u0026#34; # 判断ip检查文件是否存在 if [ ! -f \u0026#34;./ip_check.txt\u0026#34; ]; then # 清空检查文件 /usr/bin/true \u0026gt;./ip_check.txt else # 创建检查文件 /usr/bin/touch ./ip_check.txt fi # 传输文件 execut_ftp_file() { IFS=$\u0026#39;\\n\u0026#39; for file in $(cat ./ftp_file.conf); do /opt/sshpass/bin/sshpass -p $password scp -o StrictHostKeyChecking=no ./$file $username@$1:/opt/ echo \u0026#34;$1 ---- $file 文件传输完成\u0026#34; done } execut_commad_file() { IFS=$\u0026#39;\\n\u0026#39; for com in $(cat ./execut_commad.conf); do /opt/sshpass/bin/sshpass -p $password ssh -o StrictHostKeyChecking=no $username@$1 $com echo \u0026#34;$1 ---------- $com 命令执行完成\u0026#34; sleep 3 done } for line in $(cat ./ip.txt); do Ture_ip=$(/usr/bin/ping -c 2 $line) if [ $? != \u0026#34;0\u0026#34; ]; then echo \u0026#34;$line is blocked\u0026#34; \u0026gt;\u0026gt;./ip_check.txt else execut_ftp_file $line sleep 2 execut_commad_file $line fi done ","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-sshpass-batch-execcute/","summary":"#!/bin/bash username=\u0026#34;root\u0026#34; password=\u0026#34;123123\u0026#34; port=\u0026#34;22\u0026#34; # 判断ip检查文件是否存在 if [ ! -f \u0026#34;./ip_check.txt\u0026#34; ]; then # 清空检查文件 /usr/bin/true \u0026gt;./ip_check.txt else # 创建检查文件 /usr/bin/touch ./ip_check.txt fi # 传输文件 execut_ftp_file() { IFS=$\u0026#39;\\n\u0026#39; for file in $(cat ./ftp_file.conf); do /opt/sshpass/bin/sshpass -p $password scp -o StrictHostKeyChecking=no ./$file $username@$1:/opt/ echo \u0026#34;$1 ---- $file 文件传输完成\u0026#34; done } execut_commad_file() { IFS=$\u0026#39;\\n\u0026#39; for com in $(cat ./execut_commad.conf); do /opt/sshpass/bin/sshpass -p $password ssh -o StrictHostKeyChecking=no $username@$1 $com echo \u0026#34;$1 ---------- $com 命令执行完成\u0026#34; sleep 3 done } for line in $(cat ./ip.txt); do Ture_ip=$(/usr/bin/ping -c 2 $line) if [ $? != \u0026#34;0\u0026#34; ]; then echo \u0026#34;$line is blocked\u0026#34;","title":"shell | sshpass 批量传输文件及执行命令"},{"content":"0 前言 本文参考以下链接:\ndocker 文档 - 使用便利性脚本进行安装 docker engine docker 文档 - 配置 http proxy 记录一下 wsl2 原生 linux 方式安装 docker 的过程\n1 安装 安装过程中会提示建议使用 docker desktop, 等待 20s 即可\ncurl https://get.docker.com -o get-docker.sh sudo bash get-docker.sh sudo docker info 安装完之后 docker 会默认开机自启, 之后管理 docker 使用 systemctl 即可\nsudo systemctl stop|start|restart docker 2 配置 2.1 修改镜像源 proxies 部分可以不用配置, 因为我这里环境特殊, 必须走代理才能访问互联网\nsudo vim /etc/docker/daemon.json # 添加如下内容 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34; ], \u0026#34;proxies\u0026#34;: { \u0026#34;http-proxy\u0026#34;: \u0026#34;http://proxy1.bj.petrochina:8080\u0026#34;, \u0026#34;https-proxy\u0026#34;: \u0026#34;http://proxy1.bj.petrochina:8080\u0026#34;, \u0026#34;no-proxy\u0026#34;: \u0026#34;localhost,127.0.0.0/8\u0026#34; } } sudo systemctl daemon-reload sudo systemctl restart docker sudo docker info # 应看到镜像仓库信息和代理信息 2.2 docker-compose 使用安装脚本完后会默认安装 docker-compose-plugin, 可以使用 docker compose 调用, 如果你更习惯使用 docker-compose, 可以手动添加一下软连接\nsudo ln -s /usr/libexec/docker/cli-plugins/docker-compose /usr/sbin/docker-compose sudo docker-compose --version 3 测试 最后简单测试一下\nmkdir docker; cd docker cat \u0026gt; docker-compose.yml \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; version: \u0026#39;3.1\u0026#39; services: nginx: image: superng6/nginx:debian-stable-1.18.0 container_name: nginx restart: always ports: - 80:80 EOF sudo docker-compose up -d 由于 wsl2 解决了和 windows 使用相同的网络 (镜像网络), 所以可以直接通过 windows 端浏览器访问 http://localhost 即可跳转到 docker 中运行的 nginx 容器\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-wsl-6-docker/","summary":"0 前言 本文参考以下链接: docker 文档 - 使用便利性脚本进行安装 docker engine docker 文档 - 配置 http proxy 记录一下 wsl2 原生 linux 方式安装 docker 的过程 1 安装 安装过程中会提示建议使用 docker desktop, 等待 20s 即可 curl https://get.docker.com -o get-docker.sh sudo bash get-docker.sh sudo docker info 安装完之后 docker 会默认开机自启, 之后管理 docker 使用 systemctl 即可 sudo systemctl stop|start|restart docker 2 配置 2.1 修改镜像源 proxies 部分可以不用配置, 因为我这里环境特","title":"wsl | 原生 linux 方式安装 docker"},{"content":"0 前言 在 wsl2 中安装配置 nodejs 环境\n1 安装 在 此页面 选择 linux x64 版本的链接, 复制链接地址\n链接及目录自行修改, 这里我将 nodejs 的目录放到了 wsl 用户主目录下\nwget https://nodejs.org/dist/v20.11.0/node-v20.11.0-linux-x64.tar.xz mv node-v20.11.0-linux-x64 nodejs mv nodejs ~/ cat \u0026gt;\u0026gt; ~/.bashrc \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; export PATH=${HOME}/nodejs/bin:$PATH EOF source ~/.bashrc 测试\nnode -v npm -v 2 配置 2.1 修改默认源地址 npm config set registry https://registry.npmmirror.com npm config get registry 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-wsl-5-nodejs/","summary":"0 前言 在 wsl2 中安装配置 nodejs 环境 1 安装 在 此页面 选择 linux x64 版本的链接, 复制链接地址 链接及目录自行修改, 这里我将 nodejs 的目录放到了 wsl 用户主目录下 wget https://nodejs.org/dist/v20.11.0/node-v20.11.0-linux-x64.tar.xz mv node-v20.11.0-linux-x64 nodejs mv nodejs ~/ cat \u0026gt;\u0026gt; ~/.bashrc \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; export PATH=${HOME}/nodejs/bin:$PATH EOF source ~/.bashrc 测试 node -v npm -v 2 配置 2.1 修改默认源地址 npm config set registry https://registry.npmmirror.com npm config get registry 以上","title":"wsl | 安装配置 nodejs 环境"},{"content":"0 前言 之前写过一篇 windows 安装 miniconda 的文章, 后面在接触了 wsl 后发现用起来要比在原生 windows 上舒服很多, 毕竟我写 python 多是为了在 linux 服务器上跑, 用 wsl 会更顺滑一些, 虚拟环境同样选择更轻量的 miniconda\n1 安装 下载并安装, 一路 yes 即可\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh 2 配置 修改 conda 配置文件\ncat \u0026gt; ${HOME}/.condarc \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ # 不自动激活 base 环境 auto_activate_base: false # 虚拟环境存放路径 envs_dirs: - /home/lvbibir/miniconda3/envs # pkg 存放路径 pkgs_dirs: - /home/lvbibir/miniconda3/pkgs EOF 为了不搞坏 conda 的默认 base 环境, 我们创建一个虚拟环境, 每次默认进这个虚拟环境\nconda create -n py37 python=3.7 cat \u0026gt;\u0026gt; ${HOME}/.bash_profile \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; conda activate py37 EOF 退出重进后发现已经默认激活 py37 了\n最后我们修改一下 pip 的配置, 添加清华源\nmkdir ${HOME}/.pip cat \u0026gt; ${HOME}/.pip/pip.conf \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; [global] timeout = 6000 index-url = http://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn EOF pip config list 测试一下 pip\npip install paramiko 3 虚拟环境 创建虚拟环境\n# 安装一个虚拟环境 conda create -n py37 python=3.7 # 从现有环境复制一个虚拟环境 conda create -n py37-temp --clone py37 激活虚拟环境\nconda activate py37 退出虚拟环境\nconda deactivate 查看虚拟环境列表, 带有 * 的行就是当前所处的虚拟环境\nconda env list 删除虚拟环境,\nconda env remove -n py37 4 其他 conda 最为人诟病的点应该是包管理跟 pip 可能会产生一些冲突, conda 官方给出的最佳方案是\n全程使用 conda install 来安装模块, 实在不行再用 pip 使用 conda 创建完虚拟环境后, 一直用 pip 来管理模块 pip 应使用 –upgrade-strategy only-if-needed 参数运行, 以防止通过 conda 安装的软件包进行不必要的升级. 这是运行 pip 时的默认设置, 不应更改 不要将 pip 与 –user 参数一起使用，避免所有用户安装 总结一下就是不要来回地用 pip 和 conda, 专一一点 (笑\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-wsl-4-miniconda/","summary":"0 前言 之前写过一篇 windows 安装 miniconda 的文章, 后面在接触了 wsl 后发现用起来要比在原生 windows 上舒服很多, 毕竟我写 python 多是为了在 linux 服务器上跑, 用 wsl 会更顺滑一些, 虚拟环境同样选择更轻量的 miniconda 1 安装 下载并安装, 一路 yes 即可 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh 2 配置 修改 conda 配置文件 cat \u0026gt; ${HOME}/.condarc \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk:","title":"wsl | 安装配置 miniconda 虚拟环境"},{"content":"0 前言 目前我使用 wsl 过程中有以下两个场景需要使用到代理:\n场景一: 某些科学上网的场景, 比如 github 加速等 场景二: 公司内网机器需要通过公司提供的代理上网 针对场景一, 可以通过将代理设置为 clash 或者其他客户端提供的端口, 如使用 clash 记得打开设置中的允许局域网\n针对场景二, 直接设置公司提供的代理地址即可\n1 配置 wsl 中添加如下脚本, 实现常规的系统代理, git 仓库代理以及 apt 的代理\ncat \u0026gt; ~/proxy #!/bin/bash # normal proxy # 指定 url 的方式 # proxy_type=\u0026#34;http\u0026#34; # proxy_ip=\u0026#34;proxy1.bj.petrochina\u0026#34; # proxy_port=\u0026#34;8080\u0026#34; # 使用 windows 主机上运行的代理程序, 例如 clash # wsl 中的地址是不固定的, 这里通过脚本获取, 每次启动 wsl 都可以实时更新 proxy_type=\u0026#34;http\u0026#34; proxy_ip=$(cat /etc/resolv.conf |grep \u0026#34;nameserver\u0026#34; |cut -f 2 -d \u0026#34; \u0026#34;) proxy_port=\u0026#34;7890\u0026#34; proxy=\u0026#34;${proxy_type}://${proxy_ip}:${proxy_port}\u0026#34; # 系统全局代理 export ALL_PROXY=\u0026#34;${proxy}\u0026#34; export all_proxy=\u0026#34;${proxy}\u0026#34; export http_proxy=\u0026#34;${proxy}\u0026#34; export https_proxy=\u0026#34;${proxy}\u0026#34; # apt 代理 # 如果不加 sudo, 会导致用 sudo 执行 apt 等命令时无法识别 alias alias sudo=\u0026#39;sudo \u0026#39; alias apt=\u0026#34;apt -o Acquire::http::proxy=${proxy}\u0026#34; alias apt-get=\u0026#34;apt-get -o Acquire::http::proxy=${proxy}\u0026#34; # git 的 http 或者 https 代理 git config --global http.https://github.com.proxy ${proxy} git config --global https.https://github.com.proxy ${proxy} # git 的 ssh 代理 cat \u0026gt; ~/.ssh/config \u0026lt;\u0026lt;- EOF # git-bash 环境: 注意替换 connect.exe 的路径 # ProxyCommand \u0026#34;C:\\\\APP\\\\Git\\\\mingw64\\\\bin\\\\connect\u0026#34; -S ${proxy_ip}:${proxy_port} -a none %h %p # linux 环境 # ProxyCommand nc -v -x ${proxy_ip}:${proxy_port} %h %p Host github.com User git Port 22 Hostname github.com ProxyCommand nc -v -x ${proxy_ip}:${proxy_port} %h %p IdentityFile \u0026#34;/home/lvbibir/.ssh/id_rsa\u0026#34; TCPKeepAlive yes Host ssh.github.com User git Port 443 Hostname ssh.github.com ProxyCommand nc -v -x ${proxy_ip}:${proxy_port} %h %p IdentityFile \u0026#34;/home/lvbibir/.ssh/id_rsa\u0026#34; TCPKeepAlive yes EOF 加入环境变量, 每次启动 wsl 自动设置 proxy\ncat \u0026gt;\u0026gt; ~/.bashrc \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; source ${HOME}/proxy EOF source ~/.bashrc 2 docker 代理 修改 docker pull 等操作的代理可以通过 docker 的 daemon.json 文件或者 service 两种方式进行修改, 推荐使用第一种\n修改 /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34; ], \u0026#34;proxies\u0026#34;: { \u0026#34;http-proxy\u0026#34;: \u0026#34;http://proxy1.bj.petrochina:8080\u0026#34;, \u0026#34;https-proxy\u0026#34;: \u0026#34;http://proxy1.bj.petrochina:8080\u0026#34;, \u0026#34;no-proxy\u0026#34;: \u0026#34;localhost,127.0.0.0/8\u0026#34; } } 修改 docker service sudo vim /lib/systemd/system/docker.service # 在 [Service] 下添加如下三行 Environment=HTTP_PROXY=http://proxy1.bj.petrochina:8080 Environment=HTTPS_PROXY=http://proxy1.bj.petrochina:8080 Environment=NO_PROXY=localhost,127.0.0.1 上述两种方式任意一种修改完成后重启 docker 即可\nsudo systemctl daemon-reload sudo systemctl restart docker sudo docker info | grep proxy 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-wsl-3-set-proxy/","summary":"0 前言 目前我使用 wsl 过程中有以下两个场景需要使用到代理: 场景一: 某些科学上网的场景, 比如 github 加速等 场景二: 公司内网机器需要通过公司提供的代理上网 针对场景一, 可以通过将代理设置为 clash 或者其他客户端提供的端口, 如使用 clash 记得打开设置中的允许局域网 针对场景二, 直接设置公司提供的代理地址即可 1 配","title":"wsl | 自动更新系统代理"},{"content":"0 前言 装完 wsl 后发现用户目录下的 .bashrc 文件总是无法正常读取, github 上关于此问题的 讨论 也没有比较好的解决方法\n1 解决办法 我这里取巧了一下, 在 .bash_profile 中再调用一下 .bashrc, 如下\necho \u0026gt;\u0026gt; ${HOME}/.bash_profile \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; source ${HOME}/.bashrc EOF source ${HOME}/.bash_profile 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-wsl-2-bashrc/","summary":"0 前言 装完 wsl 后发现用户目录下的 .bashrc 文件总是无法正常读取, github 上关于此问题的 讨论 也没有比较好的解决方法 1 解决办法 我这里取巧了一下, 在 .bash_profile 中再调用一下 .bashrc, 如下 echo \u0026gt;\u0026gt; ${HOME}/.bash_profile \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; source ${HOME}/.bashrc EOF source ${HOME}/.bash_profile 以上","title":"wsl | bashrc 环境变量不正确加载的处理方法"},{"content":"0 前言 本文内容参考以下链接:\nhttps://zhuanlan.zhihu.com/p/466001838 https://learn.microsoft.com/zh-cn/windows/wsl/install-manual 今天不小心把我电脑的 wsl 误删了, 刚好重装记录一下安装步骤\n1 安装 1.1 打开系统功能 首先通过管理员打开 powershell 执行如下指令, 用于打开系统功能\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 然后在 Microsoft Store 中安装 Windows Subsystem for Linux\n安装好之后重启\n重启完成后在 powershell 执行\nwsl --set-default-version 2 1.2 安装内核更新包 点击 此链接 下载内核更新包, 右击安装即可\n1.3 安装 wsl 到 D 盘 如果不需要装到其他盘, 1.3 的步骤无需操作 直接 powershell 执行 wsl \u0026ndash;install -d Ubuntu-20.04 即可\n通过 chrome 或者 IDM 输入 https://aka.ms/wslubuntu2004 下载安装包, chrome 可能会提示未经验证, 直接无视后保存即可\n或者执行如下 powershell 命令下载\ncd D:\\ Invoke-WebRequest -Uri https://aka.ms/wslubuntu2004 -OutFile Ubuntu.appx -UseBasicParsing # 或者使用 curl 下载 curl.exe -L -o ubuntu-2004.appx https://aka.ms/wslubuntu2004 将下载后的文件后缀直接改为 zip, , 再将 x64 的 appx 文件后缀改成 zip, 将此 zip 解压到指定目录, 此目录就是后续 ubuntu 存放数据的地方, 我这里放到了 D:\\ubuntu 目录\n最后执行解压后的 exe 进行安装, 按照提示设置账号密码即可\ncd D:\\ubuntu .\\ubuntu2004.exe 1.4 更换系统源 cmd 或者 powershell 中执行 wsl 进入 ubuntu, 更换系统源\nsudo apt-get install --only-upgrade ca-certificates sudo cp /etc/apt/sources.list /etc/apt/sources.list.origin sudo cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse deb http://security.ubuntu.com/ubuntu/ focal-security main restricted universe multiverse EOF sudo apt-get update 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-wsl-1-install/","summary":"0 前言 本文内容参考以下链接: https://zhuanlan.zhihu.com/p/466001838 https://learn.microsoft.com/zh-cn/windows/wsl/install-manual 今天不小心把我电脑的 wsl 误删了, 刚好重装记录一下安装步骤 1 安装 1.1 打开系统功能 首先通过管理员打开 powershell 执行如下指令, 用于打开系统功能 dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 然后在 Microsoft Store 中安装 Windows Subsystem for Linux 安装好之后重启 重启完成后在 powershell 执行 wsl --set-default-version 2 1.2 安装内核更新包 点击 此链接 下载内核更新","title":"wsl | win10 安装 wsl2"},{"content":"0 前言 前段时间在配置 jenkins publish over ssh 时发现 jenkins 无法连接某个服务器, 经测试 ssh 可以正常登录, 但是 scp 时报错 subsystem request failed on channel 0, 记录一下这个问题的排查思路\n1 大致思路 影响到 ssh 的配置无非是以下这些:\n网络问题: server 和 client 之间的网络不通或者防火墙配置 认证问题: 账号密码或者密钥错误 配置问题: server 端本身 sshd 服务报错未正常 server 端拒绝 client 的加密算法, 常出现在旧版本 ssh 连接高版本时出现 server 端拒绝某些用户的登录, 比如生产环境基本都禁止 root 登录 大致思路是尽量找相同配置的 server 和 client 进行交叉验证对比, 定位问题点, 涉及到如下四个角色, 本次故障是在 client-docker 在 scp server-1 时出现的\n角色 OS 版本 ssh 版本 备注 client-1 Centos 7.9 OpenSSH_7.4p1 client-docker docker container OpenSSH_9.2p1 fail server-1 Centos 7.9 OpenSSH_7.4p1 fail server-2 Centos 7.9 OpenSSH_7.4p1 经测试, 除了上述故障, 所有 client 对所有 server 执行 ssh 或者 scp 都是没有问题的, 能 ssh 成功其实就代表出现问题的地方并不是我们之前预想的那些\n2 debug 那就纳闷了, 幸好 scp 命令提供了 -v 参数, 可以展示出更多的 debug 信息, 于是着手将异常 scp 的 debug 信息与正常 scp 的 debug 信息进行对比, 开始愉快的 找不同 环节\n(正常情况) client-1 scp server-1 的 debug 信息\n# scp -v test app@server-1:/home/app/ ...... debug1: Authentication succeeded (password). Authenticated to 11.53.57.80 ([11.53.57.80]:22). debug1: channel 0: new [client-session] debug1: Requesting no-more-sessions@openssh.com debug1: Entering interactive session. debug1: pledge: network debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0 debug1: Sending environment. debug1: Sending env LANG = en_US.UTF-8 debug1: Sending command: scp -v -t /home/app/ Sending file modes: C0644 0 test Sink: C0644 0 test test 100% 0 0.0KB/s 00:00 debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: channel 0: free: client-session, nchannels 1 debug1: fd 0 clearing O_NONBLOCK debug1: fd 1 clearing O_NONBLOCK Transferred: sent 2120, received 2344 bytes, in 0.2 seconds Bytes per second: sent 12262.0, received 13557.6 debug1: Exit status 0 (正常情况) client-docker scp server-2 的 debug 信息\n# scp -v test app@server-2:/home/app/ ...... Authenticated to 11.53.57.74 ([11.53.57.74]:22) using \u0026#34;password\u0026#34;. debug1: channel 0: new session [client-session] (inactive timeout: 0) debug1: Requesting no-more-sessions@openssh.com debug1: Entering interactive session. debug1: pledge: filesystem debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0 debug1: client_input_hostkeys: searching /root/.ssh/known_hosts for 11.53.57.74 / (none) debug1: client_input_hostkeys: searching /root/.ssh/known_hosts2 for 11.53.57.74 / (none) debug1: client_input_hostkeys: hostkeys file /root/.ssh/known_hosts2 does not exist debug1: Sending environment. debug1: channel 0: setting env LANG = \u0026#34;C.UTF-8\u0026#34; debug1: Sending subsystem: sftp debug1: client_global_hostkeys_prove_confirm: server used untrusted RSA signature algorithm ssh-rsa for key 0, disregarding debug1: update_known_hosts: known hosts file /root/.ssh/known_hosts2 does not exist debug1: pledge: fork test 100% 0 0.0KB/s 00:00 scp: debug1: truncating at 0 debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: channel 0: free: client-session, nchannels 1 Transferred: sent 3124, received 3084 bytes, in 0.0 seconds Bytes per second: sent 88478.9, received 87346.1 debug1: Exit status 0 (异常情况) client-docker scp server-1 的 debug 信息\n# scp -v test app@server-1:/home/app/ ...... Authenticated to 11.53.57.80 ([11.53.57.80]:22) using \u0026#34;password\u0026#34;. debug1: channel 0: new session [client-session] (inactive timeout: 0) debug1: Requesting no-more-sessions@openssh.com debug1: Entering interactive session. debug1: pledge: filesystem debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0 debug1: client_input_hostkeys: searching /root/.ssh/known_hosts for 11.53.57.80 / (none) debug1: client_input_hostkeys: searching /root/.ssh/known_hosts2 for 11.53.57.80 / (none) debug1: client_input_hostkeys: hostkeys file /root/.ssh/known_hosts2 does not exist debug1: Sending environment. debug1: channel 0: setting env LANG = \u0026#34;C.UTF-8\u0026#34; debug1: Sending subsystem: sftp debug1: client_global_hostkeys_prove_confirm: server used untrusted RSA signature algorithm ssh-rsa for key 0, disregarding debug1: update_known_hosts: known hosts file /root/.ssh/known_hosts2 does not exist debug1: pledge: fork subsystem request failed on channel 0 scp: Connection closed 排除冗余信息后可以发现:\n(正常情况) client-1 scp server-1 的 debug 信息中, Sending environment 之后的步骤是 Sending command: scp -v -t /home/app/ (正常情况) client-docker scp server-2 的 debug 信息中, Sending environment 之后的步骤是 Sending subsystem: sftp (异常情况) client-docker scp server-1 的 debug 信息中, Sending environment 之后的步骤是 Sending subsystem: sftp, 但是 subsystem request failed 可以推断出问题点在于 scp 的流程中调用了 sftp, 但由于 sftp 的某些原因导致出现了问题\n3 sftp 遂去对比一下两个 server 的 ssh 配置中关于 sftp 的配置\n正常 server 的配置\n# grep -i \u0026#39;sftp\u0026#39; /etc/ssh/sshd_config #Subsystem sftp /usr/libexec/openssh/sftp-server Subsystem sftp internal-sftp Match Group sftp ChrootDirectory /data/sftp/mysftp ForceCommand internal-sftp 异常 server 的配置\n# grep -i \u0026#39;sftp\u0026#39; /etc/ssh/sshd_config #Subsystem sftp /usr/libexec/openssh/sftp-server 可以看到异常 server 的 sftp 是没开的\n去掉 sftp 的注释后重启 sshd, 再次进行尝试后不出意料地恢复正常了\n4 总结 至此, 我们可以确定问题点是由于 scp 中使用 sftp 协议进行传输, 而 server 端未开启 sftp 导致 scp 失败\n最后就是确认一下为什么 scp 会调用 sftp, 在 openssh 9.0p1 release 中发现如下说明:\nThis release switches scp(1) from using the legacy scp/rcp protocol to using the SFTP protocol by default.\n从 9.0p1 开始, scp 将默认使用 sftp 进行传输, 可以使用 -O 选项使 scp 使用 legacy SCP protocol 进行传输\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/troubleshooting-ssh-success-but-scp-failed/","summary":"0 前言 前段时间在配置 jenkins publish over ssh 时发现 jenkins 无法连接某个服务器, 经测试 ssh 可以正常登录, 但是 scp 时报错 subsystem request failed on channel 0, 记录一下这个问题的排查思路 1 大致思路 影响到 ssh 的配置无非是以下这些: 网络问题: server 和 client 之间的网络不通或者防火墙配置 认证问题: 账号密码或者密钥错误 配置问题: server 端本身 sshd 服务报错未正常 server 端","title":"troubleshooting | ssh 成功但是 scp 失败"},{"content":"1 文本处理 1.1 sed 截取 rpm 包名\ncat rpms | sed -e s/-[[:digit:]]./@/ | awk -F \u0026#39;@\u0026#39; \u0026#39;{print $1}\u0026#39; 1.2 awk # 打印某列之后的所有列 awk ‘{ $1=\u0026#34;\u0026#34;; print $0 }’ file_name 1.3 grep # 去除注释和空行 grep -Ev \u0026#39;^$|#\u0026#39; filename # 查找一个目录中不包含 \u0026#34;description\u0026#34; 的文件 grep -Lr \u0026#34;description\u0026#34; /your/directory/* # 查找一个目录中包含 \u0026#34;title\u0026#34; 但是不包含 \u0026#34;description\u0026#34; 的文件 grep -r -l \u0026#39;title\u0026#39; /your/directory/* | xargs grep -L \u0026#39;description\u0026#39; 2 系统进程 2.1 ps # 查看获取服务器内占用内存较高的10个进程 ps aux | head -1; ps aux | grep -v PID | sort -rn -k +4 | head -10 # 查看进程启动时间 ps -eo pid,lstart,etime,cmd | grep java | grep 8082 3 网络 3.1 traceroute # tcp traceroute -n -T -p \u0026lt;port\u0026gt; \u0026lt;ip\u0026gt; tcptraceroute \u0026lt;ip\u0026gt; \u0026lt;port\u0026gt; # udp traceroute -n -U -p \u0026lt;port\u0026gt; \u0026lt;ip\u0026gt; 3.2 nc # tcp nc -zvw 5 10.30.214.22 7001 # udp nc -zvuw 5 11.53.89.7 5030 4 other 4.1 find # 查找文件并删除 find . -type f -name \u0026#39;*flac\u0026#39; -print0| xargs -0 rm -f # 查看所有文件的文件类型 find . -type f -exec file \u0026#34;{}\u0026#34; \u0026#34;;\u0026#34; | awk -F \u0026#39;: \u0026#39; \u0026#39;$2 !~ /ASCII/ {print $1 \u0026#34;: \u0026#34; $2}\u0026#39; # 将目录内所有的 crlf 文件转为 lf find . -type f -exec file \u0026#34;{}\u0026#34; \u0026#34;;\u0026#34; | awk -F \u0026#39;: \u0026#39; \u0026#39;$2 !~ /ASCII/ {print $1 \u0026#34;: \u0026#34; $2}\u0026#39; | grep CRLF | awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; | xargs dos2unix 4.2 tar xz 多核压缩\n# 多核压缩 tar cf - linux-3.10.0-327.36.4.el7/ | xz -4e -T8 \u0026gt; linux-3.10.0-327.36.4.el7.tar.xz rpm -qpi \u0026lt;rpm_pkg\u0026gt; --changelog rpm -qi \u0026lt;installed_pkg\u0026gt; --changelog cat /root/rpmbuild/SOURCES/openssh-5.8p1-packet.patch | patch -p1 -b --suffix .packet --fuzz=0 4.3 rsync # 将 test1 目录下的所有文件和目录复制进 test2, 如果 test1 后面没有跟 /, 则表示将 test1 目录复制进 test2 rsync -avuzc test1/* test2/ # 使 test1 与 test2 目录完全同步 rsync -avzc --delete test1/* test2/ 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-offen-used-command/","summary":"1 文本处理 1.1 sed 截取 rpm 包名 cat rpms | sed -e s/-[[:digit:]]./@/ | awk -F \u0026#39;@\u0026#39; \u0026#39;{print $1}\u0026#39; 1.2 awk # 打印某列之后的所有列 awk ‘{ $1=\u0026#34;\u0026#34;; print $0 }’ file_name 1.3 grep # 去除注释和空行 grep -Ev \u0026#39;^$|#\u0026#39; filename # 查找一个目录中不包含 \u0026#34;description\u0026#34; 的文件 grep -Lr \u0026#34;description\u0026#34; /your/directory/* # 查找一个目录中包含 \u0026#34;title\u0026#34; 但是不包含 \u0026#34;description\u0026#34; 的文件 grep -r -l \u0026#39;title\u0026#39; /your/directory/* | xargs grep -L \u0026#39;description\u0026#39; 2 系统进程 2.1 ps # 查看获取服务器内占用内存较高的10个进程 ps aux |","title":"linux | 常用命令总结"},{"content":"0 前言 感谢 piclist 作者的 不吝解答\n最近从 typora 迁移到了 obsidian, typora 可以很方便的自动调用 picgo 实现图片上传, obsidian 得益于丰富的插件市场, 可以通过 Image Auto Upload Plugin 插件调用 picgo, 但是必须手动启动 picgo 后才能正常使用\n在插件配置的注释中发现了 piclist, 经了解发现这个二开版本支持 docker 部署, 综合考虑了一下还是值得折腾一下的, 既能避免手动打开 picgo 的繁琐, 也可以在我所有的 pc 上卸载掉一个软件, 同时还能水一文\n注意本文以已有服务器/ip/域名且 web 服务使用 nginx 为前提, 如果不满足上述前提, 需要将 piclist 的 36677 端口映射到主机, 部署完 piclist 后直接通过 ip 加端口的形式调用即可\n1 部署 1.1 piclist 配置 docker-compose.yml 中添加如下配置\nversion: \u0026#39;3.1\u0026#39; services: piclist: image: \u0026#39;kuingsmile/piclist:v1.7.0\u0026#39; container_name: piclist restart: always networks: blog_net: ipv4_address: 172.19.0.5 volumes: - \u0026#39;$PWD/data/piclist:/root/.piclist\u0026#39; # 需要设置 piclist_key 环境变量 command: node /usr/local/bin/picgo-server -k ${piclist_key} networks: blog_net: driver: bridge ipam: config: - subnet: 172.19.0.0/16 添加环境变量并启动 piclist 容器, 此环境变量用于 client(obsidian) 和 piclist server 之间的鉴权\n# 将 123456 设置为自定义的密码 cat \u0026gt;\u0026gt; ${HOME}/.bash_profile \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; export piclist_key=\u0026#39;123456\u0026#39; EOF source ${HOME}/.bash_profile docker-compose up -d 修改 data/piclist/config.json 的配置, 以阿里云 OSS 为例添加图床配置, 内容自行修改, 官方没有配置文件的详细文档, 可以折中一下, 先 windows 安装 piclist, 测试无误后导出配置\n{ \u0026#34;picBed\u0026#34;: { \u0026#34;current\u0026#34;: \u0026#34;aliyun\u0026#34;, \u0026#34;uploader\u0026#34;: \u0026#34;aliyun\u0026#34;, \u0026#34;aliyun\u0026#34;: { \u0026#34;accessKeyId\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;accessKeySecret\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;lvbibir-image\u0026#34;, \u0026#34;area\u0026#34;: \u0026#34;oss-cn-beijing\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;blog/\u0026#34;, \u0026#34;customUrl\u0026#34;: \u0026#34;https://image.lvbibir.cn\u0026#34;, \u0026#34;options\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;picgoPlugins\u0026#34;: {} } 最后再重启一下 piclist\ndocker restart piclist 1.2 nginx 配置 nginx 中添加如下 location 配置\nlocation /piclist/ { proxy_pass http://172.19.0.5:36677/; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Real-Port $remote_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $host; proxy_set_header X-NginX-Proxy true; proxy_set_header Accept-Encoding \u0026#34;br\u0026#34;; } 执行 docker restart nginx-proxy 重启 nginx\n最后修改 obsidian 的 Image auto upload Plugin 插件的配置\n打开远程服务器模式 将接口 url 设置为 https://\u0026lt;你的域名\u0026gt;/piclist/upload?key=\u0026lt;你的key\u0026gt;, 这里的 key 就是启动容器时配置的环境变量的值, 需注意如果 key 中有特殊字符需要 url 转义一下 最后测试一下图片上传即可, 如果有报错可以通过 docker logs -f piclist 查看日志\n2 常见问题 2.1 上传失败 obsdian 直接提示上传失败, 可能是 key 中有特殊字符没有转义或者没有打开远程服务器模式 日志中有如下 Unauthorized access 报错, 一般是 key 不匹配 2.2 忘记 piclist_key 如果已经启动了的容器可以通过如下命令查看\ndocker exec -it piclist ps -ef | grep -v grep | grep node 以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/docker-deploy-piclist/","summary":"0 前言 感谢 piclist 作者的 不吝解答 最近从 typora 迁移到了 obsidian, typora 可以很方便的自动调用 picgo 实现图片上传, obsidian 得益于丰富的插件市场, 可以通过 Image Auto Upload Plugin 插件调用 picgo, 但是必须手动启动 picgo 后才能正常使用 在插件配置的注释中发现了 piclist, 经了解发现这个二开版本支持 docker 部署, 综合考虑了一下还是值得折腾一下的, 既能避免手动打开 picgo 的繁","title":"docker 部署 piclist"},{"content":"脚本内容如下, 替换钉钉 bot 的 token, 将脚本放至 crontab 执行即可\n#!/bin/bash export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin # 设置要检测的网页URL urls=(\u0026#34;https://emp.cnpc.com.cn/index.html\u0026#34; \u0026#34;https://mdm.cnpc.com.cn/\u0026#34;) #urls=(\u0026#34;https://emp.cnpc.com.cn/index.html\u0026#34; \u0026#34;https://mdm.cnpc.com.cn/\u0026#34; \u0026#34;https://www.956100.com\u0026#34; \u0026#34;https://mm.956100.com\u0026#34; \u0026#34;https://app.956100.com\u0026#34;) # 钉钉机器人的 webhook 地址 webhook=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=******************************\u0026#34; # 最大连续无法访问次数 max_attempts=3 # 设置并发进程数为 URL 数量 max_concurrent=${#urls[@]} # 初始化计数器 completed=0 for url in \u0026#34;${urls[@]}\u0026#34;; do # 在后台启动一个子进程进行测试 ( attempts=0 while [ $attempts -lt $max_attempts ]; do # 使用curl获取网页内容，并保存HTTP状态码到变量response_code response_code=$(curl -s --connect-timeout 5 -o /dev/null -w \u0026#34;%{http_code}\u0026#34; \u0026#34;$url\u0026#34;) # 判断HTTP状态码来确定网页是否可访问 if [ \u0026#34;$response_code\u0026#34; -eq 200 ]; then break else attempts=$((attempts + 1)) fi if [ $attempts -ge $max_attempts ]; then message=\u0026#34;告警: $(date +\u0026#34;%Y年%m月%d日-%H:%M:%S\u0026#34;) ${url} 网页无法访问，HTTP状态码: $response_code\u0026#34; curl -X POST ${webhook} -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;{\\\u0026#34;msgtype\\\u0026#34;: \\\u0026#34;text\\\u0026#34;, \\\u0026#34;text\\\u0026#34;: {\\\u0026#34;content\\\u0026#34;:\\\u0026#34;$message\\\u0026#34;}}\u0026#34; break fi sleep 60 # 等待 20 秒后再次尝试 done completed=$((completed + 1)) ) \u0026amp; # 控制并发进程数 if [ $completed -ge $max_concurrent ]; then wait completed=0 fi done # 等待剩余的并发进程完成 wait exit 0 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-check-website-available/","summary":"脚本内容如下, 替换钉钉 bot 的 token, 将脚本放至 crontab 执行即可 #!/bin/bash export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin # 设置要检测的网页URL urls=(\u0026#34;https://emp.cnpc.com.cn/index.html\u0026#34; \u0026#34;https://mdm.cnpc.com.cn/\u0026#34;) #urls=(\u0026#34;https://emp.cnpc.com.cn/index.html\u0026#34; \u0026#34;https://mdm.cnpc.com.cn/\u0026#34; \u0026#34;https://www.956100.com\u0026#34; \u0026#34;https://mm.956100.com\u0026#34; \u0026#34;https://app.956100.com\u0026#34;) # 钉钉机器人的 webhook 地址 webhook=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=******************************\u0026#34; # 最大连续无法访问次数 max_attempts=3 # 设置并发进程数为 URL 数量 max_concurrent=${#urls[@]} # 初始化计数器 completed=0 for url in \u0026#34;${urls[@]}\u0026#34;; do # 在后台启动一个子进程进行测试 ( attempts=0 while [ $attempts -lt $max_attempts ]; do # 使用curl获取网页内容，并保存HTTP","title":"shell | 检测网站存活并自动钉钉告警"},{"content":"0 前言 本文实现被检测主机到特定 ip 的特定端口的连通性, 通过 nc 命令测试端口可用性, 当 nc 超时时自动执行 traceroute 追踪路由定位网络故障点, 本文的案例是监控我们生产的短信业务服务器到运营商提供的短信接口之间的连通性.\n环境信息:\nCentOS 7.6 Zabbix 3.4 确保需要检测端口连通性的服务器安装了 nc 及 traceroute\n1 服务器配置 每个需要检测的服务器都要做如下操作\n修改 traceroute 权限\nchmod u+s /usr/bin/traceroute 创建检测脚本, 脚本传入三个参数: nc 命令的超时时间, 要检测的 ip 以及端口, traceroute 日志保存到 /var/log/smslink_monitor 目录\nvim /etc/zabbix/zabbix_agentd.d/smslink.sh #!/bin/bash start_time=$(date +\u0026#34;%Y%m%d-%H%M%S\u0026#34;) timeout=$1 ip=$2 port=$3 LOG_DIRECTORY=/var/log/smslink_monitor [ -d \u0026#34;${LOG_DIRECTORY}\u0026#34; ] || mkdir -p \u0026#34;${LOG_DIRECTORY}\u0026#34; # 开始 nc 测试连通性 nc_result=$(/bin/nc -z -w ${timeout} ${ip} ${port}; echo $?) echo ${nc_result} # 如果 nc 测试失败则执行 traceroute if [ ${nc_result} -ne 0 ]; then log_file=\u0026#34;${LOG_DIRECTORY}/${start_time}-${ip}-${port}.log\u0026#34; /bin/nohup /usr/bin/traceroute -n -T -p ${port} ${ip} \u0026gt; ${log_file} 2\u0026gt;\u0026amp;1 \u0026amp; fi exit 0 此步可以略过, 如果用 root 用户执行过脚本进行测试, 需要执行一下, 因为 root 用户执行过脚本后, 这个目录的属主将是 root, 而 zabbix 执行脚本使用的是 zabbix 用户, 会导致没有权限写入\nchown zabbix:zabbix /var/log/smslink_monitor 新增 zabbix agent 配置文件, 通过调用我们刚才创建的脚本实现\nvim /etc/zabbix/zabbix_agentd.d/smslink.conf # $1: timeout # $2: ip # $3: port UserParameter=get_smslink_status[*],/bin/sh /etc/zabbix/zabbix_agentd.d/smslink.sh $1 $2 $3 重启 zabbix agent\nsystemctl restart zabbix-agent 在 zabbix server 端测试监控项, * 为占位符, 改成自己的实际 ip, 返回值 0 为正常, 其他值为异常\n[root@klmy-kfyy-jxwh-0003 ~]# zabbix_get -s 192.168.4.72 -p 10050 -k get_smslink_status[3,10.**.**.22,7001] 0 [root@klmy-kfyy-jxwh-0003 ~]# zabbix_get -s 192.168.4.73 -p 10050 -k get_smslink_status[3,10.**.**.22,7001] 0 [root@klmy-kfyy-jxwh-0003 ~]# zabbix_get -s 192.168.4.74 -p 10050 -k get_smslink_status[3,10.**.**.22,7001] 0 改成一个错误的端口, 可以看到返回值变成了 1\n[root@klmy-kfyy-jxwh-0003 ~]# zabbix_get -s 192.168.4.72 -p 10050 -k get_smslink_status[3,10.**.**.22,7002] 1 在 4.72 上看下 traceroute 日志输出\n[root@klmy-kfyy-dxpt-0003 ~]# ls -ltr /var/log/smslink_monitor/ -rw-rw-r-- 1 zabbix zabbix 595 Dec 7 11:21 20231207-112058-10.**.**.22-7002.log [root@klmy-kfyy-dxpt-0003 ~]# cat /var/log/smslink_monitor/20231207-112058-10.**.**.22-7002.log traceroute to 10.**.**.22 (10.**.**.22), 30 hops max, 60 byte packets 1 * * * 2 * * * 3 100.32.34.2 5.202 ms 5.198 ms 5.187 ms 4 * * * 5 11.54.13.201 6.103 ms 6.076 ms 6.074 ms 6 11.54.13.1 6.873 ms 2.104 ms 1.598 ms 7 * * * 8 10.33.253.129 2.086 ms 3.160 ms 3.110 ms 9 10.11.1.153 71.703 ms 70.821 ms 69.009 ms 10 10.33.0.82 77.203 ms 69.230 ms 69.315 ms 11 * * * 12 * * * 13 * * * 可以看到中断的点, 这里中断是因为我们集团广域网没开 7002 端口的策略, 所以到广域网直接断掉了\n2 zabbix 配置 2.1 创建模板 选择链接的主机或者主机群组\n2.2 创建应用集 2.3 创建监控项 八条链路都创建一下\n在最新数据处看下键值获取是否正常\n2.4 创建触发器 这里的表达式代表如果连续的两个值的最小值不为 0 则触发告警, 即连续两次值都不为 0 触发告警, 这是考虑到整体网络比较复杂, 网络波动可能会导致误报\n八条链路分别添加一下触发器\n2.5 配置告警动作 2.6 测试验证 我这里是找网络侧的同事中断了一下链路进行测试的, 各位可以自行选择适合自己的方法\n2.7 添加聚合图形 最后可以添加一个聚合图形, 方便后续查看\n在模板处添加图形\n在 监测中 -\u0026gt; 聚合图形 处添加聚合图形, 三台主机, 选择一行显示出来\n进入刚才创建的聚合图形, 选择右上角的编辑聚合图形, 然后点击 更改 添加图形\n把三个节点都添加一下\n最后将此聚合图形通过右上角的按钮添加到常用, 就可以在首页直接点击进来了\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/zabbix-tcp-port-monitor/","summary":"0 前言 本文实现被检测主机到特定 ip 的特定端口的连通性, 通过 nc 命令测试端口可用性, 当 nc 超时时自动执行 traceroute 追踪路由定位网络故障点, 本文的案例是监控我们生产的短信业务服务器到运营商提供的短信接口之间的连通性. 环境信息: CentOS 7.6 Zabbix 3.4 确保需要检测端口连通性的服务器安装了 nc 及 traceroute 1 服务器配置 每个需要检","title":"Zabbix | 监控端口连通性并自动追踪 TCP 路由"},{"content":"0 前言 最近注意到 windows 系统中当 onedrive 和 clash 同时开机自启时会导致 onedrive 无法自动登录, 需要退出 onedrive 重新启动一下才能正常登录.\n出现这个问题的原因是 onedrive 启动速度要比 clash 快, 导致 onedrive 启动时访问不到 clash. 其实只要将这两个其中一个不设置为开机自启即可解决, 但是这两个都是刚需, 放下任何一个都会不舒服.\n一番 google 下来, 大部分的解决方案都是添加 windows 的计划任务, 我尝试了半天也没办法无法成功, 最后终于找到了满足需求的 解决方案, 使用 EarlyStart 实现在 windows explorer 启动前就启动自定义的软件.\n同时还能顺便解决之前感觉有点不舒服的两个问题:\nTranslucentTB: 自启动时会慢一拍, 刚进系统时任务栏没有透明, 等个几秒启动后才能正常 utools: 同样的, 进系统后第一时间不能使用, 得等几秒启动后才行 1 安装 EarlyStart 下载 EarlyStart.zip\n解压后使用管理员打开 powershell 并进入安装目录\ncd D:\\software\\1-portable\\EarlyStart # 安装 C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\InstallUtil.exe .\\EarlyStart.exe # 卸载 C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\InstallUtil.exe /u .\\EarlyStart.exe 2 配置启动项 首先将要配置快速启动的应用默认的开机自启给关掉\n在用户目录 C:\\Users\\\u0026lt;username\u0026gt; 创建一个名为 .earlystart 的文件, 每一行输入一个 exe 的路径\n然后还需要修改一下账户配置\n配置完成之后重启系统即可\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-early-auto-start/","summary":"0 前言 最近注意到 windows 系统中当 onedrive 和 clash 同时开机自启时会导致 onedrive 无法自动登录, 需要退出 onedrive 重新启动一下才能正常登录. 出现这个问题的原因是 onedrive 启动速度要比 clash 快, 导致 onedrive 启动时访问不到 clash. 其实只要将这两个其中一个不设置为开机自启即可解决, 但是这两个都是刚需, 放下任何一个都会不舒服. 一番 google 下来, 大部分的","title":"windows | 自定义开机快速启动项"},{"content":"0 前言 python 虚拟环境的重要性已经无需多言了, 目前所有支持 python 虚拟环境的工具中最好用的应该就是 conda 了, 最重要的一点是可以一键创建不同版本的 python 环境以适应不同的需求.\nAnaconda 比较臃肿, 本文使用无 GUI 的 miniconda.\n环境:\nwin10 miniconda3-py11-23.5.2-0 1 安装 安装前需要确认一下系统及用户的环境变量中不要存在中文, 在 CMD 中直接执行 path 或者 git-bash 中执行 echo $PATH 进行确认, 这个问题当时被折磨疯了, 还给 conda 项目提了 issue.\n最新版下载地址\n选好路径直接下一步即可, 没有需要注意的自定义配置项\n2 配置 2.1 环境变量 在用户环境变量 PATH 添加如下项, 我的安装路径是 D:\\miniconda, 按实际情况修改\nD:\\software\\miniconda D:\\software\\miniconda\\Scripts D:\\software\\miniconda\\Library\\bin 添加完后重启系统, 让系统重新读取一下环境变量\n2.2 conda 配置 参考链接\nminiconda 默认没有 .condarc 配置文件, 需要生成一下\nconda config --set show_channel_urls yes .condarc 会生成到用户目录下\n$ cat .condarc channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ # 不自动激活 base 环境 auto_activate_base: false # 虚拟环境存放路径 envs_dirs: - D:\\software\\python\\envs # pkg 存放路径 pkgs_dirs: - D:\\software\\python\\pkgs 上述配置文件中主要配置了三项: conda 的清华国内源, 虚拟环境和 pkg 的存储路径\n如不配置创建虚拟环境时可能会生成到用户目录下, 导致系统盘臃肿, 建议新建一个目录专门存放\n2.3 pip 配置 系统中直接安装的 python, 其 pip 的配置文件一般存放在用户目录的 .pip/pip.ini, 使用 conda 创建的虚拟环境的 pip 则不同, 可以使用如下命令查看, 这个问题当时也折磨了我很久\n$ pip -v config list For variant \u0026#39;global\u0026#39;, will try loading \u0026#39;C:\\ProgramData\\pip\\pip.ini\u0026#39; For variant \u0026#39;user\u0026#39;, will try loading \u0026#39;C:\\Users\\lvbibir\\pip\\pip.ini\u0026#39; For variant \u0026#39;user\u0026#39;, will try loading \u0026#39;C:\\Users\\lvbibir\\AppData\\Roaming\\pip\\pip.ini\u0026#39; For variant \u0026#39;site\u0026#39;, will try loading \u0026#39;D:\\software\\miniconda\\pip.ini\u0026#39; 这里我们使用用户目录存放配置文件, 默认也是没有的\n$ cat pip/pip.ini [global] timeout = 6000 index-url = http://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn proxy=http://127.0.0.1:7890 配置 pip 使用国内的清华源, 最后一条 proxy 可以不写, 这个问题是因为我常开代理, pip 默认用 https 访问系统代理, 导致 pip 报错.\n2.4 管理虚拟环境 上述步骤做完后就可以正式使用 conda 创建虚拟环境了\n用管理员打开 powershell 使用如下命令初始化 conda\nconda init powershell conda init cmd conda init bash 之后重新打开终端, 创建你的虚拟环境, -n 表示虚拟环境的名字, 不指定 python 版本默认最新\nconda create -n py37 python=3.7 激活虚拟环境\nconda activate py37 退出虚拟环境\nconda deactivate 查看虚拟环境列表\nconda env list 删除虚拟环境\nconda env remove -n py37 --all 3 其他 conda 最为人诟病的点应该是包管理跟 pip 可能会产生一些冲突, conda 官方给出的最佳方案是\n全程使用 conda install 来安装模块, 实在不行再用 pip 使用 conda 创建完虚拟环境后, 一直用 pip 来管理模块 pip 应使用 –upgrade-strategy only-if-needed 参数运行, 以防止通过 conda 安装的软件包进行不必要的升级. 这是运行 pip 时的默认设置, 不应更改 不要将 pip 与 –user 参数一起使用，避免所有用户安装 总结一下就是不要来回地用 pip 和 conda, 专一一点 (笑\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-miniconda/","summary":"0 前言 python 虚拟环境的重要性已经无需多言了, 目前所有支持 python 虚拟环境的工具中最好用的应该就是 conda 了, 最重要的一点是可以一键创建不同版本的 python 环境以适应不同的需求. Anaconda 比较臃肿, 本文使用无 GUI 的 miniconda. 环境: win10 miniconda3-py11-23.5.2-0 1 安装 安装前需要确认一下系统及用户的环境变量中不要存在中文, 在 CMD 中直接执行 path 或者 git-bash 中执行 echo","title":"windows | miniconda 配置 python 虚拟环境"},{"content":"0 前言 分享一下如何监控某个主机上的网卡到指定 ip 的流量大小, 测试环境已安装 tcpdump 并配置了 zabbix_agent\n被检测端 ip 为 1.1.1.11, 要检测到 1.1.1.12-17 这些 ip 的出口流量\n大致流程为:\n创建一个监控脚本, 分析 1 分钟内指定网卡发送到指定 ip 的数据包大小并输出到日志文件 将该脚本放到 crontab 中, 每分钟执行一次 配置 zabbix-agent 创建数据采集脚本, 提取日志文件中的内容 添加自定义配置, 创建采集的键值 配置 zabbix-server 添加监控项 添加触发器 添加仪表盘 1 监控脚本 添加 /opt/traffic_monitor.sh\n#!/bin/bash export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin set -e # 检查是否安装了tcpdump命令 if which tcpdump \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then # 如果已安装，则不进行任何提示 : else echo \u0026#34;系统中未安装 tcpdump 命令，请先安装 tcpdump。\u0026#34; exit 1 fi # 检查是否有 tcpdump 残留进程 existing_tcpdump_pids=$(pgrep -f \u0026#34;tcpdump -i ens32 -nn dst\u0026#34;) || true # 检查 tcpdump 进程数量 tcpdump_count=$(echo \u0026#34;${existing_tcpdump_pids}\u0026#34; | wc -w) if [ \u0026#34;$tcpdump_count\u0026#34; -gt 6 ]; then # 如果数量大于 6 视为之前的进程未正确关闭, 杀死所有 tcpdump 进程 kill -9 ${existing_tcpdump_pids} fi IPLIST=(\u0026#34;1.1.1.12\u0026#34; \u0026#34;1.1.1.13\u0026#34; \u0026#34;1.1.1.14\u0026#34; \u0026#34;1.1.1.15\u0026#34; \u0026#34;1.1.1.16\u0026#34; \u0026#34;1.1.1.17\u0026#34;) LOG_DIRECTORY=/var/log/traffic_monitor LOG_FILE=${LOG_DIRECTORY}/traffic_monitor.log [ -d \u0026#34;${LOG_DIRECTORY}\u0026#34; ] || mkdir -p \u0026#34;${LOG_DIRECTORY}\u0026#34; [ -f \u0026#34;${LOG_FILE}\u0026#34; ] || touch \u0026#34;${LOG_FILE}\u0026#34; # 获取文件大小（以字节为单位） log_file_size=$(du -b \u0026#34;$LOG_FILE\u0026#34; | cut -f1) # 设置100M对应的字节数 limit_size=$((100 * 1024 * 1024)) # 检查文件大小是否大于100M if [ \u0026#34;$log_file_size\u0026#34; -gt \u0026#34;$limit_size\u0026#34; ]; then # 清空文件 echo \u0026#34;\u0026#34; \u0026gt; \u0026#34;$LOG_FILE\u0026#34; fi start_time=$(date +\u0026#34;%Y%m%d-%H%M%S\u0026#34;) for ip in \u0026#34;${IPLIST[@]}\u0026#34;; do ( # 开始 tcpdump 抓包 output_file=/tmp/monitor-${ip}-${start_time}.output nohup tcpdump -i ens32 -nn dst ${ip} and not icmp 2\u0026gt;/dev/null \u0026gt; ${output_file} \u0026amp; # 等待 60 秒后关闭 tcpdump tcpdump_pid=$! sleep 60 \u0026amp;\u0026amp; kill ${tcpdump_pid} stop_time=$(date +\u0026#34;%Y%m%d-%H%M%S\u0026#34;) # 分析流量大小, 以 KB 为单位 traffic_size=$(cat ${output_file} | awk -F\u0026#39;length \u0026#39; \u0026#39;{print $2}\u0026#39; | awk \u0026#39;{sum+=$1} END {printf \u0026#34;%.2f\u0026#34;, sum / 1024}\u0026#39;) # 删除 tcpdump 的输出文件 rm -f ${output_file} echo \u0026#34;${ip} ==== ${start_time} ----\u0026gt; ${stop_time} ===== (KB) ${traffic_size}\u0026#34; \u0026gt;\u0026gt; ${LOG_FILE} ) \u0026amp; done # 等待所有后台任务完成 wait exit 0 放到 crontab 中\n* * * * * /bin/bash /opt/traffic_monitor.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 日志文件应有类似如下输出\n$ tail -12 /var/log/traffic_monitor/traffic_monitor.log 1.1.1.14 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1964.99 1.1.1.12 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 0.23 1.1.1.17 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.16 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.15 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.13 ==== 20230804-154601 ----\u0026gt; 20230804-154701 ===== (KB) 1029.35 1.1.1.12 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 1029.49 1.1.1.14 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 0.00 1.1.1.15 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 0.00 1.1.1.16 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 1029.35 1.1.1.13 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 0.00 1.1.1.17 ==== 20230804-154701 ----\u0026gt; 20230804-154801 ===== (KB) 3086.44 2 配置 zabbix-agent 添加 /opt/zabbix_traffic_monitor.sh, 根据 ip 筛选最后一个匹配项的数值\n#!/bin/bash LOG_DIRECTORY=/var/log/traffic_monitor LOG_FILE=${LOG_DIRECTORY}/traffic_monitor.log grep \u0026#34;$1\u0026#34; \u0026#34;${LOG_FILE}\u0026#34; | awk \u0026#39;{last_column=$NF} END {print last_column}\u0026#39; 添加 /etc/zabbix/zabbix_agentd.d/get_traffic_monitor.conf 配置文件\nUserParameter=get_traffic_monitor[*],/opt/zabbix_traffic_monitor.sh $1 重启 zabbix-agent\nsystemctl restart zabbix-agent 3 配置 zabbix-server 创建监控项, 有几个 ip 创建几个监控项\n监控项测试, 此处应有值\n创建触发器, 同样的, 有几个 ip 创建几个\n仪表盘添加图形\n4 测试 找一台服务器配置多 ip\nIPADDR=1.1.1.12 NETMASK=255.255.255.0 GATEWAY=1.1.1.254 DNS1=8.8.8.8 DNS2=114.114.114.114 IPADDR1=1.1.1.13 NETMASK1=255.255.255.0 IPADDR2=1.1.1.14 NETMASK2=255.255.255.0 IPADDR3=1.1.1.15 NETMASK3=255.255.255.0 IPADDR4=1.1.1.16 NETMASK4=255.255.255.0 IPADDR5=1.1.1.17 NETMASK5=255.255.255.0 重启 network\n配置 1.1.1.11 到 1.1.1.12-17 的免密登录\nssh-keygen ssh-copy-id root@1.1.1.12 # 每个 ip 都 ssh 一下, 添加一下 hotkey ssh root@1.1.1.13 ssh root@1.1.1.14 ssh root@1.1.1.15 ssh root@1.1.1.16 ssh root@1.1.1.17 运行一个脚本模拟网络流量\n#!/bin/bash # 设置目标IP地址列表 ip_list=(\u0026#34;1.1.1.12\u0026#34; \u0026#34;1.1.1.13\u0026#34; \u0026#34;1.1.1.14\u0026#34; \u0026#34;1.1.1.15\u0026#34; \u0026#34;1.1.1.16\u0026#34; \u0026#34;1.1.1.17\u0026#34;) dd if=/dev/zero of=/tmp/test bs=1M count=1 while true; do # 生成一个随机数，范围为 0 到 5 random_index=$((RANDOM % 6)) # 随机选择一个IP target_ip=\u0026#34;${ip_list[random_index]}\u0026#34; echo ${target_ip} /usr/bin/scp /tmp/test root@${target_ip}:/tmp/ # 等待10秒 sleep 10 done 运行脚本, 应有如下输出\n过段时间后查看仪表盘, 能看到流量数据\n触发器也应正常工作\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/zabbix-ip-traffic-monitor/","summary":"0 前言 分享一下如何监控某个主机上的网卡到指定 ip 的流量大小, 测试环境已安装 tcpdump 并配置了 zabbix_agent 被检测端 ip 为 1.1.1.11, 要检测到 1.1.1.12-17 这些 ip 的出口流量 大致流程为: 创建一个监控脚本, 分析 1 分钟内指定网卡发送到指定 ip 的数据包大小并输出到日志文件 将该脚本放到 crontab 中, 每分钟执行一次 配置 zabbix-agent 创建数据采集脚本, 提取日志文","title":"Zabbix | 监控主机到指定 ip 的流量大小"},{"content":"0 前言 以 centos7 为例, 通常我们新装完操作系统后需要进行配置 yum 源, iptables, selinux, ntp 以及优化 kernel 等操作, 现分享一些较为通用的配置. 同时博主将这些配置整理成了脚本, 可以一键执行.\n1 常用配置 1.1 iptables \u0026amp; selinux sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 iptables -F systemctl disable --now firewalld 1.2 PS1 终端美化 cat \u0026gt; /etc/profile.d/PS1_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export PS1=\u0026#34;\\n[\\[\\e[31m\\]\\u\\[\\e[m\\]@\\[\\e[32m\\]\\h\\[\\e[m\\]] -\\$?- \\[\\e[33m\\]\\$(pwd)\\[\\e[m\\] \\[\\e[34m\\]\\$(date +\u0026#39;%F %T\u0026#39;)\\[\\e[m\\] \\n(\\#)$ \u0026#34; EOF source /etc/profile.d/PS1_conf.sh 1.3 history 格式化 cat \u0026gt; /etc/profile.d/history_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 写入文件 export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups 不记录“重复”的命令；连续且相同 方为“重复” export PROMPT_COMMAND=\u0026#34;history -a\u0026#34; # 设置每条命令执行完立即写入HISTFILE(默认等待退出会话写入) export HISTTIMEFORMAT=\u0026#34;$(whoami) %F %T \u0026#34; # 设置命令执行时间格式，记录文件增加时间戳 shopt -s histappend # 防止会话退出时覆盖其他会话写到HISTFILE的内容 EOF source /etc/profile.d/history_conf.sh 1.4 ssh 公钥 mkdir /root/.ssh || true chmod 700 /root/.ssh cat \u0026gt; /root/.ssh/authorized_keys \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCeQZmPg93SNx6zzR/l4RiPnHtFPbDTSOL7AtJOIvrlMm300x1OM8a48VqYuKEx7B7WM7UhszVndg8efJv9UdOtOaa0o8L0Wd2uujn2rFKKok69c5i7c/jmU1my9MkEsKpkx1MHQWVZTFqayv/DB9L5GaE/ShChsTSlXoQ6rc6JC4k1zgSsoNSTLwPrbZcDOZWprt/AOhqCklf9mL1E50WTx9XsjxBLqJIwwVEzmHAhzIiVowjBKjJpQ6hEvygCz67gNVn0vAvHPvCz3amrkCQa333Z9r8tbY7mJpq2Anj4qWtlnL9kHreVK6YoKGvM8+DrbVoT5/zM7wMZ+tdLmreUsu4OhgDkE4IgUMHWQ3T1GyD1EjCkqCdSfJbrLaAR8v7g92uDXO5irIyYMc/iQJ8v4okus9Iid61zFF0SPgZEykOVfT7jJqH0a/630D41uD0TK90v5PicVdh1FfEfok8P4F4UHGLUly2jRVBESQ/TXVGPaMITHPEtYEpmT3kmnOk= 15810243114@163.com EOF chmod 600 /root/.ssh/authorized_keys 1.5 加速 ssh 连接 echo \u0026#34;UseDNS no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 1.6 配置 yum 源 以实测最快的清华源为例\nmkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true cat \u0026gt; /etc/yum.repos.d/centos-tuna.repo \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [base] name=CentOS-$releasever - Base baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ gpgcheck=0 [updates] name=CentOS-$releasever - Updates baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ gpgcheck=0 [extras] name=CentOS-$releasever - Extras baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ gpgcheck=0 EOF yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion ntpdate 1.7 时间配置 timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com 1.8 limit cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; * soft nofile 65535 * hard nofile 65535 * soft nproc 65535 * hard nproc 65535 EOF 1.9 kernel cat \u0026gt;\u0026gt; /etc/sysctl.d/99-sysctl.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # 关闭ipv6 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 # 避免放大攻击 net.ipv4.icmp_echo_ignore_broadcasts = 1 # 开启恶意icmp错误消息保护 net.ipv4.icmp_ignore_bogus_error_responses = 1 # 开启反向路径过滤 net.ipv4.conf.all.rp_filter = 1 net.ipv4.conf.default.rp_filter = 1 # 关闭sysrq功能 kernel.sysrq = 0 # core文件名中添加pid作为扩展名 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 # 修改消息队列长度 kernel.msgmnb = 65536 kernel.msgmax = 65536 # 设置最大内存共享段大小bytes kernel.shmmax = 68719476736 kernel.shmall = 4294967296 # timewait的数量，默认180000 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.core.netdev_max_backlog = 262144 # 限制仅仅是为了防止简单的DoS 攻击 net.ipv4.tcp_max_orphans = 3276800 # 收到客户端确认信息的连接请求的最大值 net.ipv4.tcp_max_syn_backlog = 262144 net.ipv4.tcp_timestamps = 0 # 内核放弃建立连接之前发送SYNACK 包的数量 net.ipv4.tcp_synack_retries = 1 # 内核放弃建立连接之前发送SYN 包的数量 net.ipv4.tcp_syn_retries = 1 # 启用timewait 快速回收 net.ipv4.tcp_tw_recycle = 1 # 开启重用。允许将TIME-WAIT sockets 重新用于新的TCP连接 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_fin_timeout = 1 # 当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时 net.ipv4.tcp_keepalive_time = 30 # 修改防火墙表大小，默认65536 #net.netfilter.nf_conntrack_max=655350 #net.netfilter.nf_conntrack_tcp_timeout_established=1200 EOF sysctl -p 2 一键脚本 #!/bin/bash set -e if [ \u0026#34;$(id -u)\u0026#34; -ne 0 ]; then echo \u0026#34;当前用户不是管理员。请使用管理员权限运行此脚本。\u0026#34; exit 1 fi echo \u0026#34;========start=============\u0026#34; function setup_selinux_firewalld() { echo \u0026#34;========selinux===========\u0026#34; sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 echo \u0026#34;========firewalld=========\u0026#34; iptables -F systemctl disable --now firewalld } function setup_format_history() { echo \u0026#34;========history format========\u0026#34; cat \u0026gt; /etc/profile.d/history_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 写入文件 export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups 不记录“重复”的命令；连续且相同 方为“重复” export PROMPT_COMMAND=\u0026#34;history -a\u0026#34; # 设置每条命令执行完立即写入HISTFILE(默认等待退出会话写入) export HISTTIMEFORMAT=\u0026#34;$(whoami) %F %T \u0026#34; # 设置命令执行时间格式，记录文件增加时间戳 shopt -s histappend # 防止会话退出时覆盖其他会话写到HISTFILE的内容 EOF source /etc/profile.d/history_conf.sh } function setup_ssh() { echo \u0026#34;========add ssh key========\u0026#34; mkdir /root/.ssh || true chmod 700 /root/.ssh cat \u0026gt; /root/.ssh/authorized_keys \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCeQZmPg93SNx6zzR/l4RiPnHtFPbDTSOL7AtJOIvrlMm300x1OM8a48VqYuKEx7B7WM7UhszVndg8efJv9UdOtOaa0o8L0Wd2uujn2rFKKok69c5i7c/jmU1my9MkEsKpkx1MHQWVZTFqayv/DB9L5GaE/ShChsTSlXoQ6rc6JC4k1zgSsoNSTLwPrbZcDOZWprt/AOhqCklf9mL1E50WTx9XsjxBLqJIwwVEzmHAhzIiVowjBKjJpQ6hEvygCz67gNVn0vAvHPvCz3amrkCQa333Z9r8tbY7mJpq2Anj4qWtlnL9kHreVK6YoKGvM8+DrbVoT5/zM7wMZ+tdLmreUsu4OhgDkE4IgUMHWQ3T1GyD1EjCkqCdSfJbrLaAR8v7g92uDXO5irIyYMc/iQJ8v4okus9Iid61zFF0SPgZEykOVfT7jJqH0a/630D41uD0TK90v5PicVdh1FfEfok8P4F4UHGLUly2jRVBESQ/TXVGPaMITHPEtYEpmT3kmnOk= 15810243114@163.com EOF chmod 600 /root/.ssh/authorized_keys echo \u0026#34;=========setup ssh========\u0026#34; echo \u0026#34;UseDNS no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config } function setup_yum() { echo \u0026#34;====backup repo===========\u0026#34; mkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true echo \u0026#34;====configure tuna repo====\u0026#34; cat \u0026gt; /etc/yum.repos.d/centos-tuna.repo \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [base] name=CentOS-$releasever - Base baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ gpgcheck=0 [updates] name=CentOS-$releasever - Updates baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ gpgcheck=0 [extras] name=CentOS-$releasever - Extras baseurl=http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ gpgcheck=0 EOF echo \u0026#34;====upgrade yum============\u0026#34; yum clean all yum makecache fast echo \u0026#34;====dowload tools=========\u0026#34; yum install -y wget net-tools vim bash-completion ntpdate } function setup_time_limit() { echo \u0026#34;=======setup timezone and ntp======\u0026#34; timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com echo \u0026#34;=======modify limit=========\u0026#34; cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; * soft nofile 65535 * hard nofile 65535 * soft nproc 65535 * hard nproc 65535 EOF } function setup_kernel() { echo \u0026#34;========Optimize kernel========\u0026#34; cat \u0026gt;\u0026gt; /etc/sysctl.d/99-sysctl.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # 关闭ipv6 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 # 避免放大攻击 net.ipv4.icmp_echo_ignore_broadcasts = 1 # 开启恶意icmp错误消息保护 net.ipv4.icmp_ignore_bogus_error_responses = 1 # 开启反向路径过滤 net.ipv4.conf.all.rp_filter = 1 net.ipv4.conf.default.rp_filter = 1 # 关闭sysrq功能 kernel.sysrq = 0 # core文件名中添加pid作为扩展名 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 # 修改消息队列长度 kernel.msgmnb = 65536 kernel.msgmax = 65536 # 设置最大内存共享段大小bytes kernel.shmmax = 68719476736 kernel.shmall = 4294967296 # timewait的数量，默认180000 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.core.netdev_max_backlog = 262144 # 限制仅仅是为了防止简单的DoS 攻击 net.ipv4.tcp_max_orphans = 3276800 # 收到客户端确认信息的连接请求的最大值 net.ipv4.tcp_max_syn_backlog = 262144 net.ipv4.tcp_timestamps = 0 # 内核放弃建立连接之前发送SYNACK 包的数量 net.ipv4.tcp_synack_retries = 1 # 内核放弃建立连接之前发送SYN 包的数量 net.ipv4.tcp_syn_retries = 1 # 启用timewait 快速回收 net.ipv4.tcp_tw_recycle = 1 # 开启重用。允许将TIME-WAIT sockets 重新用于新的TCP连接 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_fin_timeout = 1 # 当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时 net.ipv4.tcp_keepalive_time = 30 # 修改防火墙表大小，默认65536 #net.netfilter.nf_conntrack_max=655350 #net.netfilter.nf_conntrack_tcp_timeout_established=1200 EOF sysctl -p } setup_selinux_firewalld setup_format_history setup_ssh setup_yum setup_time_limit setup_kernel echo \u0026#34;=========finish============\u0026#34; exit 0 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-centos-init/","summary":"0 前言 以 centos7 为例, 通常我们新装完操作系统后需要进行配置 yum 源, iptables, selinux, ntp 以及优化 kernel 等操作, 现分享一些较为通用的配置. 同时博主将这些配置整理成了脚本, 可以一键执行. 1 常用配置 1.1 iptables \u0026amp; selinux sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 iptables -F systemctl disable --now firewalld 1.2 PS1 终端美化 cat \u0026gt; /etc/profile.d/PS1_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export PS1=\u0026#34;\\n[\\[\\e[31m\\]\\u\\[\\e[m\\]@\\[\\e[32m\\]\\h\\[\\e[m\\]] -\\$?- \\[\\e[33m\\]\\$(pwd)\\[\\e[m\\] \\[\\e[34m\\]\\$(date +\u0026#39;%F %T\u0026#39;)\\[\\e[m\\] \\n(\\#)$ \u0026#34; EOF source /etc/profile.d/PS1_conf.sh 1.3 history 格式化 cat \u0026gt; /etc/profile.d/history_conf.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 写入文件 export HISTSIZE=1000","title":"shell | centos 初始化"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0\n这次部署的 loki 整体架构如下, loki 使用 statefulset 的方式运行, promtail 以 daemonset 的方式运行在 k8s 集群的每个节点.\n1 promtail 1.1 部署 namespace\napiVersion: v1 kind: Namespace metadata: name: logging rbac\napiVersion: v1 kind: ServiceAccount metadata: name: loki-promtail labels: app: promtail namespace: logging --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: app: promtail name: promtail-clusterrole namespace: logging rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: promtail-clusterrolebinding labels: app: promtail namespace: logging subjects: - kind: ServiceAccount name: loki-promtail namespace: logging roleRef: kind: ClusterRole name: promtail-clusterrole apiGroup: rbac.authorization.k8s.io configmap\napiVersion: v1 kind: ConfigMap metadata: name: loki-promtail namespace: logging labels: app: promtail data: promtail.yaml: | client: # 配置Promtail如何连接到Loki的实例 backoff_config: # 配置当请求失败时如何重试请求给Loki max_period: 5m max_retries: 10 min_period: 500ms batchsize: 1048576 # 发送给Loki的最大批次大小(以字节为单位) batchwait: 1s # 发送批处理前等待的最大时间（即使批次大小未达到最大值） external_labels: {} # 所有发送给Loki的日志添加静态标签 timeout: 10s # 等待服务器响应请求的最大时间 positions: filename: /run/promtail/positions.yaml server: http_listen_port: 3101 target_config: sync_period: 10s scrape_configs: - job_name: kubernetes-pods-name pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_label_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-app pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ source_labels: - __meta_kubernetes_pod_label_name - source_labels: - __meta_kubernetes_pod_label_app target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-direct-controllers pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ separator: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_label_name - __meta_kubernetes_pod_label_app - action: drop regex: \u0026#39;[0-9a-z-.]+-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name - source_labels: - __meta_kubernetes_pod_controller_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-indirect-controller pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ separator: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_label_name - __meta_kubernetes_pod_label_app - action: keep regex: \u0026#39;[0-9a-z-.]+-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name - action: replace regex: \u0026#39;([0-9a-z-.]+)-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-static pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror - action: replace source_labels: - __meta_kubernetes_pod_label_component target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror - __meta_kubernetes_pod_container_name target_label: __path__ daemonset\napiVersion: apps/v1 kind: DaemonSet metadata: name: loki-promtail namespace: logging labels: app: promtail spec: selector: matchLabels: app: promtail updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: promtail spec: serviceAccountName: loki-promtail containers: - name: promtail image: grafana/promtail:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/etc/promtail/promtail.yaml - -client.url=http://loki:3100/loki/api/v1/push env: - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName volumeMounts: - mountPath: /etc/promtail name: config - mountPath: /run/promtail name: run - mountPath: /var/lib/docker/containers name: docker readOnly: true - mountPath: /var/log/pods name: pods readOnly: true ports: - containerPort: 3101 name: http protocol: TCP securityContext: readOnlyRootFilesystem: true runAsGroup: 0 runAsUser: 0 readinessProbe: failureThreshold: 5 httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 tolerations: - operator: Exists volumes: - name: config configMap: defaultMode: 420 name: loki-promtail - name: run hostPath: path: /run/promtail type: \u0026#34;\u0026#34; - name: docker hostPath: path: /var/lib/docker/containers - name: pods hostPath: path: /var/log/pods 1.2 配置详解 主要解释一下 promtail 中的匹配规则, 因为采集的日志可以说非常地杂乱, 如何将应用日志分类就尤为重要, 可以说匹配规则是 promtail 的核心所在\n通常我们分类 pod 的手段基本为 namespace + labels + controller , 在 loki 中也一样, 在上述 configmap 的配置中将 k8s 中的所有 pod 分为了五类:\n定义了 label_name 未定义 label_name, 定义了 label_app 未定义 label_name \u0026amp; label_app, 由 Daemonset 控制 未定义 label_name \u0026amp; label_app, 由非 Daemonset 控制 未定义 label_name \u0026amp; label_app, 由 kubelet 直接控制 对应上述 configmap 中配置的五个 job:\nkubernetes-pods-name job=namespace/label_name kubernetes-pods-app job=namespace/label_app kubernetes-pods-direct-controllers job=namespace/controller kubernetes-pods-indirect-controllers job=namespace/controller kubernetes-pods-static job=namespace/label_component 每个指标数据将由上述规则分类, 添加一个 job 的 label\n然后基于指标数据对应 pod 的所有 label 附加到指标数据上\n- action: labelmap regex: __meta_kubernetes_pod_label_(.+) 再加上指标数据本身携带的一些 label, 我们就可以对 pod 日志做一个十分细致的区分\n2 loki rbac\napiVersion: v1 kind: ServiceAccount metadata: name: loki namespace: logging --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: loki namespace: logging rules: - apiGroups: - extensions resourceNames: - loki resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: loki namespace: logging roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: loki subjects: - kind: ServiceAccount name: loki configmap\napiVersion: v1 kind: ConfigMap metadata: name: loki namespace: logging labels: app: loki data: loki.yaml: | auth_enabled: false ingester: chunk_idle_period: 3m # 如果块没有达到最大的块大小，那么在刷新之前，块应该在内存中不更新多长时间 chunk_block_size: 262144 chunk_retain_period: 1m # 块刷新后应该在内存中保留多长时间 max_transfer_retries: 0 # Number of times to try and transfer chunks when leaving before falling back to flushing to the store. Zero = no transfers are done. lifecycler: # 配置ingester的生命周期，以及在哪里注册以进行发现 ring: kvstore: store: inmemory # 用于ring的后端存储，支持consul、etcd、inmemory replication_factor: 1 # 写入和读取的ingesters数量，至少为1（为了冗余和弹性，默认情况下为3) limits_config: enforce_metric_name: false reject_old_samples: true # 旧样品是否会被拒绝 reject_old_samples_max_age: 168h # 拒绝旧样本的最大时限 schema_config: # 配置从特定时间段开始应该使用哪些索引模式 configs: - from: 2020-10-24 # 创建索引的日期。如果这是唯一的schema_config，则使用过去的日期，否则使用希望切换模式时的日期 store: boltdb-shipper # 索引使用哪个存储，如：cassandra, bigtable, dynamodb，或boltdb object_store: filesystem # 用于块的存储，如：gcs, s3， inmemory, filesystem, cassandra，如果省略，默认值与store相同 schema: v11 index: # 配置如何更新和存储索引 prefix: index_ # 所有周期表的前缀 period: 24h # 表周期 server: http_listen_port: 3100 storage_config: # 为索引和块配置一个或多个存储 boltdb_shipper: active_index_directory: /data/loki/boltdb-shipper-active cache_location: /data/loki/boltdb-shipper-cache cache_ttl: 24h shared_store: filesystem filesystem: directory: /data/loki/chunks chunk_store_config: # 配置如何缓存块，以及在将它们保存到存储之前等待多长时间 max_look_back_period: 0s # 限制查询数据的时间，默认是禁用的，这个值应该小于或等于table_manager.retention_period中的值 table_manager: retention_deletes_enabled: true # 日志保留周期开关，用于表保留删除 retention_period: 48h # 日志保留周期，保留期必须是索引/块的倍数 compactor: working_directory: /data/loki/boltdb-shipper-compactor shared_store: filesystem ruler: storage: type: local local: directory: /etc/loki/rules/rules1.yaml rule_path: /tmp/loki/rules-temp alertmanager_url: http://alertmanager-main.monitoring.svc:9093 ring: kvstore: store: inmemory enable_api: true enable_alertmanager_v2: true statefulset service, 注意修改 storageClass 为自己的\n--- apiVersion: v1 kind: Service metadata: name: loki namespace: logging labels: app: loki spec: type: ClusterIP ports: - port: 3100 protocol: TCP name: http targetPort: http selector: app: loki --- apiVersion: apps/v1 kind: StatefulSet metadata: name: loki namespace: logging labels: app: loki spec: podManagementPolicy: OrderedReady replicas: 1 selector: matchLabels: app: loki serviceName: loki updateStrategy: type: RollingUpdate template: metadata: labels: app: loki spec: serviceAccountName: loki securityContext: fsGroup: 10001 runAsGroup: 10001 runAsNonRoot: true runAsUser: 10001 initContainers: [] containers: - name: loki image: grafana/loki:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/etc/loki/loki.yaml volumeMounts: - name: config mountPath: /etc/loki - name: storage mountPath: /data ports: - name: http-metrics containerPort: 3100 protocol: TCP livenessProbe: httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 45 timeoutSeconds: 1 periodSeconds: 10 successThreshold: 1 failureThreshold: 3 readinessProbe: httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 45 timeoutSeconds: 1 periodSeconds: 10 successThreshold: 1 failureThreshold: 3 securityContext: readOnlyRootFilesystem: true terminationGracePeriodSeconds: 4800 volumes: - name: config configMap: defaultMode: 420 name: loki volumeClaimTemplates: - metadata: name: storage labels: app: loki annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;nfs\u0026#34; # 注意修改 storageClass 名称 spec: accessModes: - ReadWriteOnce resources: requests: storage: \u0026#34;2Gi\u0026#34; 应用所有配置文件, 上述配置是 loki 针对 k8s 的一套比较标准的配置, 所以目前的配置仅能抓取 k8s 中所有 pod 发送到 stdout 和 stderr 的信息, 如果需要抓取日志文件还需另外配置.\n[root@k8s-node1 ~]# kubectl apply -f /opt/loki/ [root@k8s-node1 ~]# kubectl get pods -n logging NAME READY STATUS RESTARTS AGE loki-0 1/1 Running 0 3m29s loki-promtail-4kskw 1/1 Running 0 3m36s loki-promtail-p7qzr 1/1 Running 0 3m36s loki-promtail-wc5f7 1/1 Running 0 3m37s [root@k8s-node1 ~]# kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loki ClusterIP 10.105.115.178 \u0026lt;none\u0026gt; 3100/TCP 4m26s 3 Grafana grafana 部署请参考博主的 prometheus 系列文章\n3.1 配置 在 grafana 中添加 loki 作为 data source, 这里我的 grafana 是直接部署在 k8s 中的, 所以可以通过 \u0026lt;svc-name\u0026gt;.\u0026lt;namespace\u0026gt; 访问到 loki\n在 Explore =\u0026gt; loki =\u0026gt; {job=\u0026quot;kube-system/kube-apiserver\u0026quot;} 可以看到 k8s 的 api-server 相关日志\n3.1 光标跳动问题 在 grafana 中手动写 logQL 查询数据时总会出现光标要么一直往行首跳, 要么一直往行尾跳, Github 也有很多人遇到了同样的问题, 社区仍未解决该问题, 目前可以通过 F12 控制台输入一条指令单次地修复这个问题, 指令如下\ndocument.querySelectorAll(\u0026#34;.slate-query-field \u0026gt; div\u0026#34;)[0][\u0026#39;style\u0026#39;].removeProperty(\u0026#39;-webkit-user-modify\u0026#39;); 4 dashboard 示例 4.1 traefik traefik 部署参考博主的 traefik 系列文章\n如下图所示, 已经可以看到收集到的 traefik 日志\n我们还可以通过 dashboard 实时展示 traefik 的信息, 在 grafana 导入 13713 号模板\n此 dashboard 默认的 traefik 的采集语句是 {job=\u0026quot;/var/log/traefik.log\u0026quot;} , 我们需要按照实际情况进行修改, 这里我改成了 {app=\u0026quot;traefik/traefik\u0026quot;}\n导入修改好的 yaml, 选择数据源\n可以看到已经可以正常展示数据了\n但是还有一个小报错, 是因为这个 dashboard 依赖 grafana-piechart-panel 这个插件, 我们在 grafana 容器内执行安装插件\n[root@k8s-node1 manifests]# kubectl exec -it grafana-78bb4557f5-7rbbq -n monitoring -- grafana-cli plugins install grafana-piechart-panel ✔ Downloaded grafana-piechart-panel v1.6.4 zip successfully Please restart Grafana after installing plugins. Refer to Grafana documentation for instructions if necessary. [root@k8s-node1 manifests]# kubectl delete pod grafana-78bb4557f5-7rbbq -n monitoring pod \u0026#34;grafana-78bb4557f5-7rbbq\u0026#34; deleted 等待重建 pod, 可以看到这里已经可以正常显示了\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-loki-2-deploy/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0 这次部署的 loki 整体架构如下, loki 使用 statefulset 的方式运行, promtail 以 daemonset 的方式运行在 k8s 集群的每个节点. 1 promtail 1.1 部署 namespace apiVersion: v1 kind: Namespace metadata: name: logging rbac apiVersion: v1 kind: ServiceAccount metadata: name: loki-promtail labels: app: promtail namespace: logging --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: app: promtail name: promtail-clusterrole namespace: logging rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: promtail-clusterrolebinding labels: app: promtail namespace: logging subjects: - kind: ServiceAccount name: loki-promtail namespace: logging roleRef: kind: ClusterRole name: promtail-clusterrole apiGroup: rbac.authorization.k8s.io configmap apiVersion: v1","title":"loki (二) 部署"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0\n1 简介 项目地址 官方文档\nLoki 是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，专门为 Prometheus 和 Kubernetes 用户做了相关优化。该项目受 Prometheus 启发，官方的介绍就是： Like Prometheus, But For Logs.\n2 优缺点 与其他日志聚合系统相比， Loki 具有下面的一些特性:\n低索引开销 不对日志进行全文索引。通过存储压缩非结构化日志和仅索引元数据，Loki 操作起来会更简单，更省成本。 这样做可以大幅降低索引资源开销, es 无论你查不查，巨大的索引开销必须时刻承担 并发查询 为了弥补没有全文索引带来的查询降速使用，Loki 将把查询分解成较小的分片，可以理解为并发的 grep 和 prometheus 采用相同的标签，对接 alertmanager Loki 和 Prometheus 之间的标签一致是 Loki 的超级能力之一 受到 Grafana 原生支持 避免 kibana 和 grafana 来回切换 服务发现 支持与 prometheus 一样的服务发现功能, 特别适合储存 Kubernetes Pod 日志 无需使用日志落盘或者 sidecar 当然, 它也有一定的缺点:\n技术比较新颖，相对应的论坛不是非常活跃。 功能单一，只针对日志的查看，筛选有好的表现，对于数据的处理以及清洗没有 ELK 强大，同时与 ELK 相比，对于后期，ELK 可以连用各种技术进行日志的大数据处理，但是 loki 不行。 3 架构 3.1 整体架构 在 Loki 架构中有以下几个概念：\nGrafana：相当于 EFK 中的 Kibana ，用于 UI 的展示。 Loki：相当于 EFK 中的 ElasticSearch ，用于存储日志和处理查询。 Promtail：相当于 EFK 中的 Filebeat/Fluentd ，用于采集日志并将其发送给 Loki 。 LogQL：Loki 提供的日志查询语言，类似 Prometheus 的 PromQL，而且 Loki 支持 LogQL 查询直接转换为 Prometheus 指标。 3.2 promtail 官方文档\npromtail 是 loki 架构中最常用的采集器, 相当于 EFK 中的 filebeat/fluentd\n它的主要工作流程:\n使用 fsnotify 监听指定目录下（例如：/var/log/*.log）的文件创建与删除 对每个活跃的日志文件起一个 goroutine 进行类似 tail -f 的读取，读取到的内容发送给 channel 有一个单独的 goroutine 会读取 channel 中的日志行，分批并附加上标签后推送给 Loki 3.3 loki Loki 采用读写分离架构，关键组件有：\nDistributor 分发器：日志数据传输的“第一站”，Distributor 分发器接收到日志数据后，根据元数据和 hash 算法，将日志分批并行地发送到多个 Ingester 接收器上 Ingester 接收器：接收器是一个有状态的组件，在日志进入时对其进行 gzip 压缩操作，并负责构建和刷新 chunck 块，当 chunk 块达到一定的数量或者时间后，就会刷新 chunk 块和对应的 Index 索引存储到数据库中 Querier 查询器：给定一个时间范围和标签选择器，Querier 查询器可以从数据库中查看 Index 索引以确定哪些 chunck 块匹配，并通过 greps 将结果显示出来，它还会直接从 Ingester 接收器获取尚未刷新的最新数据 Query frontend 查询前端：查询前端是一个可选的组件，运行在 Querier 查询器之前，起到缓存，均衡调度的功能，用于加速日志查询 Loki 提供了两种部署方式：\n单体模式，ALL IN ONE：Loki 支持单一进程模式，可在一个进程中运行所有必需的组件。单进程模式非常适合测试 Loki 或以小规模运行。不过尽管每个组件都以相同的进程运行，但它们仍将通过本地网络相互连接进行组件之间的通信（grpc）。使用 Helm 部署就是采用的该模式。 微服务模式：为了实现水平可伸缩性，Loki 支持组件拆分为单独的组件分开部署，从而使它们彼此独立地扩展。每个组件都产生一个用于内部请求的 gRPC 服务器和一个用于外部 API 请求的 HTTP 服务，所有组件都带有 HTTP 服务器，但是大多数只暴露就绪接口、运行状况和指标端点。 4 日志告警 Loki 支持三种模式创建日志告警：\n在 Promtail 中的 pipeline 管道的 metrics 的阶段，根据需求增加一个监控指标，然后使用 Prometheus 结合 Alertmanager 完成监控报警。 通过 Loki 自带的报警功能（ Ruler 组件）可以持续查询一个 rules 规则，并将超过阈值的事件推送给 AlertManager 或者其他 Webhook 服务。 将 LogQL 查询转换为 Prometheus 指标。可以通过 Grafana 自带的 Alert rules \u0026amp; notifications，定义有关 LogQL 指标的报警，推送到 Notification channels（ Prometheus Alertmanager ， Webhook 等）。 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-loki-1-jianjie/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 loki-2.3.0 promtail-2.3.0 1 简介 项目地址 官方文档 Loki 是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，专门为 Prometheus 和 Kubernetes 用户做了相关优化。该项目受 Prometheus 启发，官方的介绍就是： Like Prometheus, But","title":"loki (一) 简介"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1 alertmanager prometheus 架构中采集数据和发送告警是独立出来的, 告警触发后将信息转发到独立的组件 alertmanager, 由 alertmanager 对报警进行统一处理, 最后通过接收器 recevier 发送给指定用户\n1.1 工作机制 Alertmanager 收到告警信息后:\n进行分组 group 通过定义好的路由 routing 转发到正确的接收器 recevier recevier 通过 email dingtalk wechat 等方式通知给定义好的接收人 1.2 四大功能 分组 (Grouping): 将同类型的告警进行分组, 合并多条告警到一个通知中 抑制 (Inhibition): 当某条告警已经发送, 停止重复发送由此告警引起的其他异常或者故障 静默 (Silences): 根据标签快速对告警进行静默处理, 如果告警符合静默的配置, Alertmanager 则不会发送告警通知 路由 (route): 用于配置 Alertmanager 如何处理传入的特定类型的告警通知 1.3 配置详解 global: # 经过此时间后，如果尚未更新告警，则将告警声明为已恢复。(即 prometheus 没有向 alertmanager 发送告警了) resolve_timeout: 5m # 配置发送邮件信息 smtp_smarthost: \u0026#39;smtp.qq.com:465\u0026#39; smtp_from: \u0026#39;742899387@qq.com\u0026#39; smtp_auth_username: \u0026#39;742899387@qq.com\u0026#39; smtp_auth_password: \u0026#39;password\u0026#39; smtp_require_tls: false # 读取告警通知模板的目录。 templates: - \u0026#39;/etc/alertmanager/template/*.tmpl\u0026#39; # 所有报警都会进入到这个根路由下，可以根据根路由下的子路由设置报警分发策略 route: # 先解释一下分组，分组就是将多条告警信息聚合成一条发送，这样就不会收到连续的报警了。 # 将传入的告警按标签分组(标签在 prometheus 中的 rules 中定义)，例如： # 接收到的告警信息里面有许多具有 cluster=A 和 alertname=LatencyHigh 的标签，这些个告警将被分为一个组。 # # 如果不想使用分组，可以这样写group_by: [...] group_by: [\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;] # 第一组告警发送通知需要等待的时间，这种方式可以确保有足够的时间为同一分组获取多个告警，然后一起触发这个告警信息。 group_wait: 30s # 发送第一个告警后，等待\u0026#34;group_interval\u0026#34;发送一组新告警。 group_interval: 5m # 分组内发送相同告警的时间间隔。这里的配置是每3小时发送告警到分组中。举个例子：收到告警后，一个分组被创建，等待5分钟发送组内告警，如果后续组内的告警信息相同,这些告警会在3小时后发送，但是3小时内这些告警不会被发送。 repeat_interval: 3h # 这里先说一下，告警发送是需要指定接收器的，接收器在receivers中配置，接收器可以是email、webhook、pagerduty、wechat等等。一个接收器可以有多种发送方式。 # 指定默认的接收器 receiver: team-X-mails # 下面配置的是子路由，子路由的属性继承于根路由(即上面的配置)，在子路由中可以覆盖根路由的配置 # 下面是子路由的配置 routes: # 使用正则的方式匹配告警标签 - match_re: # 这里可以匹配出标签含有 service=foo1 或 service=foo2 或 service=baz 的告警 service: ^(foo1|foo2|baz)$ # 指定接收器为 team-X-mails receiver: team-X-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出 severity=critical 的告警，并使用 team-X-pager 接收器发送告警，没有匹配到的告警会由父路由进行处理。 routes: - match: severity: critical receiver: team-X-pager # 这里也是一条子路由，会匹配出标签含有 service=files 的告警，并使用 team-Y-mails 接收器发送告警 - match: service: files receiver: team-Y-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出 severity=critical 的告警，并使用 team-Y-pager 接收器发送告警，没有匹配到的会由父路由进行处理。 routes: - match: severity: critical receiver: team-Y-pager # 该路由处理来自数据库服务的所有警报。如果没有团队来处理，则默认为数据库团队。 - match: # 首先匹配标签service=database service: database # 指定接收器 receiver: team-DB-pager # 根据受影响的数据库对告警进行分组 group_by: [alertname, cluster, database] routes: - match: owner: team-X receiver: team-X-pager # 告警是否继续匹配后续的同级路由节点，默认false，下面如果也可以匹配成功，会向两种接收器都发送告警信息(猜测。。。) continue: true - match: owner: team-Y receiver: team-Y-pager # 下面是关于inhibit(抑制)的配置，先说一下抑制是什么：抑制规则允许在另一个警报正在触发的情况下使一组告警静音。其实可以理解为告警依赖。比如一台数据库服务器掉电了，会导致db监控告警、网络告警等等，可以配置抑制规则如果服务器本身down了，那么其他的报警就不会被发送出来。 inhibit_rules: #下面配置的含义：当有多条告警在告警组里时，并且他们的标签alertname,cluster,service都相等，如果severity: \u0026#39;critical\u0026#39;的告警产生了，那么就会抑制severity: \u0026#39;warning\u0026#39;的告警。 - source_match: # 源告警(我理解是根据这个报警来抑制target_match中匹配的告警) severity: \u0026#39;critical\u0026#39; # 标签匹配满足severity=critical的告警作为源告警 target_match: # 目标告警(被抑制的告警) severity: \u0026#39;warning\u0026#39; # 告警必须满足标签匹配severity=warning才会被抑制。 equal: [\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;] # 必须在源告警和目标告警中具有相等值的标签才能使抑制生效。(即源告警和目标告警中这三个标签的值相等\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;) # 下面配置的是接收器 receivers: # 接收器的名称、通过邮件的方式发送、 - name: \u0026#39;team-X-mails\u0026#39; email_configs: # 发送给哪些人 - to: \u0026#39;team-X+alerts@example.org\u0026#39; # 是否通知已解决的警报 send_resolved: true # 接收器的名称、通过邮件和pagerduty的方式发送、发送给哪些人，指定pagerduty的service_key - name: \u0026#39;team-X-pager\u0026#39; email_configs: - to: \u0026#39;team-X+alerts-critical@example.org\u0026#39; pagerduty_configs: - service_key: \u0026lt;team-X-key\u0026gt; # 接收器的名称、通过邮件的方式发送、发送给哪些人 - name: \u0026#39;team-Y-mails\u0026#39; email_configs: - to: \u0026#39;team-Y+alerts@example.org\u0026#39; # 接收器的名称、通过pagerduty的方式发送、指定pagerduty的service_key - name: \u0026#39;team-Y-pager\u0026#39; pagerduty_configs: - service_key: \u0026lt;team-Y-key\u0026gt; # 一个接收器配置多种发送方式 - name: \u0026#39;ops\u0026#39; webhook_configs: - url: \u0026#39;http://prometheus-webhook-dingtalk.kube-ops.svc.cluster.local:8060/dingtalk/webhook1/send\u0026#39; send_resolved: true email_configs: - to: \u0026#39;742899387@qq.com\u0026#39; send_resolved: true - to: \u0026#39;soulchild@soulchild.cn\u0026#39; send_resolved: true 1.4 Alertmanager CRD Prometheus Operator 为 alertmanager 抽象了两个 CRD 资源:\nalertmanager CRD: 基于 statefulset, 实现 alertmanager 的部署以及扩容缩容 alertmanagerconfig CRD: 实现模块化修改 alertmanager 的配置 通过 alertManager CRD 部署的实例配置文件由 secret/alertmanager-main-generated 提供\n# kubectl get pod alertmanager-main-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;config-volume\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;name\u0026#34;: \u0026#34;config-volume\u0026#34;, \u0026#34;secret\u0026#34;: { \u0026#34;defaultMode\u0026#34;: 420, \u0026#34;secretName\u0026#34;: \u0026#34;alertmanager-main-generated\u0026#34; } } # kubectl get secret alertmanager-main-generated -n monitoring -o jsonpath=\u0026#39;{.data.alertmanager\\.yaml}\u0026#39; | base64 --decode \u0026#34;global\u0026#34;: \u0026#34;resolve_timeout\u0026#34;: \u0026#34;5m\u0026#34; \u0026#34;inhibit_rules\u0026#34;: - \u0026#34;equal\u0026#34;: - \u0026#34;namespace\u0026#34; - \u0026#34;alertname\u0026#34; \u0026#34;source_matchers\u0026#34;: ...... secret alertmanager-main-generated 是自动生成的, 基于 secret alertmanager-main 和 CRD alertmanagerConfig\n[root@k8s-node1 manifests]# kubectl explain alertmanager.spec.configSecret DESCRIPTION: ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for this Alertmanager instance. Defaults to \u0026#39;alertmanager-\u0026lt;alertmanager-name\u0026gt;\u0026#39; The secret is mounted into /etc/alertmanager/config. 综上, 修改 alertmanager 配置可以修改 secret alertmanager-main 或者 CRD alertmanagerconfig\n2 示例 2.1 secret 新建 alertmanager.yaml 配置文件\nglobal: resolve_timeout: 5m smtp_from: \u0026#39;15810243114@163.com\u0026#39; smtp_smarthost: \u0026#39;smtp.163.com:25\u0026#39; smtp_auth_username: \u0026#39;15810243114@163.com\u0026#39; smtp_auth_password: \u0026#39;******\u0026#39; smtp_require_tls: false smtp_hello: \u0026#39;163.com\u0026#39; route: receiver: Default group_by: - namespace continue: false routes: - receiver: Critical matchers: - severity=\u0026#34;critical\u0026#34; continue: false group_wait: 30s group_interval: 5m repeat_interval: 12h inhibit_rules: - source_matchers: - severity=\u0026#34;critical\u0026#34; target_matchers: - severity=~\u0026#34;warning|info\u0026#34; equal: - namespace - alertname - source_matchers: - severity=\u0026#34;warning\u0026#34; target_matchers: - severity=\u0026#34;info\u0026#34; equal: - namespace - alertname receivers: - name: Default email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true - name: Critical email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true 修改 secret alertmanager-main\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 查看生成的 secret alertmanager-main-generated\nkubectl get secret alertmanager-main-generated -n monitoring -o jsonpath=\u0026#39;{.data.alertmanager\\.yaml}\u0026#39; | base64 --decode 之后 prometheus-operator 会自动更新 alertmanager 的配置\n# kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus-operator | tail -1 level=info ts=2023-04-30T11:43:01.104579363Z caller=operator.go:741 component=alertmanageroperator key=monitoring/main msg=\u0026#34;sync alertmanager\u0026#34; 2.2 alertmanagerconfig 默认情况下配置 alertmanager 是无法获取到的, 我们需要先修改一下 alertmanager 实例, 添加标签选择器\napiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: spec: alertmanagerConfigSelector: matchLabels: alertmanager: main 创建 alertmanager CRD 资源\napiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: dinghook namespace: monitoring labels: alertmanager: main spec: receivers: - name: web webhookConfigs: - url: http://dingtalk-hook-web sendResolved: true - name: db webhookConfigs: - url: http://dingtalk-hook-db sendResolved: true route: groupBy: [\u0026#34;app\u0026#34;] groupWait: 30s groupInterval: 5m repeatInterval: 12h continue: false receiver: web routes: - matchers: - name: app value: nginx receiver: web - matchers: - name: app value: mysql receiver: db 同样的, prometheus-operator 会更新 alertmanager 配置\n# kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus-operator | tail -1 level=info ts=2023-04-30T11:55:00.309492873Z caller=operator.go:741 component=alertmanageroperator key=monitoring/main msg=\u0026#34;sync alertmanager\u0026#34; 查看最后生成的配置\n# kubectl get secret alertmanager-main-generated -n monitoring -o jsonpath=\u0026#39;{.data.alertmanager\\.yaml}\u0026#39; | base64 --decode global: resolve_timeout: 5m smtp_from: 15810243114@163.com smtp_hello: 163.com smtp_smarthost: smtp.163.com:25 smtp_auth_username: 15810243114@163.com smtp_auth_password: ********* smtp_require_tls: false route: receiver: Default group_by: - namespace routes: - receiver: monitoring-dinghook-web group_by: - app matchers: - namespace=\u0026#34;monitoring\u0026#34; # 指定匹配了 namespace continue: true # continue 也没有按照预设配置 routes: - receiver: monitoring-dinghook-web match: app: nginx - receiver: monitoring-dinghook-db match: app: mysql group_wait: 30s group_interval: 5m repeat_interval: 12h - receiver: Critical matchers: - severity=\u0026#34;critical\u0026#34; group_wait: 30s group_interval: 5m repeat_interval: 12h inhibit_rules: - target_matchers: - severity=~\u0026#34;warning|info\u0026#34; source_matchers: - severity=\u0026#34;critical\u0026#34; equal: - namespace - alertname - target_matchers: - severity=\u0026#34;info\u0026#34; source_matchers: - severity=\u0026#34;warning\u0026#34; equal: - namespace - alertname receivers: - name: Default email_configs: - send_resolved: true to: lvbibir@foxmail.com - name: Critical email_configs: - send_resolved: true to: lvbibir@foxmail.com - name: monitoring-dinghook-web webhook_configs: - send_resolved: true url: http://dingtalk-hook-web - name: monitoring-dinghook-db webhook_configs: - send_resolved: true url: http://dingtalk-hook-db templates: [] 目前 alertmanagerconfig 这个 CRD 使用起来感觉有点麻烦, 一级 route 目前只能按照 namespace 筛选, 而且 continue 也只能设置成 false , 而且无法指定其他配置中的 receiver, 比如全局配置中的 Default\n[root@k8s-node1 ~]# kubectl explain alertmanagerconfig.spec.route.continue DESCRIPTION: Boolean indicating whether an alert should continue matching subsequent sibling nodes. It will always be overridden to true for the first-level route by the Prometheus operator. [root@k8s-node1 ~]# kubectl explain alertmanagerconfig.spec.route.matchers DESCRIPTION: List of matchers that the alert’s labels should match. For the first level route, the operator removes any existing equality and regexp matcher on the `namespace` label and adds a `namespace: \u0026lt;object namespace\u0026gt;` matcher. 2.3 告警模板 alertmanager 收到的告警大概长这个样子\nalertmanager CRD 支持 configMaps 参数, 会自动挂载到 /etc/alertmanager/configmaps 目录, 我们可以将模板文件配置成 configmap\n[root@k8s-node1 ~]# kubectl explain alertmanager.spec.configMaps DESCRIPTION: ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods. The ConfigMaps are mounted into /etc/alertmanager/configmaps/\u0026lt;configmap-name\u0026gt;. 创建模板文件 email.tmpl\n{{ define \u0026#34;email.html\u0026#34; }} {{- if gt (len .Alerts.Firing) 0 -}} {{- range $index, $alert := .Alerts -}} ========= ERROR ==========\u0026lt;br\u0026gt; 告警名称：{{ .Labels.alertname }}\u0026lt;br\u0026gt; 告警级别：{{ .Labels.severity }}\u0026lt;br\u0026gt; 告警机器：{{ .Labels.instance }} {{ .Labels.device }}\u0026lt;br\u0026gt; 告警详情：{{ .Annotations.summary }}\u0026lt;br\u0026gt; 告警时间：{{ (.StartsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;br\u0026gt; ========= END ==========\u0026lt;br\u0026gt; {{- end }} {{- end }} {{- if gt (len .Alerts.Resolved) 0 -}} {{- range $index, $alert := .Alerts -}} ========= INFO ==========\u0026lt;br\u0026gt; 告警名称：{{ .Labels.alertname }}\u0026lt;br\u0026gt; 告警级别：{{ .Labels.severity }}\u0026lt;br\u0026gt; 告警机器：{{ .Labels.instance }}\u0026lt;br\u0026gt; 告警详情：{{ .Annotations.summary }}\u0026lt;br\u0026gt; 告警时间：{{ (.StartsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;br\u0026gt; 恢复时间：{{ (.EndsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;br\u0026gt; ========= END ==========\u0026lt;br\u0026gt; {{- end }} {{- end }} {{- end }} 创建 configmap\nkubectl create configmap alertmanager-templates --from-file=email.tmpl --dry-run -o yaml -n monitoring \u0026gt; alertmanager-configmap-templates.yaml kubectl apply -f alertmanager-configmap-templates.yaml 更新 alertmanager 示例, 添加 configmap\napiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: spec: alertmanagerConfigSelector: matchLabels: alertmanager: main configMaps: - alertmanager-templates 查看挂载\n# kubectl get pod -n monitoring alertmanager-main-0 -o jsonpath=\u0026#34;{.spec.volumes[?(@.name==\u0026#39;configmap-alertmanager-templates\u0026#39;)]}\u0026#34; | python -m json.tool { \u0026#34;configMap\u0026#34;: { \u0026#34;defaultMode\u0026#34;: 420, \u0026#34;name\u0026#34;: \u0026#34;alertmanager-templates\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;configmap-alertmanager-templates\u0026#34; } 查看容器内的路径\n# kubectl exec -it alertmanager-main-0 -n monitoring -- sh /alertmanager $ cat /etc/alertmanager/configmaps/alertmanager-templates/email.tmpl 修改 alertmanager.yaml 配置文件, 指定模板文件\nreceivers: - name: Default email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true html: \u0026#39;{{ template \u0026#34;email.html\u0026#34; . }}\u0026#39; # 添加 与模板中的 define 对应 - name: Critical email_configs: - to: \u0026#39;lvbibir@foxmail.com\u0026#39; send_resolved: true html: \u0026#39;{{ template \u0026#34;email.html\u0026#34; . }}\u0026#39; # 添加 与模板中的 define 对应 templates: - \u0026#39;/etc/alertmanager/configmaps/alertmanager-templates/*.tmpl\u0026#39; 更新 secret\nkubectl create secret generic alertmanager-main -n monitoring --from-file=alertmanager.yaml --dry-run -oyaml \u0026gt; alertmanager-main-secret.yaml kubectl apply -f alertmanager-main-secret.yaml 查看配置是否生效, 在 webUI 界面查看\n查看新生成的告警邮件\n告警邮件 恢复邮件 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-6-alertmanager/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1 alertmanager prometheus 架构中采集数据和发送告警是独立出来的, 告警触发后将信息转发到独立的组件 alertmanager, 由 alertmanager 对报警进行统一处理, 最后通过接收器 recevier 发送给指定用户 1.1 工作机制 Alertmanager 收到告警信息后: 进行分组 group 通过定义好的路由 routing 转发到正确的接收器 recevier recevier 通过 email dingtalk wechat 等方式通知给定义好的接收人 1.2 四大功能 分组","title":"prometheus (六) Alertmanager"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1 告警规则 prometheus 支持两种类型的规则, 记录规则 recording rule 和告警规则 alerting rule\n1.1 recording rule 记录规则: 允许预先计算经常需要或计算量大的表达式，并将其结果保存为一组新的时间序列。查询预先计算的结果通常比每次需要时都执行原始表达式要快得多。这对于每次刷新时都需要重复查询相同表达式的仪表板特别有用。\n如下示例, 将统计 cpu 个数的表达式存为一个新的时间序列 instance:node_num_cpu:sum\ngroups: - name: node-exporter.rules rules: - record: instance:node_num_cpu:sum expr: | count without (cpu, mode) ( node_cpu_seconds_total{job=\u0026#34;node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;} ) 原始表达式结果\n新表达式结果\n1.2 alerting rule 告警规则: 当满足指定的触发条件时发送告警\nalert: 告警规则的名称 expr: 告警触发条件, 基于 PromQL 表达式, 如果表达式执行结果为 True 则推送告警 for: 等待评估时间, 可选参数. 表示当触发条件持续一定时间后才发送告警, 在等待期间告警的状态为 pending labels: 自定义标签 annotaions: 指定一组附加信息, 可以使用 $labels $externalLabels $value 格式化信息. $labels 储存报警实例的时序数据; $externalLabels 储存 prometheus 中 global.external_labels 配置的标签; $value 保存报警实例的评估值 description: 详细信息 summary: 描述信息 如下示例, 当节点的某个文件系统剩余空间不足 10% 达到 30 分钟后将发送告警\ngroups: - name: test rules: - alert: NodeFilesystemAlmostOutOfSpace expr: node_filesystem_avail_bytes{job=\u0026#34;node-exporter\u0026#34;,fstype!=\u0026#34;\u0026#34;} / node_filesystem_size_bytes{job=\u0026#34;node-exporter\u0026#34;,fstype!=\u0026#34;\u0026#34;} * 100 \u0026lt; 10 for: 30m labels: severity: warning annotations: description: \u0026#39; {{ $labels.instance }} 节点 {{ $labels.device }} 文件系统剩余空间: {{ printf \u0026#34;%.2f\u0026#34; $value }}% \u0026#39; summary: \u0026#39;文件系统剩余空间不足 10%\u0026#39; 1.3 prometheusrule CRD Prometheus Operator 抽象出来一个 prometheusrule CRD 资源, 通过管理这个 CRD 资源实现告警规则的统一管理\nkube-prometheus 默认帮我们创建了一些告警规则\n# kubectl get prometheusrule -A NAMESPACE NAME AGE monitoring alertmanager-main-rules 8d monitoring kube-prometheus-rules 8d monitoring kube-state-metrics-rules 8d monitoring kubernetes-monitoring-rules 8d monitoring node-exporter-rules 8d monitoring prometheus-k8s-prometheus-rules 8d monitoring prometheus-operator-rules 8d prometheusrule 定义一系列报警规则\napiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: name: demo namespace: monitoring spec: groups: - name: group1 rules: - alert: alert1 annotations: description: alert-1 summary: alert-1 expr: up == 0 for: 15m labels: severity: critical - alert: alert2 ...... - name: group2 rules: - alert: alert3 ...... 对于 prometheusrule 的更新操作 (create, delete, update) 都会被 watch 到, 然后更新到统一的一个 configmap 中, 然后 prometheus 自动重载配置\n每个 prometheusrule 会作为 configmap prometheus-k8s-rulefiles-0 中的一个 data , data 的命名规则为 \u0026lt;namespace\u0026gt;-\u0026lt;rulename\u0026gt;-ruleuid\n# kubectl get cm prometheus-k8s-rulefiles-0 -n monitoring NAME DATA AGE prometheus-k8s-rulefiles-0 7 41m # prometheus 实例的挂载信息 # kubectl get pod prometheus-k8s-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;prometheus-k8s-rulefiles-0\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;configMap\u0026#34;: { \u0026#34;defaultMode\u0026#34;: 420, \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-rulefiles-0\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-rulefiles-0\u0026#34; } # prometheus 中实际的存储路径 # kubectl exec -it prometheus-k8s-0 -n monitoring -- ls /etc/prometheus/rules/prometheus-k8s-rulefiles-0/ monitoring-alertmanager-main-rules-79a2aba8-1a50-4bbc-b201-e9c8ee43e6aa.yaml monitoring-kube-prometheus-rules-9867eba7-cd4c-4677-b931-4268744ae5e7.yaml monitoring-kube-state-metrics-rules-b787fea0-dba2-4d6d-9fd6-0b470ce45059.yaml monitoring-kubernetes-monitoring-rules-b1939032-6a22-4ce1-b0ce-6482db094018.yaml monitoring-node-exporter-rules-0140bdd4-b858-4672-85be-930eabdc95eb.yaml monitoring-prometheus-k8s-prometheus-rules-87a80a69-f3be-4d3e-8a26-e1da2ade3a0a.yaml monitoring-prometheus-operator-rules-8688aa7b-a157-4ddc-bd09-21781f8ac567.yaml prometheus 的配置中定义了 rule_files 路径\n2 示例 2.1 磁盘使用率 当磁盘可用空间少于 50% 时触发告警\n创建 prometheusrule\napiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: demo namespace: monitoring spec: groups: - name: demo rules: - alert: nodeDiskUsage annotations: description: | 节点 {{$labels.instance }} 挂载目录 {{ $labels.mountpoint }} 当前可用空间 {{ printf \u0026#34;%.2f\u0026#34; $value }}% summary: | 挂载目录可用空间低于 50% expr: | node_filesystem_avail_bytes{fstype!=\u0026#34;\u0026#34;,job=\u0026#34;node-exporter\u0026#34;} / node_filesystem_size_bytes{fstype!=\u0026#34;\u0026#34;,job=\u0026#34;node-exporter\u0026#34;} * 100 \u0026lt; 50 for: 1m labels: severity: warning 查看生成的告警规则, 当前状态是 inactive\nnode1 当前状态, / 目录总容量 45G, 可用空间为 82%\n[root@k8s-node1 manifests]# df -hT | head -1 \u0026amp;\u0026amp; df -hT | grep -E \u0026#34;/$\u0026#34; Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/centos_one-root xfs 45G 7.8G 38G 18% / 接下来用 dd 手动创建一个 25G 的大文件, 此时剩余空间仅剩 27%\n[root@k8s-node1 manifests]# dd if=/dev/zero of=/tmp/demo bs=1G count=25 [root@k8s-node1 manifests]# df -hT | head -1 \u0026amp;\u0026amp; df -hT | grep -E \u0026#34;/$\u0026#34; Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/centos_one-root xfs 45G 33G 13G 73% / 此时告警规则已经进入 pending 状态了, 我们设置了 1m 的评估等待时间\n一分钟过后进入 firing 状态, 正式发出告警, 此时我们设置的 $label 还没有解析\n我们去 alertmanager 看一下, 成功收到了告警, 且 $labels 和 $value 也已经正常解析了\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-5-rule/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1 告警规则 prometheus 支持两种类型的规则, 记录规则 recording rule 和告警规则 alerting rule 1.1 recording rule 记录规则: 允许预先计算经常需要或计算量大的表达式，并将其结果保存为一组新的时间序列。查询预先计算的结果通常比每次需要时都执行原始表达式要快得多。这对于每次刷新时都需要重复查询相同表达式的仪表板特别","title":"prometheus (五) 记录规则与告警规则"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1 简介 Probe 的 API 文档\n1.1 白盒监控 vs 黑盒监控 白盒监控: 我们监控主机的资源用量、容器的运行状态、数据库中间件的运行数据等等，这些都是支持业务和服务的基础设施，通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化\n黑盒监控: 以用户的身份测试服务的外部可见性，常见的黑盒监控包括 HTTP 探针 TCP 探针 等用于检测站点或者服务的可访问性，以及访问效率等。\n黑盒监控相较于白盒监控最大的不同在于黑盒监控是以故障为导向的. 当故障发生时，黑盒监控能快速发现故障，而白盒监控则侧重于主动发现或者预测潜在的问题。一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题。\n1.2 blackbox exporter Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过 HTTP HTTPS DNS TCP ICMP 以及 gPRC 的方式对 endpoints 端点进行探测。\n在 kube-prometheus 的默认配置中已经部署了 Blackbox Exporter 以供用户使用\n[root@k8s-node1 kube-prometheus]# ls manifests/blackboxExporter-* | cat manifests/blackboxExporter-clusterRoleBinding.yaml manifests/blackboxExporter-clusterRole.yaml manifests/blackboxExporter-configuration.yaml manifests/blackboxExporter-deployment.yaml manifests/blackboxExporter-serviceAccount.yaml manifests/blackboxExporter-serviceMonitor.yaml manifests/blackboxExporter-service.yaml 1.3 Probe CRD prometheus-operator 提供了一个 Probe CRD 对象，可以用来进行黑盒监控，具体的探测功能由 Blackbox-exporter 实现。\nProbe 支持 staticConfig 和 ingress 两种配置方式, 使用 ingress 时可以自动发现 ingress 代理的 url 并进行探测\n大概步骤:\n首先，用户创建一个 Probe CRD 对象，对象中指定探测方式、探测目标等参数； 然后，prometheus-operator watch 到 Probe 对象创建，然后生成对应的 prometheus 拉取配置，reload 到 prometheus 中； 最后，prometheus 使用 url=/probe?target={探测目标}\u0026amp;module={探测方式}，拉取 blackbox-exporter ，此时 blackbox-exporter 会对目标进行探测，并以 metrics 格式返回探测结果； 2 probe 示例 2.1 staticConfig 2.1.1 kube-dns 使用黑盒监控监测 kube-dns 的可用性\n默认配置下的 blackbox exporter 未开启 dns 模块, 我们手动开启一下\n修改 blackboxExporter-configuration.yaml 文件, 添加 dns 模块\napiVersion: v1 data: config.yml: |- \u0026#34;modules\u0026#34;: \u0026#34;dns\u0026#34;: # DNS 检测模块 \u0026#34;prober\u0026#34;: \u0026#34;dns\u0026#34; \u0026#34;dns\u0026#34;: \u0026#34;transport_protocol\u0026#34;: \u0026#34;tcp\u0026#34; # 默认是 udp \u0026#34;preferred_ip_protocol\u0026#34;: \u0026#34;ip4\u0026#34; # 默认是 ip6 \u0026#34;query_name\u0026#34;: \u0026#34;kubernetes.default.svc.cluster.local\u0026#34; 更新 configmap 配置文件, prometheus-opertor 会 watch 到更新然后通过 pod 中的 module-configmap-reloader 容器通知 blackbox-exporter 重载配置\n# 每个 blackbox-exporter POD 中有三个 container [root@k8s-node1 manifests]# kubectl get pods -n monitoring -l app.kubernetes.io/name=blackbox-exporter \\ -o jsonpath=\u0026#39;{.items[*].spec.containers[*].name}{\u0026#34;\\n\u0026#34;}\u0026#39; blackbox-exporter module-configmap-reloader kube-rbac-proxy [root@k8s-node1 manifests]# kubectl apply -f blackboxExporter-configuration.yaml configmap/blackbox-exporter-configuration configured [root@k8s-node1 manifests]# kubectl logs -n monitoring -l app.kubernetes.io/name=blackbox-exporter | tail -2 level=info ts=2023-04-23T02:23:49.614Z caller=tls_config.go:191 msg=\u0026#34;TLS is disabled.\u0026#34; http2=false level=info ts=2023-04-28T06:40:48.168Z caller=main.go:278 msg=\u0026#34;Reloaded config file\u0026#34; 创建 Probe CRD 资源\napiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: blackbox-kube-dns namespace: monitoring spec: jobName: blackbox-kube-dns interval: 10s module: dns prober: # 指定blackbox的地址 url: blackbox-exporter:19115 # blackbox-exporter 的 地址 和 端口 path: /probe # 路径 targets: staticConfig: static: - kube-dns.kube-system:53 # 要检测的 url 查看生成的 target\n现在可以通过:\nprobe_success{job=\u0026quot;blackbox-kube-dns\u0026quot;} 查看服务状态是否可用 probe_dns_lookup_time_seconds{job='blackbox-kube-dns'} DNS 解析耗时 查看 blackbox exporter 一次 dns 探测生成的 metrics 指标\n2.2.1 http http 探测一般使用 http_2xx 模块, 虽然默认有这个模块, 但是默认的配置不太合理, 我们修改一下\napiVersion: v1 data: config.yml: |- \u0026#34;modules\u0026#34;: \u0026#34;dns\u0026#34;: # DNS 检测模块 \u0026#34;prober\u0026#34;: \u0026#34;dns\u0026#34; \u0026#34;dns\u0026#34;: \u0026#34;transport_protocol\u0026#34;: \u0026#34;tcp\u0026#34; # 默认是 udp \u0026#34;preferred_ip_protocol\u0026#34;: \u0026#34;ip4\u0026#34; # 默认是 ip6 \u0026#34;query_name\u0026#34;: \u0026#34;kubernetes.default.svc.cluster.local\u0026#34; \u0026#34;http_2xx\u0026#34;: \u0026#34;http\u0026#34;: \u0026#34;preferred_ip_protocol\u0026#34;: \u0026#34;ip4\u0026#34; \u0026#34;valid_status_codes\u0026#34;: \u0026#34;[200]\u0026#34; # 最好加上状态码, 方便 grafana 展示数据 \u0026#34;valid_http_versions\u0026#34;: [\u0026#34;HTTP/1.1\u0026#34;, \u0026#34;HTTP/2\u0026#34;] \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34; \u0026#34;prober\u0026#34;: \u0026#34;http\u0026#34; 更新配置\n[root@k8s-node1 manifests]# kubectl apply -f blackboxExporter-configuration.yaml configmap/blackbox-exporter-configuration configured 快速创建一个 nginx 应用\nkubectl create deployment nginx --image=nginx:1.22.1 --port=80 kubectl expose deployment nginx --name=nginx --port=80 --target-port=80 创建 Probe 资源\napiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: blackbox-http-nginx namespace: monitoring spec: jobName: blackbox-http-nginx prober: url: blackbox-exporter:19115 path: /probe module: http_2xx # 配置文件中的检测模块 targets: staticConfig: static: - nginx.default 查看生成的 target\n查看一次 http 探测生成的 metrics 指标\n2.2 ingress 自动发现 接下来使用 ingrss 自动发现实现集群内的 ingress 并进行黑盒探测\n先创建两个 web 应用\nkubectl create deployment web-1 --image=nginx:1.22.1 --port=80 kubectl expose deployment web-1 --name=web-1 --port=80 --target-port=80 kubectl create deployment web-2 --image=nginx:1.22.1 --port=80 kubectl expose deployment web-2 --name=web-2 --port=80 --target-port=80 2.2.1 创建 ingress 包含三个路径: http://web1.test.com http://web1.test.com/test http://web2.test.com\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: demo-web namespace: default labels: prometheus.io/http-probe: \u0026#34;true\u0026#34; # 用于监测 annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/router.entrypoints: web spec: rules: - host: web1.test.com http: paths: - pathType: Prefix path: / backend: service: name: web-1 port: number: 80 - pathType: Prefix path: /test backend: service: name: web-1 port: number: 80 - host: web2.test.com http: paths: - pathType: Prefix path: / backend: service: name: web-2 port: number: 80 部署并访问测试\n[root@k8s-node1 manifests]# echo \u0026#34;1.1.1.1 web1.test.com web2.test.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts [root@k8s-node1 manifests]# curl -sI web1.test.com | head -1 HTTP/1.1 200 OK [root@k8s-node1 manifests]# curl -sI web1.test.com/test | head -1 HTTP/1.1 404 Not Found [root@k8s-node1 manifests]# curl -sI web2.test.com | head -1 HTTP/1.1 200 OK 2.2.2 创建 probe 可以使用 label 或者 annotation 两种方式筛选监测的 ingress, 不配置监测所有 ingress\napiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: blackbox-ingress namespace: monitoring spec: jobName: blackbox-ingress prober: url: blackbox-exporter:19115 path: /probe module: http_2xx targets: ingress: namespaceSelector: # 监测所有 namespace # any: true # 只监测指定 namespace 的 ingress matchNames: - default - monitoring # 只监测配置了标签 prometheus.io/http-probe: true 的 ingress selector: matchLabels: prometheus.io/http-probe: \u0026#34;true\u0026#34; # 只监测配置了注解 prometheus.io/http_probe: true 的 ingress # relabelingConfigs: # - action: keep # sourceLabels: # - __meta_kubernetes_ingress_annotation_prometheus_io_http_probe # regex: \u0026#34;true\u0026#34; 查看 targets\n不过由于 ingress 配置的域名无法解析, 所以监测到的状态是失败的:\n2.2.3 配置 coredns 我们为 coredns 添加自定义 hosts, 实现 blackbox 可以解析到我们的自定义域名\n# kubectl edit cm coredns -n kube-system apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } hosts { # 添加 hosts 配置 1.1.1.1 web1.test.com 1.1.1.1 web2.test.com fallthrough } cache 30 loop reload loadbalance } kind: ConfigMap 等待 coredns 的配置生效后, 我们重新再看一下 target 状态\n3 additionalScrapeConfigs 目前 prometheus operator 只支持 ingress 方式的自动发现, 而且自定义配置其实不是很多, 更推荐使用 additionalScrapeConfigs 静态配置的方式实现\n3.2 service 自动发现 沿用 2.2 章节中创建的两个 service\n修改 service web-1, 添加 annotation\n# kubectl edit svc web-1 metadata: annotations: prometheus.io/http-probe: \u0026#34;true\u0026#34; # 控制是否监测 prometheus.io/http-probe-path: / # 控制监测路径 prometheus.io/http-probe-port: \u0026#34;80\u0026#34; # 控制监测端口 修改静态配置\n# vim prometheus-additional.yaml - job_name: \u0026#34;kubernetes-services\u0026#34; metrics_path: /probe params: module: - \u0026#34;http_2xx\u0026#34; ## 使用 Kubernetes 动态服务发现,且使用 Service 类型的发现 kubernetes_sd_configs: - role: service relabel_configs: ## 设置只监测 Annotation 里配置了 prometheus.io/http_probe: true 的 service - action: keep source_labels: [__meta_kubernetes_service_annotation_prometheus_io_http_probe] regex: \u0026#34;true\u0026#34; - action: replace source_labels: - \u0026#34;__meta_kubernetes_service_name\u0026#34; - \u0026#34;__meta_kubernetes_namespace\u0026#34; - \u0026#34;__meta_kubernetes_service_annotation_prometheus_io_http_probe_port\u0026#34; - \u0026#34;__meta_kubernetes_service_annotation_prometheus_io_http_probe_path\u0026#34; target_label: __param_target regex: (.+);(.+);(.+);(.+) replacement: $1.$2:$3$4 - target_label: __address__ replacement: blackbox-exporter:19115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name 更新 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 确保 prometheus CRD 实例配置了 secret\n[root@k8s-node1 manifests]# grep -A2 additionalScrapeConfigs prometheus-prometheus.yaml additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 查看 target, 可以看到只有 web-1 发现成功, 因为 web-2 没有配置 annotation\n查看一下监测状态, 直接使用 \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt; 访问, 在集群内是可以正常解析的, 所以这里 http 状态码为正常的 200\n3.1 ingress 自动发现 依旧使用 2.2 章节中创建的 ingress\n为 ingress 添加 annotation 注解\n# kubectl edit ingress demo-web annotations: # 添加如下项 prometheus.io/http-probe: \u0026#34;true\u0026#34; # 用于控制是否监测 prometheus.io/http-probe-port: \u0026#34;80\u0026#34; # 用于控制监测端口 修改静态配置\n# cat prometheus-additional.yaml - job_name: \u0026#34;kubernetes-ingresses\u0026#34; metrics_path: /probe params: module: - \u0026#34;http_2xx\u0026#34; ## 使用 Kubernetes 动态服务发现,且使用 ingress 类型的发现 kubernetes_sd_configs: - role: ingress relabel_configs: ## 设置只监测 Annotation 里配置了 prometheus.io/http_probe: true 的 ingress - action: keep source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_http_probe] regex: \u0026#34;true\u0026#34; - action: replace source_labels: - \u0026#34;__meta_kubernetes_ingress_scheme\u0026#34; - \u0026#34;__meta_kubernetes_ingress_host\u0026#34; - \u0026#34;__meta_kubernetes_ingress_annotation_prometheus_io_http_probe_port\u0026#34; - \u0026#34;__meta_kubernetes_ingress_path\u0026#34; target_label: __param_target regex: (.+);(.+);(.+);(.+) replacement: ${1}://${2}:${3}${4} - target_label: __address__ replacement: blackbox-exporter:19115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: ingress_name 更新 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 确保 prometheus CRD 实例配置了 secret\n[root@k8s-node1 manifests]# grep -A2 additionalScrapeConfigs prometheus-prometheus.yaml additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 查看 target\n查看监测状态, 与预期配置一样\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-4-blackbox-probe/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1 简介 Probe 的 API 文档 1.1 白盒监控 vs 黑盒监控 白盒监控: 我们监控主机的资源用量、容器的运行状态、数据库中间件的运行数据等等，这些都是支持业务和服务的基础设施，通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化","title":"prometheus (四) 黑盒监控"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1 简介 手动添加 job 配置未免过于繁琐, prometheus 支持很多种方式的服务发现, 在 k8s 中是通过 kubernetes_sd_config 配置实现的. 通过抓取 k8s REST API 自动发现我们部署在 k8s 集群中的 exporter 实例\n在 Prometheus Operator 中, 我们无需手动编辑配置文件添加 kubernetes_sd_config 配置, Prometheus Operator 提供了下述资源:\nserviceMonitor: 创建 endpoints 级别的服务发现 podMonitor: 创建 pod 级别的服务发现 probe: 创建 ingress 级别的服务发现 (用于黑盒监控) 通过对这三种 CRD 资源的管理实现 prometheus 动态的服务发现.\n1.1 kubernetes_sd_config kubernets_sd_config 官方文档\nkubernetes_sd_config 支持 node service pod endpoints ingress 5 种服务发现模式.\nnode 适用于与主机相关的监控资源，如 Kubernetes 组件状态、节点上运行的容器状态等； service 和 ingress 用于黑盒监控的场景，如对服务的可用性以及服务质量的监控； endpoints 和 pod 用于获取 Pod 的监控数据，如监控用户部署的支持 Prometheus 的应用。 每种发现模式都支持很多 label, prometheus 可以通过 relabel_config 分析这些标签进行标签重写或者丢弃 target\n在 kube-prometheus 的模板配置中, 所有的 target 都是通过 endpoints 模式实现的.\nendpoints 模式的自动发现会添加 endpoints 后端所有 pod 暴露出来的所有 port. 如下所示\n# 共有 10 个 endpoints, 后端包含 15 个 pod, 所有 ip+port 的组合有 44 个 endpoints ===\u0026gt; pods ===\u0026gt; pod-ip+port 10 15 44 同样, 在 prometheus 后端看到的 targets 将会是 44 个, 然后按照 relabel 规则在这些所有的 target 中选择合适的 target 并进行 active\n2 serviceMonitor CRD 2.1 node-exporter 以上节部署的 kube-prometheus 为例, 学习 prometheus 如何通过 endpoints 模式的服务发现来添加我们创建的 node-exporter 为 target\n需要注意的是, 与一般部署的 node-exporter 不同, kube-prometheus 额外创建了一个 headless service, 随着 service 创建的 endpoints 将用于 prometheus 的自动发现.\n# cat manifests/nodeExporter-service.yaml apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 1.3.1 name: node-exporter namespace: monitoring spec: clusterIP: None ports: - name: https port: 9100 targetPort: https selector: app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus 查看 endpoints\n[root@k8s-node1 ~]# kubectl get ep -n monitoring -l app.kubernetes.io/name=node-exporter NAME ENDPOINTS AGE node-exporter 1.1.1.1:9100,1.1.1.2:9100,1.1.1.3:9100 10d 查看 serviceMonitor\n# cat manifests/nodeExporter-serviceMonitor.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 1.3.1 name: node-exporter namespace: monitoring spec: endpoints: - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token interval: 15s port: https relabelings: - action: replace regex: (.*) replacement: $1 sourceLabels: - __meta_kubernetes_pod_node_name targetLabel: instance scheme: https tlsConfig: insecureSkipVerify: true jobLabel: app.kubernetes.io/name selector: matchLabels: # 标签匹配规则, 符合 label 条件的 target 才会被 active app.kubernetes.io/component: exporter app.kubernetes.io/name: node-exporter app.kubernetes.io/part-of: kube-prometheus 上述的 serviceMonitor 将会为 prometheus 生成一个 job, 使用了 endpoints 模式的 kubernetes_sd_config, 用于自动发现集群内符合条件的 node-exporter\n##### job 基础信息 ######## - job_name: serviceMonitor/monitoring/node-exporter/0 honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: https authorization: type: Bearer credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: insecure_skip_verify: true follow_redirects: true ##### kubernetes_sd_configs 自动发现的配置 ######### kubernetes_sd_configs: - role: endpoints kubeconfig_file: \u0026#34;\u0026#34; follow_redirects: true namespaces: names: - monitoring ##### 开始执行 relabel, 第一个规则是替换任务名 ################ relabel_configs: - source_labels: [job] separator: ; regex: (.*) target_label: __tmp_prometheus_job_name replacement: $1 action: replace ###### 以下是匹配规则, 如果不满足 label 匹配规则就丢弃 target, 对应 serviceMonitor 配置的 matchlabels ###### - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_component, __meta_kubernetes_service_labelpresent_app_kubernetes_io_component] separator: ; regex: (exporter);true replacement: $1 action: keep # 如果不满足, 丢弃此 target - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name, __meta_kubernetes_service_labelpresent_app_kubernetes_io_name] separator: ; regex: (node-exporter);true replacement: $1 action: keep - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_part_of, __meta_kubernetes_service_labelpresent_app_kubernetes_io_part_of] separator: ; regex: (kube-prometheus);true replacement: $1 action: keep - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: https replacement: $1 action: keep ##### 以下是 replace 替换标签的操作, 可以使我们的 target 标签有更好的可读性 ##### - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Node;(.*) target_label: node replacement: ${1} action: replace - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Pod;(.*) target_label: pod replacement: ${1} action: replace - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: service replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_container_name] separator: ; regex: (.*) target_label: container replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: job replacement: ${1} action: replace - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name] separator: ; regex: (.+) target_label: job replacement: ${1} action: replace - separator: ; regex: (.*) target_label: endpoint replacement: https action: replace - source_labels: [__meta_kubernetes_pod_node_name] separator: ; regex: (.*) target_label: instance replacement: $1 action: replace - source_labels: [__address__] separator: ; regex: (.*) modulus: 1 target_label: __tmp_hash replacement: $1 action: hashmod - source_labels: [__tmp_hash] separator: ; regex: \u0026#34;0\u0026#34; replacement: $1 action: keep 在 prometheus 的服务发现界面可以看到采集到的所有 target, 每个 target 就对应了一个 pod-ip+Port ,每个 target 含有许多原始标签, relebal_config 就是针对这些标签进行筛选和重写等其他操作.\nendpoints 级别的标签\nservice 和 pod 级别的标签\n查看自动注册到 prometheus 的 node-exporter\n可以发现:\n经过 keep 规则成功从 44 个 target 中筛选到了对应的 node-exporter 经过 replace 规则之后 target-labels 有了更好的可读性 2.2 traefik 接下来演示一下通过创建 serviceMonitor 实现采集 traefik 的 metrics 指标, traefik 安装请参考 traefik系列文章\n在配置中开启 metric\n访问测试\n[root@k8s-node1 ~]# kubectl get svc traefik-metrics -n traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-metrics ClusterIP 10.103.102.23 \u0026lt;none\u0026gt; 9100/TCP 19s [root@k8s-node1 ~]# [root@k8s-node1 ~]# curl -s 10.103.102.23:9100/metrics | head -5 # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 4.8217e-05 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 7.3819e-05 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000203355 2.2.1 rbac 创建一个用于访问 traefik 命名空间的 role\n修改 manifests/prometheus-roleSpecificNamespaces.yaml, 新增如下配置\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.32.1 name: prometheus-k8s namespace: traefik rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - pods verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch 将上一步创建的 role 与 serviceAccount prometheus-k8s 绑定\n修改 manifests/prometheus-roleBindingSpecificNamespaces.yaml, 新增如下配置\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.32.1 name: prometheus-k8s namespace: traefik roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring 验证\n[root@k8s-node1 kube-prometheus]# kubectl get role,rolebinding -n traefik NAME CREATED AT role.rbac.authorization.k8s.io/prometheus-k8s 2023-04-26T06:50:22Z NAME ROLE AGE rolebinding.rbac.authorization.k8s.io/prometheus-k8s Role/prometheus-k8s 75m 2.2.2 serviceMonitor 创建 serviceMonitor\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: traefik namespace: monitoring labels: app.kubernetes.io/name: traefik spec: jobLabel: app.kubernetes.io/name endpoints: - interval: 15s port: metrics # endpoint(service) 中定义的 portName path: /metrics # metrics 访问路径 namespaceSelector: # 指定 namespace matchNames: - traefik selector: matchLabels: app: traefik-metrics # endpoint 的 label 筛选 部署\nkubectl apply -f manifests/traefik-serviceMonitor.yml 2.2.3 验证 prometheus 的 configuration 界面自动生成的配置如下\n- job_name: serviceMonitor/monitoring/traefik/0 honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http follow_redirects: true relabel_configs: - source_labels: [job] separator: ; regex: (.*) target_label: __tmp_prometheus_job_name replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_label_app, __meta_kubernetes_service_labelpresent_app] separator: ; regex: (traefik-metrics);true replacement: $1 action: keep - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: metrics replacement: $1 action: keep - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Node;(.*) target_label: node replacement: ${1} action: replace - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name] separator: ; regex: Pod;(.*) target_label: pod replacement: ${1} action: replace - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: service replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_container_name] separator: ; regex: (.*) target_label: container replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: job replacement: ${1} action: replace - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name] separator: ; regex: (.+) target_label: job replacement: ${1} action: replace - separator: ; regex: (.*) target_label: endpoint replacement: metrics action: replace - source_labels: [__address__] separator: ; regex: (.*) modulus: 1 target_label: __tmp_hash replacement: $1 action: hashmod - source_labels: [__tmp_hash] separator: ; regex: \u0026#34;0\u0026#34; replacement: $1 action: keep kubernetes_sd_configs: - role: endpoints kubeconfig_file: \u0026#34;\u0026#34; follow_redirects: true namespaces: names: - traefik Service Discovery 界面\nTargets 界面\n3 podMonitor CRD 3.1 calico-node 以 calico 为例, 使用 podMonitor 资源监控 calico-node\ncalico 中核心的组件是 Felix，它负责设置路由表和 ACL 规则，同时还负责提供网络健康状况的数据；这些数据会被写入 etcd。\n监控 calico 的核心便是监控 felix，felix 相当于 calico 的大脑。\n如下所示, 一般 calico-node 都是使用 daemonset 方式部署在集群中的\n[root@k8s-node1 ~]# kubectl get pods -n kube-system -l k8s-app=calico-node -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES calico-node-8p9w5 1/1 Running 2 (5d16h ago) 7d23h 1.1.1.2 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-bw2kb 1/1 Running 2 (5d16h ago) 7d23h 1.1.1.1 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node-gt688 1/1 Running 2 (5d16h ago) 7d23h 1.1.1.3 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-node 默认是没有打开 metrics 监听的, 我们修改 calico 的 daemonset 配置文件, 可以直接修改部署时的 yaml, 也可以 kubectl edit ds calico-node -n kube-system\nkind: DaemonSet spec: template: spec: containers: - name: calico-node env: # 添加如下配置, 开启 FELIX 的 metrics 的监听端口为 9101 (9100 端口被 node-exporter 占据了) - name: FELIX_PROMETHEUSMETRICSENABLED value: \u0026#34;True\u0026#34; - name: FELIX_PROMETHEUSMETRICSPORT value: \u0026#34;9101\u0026#34; ports: - containerPort: 9101 name: metrics protocol: TCP 修改完等待 calico-node 的 pod 重新部署, 然后访问测试\n[root@k8s-node1 ~]# curl -s k8s-node2:9101/metrics | head -6 # HELP felix_active_local_endpoints Number of active endpoints on this host. # TYPE felix_active_local_endpoints gauge felix_active_local_endpoints 9 # HELP felix_active_local_policies Number of active policies on this host. # TYPE felix_active_local_policies gauge felix_active_local_policies 0 由于 kube-prometheus 默认已经帮我们创建了基于 kube-sysetm 命名空间的授权, 我们无需再额外配置\n创建 podMonitor\napiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: labels: app.kubernetes.io/name: calico-node name: calico-node namespace: monitoring spec: jobLabel: app.kubernetes.io/name podMetricsEndpoints: - interval: 15s path: /metrics port: metrics # 确保与 calico-node 的 containerPort 名称一致 namespaceSelector: matchNames: - kube-system # 确保命名空间正确 selector: matchLabels: k8s-app: calico-node # 确保 label 配置正确 查看 prometheus\n4 集群范围的自动发现 当我们的 k8s 集群中 service 和 pod 达到一定规模后手动一个一个创建 serviceMonitor 和 podMonitor 不免又麻烦了起来, 我们可以使用不限制 namespace 的 kubernetes_sd_configs 实现集群范围内自动发现所有的 exporter 实例\n接下来的演示中我们监控集群范围内的所有 endpoints, 并且将带有 prometheus.io/scrape=true 这个 annotations 的 service 注册到 prometheus\n4.1 rbac 由于需要访问访问集群范围内的资源对象, 继续使用 role+roleBinding 模式显然不适合, prometheus-k8s 这个 serviceAccount 还绑定一个名为 prometheus-k8s 的 clusterRole, 该 clusterRole 默认权限是不够的, 添加需要的权限:\n# vim manifests/prometheus-clusterRole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/instance: k8s app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.32.1 name: prometheus-k8s namespace: monitoring rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes - services - endpoints - pods - nodes/metrics verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get # kubectl apply -f manifests/prometheus-clusterRole.yaml 4.2 自动发现配置 通过上一篇文章中的 additionalScrapeConfigs 添加自动发现配置\n# vim prometheus-additional.yaml - job_name: \u0026#39;kubernetes-service-endpoints\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name 修改 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 确保 prometheus CRD 中添加了 additionalScrapeConfigs 配置\nkind: Prometheus spec: additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 4.3 验证 service kube-dns 默认有 prometheus.io/scrape=true 这个注解, 已经成功注册:\n创建一个示例应用\n# vim node-exporter-deploy-svc.yml apiVersion: v1 kind: Namespace metadata: name: test --- apiVersion: v1 kind: Service metadata: name: test-node-exporter namespace: test labels: app: test-node-exporter annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;9200\u0026#34; spec: selector: app: test-node-exporter ports: - name: metrics port: 9200 targetPort: metrics --- apiVersion: apps/v1 kind: Deployment metadata: name: test-node-exporter namespace: test labels: app: test-node-exporter spec: replicas: 1 selector: matchLabels: app: test-node-exporter template: metadata: labels: app: test-node-exporter spec: containers: - args: - --web.listen-address=:9200 image: registry.cn-hangzhou.aliyuncs.com/lvbibir/node-exporter:v1.3.1 name: node-exporter ports: - name: metrics containerPort: 9200 # kubectl apply -f node-exporter-deploy-svc.yml 如下, 我们部署的 node-exporter 已经成功注册, 只要 service 设置了 annotations 即可\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-3-servicemonitor-podmonitor/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1 简介 手动添加 job 配置未免过于繁琐, prometheus 支持很多种方式的服务发现, 在 k8s 中是通过 kubernetes_sd_config 配置实现的. 通过抓取 k8s REST API 自动发现我们部署在 k8s 集群中的 exporter 实例 在 Prometheus Operator 中, 我们无需手动编辑配置文件添加 kubernetes_sd_config 配置, Prometheus Operator 提供了下述资源: serviceMonitor: 创建 endpoints 级别的服务发现 podMonitor: 创建 pod 级别的服务发现 probe: 创建 ingress 级别的","title":"prometheus (三) 服务发现"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1 简介 使用原生的 prometheus 时, 我们创建 job 直接修改配置文件即可, 然而在 prometheus-operator 中所有的配置都抽象成了 k8s CRD 资源, 手动配置 job 需要:\n创建 secret 在 prometheus CRD 资源中配置 additionalScrapeConfigs additional-scrape-config 官方示例\n2 示例 2.1 node-exporter 添加 k8s 集群外的 node-exporter metrics\n在 1.1.1.4 部署 node-exporter\ndocker run -d --name node-exporter \\ -p 9102:9100 \\ -v \u0026#34;/proc:/host/proc:ro\u0026#34; \\ -v \u0026#34;/sys:/host/sys:ro\u0026#34; \\ -v \u0026#34;/:/rootfs:ro\u0026#34; \\ registry.cn-hangzhou.aliyuncs.com/lvbibir/node-exporter:v1.3.1 \\ --path.sysfs=/host/sys \\ --path.rootfs=/roofs # 验证可用性 [root@1-1-1-4 ~]# curl -s 1.1.1.4:9100/metrics | head -5 # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 0.000326036 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 0.000326036 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000714158 创建 job 配置 prometheus-additional.yaml\n- job_name: \u0026#34;node-exporter\u0026#34; static_configs: - targets: - \u0026#34;1.1.1.4:9100\u0026#34; 创建 secret additional-scrape-configs.yaml\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml 修改 prometheus 资源 prometheus-prometheus.yaml , 添加 additionalScrapeConfigs\nkind: Prometheus spec: # 添加如下三行 additionalScrapeConfigs: name: additional-scrape-configs # secret name key: prometheus-additional.yaml # secret key 更新一下 prometheus\n[root@k8s-node1 demo]# kubectl apply -f ../prometheus-prometheus.yaml 查看结果\n3 动态更新 后续所有的自定义配置直接更新现有的 secret 即可\n比如在之前的 node-exporter 的 job 中新增一个 target\n修改 prometheus-additional.yaml\n- job_name: \u0026#34;node-exporter\u0026#34; static_configs: - targets: - \u0026#34;1.1.1.4:9100\u0026#34; - \u0026#34;192.168.17.99:59100\u0026#34; 更新 secret\nkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml prometheus 会自动重载配置\n4 更新失败排查 如果修改了 secret 不生效一定要注意 secret 部署的 namespace 是不是 monitoring\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-2-additionalscrapeconfig/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1 简介 使用原生的 prometheus 时, 我们创建 job 直接修改配置文件即可, 然而在 prometheus-operator 中所有的配置都抽象成了 k8s CRD 资源, 手动配置 job 需要: 创建 secret 在 prometheus CRD 资源中配置 additionalScrapeConfigs additional-scrape-config 官方示例 2 示例 2.1 node-exporter 添加 k8s 集群外的 node-exporter metrics 在 1.1.1.4 部署 node-exporter docker run -d --name node-exporter \\ -p 9102:9100 \\ -v \u0026#34;/proc:/host/proc:ro\u0026#34; \\ -v \u0026#34;/sys:/host/sys:ro\u0026#34; \\ -v \u0026#34;/:/rootfs:ro\u0026#34; \\ registry.cn-hangzhou.aliyuncs.com/lvbibir/node-exporter:v1.3.1 \\ --path.sysfs=/host/sys \\ --path.rootfs=/roofs # 验证可用性 [root@1-1-1-4 ~]# curl -s 1.1.1.4:9100/metrics | head -5 # HELP","title":"prometheus (二) 静态配置"},{"content":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1\n1 简介 1.1 prometheus operator Prometheus Operator: 在 Kubernetes 上管理 Prometheus 集群。该项目的目的是简化和自动化基于 Prometheus 的 Kubernetes 集群监控堆栈的配置。\n所有 CRD 资源的 API 文档\nPrometheus Operator 的核心特性是 watch Kubernetes API 服务器对特定对象的更改，并确保当前 Prometheus 部署与这些对象匹配。\nmonitoring.coreos.com/v1:\nprometheus 相关 Prometheus: 配置 Prometheus statefulset 及 Prometheus 的一些配置。 ServiceMonitor: 用于通过 Service 对 K8S 中的资源进行监控，推荐首选 ServiceMonitor. 它声明性地指定了 Kubernetes service 应该如何被监控。 PodMonitor: 用于对 Pod 进行监控，推荐首选 ServiceMonitor. PodMonitor 声明性地指定了应该如何监视一组 pod。 Probe: 它声明性地指定了应该如何监视 ingress 或静态目标组. 一般用于黑盒监控. PrometheusRule: 用于管理 Prometheus 告警规则；它定义了一套所需的 Prometheus 警报和/或记录规则。可以被 Prometheus 实例挂载使用。 Alertmanager 相关 Alertmanager: 配置 AlertManager statefulset 及 AlertManager 的一些配置。 AlertmanagerConfig: 用于管理 AlertManager 配置文件；它声明性地指定 Alertmanager 配置的子部分，允许将警报路由到自定义接收器，并设置禁止规则。 其他 ThanosRuler: 管理 ThanosRuler deployment； 1.2 kube-prometheus kube-prometheus 提供了一个基于 Prometheus 和 Prometheus Operator 的完整集群监控堆栈的示例配置。这包括部署多个 Prometheus 和 Alertmanager 实例、用于收集节点指标的指标导出器（如 node_exporters)、将 Prometheus 链接到各种指标端点的目标配置，以及用于通知集群中潜在问题的示例警报规则。\n2 部署 kubernets 与 kube-prometheus 的兼容性关系如下\nkube-prometheus stack Kubernetes 1.21 Kubernetes 1.22 Kubernetes 1.23 Kubernetes 1.24 Kubernetes 1.25 release-0.9 ✔ ✔ ✗ ✗ ✗ release-0.10 ✗ ✔ ✔ ✗ ✗ release-0.11 ✗ ✗ ✔ ✔ ✗ release-0.12 ✗ ✗ ✗ ✔ ✔ kube-prometheus 项目提供的 yaml 中使用的镜像大部分是 quay.io 或者 k8s.gcr.io 等外网仓库的镜像，博主已经将所需镜像上传到了阿里云，且 fork 官方仓库后修改了 yaml 中的镜像仓库地址，可以直接拉取我修改后的 yaml\n这里我的 k8s 测试集群版本是 1.22.3，部署 release-0.10 版本的 kube-prometheus\n[root@k8s-node1 opt]# cd /opt/ \u0026amp;\u0026amp; git clone https://github.com/lvbibir/kube-prometheus -b release-0.10 [root@k8s-node1 kube-prometheus]# cd /opt/kube-prometheus/ [root@k8s-node1 kube-prometheus]# kubectl create -f manifests/setup/ [root@k8s-node1 kube-prometheus]# kubectl create -f manifests/ 验证\n[root@k8s-node1 kube-prometheus]# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 5m16s alertmanager-main-1 2/2 Running 0 5m16s alertmanager-main-2 2/2 Running 0 5m16s blackbox-exporter-7c8787786-r9lmv 3/3 Running 0 8m12s grafana-795c6dd64b-8cspz 1/1 Running 0 8m11s kube-state-metrics-56f79b8fdc-9p97x 3/3 Running 0 8m11s node-exporter-4scm6 2/2 Running 0 8m11s node-exporter-7hlrp 2/2 Running 0 8m11s node-exporter-pph2d 2/2 Running 0 8m11s prometheus-adapter-5595dcc894-nmn2w 1/1 Running 0 8m10s prometheus-adapter-5595dcc894-rzdhv 1/1 Running 0 8m10s prometheus-k8s-0 2/2 Running 0 5m15s prometheus-k8s-1 2/2 Running 0 5m15s prometheus-operator-7575c94984-7r4l9 2/2 Running 0 8m10s [root@k8s-node1 kube-prometheus]# kubectl get svc -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-main NodePort 10.104.94.103 \u0026lt;none\u0026gt; 9093:39093/TCP,8080:10422/TCP 9m12s alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 6m16s blackbox-exporter ClusterIP 10.103.168.130 \u0026lt;none\u0026gt; 9115/TCP,19115/TCP 9m12s grafana NodePort 10.108.134.168 \u0026lt;none\u0026gt; 3000:33000/TCP 9m11s kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 8443/TCP,9443/TCP 9m11s node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 9m11s prometheus-adapter ClusterIP 10.111.22.126 \u0026lt;none\u0026gt; 443/TCP 9m10s prometheus-k8s NodePort 10.111.183.173 \u0026lt;none\u0026gt; 9090:39090/TCP,8080:43263/TCP 9m11s prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 6m15s prometheus-operator ClusterIP None \u0026lt;none\u0026gt; 8443/TCP 9m10s 可以通过 nodePort 访问，也可以通过 ingress 将 grafana 暴露到外部\ngrafana 默认用户名密码为 admin/admin\n2.1 组件介绍 kube-prometheus 中部署了如下组件:\nCRD 资源: manifests/setup 目录中的内容 实际的组件部署和配置: manifests 目录中的内容 exporter 数据采集器: node-exporter blackbox-exporter 用于黑盒监控 prometheus 实例 grafana 实例 alertmanager 实例 adapter 实例, 替代原始 metrics-server 组件, 实现自定义 HPA 指标, 参考 3 数据持久化 3.1 prometheus prometheus 默认的数据文件使用的是 emptydir 方式进行的持久化, 我们改为 nfs\n修改 manifests/prometheus-prometheus.yaml\n在文件最后新增配置\nretention: 15d # 监控数据保存的时间为 15 天 storage: # 存储配置, 使用 nfs 的 storageClass volumeClaimTemplate: spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 应用后查看新建的 pod 中的 volumes 信息\n# pod.spec.container.volumeMounts [root@k8s-node1 ~]# kubectl get pod prometheus-k8s-0 -n monitoring -o jsonpath=\u0026#39;{.spec.containers[?(@.name==\u0026#34;prometheus\u0026#34;)].volumeMounts[?(@.name==\u0026#34;prometheus-k8s-db\u0026#34;)]}{\u0026#34;\\n\u0026#34;}\u0026#39; | python -m json.tool { \u0026#34;mountPath\u0026#34;: \u0026#34;/prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-db\u0026#34;, \u0026#34;subPath\u0026#34;: \u0026#34;prometheus-db\u0026#34; } # pod.spec.volumes [root@k8s-node1 ~]# kubectl get pod prometheus-k8s-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;prometheus-k8s-db\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;name\u0026#34;: \u0026#34;prometheus-k8s-db\u0026#34;, \u0026#34;persistentVolumeClaim\u0026#34;: { \u0026#34;claimName\u0026#34;: \u0026#34;prometheus-k8s-db-prometheus-k8s-0\u0026#34; } } 与传统 statefulset 不同的是, prometheus 识别到 .spec.storage.volumeClaimTemplate 配置后会自动将 prometheus 的数据文件挂载到自动创建的 pvc 上, 无需手动指定 name 然后挂载\n3.2 alertmanager 与 prometheus 类似, 这里就不赘述了\nmanifests/alertmanager-alertmanager.yaml\nstorage: # 存储配置, 使用 nfs 的 storageClass volumeClaimTemplate: spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 重新 apply 之后, 查看新生成 pod 的 volumes\n# pod.spec.container.volumeMounts [root@k8s-node1 ~]# kubectl get pod alertmanager-main-0 -n monitoring -o jsonpath=\u0026#39;{.spec.containers[?(@.name==\u0026#34;alertmanager\u0026#34;)].volumeMounts[?(@.name==\u0026#34;alertmanager-main-db\u0026#34;)]}{\u0026#34;\\n\u0026#34;}\u0026#39; | python -m json.tool { \u0026#34;mountPath\u0026#34;: \u0026#34;/alertmanager\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;alertmanager-main-db\u0026#34;, \u0026#34;subPath\u0026#34;: \u0026#34;alertmanager-db\u0026#34; } # pod.spec.volumes [root@k8s-node1 ~]# kubectl get pod alertmanager-main-0 -n monitoring -o jsonpath=\u0026#39;{.spec.volumes[?(@.name==\u0026#34;alertmanager-main-db\u0026#34;)]}\u0026#39; | python -m json.tool { \u0026#34;name\u0026#34;: \u0026#34;alertmanager-main-db\u0026#34;, \u0026#34;persistentVolumeClaim\u0026#34;: { \u0026#34;claimName\u0026#34;: \u0026#34;alertmanager-main-db-alertmanager-main-0\u0026#34; } } 3.3 grafana grafana 就是一个普通的 deployment 应用, 直接修改 yaml 中的 volume 配置即可\n[root@k8s-node1 ~]# mkdir /nfs/kubernetes/grafana-data [root@k8s-node1 ~]# chmod -R 777 /nfs/kubernetes/grafana-data 修改 manifests/grafana-deployment.yaml 直接将默认的 emptydir 修改为 nfs 即可\nvolumes: - name: grafana-storage nfs: server: k8s-node1 path: /nfs/kubernetes/grafana-data 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-prometheus-1-deploy/","summary":"0 前言 基于 centos7.9 docker-ce-20.10.18 kubelet-1.22.3-0 kube-prometheus-0.10 prometheus-v2.32.1 1 简介 1.1 prometheus operator Prometheus Operator: 在 Kubernetes 上管理 Prometheus 集群。该项目的目的是简化和自动化基于 Prometheus 的 Kubernetes 集群监控堆栈的配置。 所有 CRD 资源的 API 文档 Prometheus Operator 的核心特性是 watch Kubernetes API 服务器对特定对象的更改，并确保当前 Prometheus 部署与这些对象匹配。 monitoring.coreos.com/v1: prometheus 相关 Prometheus: 配置 Prometheus statefulset 及 Prometheus 的一些配置。 ServiceMonitor: 用于通过 Service 对 K8S 中的资源进行监控，推荐首","title":"prometheus (一) 简介及部署"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示\n1 简介 traefik 的路由规则就可以实现 4 层和 7 层的基本负载均衡操作，使用 IngressRoute IngressRouteTCP IngressRouteUDP 资源即可。但是如果想要实现 加权轮询、流量复制 等高级操作，traefik 抽象出了一个 TraefikService 资源。此时整体流量走向为：外部流量先通过 entryPoints 端口进入 traefik，然后由 IngressRoute/IngressRouteTCP/IngressRouteUDP 匹配后进入 TraefikService，在 TraefikService 这一层实现加权轮循和流量复制，最后将请求转发至 kubernetes 的 service。\n除此之外 traefik 还支持 7 层的粘性会话、健康检查、传递请求头、响应转发、故障转移等操作。\n2 灰度发布 (加权轮询) 官方文档\n灰度发布也称为金丝雀发布，让一部分即将上线的服务发布到线上，观察是否达到上线要求，主要通过加权轮询的方式实现。\n创建 traefikService 和 inressRoute 资源，实现 wrr 加权轮询\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroutewrr namespace: default spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/`) kind: Rule services: - name: wrr namespace: default kind: TraefikService --- apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: wrr namespace: default spec: weighted: services: - name: myapp1 port: 80 weight: 1 # 定义权重 kind: Service # 可选，默认就是 Service - name: myapp2 port: 80 weight: 2 测试结果如下，可以看到每 3 次访问会有 1 次流量落到 v1 应用， 2 次流量落到 v2 应用\n[root@k8s-node1 ~]# for i in {1..9}; do curl http://myapp.test.com \u0026amp;\u0026amp; sleep 1; done Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 3 会话保持 (粘性会话) 官方文档\n会话保持功能依赖加权轮询功能\n当我们使用 traefik 的负载均衡时，默认情况下轮循多个 k8s 的 service 服务，如果用户对同一内容的多次请求，可能被转发到了不同的后端服务器。假设用户发出请求被分配至服务器 A，保存了一些信息在 session 中，该用户再次发送请求被分配到服务器 B，要用之前保存的信息，若服务器 A 和 B 之间没有 session 粘滞，那么服务器 B 就拿不到之前的信息，这样会导致一些问题。traefik 同样也支持粘性会话，可以让用户在一次会话周期内的所有请求始终转发到一台特定的后端服务器上。\n创建 traefikervie 和 ingressRoute，实现基于 cookie 的会话保持\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroute-sticky namespace: default spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/`) kind: Rule services: - name: sticky namespace: default kind: TraefikService --- apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: sticky namespace: default spec: weighted: services: - name: myapp1 port: 80 weight: 1 # 定义权重 - name: myapp2 port: 80 weight: 2 sticky: # 开启粘性会话 cookie: # 基于cookie区分客户端 name: test-cookie # 指定客户端请求时，包含的cookie名称 客户端访问测试，携带 cookie\n[root@k8s-node1 ~]# for i in {1..5}; do curl -b \u0026#34;test-cookie=default-myapp2-80\u0026#34; http://myapp.test.com; done Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# for i in {1..5}; do curl -b \u0026#34;test-cookie=default-myapp1-80\u0026#34; http://myapp.test.com; done Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 4 流量复制 官方文档\n所谓的流量复制，也称为镜像服务是指将请求的流量按规则复制一份发送给其它服务，并且会忽略这部分请求的响应，这个功能在做一些压测或者问题复现的时候很有用。\n创建 traefikService 和 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroute-mirror namespace: default spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/`) kind: Rule services: - name: mirror-from-service namespace: default kind: TraefikService --- apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: mirror-from-service namespace: default spec: mirroring: name: myapp1 # 发送 100% 的请求到 myapp1 port: 80 mirrors: - name: myapp2 # 然后复制 10% 的请求到 myapp2 port: 80 percent: 10, 测试如下，可以看到只有 myapp1 应用会有数据返回\n[root@k8s-node1 ~]# for i in {1..20}; do curl http://myapp.test.com \u0026amp;\u0026amp; sleep 1; done Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; ....... myapp2 应用同样收到了请求，与预期相同，收到了 10% 的流量，\n[root@k8s-node1 ~]# kubectl logs -l app=bar .... 10.244.36.64 - - [20/Apr/2023:07:04:33 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 65 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; 10.244.36.64 - - [20/Apr/2023:07:04:43 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 65 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-4-traefikservice/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示 1 简介 traefik 的路由规则就可以实现 4 层和 7 层的基本负载均衡操作，使用 IngressRoute IngressRouteTCP IngressRouteUDP 资源即可。但是如果想要实现 加权轮询、流量复制 等高级操作，t","title":"traefik (四) TraefikService 服务"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示\n1 简介 官方文档\nTraefik Middlewares 是一个处于路由和后端服务之前的中间件，在外部流量进入 Traefik，且路由规则匹配成功后，将流量发送到对应的后端服务前，先将其发给中间件进行一系列处理（类似于过滤器链 Filter，进行一系列处理），例如，添加 Header 头信息、鉴权、流量转发、处理访问路径前缀、IP 白名单等等，经过一个或者多个中间件处理完成后，再发送给后端服务，这个就是中间件的作用。\nTraefik 内置了很多不同功能的 Middleware，主要是针对 HTTP 和 TCP，这里挑选几个比较常用的进行演示。\n1.1 重定向 -redirectScheme 官方文档\n定义一个 ingressroute，包含一个自动将 http 跳转到 https 的中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp2 spec: entryPoints: - web routes: - match: Host(`myapp2.test.com`) kind: Rule services: - name: myapp2 port: 80 middlewares: - name: redirect-https-middleware # 指定使用RedirectScheme中间件，完成http强制跳转至https --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: redirect-https-middleware spec: redirectScheme: scheme: https # 自动跳转到 https 测试，可以看到访问 http 自动 307 重定向到了 https\n[root@k8s-node1 ~]# curl -I http://myapp2.test.com HTTP/1.1 307 Temporary Redirect Location: https://myapp2.test.com/ Date: Wed, 19 Apr 2023 07:58:34 GMT Content-Length: 18 Content-Type: text/plain; charset=utf-8 1.2 去除请求路径前缀 -stripPrefix 官方文档\n假设现在有这样一个需求，当访问 http://myapp.test.com/v1 时，流量调度至 myapp1。当访问 http://myapp.test.com/v2 时，流量调度至 myapp2。这种需求是非常常见的，在 NGINX 中，我们可以配置多个 Location 来定制规则，使用 Traefik 也可以这么做。但是定制不同的前缀后，由于应用本身并没有这些前缀，导致请求返回 404，这时候我们就需要对请求的 path 进行处理。\n创建一个 IngressRoute，并设置两条规则，根据不同的访问路径代理至相对应的 service\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp spec: entryPoints: - web routes: - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/v1`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: prefix-url-middleware - match: Host(`myapp.test.com`) \u0026amp;\u0026amp; PathPrefix(`/v2`) kind: Rule services: - name: myapp2 port: 80 middlewares: - name: prefix-url-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: prefix-url-middleware spec: stripPrefix: # 去除前缀的中间件 stripPrefix，指定将请求路径中的v1、v2去除。 prefixes: - /v1 - /v2 部署测试\n[root@k8s-node1 ~]# curl http://myapp.test.com/v1 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://myapp.test.com/v2 Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# kubectl logs -l app=myapp1 | tail -2 # 未添加插件的访问路径为 /v1/ 10.244.36.64 - - [19/Apr/2023:08:02:03 +0000] \u0026#34;GET /v1/ HTTP/1.1\u0026#34; 404 169 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; # 添加插件后的访问路径为 / 10.244.36.64 - - [19/Apr/2023:08:04:31 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 65 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;1.1.1.1\u0026#34; 1.3 白名单 -IPWhiteList 官方文档\n为提高安全性，通常情况下一些管理员界面会设置 ip 访问白名单，只希望个别用户可以访问。\n示例\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: ip-white-list-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: ip-white-list-middleware spec: ipWhiteList: sourceRange: - 127.0.0.1/32 - 1.1.1.253 测试\n# 白名单外主机 [root@k8s-node1 ~]# curl -I http://myapp1.test.com HTTP/1.1 403 Forbidden # 白名单内主机 Admin@BJLPT0152 MINGW64 ~ $ ipconfig | grep 1.1.1.253 IPv4 地址 . . . . . . . . . . . . : 1.1.1.253 Admin@BJLPT0152 MINGW64 ~ $ curl -i http://myapp1.test.com Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 1.4 基础用户认证 -basicAuth 官方文档\n通常企业安全要求规范除了要对管理员页面限制访问 ip 外，还需要添加账号密码认证，而 traefik 默认没有提供账号密码认证功能，此时就可以通过 BasicAuth 中间件完成用户认证，只有认证通过的授权用户才可以访问页面。\n安装 htpasswd 工具生成密码文件\n[root@k8s-node1 ~]# yum install -y httpd [root@k8s-node1 ~]# htpasswd -bc basic-auth-secret-lvbibir lvbibir 123 Adding password for user lvbibir [root@k8s-node1 ~]# kubectl create secret generic basic-auth-lvbibir --from-file=basic-auth-secret-lvbibir secret/basic-auth-lvbibir created 创建 ingressroute，使用 basicAuth 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: basic-auth-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: basic-auth-middleware spec: basicAuth: secret: basic-auth-lvbibir 访问测试，可以看到弹出界面提示需要输入用户名和密码，输入后回车显示正常页面\n1.5 修改请求/响应头信息 -headers 官方文档\n为了提高业务的安全性，安全团队会定期进行漏洞扫描，其中有些 web 漏洞就需要通过修改响应头处理，traefik 的 Headers 中间件不仅可以修改返回客户端的响应头信息，还能修改反向代理后端 service 服务的请求头信息。\n例如对 https://myapp2.test.com 提高安全策略，强制启用 HSTS\nHSTS：即 HTTP 严格传输安全响应头，收到该响应头的浏览器会在 63072000s（约 2 年）的时间内，只要访问该网站，即使输入的是 http，浏览器会自动跳转到 https。（HSTS 是浏览器端的跳转，之前的 HTTP 重定向到 HTTPS 是服务器端的跳转）\n创建 ingressRoute 和 headers 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp2-tls spec: entryPoints: - web - websecure routes: - match: Host(`myapp2.test.com`) kind: Rule services: - name: myapp2 port: 80 middlewares: - name: hsts-header-middleware tls: secretName: myapp2-tls # 指定tls证书名称 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: hsts-header-middleware spec: headers: customResponseHeaders: Strict-Transport-Security: \u0026#39;max-age=63072000\u0026#39; 访问测试\n[root@k8s-node1 ~]# curl -kI https://myapp2.test.com HTTP/1.1 200 OK Accept-Ranges: bytes Content-Length: 65 Content-Type: text/html Date: Wed, 19 Apr 2023 08:37:07 GMT Etag: \u0026#34;5a9251f0-41\u0026#34; Last-Modified: Sun, 25 Feb 2018 06:04:32 GMT Server: nginx/1.12.2 Strict-Transport-Security: max-age=63072000 # headers 插件添加的响应头 1.6 限流 -rateLimit 官方文档\n在实际生产环境中，流量限制也是经常用到的，它可以用作安全目的，比如可以减慢暴力密码破解的速率。通过将传入请求的速率限制为真实用户的典型值，并标识目标 URL 地址 (通过日志)，还可以用来抵御 DDOS 攻击。更常见的情况，该功能被用来保护下游应用服务器不被同时太多用户请求所压垮。\n创建 ingressRoute 和 rateLimit 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: rate-limit-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: rate-limit-middleware spec: rateLimit: # 指定 1s 内请求数平均值不大于 10 个，高峰最大值不大于 50 个。 burst: 10 average: 50 压力测试，使用 ab 工具进行压力测试，一共请求 100 次，每次并发 10。测试结果失败的请求为 72 次，总耗时 0.409 秒\n[root@k8s-node1 ~]# ab -n 100 -c 10 \u0026#34;http://myapp1.test.com/\u0026#34; Concurrency Level: 10 Time taken for tests: 0.409 seconds Complete requests: 100 Failed requests: 72 (Connect: 0, Receive: 0, Length: 72, Exceptions: 0) Non-2xx responses: 72 1.7 熔断 -circuitBreaker 官方文档\n服务熔断的作用类似于保险丝，当某服务出现不可用或响应超时的情况时，为了防止整个系统出现雪崩，暂时停止对该服务的调用。\n熔断器三种状态\nClosed：关闭状态，所有请求都正常访问。 Open：打开状态，所有请求都会被降级。traefik 会对请求情况计数，当一定时间内失败请求百分比达到阈值，则触发熔断，断路器会完全打开。 Recovering：半开恢复状态，open 状态不是永久的，打开后会进入休眠时间。随后断路器会自动进入半开状态。此时会释放部分请求通过，若这些请求都是健康的，则会完全关闭断路器，否则继续保持打开，再次进行休眠计时 服务熔断原理 (断路器的原理)\n统计用户在指定的时间范围（默认 10s）之内的请求总数达到指定的数量之后，如果不健康的请求 (超时、异常) 占总请求数量的百分比（50%）达到了指定的阈值之后，就会触发熔断。触发熔断，断路器就会打开 (open),此时所有请求都不能通过。在 5s 之后，断路器会恢复到半开状态 (half open)，会允许少量请求通过，如果这些请求都是健康的，那么断路器会回到关闭状态 (close).如果这些请求还是失败的请求,断路器还是恢复到打开的状态 (open).\ntraefik 支持的触发器\nNetworkErrorRatio：网络错误率 ResponseCodeRatio：状态代码比率 LatencyAtQuantileMS：分位数的延迟（以毫秒为单位） 创建 ingressRoute ，添加 circuitBreaker 中间件，指定 50% 的请求比例响应时间大于 1MS 时熔断。\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web routes: - match: Host(`myapp1.test.com`) kind: Rule services: - name: myapp1 port: 80 middlewares: - name: circuit-breaker-middleware --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: circuit-breaker-middleware spec: circuitBreaker: expression: LatencyAtQuantileMS(50.0) \u0026gt; 1 压力测试，一共请求 1000 次，每次并发 100 次。触发熔断机制，测试结果失败的请求为 999 次，总耗时 1.742 秒。\n[root@k8s-node1 traefik]# ab -n 1000 -c 100 \u0026#34;http://myapp1.test.com/\u0026#34; Concurrency Level: 100 Time taken for tests: 1.742 seconds Complete requests: 1000 Failed requests: 999 (Connect: 0, Receive: 0, Length: 2, Exceptions: 0) Write errors: 0 Non-2xx responses: 999 1.8 自定义错误页 -errorPages 官方文档\n在实际的业务中，肯定会存在 4XX 5XX 相关的错误异常，如果每个应用都开发一个单独的错误页，无疑大大增加了开发成本，traefik 同样也支持自定义错误页，但是需要注意的是，错误页面不是由 traefik 存储处理，而是通过定义中间件，将错误的请求重定向到其他的页面。\n首先，我们先创建一个应用。这个 web 应用的功能是：\n当请求 / 时，返回状态码为 200 当请求 /400 时，返回 400 状态码 当请求 /500 时，返回 500 状态码 创建 deployment svc\napiVersion: apps/v1 kind: Deployment metadata: name: flask spec: selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: cuiliang0302/request-code:v2.0 imagePullPolicy: IfNotPresent resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: flask spec: type: ClusterIP selector: app: flask ports: - port: 5000 targetPort: 5000 创建 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: flask spec: entryPoints: - web routes: - match: Host(`flask.test.com`) kind: Rule services: - name: flask port: 5000 访问测试，模拟 400 500 错误\n[root@k8s-node1 ~]# curl -I http://flask.test.com HTTP/1.1 200 OK [root@k8s-node1 ~]# curl -I http://flask.test.com/400 HTTP/1.1 400 Bad Request [root@k8s-node1 ~]# curl -I http://flask.test.com/500 HTTP/1.1 500 Internal Server Error [root@k8s-node1 ~]# curl -I http://flask.test.com/404 HTTP/1.1 404 Not Found 现在提出一个新的需求，当我访问 flask 项目时，如果错误码为 400，返回 myapp1 的页面，如果错误码为 500，返回 myapp2 的页面 (前提是 myapp1 和 myapp2 服务已创建)。\n创建 errors 中间件\napiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: errors5 spec: errors: status: - \u0026#34;500-599\u0026#34; # query: /{status}.html # 可以为每个页面定义一个状态码，也可以指定5XX使用统一页面返回 query : / # 指定返回myapp2的请求路径 service: name: myapp2 port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: errors4 spec: errors: status: - \u0026#34;400-499\u0026#34; # query: /{status}.html # 可以为每个页面定义一个状态码，也可以指定5XX使用统一页面返回 query : / # 指定返回myapp1的请求路径 service: name: myapp1 port: 80 修改 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: flask spec: entryPoints: - web routes: - match: Host(`flask.test.com`) kind: Rule services: - name: flask port: 5000 middlewares: - name: errors4 - name: errors5 访问测试，可以看到 400 页面和 500 页面已经成功重定向了\n[root@k8s-node1 ~]# curl http://flask.test.com/ \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;flask\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;hello flask\u0026lt;/h1\u0026gt; \u0026lt;img src=\u0026#34;/static/photo.jpg\u0026#34; alt=\u0026#34;photo\u0026#34;\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@k8s-node1 ~]# curl http://flask.test.com/400 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://flask.test.com/500 Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 1.9 数据压缩 -compress 官方文档\n有时候客户端和服务器之间会传输比较大的报文数据，这时候就占用较大的网络带宽和时长。为了节省带宽，加速报文的响应速速，可以将传输的报文数据先进行压缩，然后再进行传输，traefik 也同样支持数据压缩。\ntraefik 默认只对大于 1024 字节，且请求标头包含 Accept-Encoding gzip 的资源进行压缩。可以指定排除特定类型不启用压缩或者根据内容大小来决定是否压缩。\n继续使用上面创建的 flask 应用，现在创建中间件并修改 ingressRoute，使用默认配置策略即可。\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: flask spec: entryPoints: - web routes: - match: Host(`flask.test.com`) kind: Rule services: - name: flask port: 5000 middlewares: - name: compress --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: compress spec: compress: {} 访问测试\nhtml 文件小于 1024 字节，未开启压缩\n图片资源大于 1024 字节，开启了压缩\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-3-middleware/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 示例中用到的 myapp 和 secret 资源请查看系列文章第二篇中的演示 1 简介 官方文档 Traefik Middlewares 是一个处于路由和后端服务之前的中间件，在外部流量进入 Traefik，且路由规则匹配成功后，将流量发送到对应的","title":"traefik (三) Middleware 中间件"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n1 简介 官方文档\n1.1 三种方式 Traefik 创建路由规则有多种方式，比如：\n原生 Ingress 写法 使用 CRD IngressRoute 方式 使用 GatewayAPI 的方式 相较于原生 Ingress 写法，ingressRoute 是 2.1 以后新增功能，简单来说，他们都支持路径 (path) 路由和域名 (host) HTTP 路由，以及 HTTPS 配置，区别在于 IngressRoute 需要定义 CRD 扩展，但是它支持了 TCP、UDP 路由以及中间件等新特性，强烈推荐使用 ingressRoute\n1.2 匹配规则 规则 描述 Headers(key, value) 检查 headers 中是否有一个键为 key 值为 value 的键值对 HeadersRegexp(key, regexp) 检查 headers 中是否有一个键位 key 值为正则表达式匹配的键值对 Host(example.com, …) 检查请求的域名是否包含在特定的域名中 HostRegexp(example.com, {subdomain:[a-z]+}.example.com, …) 检查请求的域名是否包含在特定的正则表达式域名中 Method(GET, …) 检查请求方法是否为给定的 methods(GET、POST、PUT、DELETE、PATCH) 中 Path(/path, /articles/{cat:[a-z]+}/{id:[0-9]+}, …) 匹配特定的请求路径，它接受一系列文字和正则表达式路径 PathPrefix(/products/, /articles/{cat:[a-z]+}/{id:[0-9]+}) 匹配特定的前缀路径，它接受一系列文字和正则表达式前缀路径 Query(foo=bar, bar=baz) 匹配查询字符串参数，接受 key=value 的键值对 ClientIP(10.0.0.0/16, ::1) 如果请求客户端 IP 是给定的 IP/CIDR 之一，则匹配。它接受 IPv4、IPv6 和网段格式。 2 dashboard 案例 之前的部署章节中我们是以 nodePort 和 service nodePort 的方式访问的 traefik 的 dashboard，接下来以三种方式演示通过域名访问 dashboard\n2.1 ingress apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: traefik-dashboard namespace: traefik annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/router.entrypoints: web spec: rules: - host: ingress.test.com http: paths: - pathType: Prefix path: / backend: service: name: traefik port: number: 9000 访问: http://ingress.test.com\n2.2 ingressRoute apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard namespace: traefik spec: entryPoints: - web routes: - match: Host(`traefik.test.com`) kind: Rule services: - name: api@internal kind: TraefikService namespace: traefik 访问：http://traefik.test.com\n2.3 Gateway API 目前，Traefik 对 Gateway APIs 的实现是基于 v1alpha1 版本的规范，目前最新的规范是 v1alpha2，所以和最新的规范可能有一些出入的地方。\n创建 gatewayClass\napiVersion: networking.x-k8s.io/v1alpha1 kind: GatewayClass metadata: name: traefik spec: controller: traefik.io/gateway-controller 创建 gateway\napiVersion: networking.x-k8s.io/v1alpha1 kind: Gateway metadata: name: http-gateway namespace: kube-system spec: gatewayClassName: traefik listeners: - protocol: HTTP port: 80 routes: kind: HTTPRoute namespaces: from: All selector: matchLabels: app: traefik 创建 httproute\napiVersion: networking.x-k8s.io/v1alpha1 kind: HTTPRoute metadata: name: traefik-dashboard namespace: kube-system labels: app: traefik spec: hostnames: - \u0026#34;gateway.test.com\u0026#34; rules: - matches: - path: type: Prefix value: / forwardTo: - serviceName: traefik port: 9000 weight: 1 访问：http://gateway.test.com\n3 myapp 环境准备 myapp1\napiVersion: apps/v1 kind: Deployment metadata: name: myapp1 spec: selector: matchLabels: app: myapp1 template: metadata: labels: app: myapp1 spec: containers: - name: myapp1 image: ikubernetes/myapp:v1 resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp1 spec: type: ClusterIP selector: app: myapp1 ports: - port: 80 targetPort: 80 myapp2\napiVersion: apps/v1 kind: Deployment metadata: name: myapp2 spec: selector: matchLabels: app: myapp2 template: metadata: labels: app: myapp2 spec: containers: - name: myapp2 image: ikubernetes/myapp:v2 resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp2 spec: type: ClusterIP selector: app: myapp2 ports: - port: 80 targetPort: 80 创建资源并访问测试\n[root@k8s-node1 ~]# vim demo/app/myapp1.yml [root@k8s-node1 ~]# vim demo/app/myapp2.yml [root@k8s-node1 ~]# kubectl apply -f demo/app/ deployment.apps/myapp1 created service/myapp1 created deployment.apps/myapp2 created service/myapp2 created [root@k8s-node1 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 12d myapp1 ClusterIP 10.100.229.135 \u0026lt;none\u0026gt; 80/TCP 33s myapp2 ClusterIP 10.96.56.49 \u0026lt;none\u0026gt; 80/TCP 33s [root@k8s-node1 ~]# curl 10.100.229.135 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl 10.96.56.49 Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 4 ingressRoute 4.1 http 路由 实现目标：集群外部用户通过访问 http://myapp1.test.com 域名时，将请求代理至 myapp1 应用。\n创建 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web # 与 configmap 中定义的 entrypoint 名字相同 routes: - match: Host(`myapp1.test.com`) # 域名 kind: Rule services: - name: myapp1 # 与svc的name一致 port: 80 # 与svc的port一致 部署\n[root@k8s-node1 ~]# vim demo/ingressroute/http-myapp1.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressroute/http-myapp1.yml ingressroute.traefik.containo.us/myapp1 created 访问测试\n4.2 https 路由 自签名证书\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=myapp2.test.com\u0026#34; 创建 tls 类型的 secret\nkubectl create secret tls myapp2-tls --cert=tls.crt --key=tls.key 创建 ingressRoute\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp2 spec: entryPoints: - websecure # 监听 websecure 这个入口点，也就是通过 443 端口来访问 routes: - match: Host(`myapp2.test.com`) kind: Rule services: - name: myapp2 port: 80 tls: secretName: myapp2-tls # 指定tls证书名称 部署\n[root@k8s-node1 ~]# vim demo/ingressroute/https-myapp2.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressroute/https-myapp2.yml ingressroute.traefik.containo.us/myapp2 created 访问测试，由于是自签名证书，所以会提示不安全\n5 ingressRouteTCP ingreeRouteTCP 官方文档\n5.1 不带 TLS 证书 部署 mysql\napiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql namespace: default data: my.cnf: | [mysqld] character-set-server = utf8mb4 collation-server = utf8mb4_unicode_ci skip-character-set-client-handshake = 1 default-storage-engine = INNODB max_allowed_packet = 500M explicit_defaults_for_timestamp = 1 long_query_time = 10 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: mysql name: mysql spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD value: abc123 ports: - containerPort: 3306 volumeMounts: - mountPath: /etc/mysql/conf.d/my.cnf subPath: my.cnf name: cm volumes: - name: cm configMap: name: mysql --- apiVersion: v1 kind: Service metadata: name: mysql namespace: default spec: ports: - port: 3306 protocol: TCP targetPort: 3306 selector: app: mysql ingressRouteTCP\nSNI 为服务名称标识，是 TLS 协议的扩展。因此，只有 TLS 路由才能使用该规则指定域名。非 TLS 路由使用带有 * 的规则来声明每个非 TLS 请求都将由路由进行处理。\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: mysql namespace: default spec: entryPoints: - tcpep # 9200 端口 routes: - match: HostSNI(`*`) # 由于 Traefik 中使用 TCP 路由配置需要 SNI，而 SNI 又是依赖 TLS 的，所以我们需要配置证书才行，如果没有证书的话，我们可以使用通配符*(适配ip)进行配置 services: - name: mysql port: 3306 部署\n[root@k8s-node1 ~]# vim demo/ingressrouteTCP/mysql.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressrouteTCP/mysql.yml configmap/mysql created deployment.apps/mysql created service/mysql created [root@k8s-node1 ~]# vim demo/ingressrouteTCP/route.yml [root@k8s-node1 ~]# kubectl apply -f demo/ingressrouteTCP/route.yml ingressroutetcp.traefik.containo.us/mysql created 集群外主机验证\n添加 hosts (mysql.test.com) 以 root \u0026amp; abc123 访问 9200 端口 5.2 带 TLS 证书 大多数情况下 tcp 路由不需要配置 TLS ，下面仅演示两个关键步骤\n创建 tls 类型的 secret\nkubectl create secret tls redis-tls --key=redis.key --cert=redis.crt 创建 ingressRouteTCP，需要携带 secret\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: redis spec: entryPoints: - redisep routes: - match: HostSNI(`redis.test.com`) services: - name: redis port: 6379 tls: secretName: redis-tls 6 ingressRouteUDP 创建应用\nkind: Deployment apiVersion: apps/v1 metadata: name: whoamiudp labels: app: whoamiudp spec: replicas: 2 selector: matchLabels: app: whoamiudp template: metadata: labels: app: whoamiudp spec: containers: - name: whoamiudp image: traefik/whoamiudp:latest ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: whoamiudp spec: ports: - port: 8080 protocol: UDP selector: app: whoamiudp 配置 ingressRouteUDP\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteUDP metadata: name: whoamiudp spec: entryPoints: - udpep routes: - services: - name: whoamiudp port: 8080 直接访问 svc 验证\n[root@k8s-node1 traefik]# kubectl get svc whoamiudp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE whoamiudp ClusterIP 10.96.119.116 \u0026lt;none\u0026gt; 8080/UDP 2m22s [root@k8s-node1 traefik]# echo \u0026#34;WHO\u0026#34; | socat - udp4-datagram:10.96.119.116:8080 Hostname: whoamiudp-6ff7dd6fb9-8qfc7 IP: 127.0.0.1 IP: 10.244.169.174 [root@k8s-node1 traefik]# echo \u0026#34;test\u0026#34; | socat - udp4-datagram:10.96.119.116:8080 Received: test 访问 udp 路由验证\n[root@k8s-node1 traefik]# echo \u0026#34;WHO\u0026#34; | socat - udp4-datagram:k8s-node1:9300 Hostname: whoamiudp-6ff7dd6fb9-5l8rd IP: 127.0.0.1 IP: 10.244.107.243 [root@k8s-node1 traefik]# echo \u0026#34;test\u0026#34; | socat - udp4-datagram:1.1.1.1:9300 Received: test 7 负载均衡 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: myapp1 spec: entryPoints: - web # 与 configmap 中定义的 entrypoint 名字相同 routes: - match: Host(`lb.test.com`) # 域名 kind: Rule services: - name: myapp1 # 与svc的name一致 port: 80 # 与svc的port一致 - name: myapp2 # 与svc的name一致 port: 80 # 与svc的port一致 部署\n[root@k8s-node1 ~]# vim demo/lb/lb.yml [root@k8s-node1 ~]# kubectl apply -f demo/lb/lb.yml ingressroute.traefik.containo.us/myapp1 created 访问测试，可以发现循环相应 myapp1 和 myapp2 的内容\n[root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@k8s-node1 ~]# curl http://lb.test.com Hello MyApp | Version: v2 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-2-router/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 1 简介 官方文档 1.1 三种方式 Traefik 创建路由规则有多种方式，比如： 原生 Ingress 写法 使用 CRD IngressRoute 方式 使用 GatewayAPI 的方式 相较于原生 Ingress 写法，ingressRoute 是 2.1 以后新增功能，简单来说，他们都支持路径 (path)","title":"traefik (二) ingressRoute 路由"},{"content":"0 前言 本文参考以下链接:\nhttps://www.cuiliangblog.cn/detail/section/29427812 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10\n1 简介 1.1 Traefik 简介 Traefik 是一个为了让部署微服务更加便捷而诞生的现代 HTTP 反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。\n它是一个边缘路由器，它会拦截外部的请求并根据逻辑规则选择不同的操作方式，这些规则决定着这些请求到底该如何处理。Traefik 提供自动发现能力，会实时检测服务，并自动更新路由规则。\n1.2 Traefik 核心组件 从上图可知，当请求 Traefik 时，请求首先到 entrypoints，然后分析传入的请求，查看他们是否与定义的 Routers 匹配。如果匹配，则会通过一系列 middlewares 处理，再到 traefikServices 上做流量转发，最后请求到 kubernetes的services上 。\n这就涉及到以下几个重要的核心组件:\nProviders 是基础组件，Traefik 的配置发现是通过它来实现的，它可以是协调器，容器引擎，云提供商或者键值存储。Traefik 通过查询 Providers 的 API 来查询路由的相关信息，一旦检测到变化，就会动态的更新路由。 Entrypoints 是 Traefik 的网络入口，它定义接收请求的接口，以及是否监听 TCP 或者 UDP。 Routers 主要用于分析请求，并负责将这些请求连接到对应的服务上去，在这个过程中，Routers 还可以使用 Middlewares 来更新请求，比如在把请求发到服务之前添加一些 Headers。 Services 负责配置如何到达最终将处理传入请求的实际服务。 Middlewares 用来修改请求或者根据请求来做出一些判断（authentication, rate limiting, headers, …），中间件被附件到路由上，是一种在请求发送到你的服务之前（或者在服务的响应发送到客户端之前）调整请求的一种方法。 1.3 Traefik CRD 资源 官方文档\ntraefik 通过自定义资源实现了对 traefik 资源的创建和管理，支持的 crd 资源类型如下所示：\nkind 功能 IngressRoute HTTP 路由配置 Middleware HTTP 中间件配置 TraefikService HTTP 负载均衡/流量复制配置 IngressRouteTCP TCP 路由配置 MiddlewareTCP TCP 中间件配置 IngressRouteUDP UDP 路由配置 TLSOptions TLS 连接参数配置 TLSStores TLS 存储配置 ServersTransport traefik 与后端之间的传输配置 2 Traefik 部署 traefik 是支持 helm 部署的，但是查看 helm 包的 value.yaml 配置发现总共有 500 多行配置，当需要修改配置项或者对 traefik 做一下自定义配置时，并不灵活。如果只是使用 traefik 的基础功能，推荐使用 helm 部署。如果想深入研究使用 traefik 的话，推荐使用自定义方式部署。\n2.1 crd rbac serviceaccount crd\n[root@k8s-node1 ~]# mkdir /opt/traefik [root@k8s-node1 ~]# cd /opt/traefik [root@k8s-node1 traefik]# wget https://raw.githubusercontent.com/traefik/traefik/v2.9/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml [root@k8s-node1 traefik]# kubectl apply -f kubernetes-crd-definition-v1.yml rbac\n[root@k8s-node1 traefik]# wget https://raw.githubusercontent.com/traefik/traefik/v2.9/docs/content/reference/dynamic-configuration/kubernetes-crd-rbac.yml [root@k8s-node1 traefik]# kubectl apply -f kubernetes-crd-rbac.yml clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created serviceaccount.yml\napiVersion: v1 kind: Namespace metadata: name: traefik --- apiVersion: v1 kind: ServiceAccount metadata: namespace: traefik name: traefik-ingress-controller --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: traefik 2.2 configmap 在 Traefik 中有三种方式定义静态配置：在配置文件中、在命令行参数中、通过环境变量传递，由于 Traefik 配置很多，通过 CLI 定义不是很方便，一般时候选择将其配置选项放到配置文件中，然后存入 ConfigMap，将其挂入 traefik 中。\nconfigmap.yml 文件内容：\napiVersion: v1 kind: ConfigMap metadata: name: traefik-config namespace: traefik data: traefik.yaml: |- global: checkNewVersion: false # 周期性的检查是否有新版本发布 sendAnonymousUsage: false # 周期性的匿名发送使用统计信息 serversTransport: insecureSkipVerify: true # Traefik忽略验证代理服务的TLS证书 api: insecure: true # 允许HTTP 方式访问API dashboard: true # 启用Dashboard debug: false # 启用Debug调试模式 metrics: prometheus: # 配置Prometheus监控指标数据，并使用默认配置 addRoutersLabels: true # 添加routers metrics entryPoint: \u0026#34;metrics\u0026#34; # 指定metrics监听地址 entryPoints: web: address: \u0026#34;:80\u0026#34; # 配置80端口，并设置入口名称为 web forwardedHeaders: insecure: true # 信任所有的forward headers websecure: address: \u0026#34;:443\u0026#34; # 配置443端口，并设置入口名称为 websecure forwardedHeaders: insecure: true traefik: address: \u0026#34;:9000\u0026#34; # 配置9000端口为 dashboard 的端口，不设置默认值为 8080 metrics: address: \u0026#34;:9100\u0026#34; # 配置9100端口，作为metrics收集入口 tcpep: address: \u0026#34;:9200\u0026#34; # 配置9200端口，作为tcp入口 udpep: address: \u0026#34;:9300/udp\u0026#34; # 配置9300端口，作为udp入口 providers: kubernetesIngress: \u0026#34;\u0026#34; # 启用 Kubernetes Ingress 方式来配置路由规则 kubernetesGateway: \u0026#34;\u0026#34; # 启用 Kubernetes Gateway API kubernetesCRD: # 启用Kubernetes CRD方式来配置路由规则 ingressClass: \u0026#34;\u0026#34; # 指定traefik的ingressClass名称 allowCrossNamespace: true #允许跨namespace allowEmptyServices: true #允许空endpoints的service log: filePath: \u0026#34;/etc/traefik/logs/traefik.log\u0026#34; # 设置调试日志文件存储路径，如果为空则输出到控制台 level: \u0026#34;DEBUG\u0026#34; # 设置调试日志级别 format: \u0026#34;json\u0026#34; # 设置调试日志格式 accessLog: filePath: \u0026#34;/etc/traefik/logs/access.log\u0026#34; # 设置访问日志文件存储路径，如果为空则输出到 stdout 和 stderr format: \u0026#34;json\u0026#34; # 设置访问调试日志格式 bufferingSize: 0 # 设置访问日志缓存行数 fields: # 设置访问日志中的字段是否保留（keep保留、drop不保留） defaultMode: keep # 设置默认保留访问日志字段 names: # 针对访问日志特别字段特别配置保留模式 ClientUsername: drop StartUTC: drop # 禁用日志timestamp使用UTC headers: # 设置Header中字段是否保留 defaultMode: keep # 设置默认保留Header中字段 names: # 针对Header中特别字段特别配置保留模式 # User-Agent: redact# 可以针对指定agent Authorization: drop Content-Type: keep 设置节点 label，用于控制在哪些节点部署 Traefik，这里我们使用 k8s-node1(master) 节点作为边缘节点部署\nkubectl label node k8s-node1 IngressProxy=true 2.3 deployment service 使用 DeamonSet 或者 Deployment 均可，此处使用 Deployment 方式部署 Traefik，调度至含有 IngressProxy=true 的边缘节点\n同时使用 podAntiAffinity 避免多个 traefik 实例运行在同一节点造成单点故障.\nkubectl apply -f deployment.yml\napiVersion: apps/v1 kind: Deployment metadata: name: traefik namespace: traefik labels: app: traefik spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: name: traefik labels: app: traefik spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - traefik topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 5 # 等待容器优雅退出的时长 tolerations: # 设置容忍所有污点，防止节点被设置污点 - operator: \u0026#34;Exists\u0026#34; nodeSelector: # 设置node筛选器，在特定label的节点上启动 IngressProxy: \u0026#34;true\u0026#34; # 调度至IngressProxy: \u0026#34;true\u0026#34;的节点 containers: - name: traefik image: traefik:v2.9 env: - name: KUBERNETES_SERVICE_HOST # 手动指定k8s api,避免网络组件不稳定。 value: \u0026#34;1.1.1.1\u0026#34; - name: KUBERNETES_SERVICE_PORT_HTTPS # API server端口 value: \u0026#34;6443\u0026#34; - name: KUBERNETES_SERVICE_PORT # API server端口 value: \u0026#34;6443\u0026#34; - name: TZ # 指定时区 value: \u0026#34;Asia/Shanghai\u0026#34; ports: - name: web containerPort: 80 - name: websecure containerPort: 443 - name: dashboard containerPort: 9000 # Traefik Dashboard 端口 - name: metrics containerPort: 9100 - name: tcpep containerPort: 9200 # tcp端口 - name: udpep containerPort: 9300 # udp端口 securityContext: # 只开放网络权限 capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --configfile=/etc/traefik/config/traefik.yaml volumeMounts: - mountPath: /etc/traefik/config name: config - mountPath: /etc/traefik/logs name: logdir - mountPath: /etc/localtime name: timezone readOnly: true resources: requests: memory: \u0026#34;5Mi\u0026#34; cpu: \u0026#34;10m\u0026#34; limits: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; volumes: - name: config # traefik配置文件 configMap: name: traefik-config - name: logdir # traefik日志目录 hostPath: path: /var/log/traefik type: \u0026#34;DirectoryOrCreate\u0026#34; - name: timezone # 挂载时区文件 hostPath: path: /etc/localtime type: File service kubectl apply -f service.yml\napiVersion: v1 kind: Service metadata: labels: app: traefik name: traefik # 实际提供服务的 service, 使用 NodePort 模式 namespace: traefik spec: type: NodePort selector: app: traefik ports: - name: web protocol: TCP port: 80 targetPort: 80 nodePort: 80 - name: websecure protocol: TCP port: 443 targetPort: 443 nodePort: 443 - name: dashboard protocol: TCP port: 9000 targetPort: 9000 nodePort: 9000 - name: tcpep protocol: TCP port: 9200 targetPort: 9200 nodePort: 9200 - name: udpep protocol: UDP port: 9300 targetPort: 9300 nodePort: 9300 --- apiVersion: v1 kind: Service metadata: labels: app: traefik-metrics name: traefik-metrics # metrics 用于给集群内的 prometheus 提供数据 namespace: traefik spec: selector: app: traefik ports: - name: metrics protocol: TCP port: 9100 targetPort: 9100 2.4 验证 [root@k8s-node1 traefik]# kubectl get pod,cm,sa,svc -n traefik |grep traefik pod/traefik-69bd67497f-v27qp 1/1 Running 0 12m configmap/traefik-config 1 7d5h serviceaccount/traefik-ingress-controller 1 7d5h service/traefik NodePort 10.101.142.158 \u0026lt;none\u0026gt; 80:80/TCP,443:443/TCP,9000:9000/TCP,9200:9200/TCP,9300:9300/UDP 11m service/traefik-metrics ClusterIP 10.98.89.13 \u0026lt;none\u0026gt; 9100/TCP 12m 可以直接通过 http://1.1.1.1:9000 访问到 dashboard\n2.5 其他配置 2.5.1 强制使用 TLS v1.2+ 官方文档\n如今，TLS v1.0 和 v1.1 因为存在安全问题，现在已被弃用。为了保障系统安全，所有入口路由都应该强制使用 TLS v1.2 或更高版本。\n[root@k8s-node1 traefik]# tee traefik-tlsoption.yml \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; apiVersion: traefik.containo.us/v1alpha1 kind: TLSOption metadata: name: default namespace: traefik spec: minVersion: VersionTLS12 cipherSuites: - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # TLS 1.2 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 # TLS 1.2 - TLS_AES_256_GCM_SHA384 # TLS 1.3 - TLS_CHACHA20_POLY1305_SHA256 # TLS 1.3 curvePreferences: - CurveP521 - CurveP384 sniStrict: true EOF [root@k8s-node1 traefik]# kubectl apply -f traefik-tlsoption.yml tlsoption.traefik.containo.us/default created 2.5.2 日志切割 官方并没有日志轮换的功能，但是 traefik 收到 USR1 信号后会重建日志文件，因此可以通过 logrotate 实现日志轮换\nmkdir -p /etc/logrotate.d/traefik tee /etc/logrotate.d/traefik/config \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; /var/log/traefik/*.log { daily rotate 15 missingok notifempty compress dateext dateyesterday dateformat .%Y-%m-%d create 0644 root root postrotate docker kill --signal=\u0026#34;USR1\u0026#34; $(docker ps | grep traefik |grep -v pause| awk \u0026#39;{print $1}\u0026#39;) endscript } EOF 创建定时任务\ncrontab -e 0 0 * * * /usr/sbin/logrotate -f /etc/logrotate.d/traefik/config \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 2.6 多控制器 有的业务场景下可能需要在一个集群中部署多个 traefik，例如：避免单个 traefik 配置规则过多导致加载处理缓慢。每个 namespace 部署一个 traefik。或者 traefik 生产与测试环境区分等场景，需要不同的实例控制不同的 IngressRoute 资源对象，要实现该功能有两种方法\n2.6.1 annotations 注解筛选 首先在 traefik 配置文件中的 providers 下增加 Ingressclass 参数，指定具体的值\nproviders: kubernetesCRD: # 启用Kubernetes CRD方式来配置路由规则 ingressClass: \u0026#34;traefik-v2.9\u0026#34; # 指定traefik的ingressClass实例名称 allowCrossNamespace: true #允许跨namespace allowEmptyServices: true #允许空endpoints的service 接下来在 IngressRoute 资源对象中的 annotations 参数中添加 kubernetes.io/ingress.class: traefik-v2.9 即可\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard namespace: traefik annotations: kubernetes.io/ingress.class: traefik-v2.9 # 因为静态配置文件指定了ingressclass，所以这里的annotations 要指定，否则访问会404 spec: entryPoints: - web routes: - match: Host(`traefik.test.com`) kind: Rule services: - name: api@internal kind: TraefikService namespace: traefik 2.6.2 label 标签选择器筛选 首先在 traefik 配置文件中的 providers 下增加 labelSelector 参数，指定具体的标签键值。\nproviders: kubernetesCRD: # 启用Kubernetes CRD方式来配置路由规则 # ingressClass: \u0026#34;traefik-v2.9\u0026#34; # 指定traefik的ingressClass名称 labelSelector: \u0026#34;app=traefik-v2.9\u0026#34; # 通过标签选择器指定traefik标签 allowCrossNamespace: true #允许跨namespace allowEmptyServices: true #允许空endpoints的service 然后在 IngressRoute 资源对象中添加 labels 标签选择器，选择 app: traefik-v2.9 这个标签即可\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard labels: # 通过标签选择器，该IngressRoute资源由配置了app=traefik-v2.9的traefik处理 app: traefik-v2.9 # annotations: # kubernetes.io/ingress.class: traefik-v2.9 spec: entryPoints: - web routes: - match: Host(`traefik.test.com`) kind: Rule services: - name: api@internal kind: TraefikService namespace: traefik 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-traefik-1-deploy/","summary":"0 前言 本文参考以下链接: https://www.cuiliangblog.cn/detail/section/29427812 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0， traefik-2.9.10 1 简介 1.1 Traefik 简介 Traefik 是一个为了让部署微服务更加便捷而诞生的现代 HTTP 反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的","title":"traefik (一) 简介、部署和配置"},{"content":"1 简介 Gateway API（之前叫 Service API）是由 SIG-NETWORK 社区管理的开源项目，项目地址\nIngress 资源对象不能很好的满足网络需求，很多场景下 Ingress 控制器都需要通过定义 annotations 或者 crd 来进行功能扩展，这对于使用标准和支持是非常不利的，新推出的 Gateway API 旨在通过可扩展的面向角色的接口来增强服务网络。\nGateway API 是 Kubernetes 中的一个 API 资源集合，包括 GatewayClass、Gateway、HTTPRoute、TCPRoute、Service 等，这些资源共同为各种网络用例构建模型。\n2 部署 2.1 crd 内容较长，直接复制 官网yaml\n[root@k8s-node1 traefik]# kubectl apply -f gateway-api-crd.yml customresourcedefinition.apiextensions.k8s.io/gatewayclasses.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/gateways.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/httproutes.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/tcproutes.networking.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/tlsroutes.networking.x-k8s.io created 2.2 rbac --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: gateway-role rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - networking.x-k8s.io resources: - gatewayclasses - gateways - httproutes - tcproutes - tlsroutes verbs: - get - list - watch - apiGroups: - networking.x-k8s.io resources: - gatewayclasses/status - gateways/status - httproutes/status - tcproutes/status - tlsroutes/status verbs: - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: gateway-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: gateway-role subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: default 应用 yaml\n[root@k8s-node1 traefik]# kubectl apply -f gateway-api-rbac.yml clusterrole.rbac.authorization.k8s.io/gateway-role created clusterrolebinding.rbac.authorization.k8s.io/gateway-controller created 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-gatewayapi/","summary":"1 简介 Gateway API（之前叫 Service API）是由 SIG-NETWORK 社区管理的开源项目，项目地址 Ingress 资源对象不能很好的满足网络需求，很多场景下 Ingress 控制器都需要通过定义 annotations 或者 crd 来进行功能扩展，这对于使用标准和支持是非常不利的，新推出的 Gateway API 旨在通过可扩展的面向角色的接口来增强服务网络。 Gateway API 是 Kubernetes 中的一个 API 资源集合，包括","title":"kubernetes | Gateway API 简介及部署"},{"content":"0 前言 dns 配置文件 /etc/resolv.conf 中常看到有 search 设置，以前以为是根据 search 中的域去指定 nameserver，其实不是这样用的。它的一个用处是程序只需要知道主机名就可以解析到 ip，不必知道域名后缀 domain 是什么\nFQDN (Fully Qualified Domain Name) 含义是完整的域名. 例如, 一台机器主机名 (hostname) 是 www, 域名后缀 (domain) 是 baidu.com, 那么该主机的 FQDN 应该是 www.baidu.com. 最后是以 . 来结尾的, 但是大部分的应用和服务器都允许忽略最后这个点 . 所有大家直接输入 www.baidu.com 也可以识别\nhttps://www.man7.org/linux/man-pages/man5/resolv.conf.5.html\n1 search 下面以几个示例演示一下 search 是如何工作的\n/etc/resolv.conf 配置文件内容\nnameserver 8.8.8.8 search foo.local bar.local 解析 test ，优先以 hostname 的形式拼接到 search 中配置的 domain 上进行查询，如果失败直接以 FQDN 的形式查询\n[root@k8s-node1 ~]# host -a test Trying \u0026#34;test.foo.local\u0026#34; Trying \u0026#34;test.bar.local\u0026#34; Trying \u0026#34;test\u0026#34; Host test not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 53 ms 解析 test.hello ，优先以 FQDN 的形式查询，如果失败则以 hostname 的形式拼接到 search 中配置的 domain 上进行查询\n[root@k8s-node1 ~]# host -a test.hello Trying \u0026#34;test.hello\u0026#34; Received 103 bytes from 8.8.8.8#53 in 48 ms Trying \u0026#34;test.hello.foo.local\u0026#34; Trying \u0026#34;test.hello.bar.local\u0026#34; Host test.hello not found: 3(NXDOMAIN) Received 113 bytes from 8.8.8.8#53 in 49 ms 解析 test. ，直接认定为 FQDN ，以 FQDN 的形式查询，不会进行拼接查询\n[root@k8s-node1 ~]# host -a test. Trying \u0026#34;test\u0026#34; Host test. not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 54 ms 2 options ndots 可以发现，配置了 search 之后，除非以最后一种形式查询，总会将 hostname 和 search 进行拼接查询\n其实它是由 options ndots:[number] 选项控制的：当查询的域名有 \u0026gt;= number 个 . 时，优先以 FQDN 的形式查询，如果失败再拼接查询\n配置 /etc/resolv.conf\nnameserver 8.8.8.8 search foo.local bar.local options ndots:2 观察下述示例\ntest\n[root@k8s-node1 ~]# host -a test Trying \u0026#34;test.foo.local\u0026#34; Trying \u0026#34;test.bar.local\u0026#34; Trying \u0026#34;test\u0026#34; Host test not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 45 ms test.hello\n[root@k8s-node1 ~]# host -a test.hello Trying \u0026#34;test.hello.foo.local\u0026#34; Trying \u0026#34;test.hello.bar.local\u0026#34; Trying \u0026#34;test.hello\u0026#34; Host test.hello not found: 3(NXDOMAIN) Received 103 bytes from 8.8.8.8#53 in 46 ms test.hello.world\n[root@k8s-node1 ~]# host -a test.hello.world Trying \u0026#34;test.hello.world\u0026#34; Received 119 bytes from 8.8.8.8#53 in 57 ms Trying \u0026#34;test.hello.world.foo.local\u0026#34; Trying \u0026#34;test.hello.world.bar.local\u0026#34; Host test.hello.world not found: 3(NXDOMAIN) Received 119 bytes from 8.8.8.8#53 in 45 ms test.\n[root@k8s-node1 ~]# host -a test. Trying \u0026#34;test\u0026#34; Host test. not found: 3(NXDOMAIN) Received 97 bytes from 8.8.8.8#53 in 45 ms 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-dns-search/","summary":"0 前言 dns 配置文件 /etc/resolv.conf 中常看到有 search 设置，以前以为是根据 search 中的域去指定 nameserver，其实不是这样用的。它的一个用处是程序只需要知道主机名就可以解析到 ip，不必知道域名后缀 domain 是什么 FQDN (Fully Qualified Domain Name) 含义是完整的域名. 例如, 一台机器主机名 (hostname) 是 www, 域名后缀 (domain) 是 baidu.com, 那么该主机的 FQDN 应该是 www.baidu.com. 最后是以 .","title":"linux | dns 配置文件中 search 和 options ndots 详解"},{"content":"1 command args 如果指定了 containers.command，Dockerfile 中的 ENTRYPOINT 会被覆盖且 CMD 指令被忽略 如果指定了 containers.args，Dockerfile 中的 ENTRYPOINT 继续执行， CMD 指令 被覆盖 ENTRYPOINT CMD command args finally [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] ep-1 foo bar [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] ep-2 [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-1 zoo boo [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-2 zoo boo 2 CMD ENTRYPOINT 我们大概可以总结出下面几条规律：\n如果 ENTRYPOINT 使用了 shell 模式，CMD 指令会被忽略。 如果 ENTRYPOINT 使用了 exec 模式，CMD 指定的内容被追加为 ENTRYPOINT 指定命令的参数。 如果 ENTRYPOINT 使用了 exec 模式，CMD 也应该使用 exec 模式。 还有一点需要注意，如果使用 docker run --entrypoint 覆盖了 Dockerfile 中的 ENTRYPOINT , 同时 CMD 指令也会被忽略\n真实的情况要远比这三条规律复杂，好在 docker 给出了 官方的解释，如下图所示：\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-command-args-docker-entrypoint-cmd/","summary":"1 command args 如果指定了 containers.command，Dockerfile 中的 ENTRYPOINT 会被覆盖且 CMD 指令被忽略 如果指定了 containers.args，Dockerfile 中的 ENTRYPOINT 继续执行， CMD 指令 被覆盖 ENTRYPOINT CMD command args finally [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] ep-1 foo bar [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] ep-2 [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-1 zoo boo [\u0026quot;/ep1\u0026quot;] [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] [\u0026quot;/ep-2\u0026quot;] [\u0026ldquo;zoo\u0026rdquo;, \u0026ldquo;boo\u0026rdquo;] ep-2 zoo boo 2 CMD ENTRYPOINT 我们大概可以总结","title":"kubernetes |  command args 和 dockerfile 中的 ENTRYPOINT CMD"},{"content":"1 基础概念 StatefulSet 应用场景：分布式应用、集群\n部署有状态应用 解决 Pod 独立生命周期，保持 Pod 启动顺序和唯一性 稳定，唯一的网络标识符，持久存储 有序，优雅的部署和扩展、删除和终止 有序，滚动更新 StatefulSet 控制器的优势\n稳定的存储 StatefulSet 的存储卷使用 VolumeClaimTemplate 创建，称为卷申请模板，当 StatefulSet 使用 VolumeClaimTemplate 创建一个 PersistentVolume 时，同样也会为每个 Pod 分配并创建一个编号的 PVC。该 PVC 和 PV 不会随着 StatefulSet 的删除而删除 稳定的网络 ID StatefulSet 中的每个 POD 名称固定：\u0026lt;statefulset-name\u0026gt;-\u0026lt;number\u0026gt; 通过 serviceName 字段指定 Headless Service ，可以为每个 POD 分配一个固定的 DNS 解析，重启或者重建 POD 时虽然 ip 有所变动，但 DNS 解析会保持稳定 示例 yaml\napiVersion: v1 kind: Service metadata: name: statefulset-nginx labels: app: statefulset-nginx spec: selector: app: statefulset-nginx clusterIP: None ports: - name: web port: 80 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: statefulset-nginx spec: serviceName: \u0026#34;statefulset-nginx\u0026#34; replicas: 2 selector: matchLabels: app: statefulset-nginx template: metadata: labels: app: statefulset-nginx spec: terminationGracePeriodSeconds: 5 containers: - name: statefulset-nginx image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - name: web containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 2 稳定的存储 可以看到与 deployment 不同，statefulset 中的每个 pod 都分配到了独立的 pv，且重启 pod 后存储对应关系不变\n[root@k8s-node1 ~]# kubectl get pod,pvc,pv | awk \u0026#39;{print $1}\u0026#39; # pod NAME pod/nfs-client-provisioner-66d6cb77fd-47hsf pod/statefulset-nginx-0 pod/statefulset-nginx-1 # pvc NAME persistentvolumeclaim/www-statefulset-nginx-0 persistentvolumeclaim/www-statefulset-nginx-1 # pv NAME persistentvolume/pvc-17751fde-1b23-4535-98bb-a70342ddd6fe persistentvolume/pvc-b7519f46-b2af-42e4-b66d-d7459be2e87c [root@k8s-node1 ~]# ls /nfs/ default-www-statefulset-nginx-0-pvc-17751fde-1b23-4535-98bb-a70342ddd6fe default-www-statefulset-nginx-1-pvc-b7519f46-b2af-42e4-b66d-d7459be2e87c 3 稳定的网络 ID 手动删除 pod 后除了 pod 的 ip 会变动，主机名和 dns 解析都正常\n# POD名字固定 [root@k8s-node1 ~]# kubectl get pods -l app=statefulset-nginx NAME READY STATUS RESTARTS AGE statefulset-nginx-0 1/1 Running 0 5m18s statefulset-nginx-1 1/1 Running 0 5m17s # 主机名固定 [root@k8s-node1 ~]# for i in 0 1; do kubectl exec \u0026#34;statefulset-nginx-$i\u0026#34; -- hostname; done statefulset-nginx-0 statefulset-nginx-1 # DNS解析固定 [root@k8s-node1 ~]# kubectl run -it --rm --restart=Never --image busybox:1.28 dns-test -- nslookup statefulset-nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: statefulset-nginx Address 1: 10.244.107.230 statefulset-nginx-1.statefulset-nginx.default.svc.cluster.local Address 2: 10.244.169.157 statefulset-nginx-0.statefulset-nginx.default.svc.cluster.local pod \u0026#34;dns-test\u0026#34; deleted 4 暴露应用 由于使用的是 Headless Service ，无法使用 NodePort 的方式暴露应用端口，我们可以单独创建 service 来暴露特定 pod 应用\nStatefulSet 控制器中的 pod 名称都是固定的： \u0026lt;statefulset-name\u0026gt;-\u0026lt;number\u0026gt; ，可以通过 statefulset.kubernetes.io/pod-name 标签固定 pod\n示例如下\napiVersion: v1 kind: Service metadata: name: ss-nginx-0 labels: app: ss-nginx-0 spec: selector: statefulset.kubernetes.io/pod-name: statefulset-nginx-0 type: NodePort ports: - name: web port: 80 targetPort: 80 nodePort: 30003 验证\n[root@k8s-node1 ~]# kubectl get svc ss-nginx-0 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ss-nginx-0 NodePort 10.111.69.1 \u0026lt;none\u0026gt; 80:30003/TCP 3h53m [root@k8s-node1 ~]# kubectl get ep ss-nginx-0 NAME ENDPOINTS AGE ss-nginx-0 10.244.169.188:80 3h53m 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-statefulset/","summary":"1 基础概念 StatefulSet 应用场景：分布式应用、集群 部署有状态应用 解决 Pod 独立生命周期，保持 Pod 启动顺序和唯一性 稳定，唯一的网络标识符，持久存储 有序，优雅的部署和扩展、删除和终止 有序，滚动更新 StatefulSet 控制器的优势 稳定的存储 StatefulSet 的存储卷使用 VolumeClaimTemplate 创建，称为卷申请模板，当 StatefulSet 使用 VolumeClaimTemplate 创建一个 PersistentVolume 时，同样也会为每个 Pod 分配","title":"kubernetes | statefulset 控制器详解"},{"content":"0 前言 本文参考以下内容:\n如何编写最佳的 Dockerfile 在使用 Docker 的过程中，编写 Dockerfile 是非常重要的一部分工作。合理编写 Dockerfile 会使我们构建出来的 Docker image 拥有更佳的性能和健壮性\n目标:\n更快的构建速度 更小的 Docker 镜像大小 更少的 Docker 镜像层 充分利用镜像缓存 增加 Dockerfile 可读性 让 Docker 容器使用起来更简单 总结\n编写.dockerignore 文件 容器只运行单个应用 将多个 RUN 指令合并为一个 基础镜像的标签不要用 latest 每个 RUN 指令后删除多余文件 选择合适的基础镜像 (alpine 版本最好) 设置 WORKDIR 和 CMD 使用 ENTRYPOINT (可选) 在 entrypoint 脚本中使用 exec COPY 与 ADD 优先使用前者 合理调整 COPY 与 RUN 的顺序 设置默认的环境变量，映射端口和数据卷 使用 LABEL 设置镜像元数据 添加 HEALTHCHECK 可以说每条 Dockerfile 指令都有相关的优化项，这里就不一一赘述了，下面仅列举一些常见且重要的设置\n1 容器的优雅退出 众所周知，docker 容器本质上是一个个进程，进程的优雅退出需要考虑的是如何正确处理 SIGTERM 信号，关于这点在我的另一篇博文中介绍过 kill命令详解以及linux中的信号\n无论是 docker stop 还是在 kubernetes 中使用容器，一般关闭容器都是向容器内的 1 号进程发送 SIGTERM 信号，等待容器自行进行资源清理等操作，等待时间 docker 默认 10s，k8s 默认 30s，如果容器仍未退出，则发送 SIGKILL 信号强制杀死进程\n综上，我们只需要考虑 2 点\n应用程序如何处理信号 这就需要在应用程序中定义对信号的处理逻辑了，包括对每个信号如何处理如何转发给子进程等。\n应用程序如何获取信号 docker 容器的一号进程是由 CMD ENTRYPOINT 这两个指令决定的，所以正确使用这两个指令十分关键\nCMD 和 ENTRYPOINT 分别都有 exec 和 shell 两种格式：\n使用 exec 格式时，我们执行的命令就是一号进程 使用 shell 格式时，实际会以 /bin/sh -c command arg… 的方式运行，这种情况下容器的一号进程将会是 /bin/sh，当收到信号时 /bin/sh 不会将信号转发给我们的应用程序，导致意料之外的错误，所以十分不推荐使用 shell 格式 我们还可以使用 tini 作为 init 系统管理进程\n官方地址：https://github.com/krallin/tini\nTini (Tiny but Independent) 是一个小型的、可执行的程序，它的主要目的是作为一个 init 系统的替代品，用于在容器中启动应用程序。\n在容器中启动应用程序时，通常会使用 init 系统来管理进程。然而，由于容器的特殊性，传统的 init 系统可能无法完全满足容器化应用程序的需求。Tini 作为一个小巧而独立的程序，可以帮助解决容器启动时可能遇到的各种问题，如僵尸进程、信号处理等。\n在 Docker 中使用 Tini 的主要意义在于提高容器的稳定性和可靠性。Tini 可以确保容器中的应用程序在启动和退出时正确处理信号，避免僵尸进程和其它常见问题的出现。此外，Tini 还可以有效地限制容器中的资源使用，避免应用程序崩溃或者占用过多的系统资源，从而提高容器的可用性和可维护性。\n总之，使用 Tini 可以让容器中的应用程序更加健壮、稳定和可靠，这对于运行生产环境中的应用程序非常重要。\n使用示例\nFROM nginx ENV TINI_VERSION=v0.19.0 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [\u0026#34;/tini\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;/docker-entrypoint.sh\u0026#34;] CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] Alpine Linux\nRUN apk add --no-cache tini # Tini is now available at /sbin/tini ENTRYPOINT [\u0026#34;/sbin/tini\u0026#34;, \u0026#34;--\u0026#34;] NixOS\nnix-env --install tini Debian\napt-get install tini Arch Linux\npacaur -S tini 2 RUN 指令 RUN 指令一般用于安装配置软件包等操作，通常需要比较多的步骤，如果每条命令都单独用 RUN 指令去跑会导致镜像层数非常多，所以尽可能将所有 RUN 指令拼接起来是当前的事实标准\n也要将 RUN 指令中生产的一些附属文件删除以缩小最终镜像的大小\n如下示例\nFROM debian:stretch RUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y $buildDeps \\ \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\ \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026amp;\u0026amp; make -C /usr/src/redis \\ \u0026amp;\u0026amp; make -C /usr/src/redis install \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm redis.tar.gz \\ \u0026amp;\u0026amp; rm -r /usr/src/redis \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps 3 多阶段构建 很多时候我们的应用容器会包含 构建 和 运行 两大功能，而运行所需要的依赖数量明显少于构建时的依赖，我们最终的 image 交付物有运行环境就足够了\n在很多的场景中，我们都会制作两个 Dockerfile 分别用于构建和运行，文件交付起来十分麻烦\n在 Docker Engine 17.05 中引入了多阶段构建，以此降低构建复杂度，同时使缩小镜像尺寸更为简单\n如下示例，go 程序编译完后几乎不需要任何依赖环境即可运行\n# 阶段1 FROM golang:1.16 WORKDIR /go/src COPY app.go ./ RUN go build app.go -o myapp # 阶段2，引用空镜像 scratch FROM scratch WORKDIR /server # 复制文件，通过编号引用，0 代表阶段 1 COPY --from=0 /go/src/myapp ./ CMD [\u0026#34;./myapp\u0026#34;] 上述例子可以修改一下，可读性更强\n# 阶段1命名为builder FROM golang:1.16 as builder WORKDIR /go/src COPY app.go ./ RUN go build app.go -o myapp # 阶段2，引用空镜像 scratch FROM scratch WORKDIR /server # 复制文件，通过名称引用 COPY --from=builder /go/src/myapp ./ CMD [\u0026#34;./myapp\u0026#34;] 只构建某个阶段\n构建镜像时，不一定需要构建整个 Dockerfile，我们可以通过 --target 参数指定某个目标阶段构建，比如我们开发阶段我们只构建 builder 阶段进行测试。\ndocker build --target builder -t builder_app:v1 . 使用外部镜像\nCOPY --from httpd:latest /usr/local/apache2/conf/httpd.conf ./httpd.conf 从上一阶段创建新的阶段\n# 阶段1命名为builder FROM golang:1.16 as builder WORKDIR /go/src COPY app.go ./ RUN go build app.go -o myapp # 阶段2，引用阶段1再进行一次构建 FROM builder as builder_ex ADD dest.tar ./ ... 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-dockerfile-optimization/","summary":"0 前言 本文参考以下内容: 如何编写最佳的 Dockerfile 在使用 Docker 的过程中，编写 Dockerfile 是非常重要的一部分工作。合理编写 Dockerfile 会使我们构建出来的 Docker image 拥有更佳的性能和健壮性 目标: 更快的构建速度 更小的 Docker 镜像大小 更少的 Docker 镜像层 充分利用镜像缓存 增加 Dockerfile 可读性 让 Docker 容器使用起来更简单 总结 编写.dockerignore 文件","title":"docker | dockerfile 最佳实践"},{"content":"0 前言 本文参考以下链接：\nDockerfile 指令详解 Dockerfile 用于构建 docker 镜像, 实际上就是把在 linux 下的命令操作写到了 Dockerfile 中, 通过 Dockerfile 去执行设置好的操作命令, 保证通过 Dockerfile 的构建镜像是一致的.\nDockerfile 是一个文本文件, 其内包含了一条条的指令 (Instruction), 每一条指令构建一层, 因此每一条指令的内容, 就是描述该层应当如何构建.\n1 FROM 指定基础镜像 命令格式\nFROM IMAGE[:TAG][@DIGEST] 我们可以用任意已存在的镜像为基础构建我们的自定义镜像\n比如:\n系统镜像: centos, ubuntu, debian, alpine 应用镜像: nginx, redis, mongo, mysql, httpd 运行环境镜像: php, java, golang 工具镜像: busybox 示例\n# tag 默认使用 latest FROM alpine # 指定 tag FROM alpine:3.17.3 # 指定 digest FROM alpine@sha256:b6ca290b6b4cdcca5b3db3ffa338ee0285c11744b4a6abaa9627746ee3291d8d # 同时指定 tag 和 digest FROM alpine:3.17.3@sha256:b6ca290b6b4cdcca5b3db3ffa338ee0285c11744b4a6abaa9627746ee3291d8d 除了选择现有镜像为基础镜像外, Docker 还存在一个特殊的镜像, 名为 scratch. 这个镜像无法从别处拉取, 可以理解为是 Docker 自 1.5.0 版本开始的自带镜像, 它仅包含一个空的文件系统.\nscratch 镜像一般用于构建基础镜像, 比如官方镜像 Ubuntu\n2 COPY 复制文件 格式:\nCOPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;源路径1\u0026gt; [源路径2] … \u0026lt;目标路径\u0026gt; COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026quot;\u0026lt;源路径1\u0026gt;\u0026quot;, \u0026quot;[源路径2]\u0026quot;, …, \u0026quot;\u0026lt;目标路径\u0026gt;\u0026quot;] COPY 指令将从构建上下文目录中 \u0026lt;源路径\u0026gt; 的文件/目录复制到新的镜像层内的 \u0026lt;目标路径\u0026gt; 位置\n\u0026lt;源路径\u0026gt; 可以是多个, 甚至可以是通配符, 其通配符规则要满足 Go 的 filepath.Match 规则\nCOPY hom* /mydir/ COPY hom?.txt /mydir/ 目标路径 可以是容器内的绝对路径, 也可以是相对于工作目录的相对路径 (工作目录可以用 WORKDIR 指令来指定), 目标路径不需要事先创建, 如果目录不存在会在复制文件前先行创建缺失目录\n此外, 还需要注意一点, 使用 COPY 指令, 源文件的各种元数据都会保留. 比如读、写、执行权限、文件变更时间等. 这个特性对于镜像定制很有用. 特别是构建相关文件都在使用 Git 进行管理的时候\n在使用该指令的时候还可以加上 --chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt; 选项来改变文件的所属用户及所属组.\nCOPY --chown=55:mygroup files* /mydir/ COPY --chown=bin files* /mydir/ COPY --chown=1 files* /mydir/ COPY --chown=10:11 files* /mydir/ 3 ADD 更高级的复制文件 ADD 指令和 COPY 的格式和性质基本一致. 同样支持 --chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt; 指令修改属主和属组.\n但是在 COPY 基础上增加了一些功能:\n\u0026lt;源路径\u0026gt; 可以是一个 URL, 这种情况下, Docker 引擎会试图去下载这个链接的文件放到 \u0026lt;目标路径\u0026gt; 去. 下载后的文件权限自动设置为 600, 如果这并不是想要的权限, 那么还需要增加额外的一层 RUN 进行权限调整. 如果 \u0026lt;源路径\u0026gt; 为一个 tar 压缩文件的话, 压缩格式为 gzip, bzip2 以及 xz 的情况下, ADD 指令将会自动解压缩这个压缩文件到 \u0026lt;目标路径\u0026gt; 去. Docker 官方要求尽可能的使用 COPY, 因为 COPY 的语义很明确, 就是复制文件而已, 而 ADD 则包含了更复杂的功能, 其行为也不一定很清晰. 最适合使用 ADD 的场合, 就是所提及的需要自动解压缩的场合.\n另外需要注意的是, ADD 指令会令镜像构建缓存失效, 从而可能会令镜像构建变得比较缓慢.\n4 RUN 执行命令 格式:\nshell 格式:RUN [command] \u0026lt;parameter1\u0026gt; \u0026lt;parameter2\u0026gt; …, 等价于在 linux 中执行 /bin/sh -c \u0026quot;command parameter1 parameter2 …\u0026quot;\nRUN ls -l exec 格式:RUN [\u0026quot;command\u0026quot;, \u0026quot;parameter1\u0026quot;, \u0026quot;parameter2\u0026quot;…], 不会通过 shell 执行, 所以像 $HOME 这样的变量就无法获取.\nRUN [\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;] RUN [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls -l\u0026#34;] # 可以获取环境变量 RUN 指令用于指定构建镜像时执行的命令, Dockerfile 允许多个 RUN 指令, 并且每个 RUN 指令都会创建一个镜像层.\nRUN 指令一般用于安装配置软件包等操作, 为避免镜像层数过多, 一般 RUN 指令使用 shell 格式且使用换行符来执行多个命令, 且尽量将 RUN 指令产生的附属物删除以缩小镜像大小\n如下示例\nFROM debian:stretch RUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y $buildDeps \\ \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\ \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026amp;\u0026amp; make -C /usr/src/redis \\ \u0026amp;\u0026amp; make -C /usr/src/redis install \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm redis.tar.gz \\ \u0026amp;\u0026amp; rm -r /usr/src/redis \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps 5 CMD 容器启动命令 CMD 指令的格式和 RUN 相似, 也是两种格式：\nshell 格式：CMD [command] \u0026lt;parameters\u0026gt; exec 格式：CMD [\u0026quot;command\u0026quot;, \u0026quot;\u0026lt;parameter1\u0026gt;\u0026quot;, \u0026quot;parameter2\u0026quot;, …] 参数列表格式：CMD [\u0026quot;参数1\u0026quot;, \u0026quot;参数2\u0026quot;…]. 在指定了 ENTRYPOINT 指令后, 用 CMD 指定具体的参数. CMD 指令用于设置容器启动时 默认执行 的指令, 一般会设置为应用程序的启动脚本或者工具镜像的 bash, 设置了多条 CMD 指令时, 只有最后一条 CMD 会被执行.\n在运行时可以指定新的命令来替代镜像设置中的这个默认命令, 比如, ubuntu 镜像默认的 CMD 是 /bin/bash, 如果我们直接 docker run -it ubuntu 的话, 会直接进入 bash. 我们也可以在运行时指定运行别的命令, 如 docker run -it ubuntu cat /etc/os-release. 这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了, 会输出系统版本信息.\n在指令格式上, 一般推荐使用 exec 格式, 这类格式在解析时会被解析为 JSON 数组, 因此一定要使用双引号 \u0026quot;, 而不要使用单引号.\n例如一般 nginx 容器的 CMD 指令:\nCMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 6 ENTRYPOINT 入口点 ENTRYPOINT 的格式和 RUN 指令格式一样, 分为 exec 格式和 shell 格式.\nshell 格式：ENTRYPOINT [command] \u0026lt;parameters\u0026gt; exec 格式：ENTRYPOINT [\u0026quot;command\u0026quot;, \u0026quot;\u0026lt;parameter1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;parameter2\u0026gt;\u0026quot;, …] ENTRYPOINT 的目的和 CMD 一样, 都是在指定容器启动程序及参数. ENTRYPOINT 在运行时也可以替代, 不过比 CMD 要略显繁琐, 需要通过 docker run 的参数 --entrypoint 来指定.\n当指定了 ENTRYPOINT 且使用的是 exec 格式时, CMD 的含义就发生了改变, 不再是直接的运行其命令, 而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令, 换句话说实际执行时, 将变为：\nENTRYPOINT [\u0026#34;command\u0026#34;, \u0026#34;\u0026lt;parameter1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;parameter2\u0026gt;\u0026#34;, \u0026#34;CMD\u0026#34;] 以下示例将展示 CMD 指令作为参数传给 ENTRYPOINT 的场景\n场景一：我们自己构建了一个用于查看外网 ip 和归属地的镜像\nFROM alpine RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\u0026#39; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk --update add curl CMD [ \u0026#34;-s\u0026#34; ] ENTRYPOINT [ \u0026#34;curl\u0026#34;, \u0026#34;http://myip.ipip.net\u0026#34; ] 构建\ndocker build -t busybox-curl . 以两种方式运行\n# 容器中实际执行的指令为 curl http://myip.ipip.net -s [root@lvbibir learn]# docker run -it --rm busybox-curl 当前 IP：101.201.150.47 来自于：中国 北京 北京 阿里云 # 容器中实际执行的指令为 curl http://myip.ipip.net -i [root@lvbibir learn]# docker run -it --rm busybox-curl -i HTTP/1.1 200 OK Date: Mon, 10 Apr 2023 03:21:59 GMT Content-Type: text/plain; charset=utf-8 Content-Length: 72 Connection: keep-alive Node: ipip-myip5 X-Cache: BYPASS X-Request-Id: e309720b9197e8b94cec18b409c69d1d Server: WAF Connection: close Accept-Ranges: bytes 当前 IP：101.201.150.47 来自于：中国 北京 北京 阿里云 场景二：应用运行前的准备工作\n启动容器就是启动主进程, 但有些时候, 启动主进程前, 需要一些准备工作.\n比如 mysql 类的数据库, 可能需要一些数据库配置、初始化的工作, 这些工作要在最终的 mysql 服务器运行之前解决.\n此外, 可能希望避免使用 root 用户去启动服务, 从而提高安全性, 而在启动服务前还需要以 root 身份执行一些必要的准备工作, 最后切换到服务用户身份启动服务. 或者除了服务外, 其它命令依旧可以使用 root 身份执行, 方便调试等.\n这些准备工作是和容器 CMD 无关的, 无论 CMD 为什么, 都需要事先进行一个预处理的工作. 这种情况下, 可以写一个脚本, 然后放入 ENTRYPOINT 中去执行, 而这个脚本会将接到的参数（也就是 \u0026lt;CMD\u0026gt;）作为命令, 在脚本最后执行. 比如官方镜像 redis 中就是这么做的：\nFROM alpine:3.4 ... RUN addgroup -S redis \u0026amp;\u0026amp; adduser -S -G redis redis ... ENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;] EXPOSE 6379 CMD [ \u0026#34;redis-server\u0026#34; ] 可以看到其中为了 redis 服务创建了 redis 用户, 并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本：\n#!/bin/sh ... # allow the container to be started with `--user` if [ \u0026#34;$1\u0026#34; = \u0026#39;redis-server\u0026#39; -a \u0026#34;$(id -u)\u0026#34; = \u0026#39;0\u0026#39; ]; then find . \\! -user redis -exec chown redis \u0026#39;{}\u0026#39; + exec gosu redis \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; fi exec \u0026#34;$@\u0026#34; 该脚本的内容就是根据 CMD 的内容来判断, 如果是 redis-server 的话, 则切换到 redis 用户身份启动服务器, 否则依旧使用 root 身份执行. 比如：\n[root@lvbibir learn]# docker run -it redis id uid=0(root) gid=0(root) groups=0(root) 7 ENV 设置环境变量 格式有两种：\nENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt;… ENV 用于设置环境变量, 既可以在 Dockerfile 中调用, 也可以在构建完的容器运行时中使用.\n支持的指令： ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN\n下面这个例子中演示了如何换行, 以及对含有空格的值用双引号括起来的办法, 这和 Shell 下的行为是一致的.\nENV VERSION=1.0 DEBUG=on \\ NAME=\u0026#34;Happy Feet\u0026#34; 示例\nFROM alpine ENV VERSION=1.0 \\ DEBUG=on \\ NAME=\u0026#34;Happy Feet\u0026#34; RUN echo \u0026#34;name: ${NAME}\u0026#34; \u0026gt; /test \\ \u0026amp;\u0026amp; echo \u0026#34;version: ${VERSION}\u0026#34; \u0026gt;\u0026gt; /test 构建\n[root@lvbibir learn]# docker build -t demo-env . 构建时调用了环境变量\n[root@lvbibir learn]# docker run -it --rm demo-env cat /test name: Happy Feet version: 1.0 构建后的容器运行时中调用, 这里需要使用 /bin/sh -c 的方式, 不然无法读取变量. 且对 $ 进行转义, 不然读取的将会是宿主机的变量\n[root@lvbibir learn]# docker run -it --rm demo-env /bin/sh -c \u0026#34;echo \\${DEBUG}\u0026#34; on 8 ARG 构建参数 构建参数和 ENV 的效果一样, 都是设置环境变量. 所不同的是, ARG 所设置的构建环境的环境变量, 在将来容器运行时是不会存在这些环境变量的. 但是不要因此就使用 ARG 保存密码之类的信息, 因为 docker history 还是可以看到所有值的.\nDockerfile 中的 ARG 指令是定义参数名称, 以及定义其默认值. 该默认值可以在构建命令 docker build 中用 --build-arg \u0026lt;参数名\u0026gt;=\u0026lt;值\u0026gt; 来覆盖.\n灵活的使用 ARG 指令, 能够在不修改 Dockerfile 的情况下, 构建出不同的镜像.\nARG 指令有生效范围, 如果在 FROM 指令之前指定, 那么只能用于 FROM 指令中, FROM 指令可以是多个\nARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine RUN set -x ; echo ${DOCKER_USERNAME} 使用上述 Dockerfile 会发现无法输出 ${DOCKER_USERNAME} 变量的值, 要想正常输出, 你必须在 FROM 之后再次指定 ARG, 如下示例\n# 只在 FROM 中生效 ARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine # 要想在 FROM 之后使用, 必须再次指定 ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} 如下示例, 变量将会在每个 FROM 指令中生效\n# 这个变量在每个 FROM 中都生效 ARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine RUN set -x ; echo 1 FROM ${DOCKER_USERNAME}/alpine RUN set -x ; echo 2 如下示例, 对于在各个阶段中使用的变量都必须在每个阶段分别指定\nARG DOCKER_USERNAME=library FROM ${DOCKER_USERNAME}/alpine # 在FROM 之后使用变量, 必须在每个阶段分别指定 ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} FROM ${DOCKER_USERNAME}/alpine # 在FROM 之后使用变量, 必须在每个阶段分别指定 ARG DOCKER_USERNAME=library RUN set -x ; echo ${DOCKER_USERNAME} 9 VOLUME 定义匿名卷 格式为：\nVOLUME [\u0026quot;\u0026lt;路径1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;路径2\u0026gt;\u0026quot;…] VOLUME \u0026lt;路径\u0026gt; 容器运行时应该尽量保持容器存储层不发生写操作, 对于数据库类需要保存动态数据的应用, 其数据库文件应该保存于卷 (volume) 中.\n为了防止运行时用户忘记将动态文件所保存目录挂载为卷, 在 Dockerfile 中, 我们可以事先指定某些目录挂载为匿名卷, 这样在运行时如果用户不指定挂载, 其应用也可以正常运行, 不会向容器存储层写入大量数据, 从而保证了容器存储层的无状态化.\nVOLUME 创建的匿名卷会挂载到系统 /var/lib/docker/volumes/\u0026lt;CONTAINER-ID\u0026gt;/\u0026lt;_VOLUME\u0026gt; 目录下, 且不会随着容器删除而删除, 需要手动删除\n如下示例\nFROM alpine VOLUME /data 构建运行\n[root@lvbibir learn]# docker build -t demo-volume . [root@lvbibir learn]# docker run -itd --name=demo-volume demo-volume [root@lvbibir learn]# docker exec -it demo-volume ls -ld /data drwxr-xr-x 2 root root 4096 Apr 10 05:24 /data 查看挂载目录\n[root@lvbibir learn]# docker inspect --format=\u0026#39;{{json .Mounts}}\u0026#39; demo-volume | python -m json.tool [ { \u0026#34;Destination\u0026#34;: \u0026#34;/data\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;49cf915dd297292e3d0e4b2c7a66ead6875cfb0dbd010de15189040ab1158b3b\u0026#34;, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/49cf915dd297292e3d0e4b2c7a66ead6875cfb0dbd010de15189040ab1158b3b/_data\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34; } ] 如下示例, 运行容器时, 可以指定 -v 参数将目录挂载到指定位置\n[root@lvbibir learn]# docker run -itd -v /mydata:/data --name demo-volume-2 demo-volume [root@lvbibir learn]# docker inspect --format=\u0026#39;{{json .Mounts}}\u0026#39; demo-volume-2 | python -m json.tool [ { \u0026#34;Destination\u0026#34;: \u0026#34;/data\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Propagation\u0026#34;: \u0026#34;rprivate\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Source\u0026#34;: \u0026#34;/mydata\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;bind\u0026#34; } ] 10 EXPOSE 暴露端口 格式为 EXPOSE \u0026lt;端口1\u0026gt; [端口2] …\nEXPOSE 指令是声明容器运行时提供服务的端口, 这只是一个声明, 在容器运行时并不会因为这个声明应用就会开启这个端口的服务\n在 Dockerfile 中写入这样的声明有两个好处：\n一个是帮助镜像使用者理解这个镜像服务的守护端口, 以方便配置映射； 另一个用处则是在运行时使用随机端口映射时, 也就是 docker run -P 时, 会自动随机映射 EXPOSE 的端口. 要将 EXPOSE 和在运行时使用 -p \u0026lt;宿主端口\u0026gt;:\u0026lt;容器端口\u0026gt; 区分开来. -p, 是映射宿主端口和容器端口, 换句话说, 就是将容器的对应端口服务公开给外界访问, 而 EXPOSE 仅仅是声明容器打算使用什么端口而已, 并不会自动在宿主进行端口映射.\n11 WORKDIR 指定工作目录 格式为 WORKDIR \u0026lt;路径\u0026gt;\n使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录）, 以后各层的当前目录就被改为指定的目录, 如该目录不存在, WORKDIR 会帮你建立目录\n如下示例, 是一个常见的错误, world.txt 最终会在 /app 目录下, 而不是期望的 /app/demo 目录\nWORKDIR /app RUN mkdir demo \u0026amp;\u0026amp; cd demo RUN echo \u0026#34;hello\u0026#34; \u0026gt; world.txt 上述需求可以进行如下优化, 推荐使用第二种写法\nWORKDIR /app/demo RUN echo \u0026#34;hello\u0026#34; \u0026gt; world.txt # 或者 WORKDIR /app RUN mkdir demo \\ \u0026amp;\u0026amp; echo \u0026#34;hello\u0026#34; \u0026gt; demo/world.txt # 或者 WORKDIR /app RUN mkdir demo \\ \u0026amp;\u0026amp; cd demo \\ \u0026amp;\u0026amp; echo \u0026#34;hello\u0026#34; \u0026gt; demo/world.txt 如果你的 WORKDIR 指令使用的相对路径, 那么所切换的路径与之前的 WORKDIR 有关\n如下示例, pwd 的输出将会是 /a/b/c\nWORKDIR /a WORKDIR b WORKDIR c RUN pwd 12 USER 指定当前用户 格式：USER \u0026lt;用户名\u0026gt;[:\u0026lt;用户组\u0026gt;]\nUSER 指令和 WORKDIR 相似, 都是改变环境状态并影响以后的层. WORKDIR 是改变工作目录, USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份.\n注意, USER 只是帮助你切换到指定用户而已, 这个用户必须是事先建立好的, 否则无法切换.\nRUN groupadd -r redis \u0026amp;\u0026amp; useradd -r -g redis redis USER redis RUN [ \u0026#34;redis-server\u0026#34; ] 如果以 root 执行的脚本, 在执行期间希望改变身份, 比如希望以某个已经建立好的用户来运行某个服务进程, 不要使用 su 或者 sudo, 这些都需要比较麻烦的配置, 而且在 TTY 缺失的环境下经常出错. 建议使用 gosu\n不过更推荐的还是 上文 中提到过的通过 ENTRYPOINT 脚本的方式\n使用 gosu 示例\n# 建立 redis 用户, 并使用 gosu 换另一个用户执行命令 RUN groupadd -r redis \u0026amp;\u0026amp; useradd -r -g redis redis # 下载 gosu RUN wget -O /usr/local/bin/gosu \u0026#34;https://github.com/tianon/gosu/releases/download/1.12/gosu-amd64\u0026#34; \\ \u0026amp;\u0026amp; chmod +x /usr/local/bin/gosu \\ \u0026amp;\u0026amp; gosu nobody true # 设置 CMD, 并以另外的用户执行 CMD [ \u0026#34;exec\u0026#34;, \u0026#34;gosu\u0026#34;, \u0026#34;redis\u0026#34;, \u0026#34;redis-server\u0026#34; ] 13 HEALTHCHECK 健康检查 格式：\nHEALTHCHECK [选项] CMD \u0026lt;命令\u0026gt;：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令, 使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常, 这是 Docker 1.12 引入的新指令.\n在没有 HEALTHCHECK 指令前, Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常. 很多情况下这没问题, 但是如果程序进入死锁状态, 或者死循环状态, 应用进程并不退出, 但是该容器已经无法提供服务了. 在 1.12 以前, Docker 不会检测到容器的这种状态, 从而不会重新调度, 导致可能会有部分容器已经无法提供服务了却还在接受用户请求.\nHEALTHCHECK 支持下列选项：\n--interval=\u0026lt;间隔\u0026gt;：两次健康检查的间隔, 默认为 30 秒； --timeout=\u0026lt;时长\u0026gt;：健康检查命令运行超时时间, 如果超过这个时间, 本次健康检查就被视为失败, 默认 30 秒； --retries=\u0026lt;次数\u0026gt;：当连续失败指定次数后, 则将容器状态视为 unhealthy, 默认 3 次. 和 CMD, ENTRYPOINT 一样, HEALTHCHECK 只可以出现一次, 如果写了多个, 只有最后一个生效.\n在 HEALTHCHECK [选项] CMD 后面的命令, 格式和 ENTRYPOINT 一样, 分为 shell 格式, 和 exec 格式. 命令的返回值决定了该次健康检查的成功与否：\n0：成功； 1：失败； 2：保留, 不要使用这个值. 如下示例, 假设我们有个镜像是个最简单的 Web 服务, 我们希望增加健康检查来判断其 Web 服务是否在正常工作, 我们可以用 curl 来帮助判断\nFROM nginx HEALTHCHECK --interval=5s --timeout=3s --retries=3\\ CMD curl -fs http://localhost/ || exit 1 构建运行\n[root@lvbibir learn]# docker build -t myweb . [root@lvbibir learn]# docker run -d --name demo-myweb -p 800:80 myweb # 此时是 starting 状态 [root@lvbibir learn]# docker ps | grep myweb e6f585df60a6 myweb \u0026#34;/docker-entrypoint.…\u0026#34; About a minute ago Up 2 seconds (health: starting) 0.0.0.0:800-\u0026gt;80/tcp demo-myweb # 等待几秒变为 healthy 状态 [root@lvbibir learn]# docker ps | grep myweb e6f585df60a6 myweb \u0026#34;/docker-entrypoint.…\u0026#34; 2 minutes ago Up About a minute (healthy) 0.0.0.0:800-\u0026gt;80/tcp demo-myweb 删除 index.html 文件模拟故障\n[root@lvbibir learn]# docker exec -it demo-myweb rm -f /usr/share/nginx/html/index.html # 状态变为unhealthy [root@lvbibir learn]# docker ps | grep myweb e6f585df60a6 myweb \u0026#34;/docker-entrypoint.…\u0026#34; 6 minutes ago Up 5 minutes (unhealthy) 0.0.0.0:800-\u0026gt;80/tcp demo-myweb 为了帮助排障, 健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里, 可以用 docker inspect 来查看.\n[root@lvbibir learn]# docker inspect --format \u0026#39;{{json .State.Health}}\u0026#39; demo-myweb | python -m json.tool { \u0026#34;FailingStreak\u0026#34;: 25, \u0026#34;Log\u0026#34;: [ { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:41:51.393698555+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 1, \u0026#34;Output\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:41:51.285647058+08:00\u0026#34; }, { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:41:56.504282619+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 1, \u0026#34;Output\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:41:56.401745529+08:00\u0026#34; }, ........... ], \u0026#34;Status\u0026#34;: \u0026#34;unhealthy\u0026#34; } 恢复文件\n[root@lvbibir learn]# docker exec -it demo-myweb /bin/bash root@e6f585df60a6:/# echo test \u0026gt; /usr/share/nginx/html/index.html root@e6f585df60a6:/# exit [root@lvbibir learn]# docker inspect --format \u0026#39;{{json .State.Health}}\u0026#39; demo-myweb | python -m json.tool { \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:48:30.482498808+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;test\\n\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:48:30.378197999+08:00\u0026#34; }, { \u0026#34;End\u0026#34;: \u0026#34;2023-04-10T14:48:35.599150547+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;test\\n\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2023-04-10T14:48:35.490433323+08:00\u0026#34; }, ....... ], \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34; } 14 ONBUILD 为他人作嫁衣裳 格式：ONBUILD \u0026lt;其它指令\u0026gt;.\nONBUILD 是一个特殊的指令, 它后面跟的是其它指令, 比如 RUN, COPY 等, 而这些指令, 在当前镜像构建时并不会被执行. 只有当以当前镜像为基础镜像, 去构建下一级镜像的时候才会被执行.\nDockerfile 中的其它指令都是为了定制当前镜像而准备的, 唯有 ONBUILD 是为了帮助别人定制自己而准备的.\n假设我们要制作 Node.js 所写的应用的镜像. 我们都知道 Node.js 使用 npm 进行包管理, 所有依赖、配置、启动信息等会放到 package.json 文件里. 在拿到程序代码后, 需要先进行 npm install 才可以获得所有需要的依赖. 然后就可以通过 npm start 来启动应用. 因此, 一般来说会这样写 Dockerfile：\nFROM node:slim WORKDIR /app COPY ./package.json /app RUN [ \u0026#34;npm\u0026#34;, \u0026#34;install\u0026#34; ] COPY . /app/ CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 把这个 Dockerfile 放到 Node.js 项目的根目录, 构建好镜像后, 就可以直接拿来启动容器运行. 但是如果我们还有第二个 Node.js 项目也差不多呢？好吧, 那就再把这个 Dockerfile 复制到第二个项目里. 那如果有第三个项目呢？再复制么？文件的副本越多, 版本控制就越困难, 让我们继续看这样的场景维护的问题.\n如果第一个 Node.js 项目在开发过程中, 发现这个 Dockerfile 里存在问题, 比如敲错字了、或者需要安装额外的包, 然后开发人员修复了这个 Dockerfile, 再次构建, 问题解决. 第一个项目没问题了, 但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的, 但是并不会因为第一个项目修复了他们的 Dockerfile, 而第二个项目的 Dockerfile 就会被自动修复.\n那么我们可不可以做一个基础镜像, 然后各个项目使用这个基础镜像呢？这样基础镜像更新, 各个项目不用同步 Dockerfile 的变化, 重新构建后就继承了基础镜像的更新？好吧, 可以, 让我们看看这样的结果.\n基础镜像 (my-node) Dockerfile\nFROM node:slim WORKDIR /app CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 应用镜像 (my-app1) Dockerfile\nFROM my-node COPY ./package.json /app RUN [ \u0026#34;npm\u0026#34;, \u0026#34;install\u0026#34; ] COPY . /app/ 基础镜像变化后, 各个项目都用这个 Dockerfile 重新构建镜像, 会继承基础镜像的更新.\n那么, 问题解决了么？没有. 准确说, 只解决了一半. 如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数, 那怎么办？这一行 RUN 是不可能放入基础镜像的, 因为涉及到了当前项目的 ./package.json, 难道又要一个个修改么？所以说, 这样制作基础镜像, 只解决了原来的 Dockerfile 的前 4 条指令的变化问题, 而后面三条指令的变化则完全没办法处理.\nONBUILD 可以解决这个问题. 让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile:\nFROM node:slim WORKDIR /app ONBUILD COPY ./package.json /app ONBUILD RUN [ \u0026#34;npm\u0026#34;, \u0026#34;install\u0026#34; ] ONBUILD COPY . /app/ CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 应用镜像 Dcokerfile\nFROM my-node 是的, 只有这么一行. 当在各个项目目录中, 用这个只有一行的 Dockerfile 构建镜像时, 之前基础镜像的那三行 ONBUILD 就会开始执行, 成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install, 生成应用镜像.\n15 LABEL 为镜像添加元数据 LABEL 指令用来给镜像以键值对的形式添加一些元数据（metadata）.\nLABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... 我们还可以用一些标签来申明镜像的作者、文档地址等：\nLABEL org.opencontainers.image.authors=\u0026#34;yeasy\u0026#34; LABEL org.opencontainers.image.documentation=\u0026#34;https://yeasy.gitbooks.io\u0026#34; 具体可以参考 https://github.com/opencontainers/image-spec/blob/master/annotations.md\n16 SHELL 指定 shell 格式：SHELL [\u0026quot;executable\u0026quot;, \u0026quot;parameters\u0026quot;]\nSHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell, Linux 中默认为 `[\u0026quot;/bin/sh\u0026quot;, \u0026ldquo;-c\u0026rdquo;]\n如下示例, 两个 RUN 运行同一命令, 第二个 RUN 运行的命令会打印出每条命令并当遇到错误时退出.\nSHELL [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] RUN lll ; ls SHELL [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-cex\u0026#34;] RUN lll ; ls 如下示例, 当 ENTRYPOINT CMD 以 shell 格式指定时, SHELL 指令所指定的 shell 也会成为这两个指令的 shell\nSHELL [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-cex\u0026#34;] # /bin/sh -cex \u0026#34;nginx\u0026#34; ENTRYPOINT nginx # /bin/sh -cex \u0026#34;nginx\u0026#34; CMD nginx 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-dockerfile-instruction/","summary":"0 前言 本文参考以下链接： Dockerfile 指令详解 Dockerfile 用于构建 docker 镜像, 实际上就是把在 linux 下的命令操作写到了 Dockerfile 中, 通过 Dockerfile 去执行设置好的操作命令, 保证通过 Dockerfile 的构建镜像是一致的. Dockerfile 是一个文本文件, 其内包含了一条条的指令 (Instruction), 每一条指令构建一层, 因此每一条指令的内容, 就是描述该层应当如何构建. 1 FROM 指定基础镜像 命","title":"docker | dockerfile 指令详解"},{"content":"1 简介 kill 命令很容易让人产生误解, 以为仅仅是用来终止 linux 中的进程.\n在 man 手册中对 kill 命令的解释如下, 不难看出, kill 命令是一个用于将指定的 signal 发送给进程的工具\nDESCRIPTION The command kill sends the specified signal to the specified process or process group. If no signal is specified, the TERM signal is sent. The TERM signal will kill processes which do not catch this signal. For other processes, it may be necessary to use the KILL (9) signal, since this signal cannot be caught. Most modern shells have a builtin kill function, with a usage rather similar to that of the command described here. The \u0026lsquo;-a\u0026rsquo; and \u0026lsquo;-p\u0026rsquo; options, and the possibility to specify processes by command name are a local extension. If sig is 0, then no signal is sent, but error checking is still performed.\n命令格式\nkill [-s signal|-p] [-q sigval] [-a] [--] pid... 常用参数\n-l # 列出所有支持的signal -s NAME # 使用NAME指定signal -NUM # 使用编号指定signal kill -s HUP 和 kill -1 效果一样 2 支持的信号 [root@lvbibir ~]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 可以看到 kill 支持的信号非常多, 在这些信号中只有 9) SIGKILL 可以无条件地终止 process, 其他信号都将依照 process 中定义的信号处理规则来进行忽略或者处理.\n上述信号中常用的其实很少, 如下表所示\n编号 名称 解释 1 SIGHUP 启动被终止的程序, 也可以让进程重新读取自己的配置文件, 类似重新启动 2 SIGINT 相当于输入 ctrl-c 来中断一个程序 9 SIGKILL 强制中断一个程序, 不会进行资源的清理工作. 如果该程序进行到一半, 可能会有半成品产生, 类似 vim 的 .filename.swp 保留下来 15 SIGTERM 以正常 (优雅) 的方式来终止进程, 由程序自身决定该如何终止 19 SIGSTOP 相当于输入 ctrl-z 来暂停一个程序 3 常用命令 以正常的方式终止进程, 由于信号 15 是最常用也是最佳的程序退出方式, 所以 kill 命令不指定信号时, 默认使用的就是信号 15\nkill pid # 或者 kill -15 pid 强制终止进程\nkill -9 pid 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-command-kill/","summary":"1 简介 kill 命令很容易让人产生误解, 以为仅仅是用来终止 linux 中的进程. 在 man 手册中对 kill 命令的解释如下, 不难看出, kill 命令是一个用于将指定的 signal 发送给进程的工具 DESCRIPTION The command kill sends the specified signal to the specified process or process group. If no signal is specified, the TERM signal is sent. The TERM signal will kill processes which do not catch this signal. For other processes, it may be necessary to use the KILL (9) signal, since this signal cannot be caught. Most modern shells have a builtin kill function, with a usage rather","title":"linux | kill 命令详解以及 linux 中的信号"},{"content":"0 前言 本文参考以下链接:\ngithub issue # 638 1 报错信息 报错详细信息\nException in thread \u0026#34;main\u0026#34; java.nio.file.NotDirectoryException: /usr/share/elasticsearch/plugins/plugin-descriptor.properties at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:439) at java.base/java.nio.file.Files.newDirectoryStream(Files.java:482) at java.base/java.nio.file.Files.list(Files.java:3793) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.getPluginInfo(BootstrapJvmOptions.java:49) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.bootstrapJvmOptions(BootstrapJvmOptions.java:34) at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:137) at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:86) 安装插件时直接将插件的 zip 解压到了 plugins 目录 导致的，每个插件应以目录的形式存放在 plugins 目录 中\n[root@21-centos-7 ~]# ls /data/elasticsearch/plugins/ commons-codec-1.9.jar commons-logging-1.2.jar config elasticsearch-analysis-ik-7.17.3.jar httpclient-4.5.2.jar httpcore-4.4.4.jar plugin-descriptor.properties plugin-security.policy 2 解决 只需要为每个插件创建一个目录，并把插件解压到对应目录即可\nmkdir /data/elasticsearch/plugins/elasticsearch-analysis-ik/ unzip elasticsearch-analysis-ik-7.17.3.zip -d /data/elasticsearch/plugins/elasticsearch-analysis-ik/ 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/troubleshooting-elasticsearch-plugin-error/","summary":"0 前言 本文参考以下链接: github issue # 638 1 报错信息 报错详细信息 Exception in thread \u0026#34;main\u0026#34; java.nio.file.NotDirectoryException: /usr/share/elasticsearch/plugins/plugin-descriptor.properties at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:439) at java.base/java.nio.file.Files.newDirectoryStream(Files.java:482) at java.base/java.nio.file.Files.list(Files.java:3793) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.getPluginInfo(BootstrapJvmOptions.java:49) at org.elasticsearch.tools.launchers.BootstrapJvmOptions.bootstrapJvmOptions(BootstrapJvmOptions.java:34) at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:137) at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:86) 安装插件时直接将插件的 zip 解压到了 plugins 目录 导致的，每个插件应以目录的形式存放在 plugins 目录 中 [root@21-centos-7 ~]# ls /data/elasticsearch/plugins/ commons-codec-1.9.jar commons-logging-1.2.jar config elasticsearch-analysis-ik-7.17.3.jar httpclient-4.5.2.jar httpcore-4.4.4.jar plugin-descriptor.properties plugin-security.policy 2 解决 只需要为每个插件创建一个目录，并把插件解压到对应目录即可 mkdir /data/elasticsearch/plugins/elasticsearch-analysis-ik/ unzip elasticsearch-analysis-ik-7.17.3.zip -d /data/elasticsearch/plugins/elasticsearch-analysis-ik/ 以上","title":"troubleshooting | elasticsearch 安装插件报错"},{"content":"1 基本语法 if [ command ];then 符合该条件执行的语句 elif [ command ];then 符合该条件执行的语句 else 符合该条件执行的语句 fi 2 字符串判断 表达式 解释 [ -z STRING ] 如果 STRING 的长度为零则为真 ，即判断是否为空，空即是真； [ -n STRING ] or [ STRING ] 如果 STRING 的长度非零则为真 ，即判断是否为非空，非空即是真； [ STRING1 = STRING2 ] 如果两个字符串相同则为真 ； [ STRING1 != STRING2 ] 如果字符串不相同则为真 ； 3 数值判断 表达式 解释 [ INT1 -eq INT2 ] INT1 和 INT2 两数相等为真，= [ INT1 -ne INT2 ] INT1 和 INT2 两数不等为真，!= [ INT1 -gt INT2 ] INT1 大于 INT1 为真，\u0026gt; [ INT1 -ge INT2 ] INT1 大于等于 INT2 为真，\u0026gt;= [ INT1 -lt INT2 ] INT1 小于 INT2 为真，\u0026lt; [ INT1 -le INT2 ] INT1 小于等于 INT2 为真，\u0026lt;= 4 文件/目录判断 表达式 解释 [ -b FILE ] 如果 FILE 存在且是一个块特殊文件则为真 [ -c FILE ] 如果 FILE 存在且是一个字特殊文件则为真 [ -d DIR ] 如果 FILE 存在且是一个目录则为真 [ -e FILE ] 如果 FILE 存在则为真 [ -f FILE ] 如果 FILE 存在且是一个普通文件则为真 [ -g FILE ] 如果 FILE 存在且已经设置了 SGID 则为真 [ -k FILE ] 如果 FILE 存在且已经设置了粘制位则为真 [ -p FILE ] 如果 FILE 存在且是一个名字管道 (F 如果 O) 则为真 [ -r FILE ] 如果 FILE 存在且是可读的则为真 [ -s FILE ] 如果 FILE 存在且大小不为 0 则为真 [ -t FD ] 如果文件描述符 FD 打开且指向一个终端则为真 [ -u FILE ] 如果 FILE 存在且设置了 SUID (set user ID) 则为真 [ -w FILE ] 如果 FILE 存在且是可写的则为真 [ -x FILE ] 如果 FILE 存在且是可执行的则为真 [ -O FILE ] 如果 FILE 存在且属有效用户 ID 则为真 [ -G FILE ] 如果 FILE 存在且属有效用户组则为真 [ -L FILE ] 如果 FILE 存在且是一个符号连接则为真 [ -N FILE ] 如果 FILE 存在且自上次阅读以来已进行了修改则为真 [ -S FILE ] 如果 FILE 存在且是一个套接字则为真 [ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 更新，或者 FILE1 存在且 FILE2 不存在则为真 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 要老，或者 FILE2 存在且 FILE1 不存在则为真 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则为真 5 与或非 -a \u0026amp;\u0026amp; 与，两个条件都满足 -o || 或，两个条件只满足一个条件 ! 非，两个条件都不满足 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-if/","summary":"1 基本语法 if [ command ];then 符合该条件执行的语句 elif [ command ];then 符合该条件执行的语句 else 符合该条件执行的语句 fi 2 字符串判断 表达式 解释 [ -z STRING ] 如果 STRING 的长度为零则为真 ，即判断是否为空，空即是真； [ -n STRING ] or [ STRING ] 如果 STRING 的长度非零则为真 ，即判断是否为非空，非空即是真； [ STRING1 = STRING2 ] 如果两个字符串相同则为真 ； [ STRING1","title":"shell | if 条件判断"},{"content":"1 阿里云构建 1.1 git 仓库设置 1.1.1 创建仓库 用于存放 Dockerfile\n1.1.2 上传 Dockerfile #换成自己的仓库地址 git clone https://github.com/lvbibir/docker-images cd docker-images mkdir -p k8s.gcr.io/pause-3.2/ echo \u0026#34;FROM k8s.gcr.io/pause:3.2\u0026#34; \u0026gt; k8s.gcr.io/pause-3.2/Dockerfile git add . git commit -m \u0026#39;new image: k8s.gcr.io/pause:3.2\u0026#39; # 默认分支可能是main，取决于你的github设置 git push origin master 1.2 阿里云设置 登陆阿里云，访问 阿里云容器镜像服务\n1.2.1 创建个人实例 1.2.2 进入个人实例创建命名空间 1.2.3 创建访问凭证 1.2.4 绑定 github 账号 1.2.5 新建镜像仓库 指定刚才创建的 github 仓库，记得勾选海外机器构建\n1.2.6 新建构建 手动触发构建，正常状况应该可以看到构建成功\n1.2.7 镜像下载 操作指南里可以看到如何下载镜像，标签即刚才新建构建时指定的镜像版本\n2 gcr.io_mirror 项目地址\n该项目通过 Github Actions 将 gcr.io、k8s.gcr.io、registry.k8s.io、quay.io、ghcr.io 镜像仓库的镜像搬运至 dockerhub\n直接提交 issue，在模板 issue 的 [PORTER] 后面添加想要搬运的镜像 tag，也可以直接在关闭的 issue 列表中检索，可能也会有其他人搬运过，直接用就行了\n稍等一小会可以看到镜像已经搬运到 dockerhub 了\n3 Docker Playground Docker Playground 是一个免费的线上 docker 环境，由于是外网环境所以下载镜像、推送到 dockerhub 都很快，也可以直接推到阿里云的仓库\ndocker login --username=lvbibir registry.cn-hangzhou.aliyuncs.com docker pull \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; dokcer tag \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; registry.cn-hangzhou.aliyuncs.com/lvbibir/\u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; docker push registry.cn-hangzhou.aliyuncs.com/lvbibir/\u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; 4 http proxy 如果有代理软件可以在 docker 中配置代理实现, 参考 docker 文档 - 配置 http proxy\n{ \u0026#34;proxies\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;httpProxy\u0026#34;: \u0026#34;http://127.0.0.1:1080\u0026#34;, \u0026#34;httpsProxy\u0026#34;: \u0026#34;http://127.0.0.1:1080\u0026#34;, \u0026#34;noProxy\u0026#34;: \u0026#34;*.test.example.com,.example2.com\u0026#34; } } } 5 使用国内现成的镜像站 这种方式的问题主要是镜像不全，且没有统一的管理，建议使用之前的四种方式\n阿里云仓库\ndocker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0 # 换成 docker pull registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.3.0 也可以使用 lank8s.cn，他们的对应关系 k8s.gcr.io –\u0026gt; lank8s.cn，gcr.io –\u0026gt; gcr.lank8s.cn\ndocker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0 # 换成 docker pull lank8s.cn/sig-storage/csi-node-driver-registrar:v2.3.0 中科大\ndocker image pull quay.io/kubevirt/virt-api:v0.45.0 # 换成 docker pull quay.mirrors.ustc.edu.cn/kubevirt/virt-api:v0.45.0 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-download-foreign-images/","summary":"1 阿里云构建 1.1 git 仓库设置 1.1.1 创建仓库 用于存放 Dockerfile 1.1.2 上传 Dockerfile #换成自己的仓库地址 git clone https://github.com/lvbibir/docker-images cd docker-images mkdir -p k8s.gcr.io/pause-3.2/ echo \u0026#34;FROM k8s.gcr.io/pause:3.2\u0026#34; \u0026gt; k8s.gcr.io/pause-3.2/Dockerfile git add . git commit -m \u0026#39;new image: k8s.gcr.io/pause:3.2\u0026#39; # 默认分支可能是main，取决于你的github设置 git push origin master 1.2 阿里云设置 登陆阿里云，访问 阿里云容器镜像服务 1.2.1 创建个人实例 1.2.2 进入个人实例创建命名空间 1.2.3 创建访问凭证 1.2.4 绑","title":"docker | 下载外网镜像的几种方式"},{"content":"原题如下\n鬼谷子随意从 2-99 中选取了两个数。他把这两个数的和告诉了庞涓， 把这两个数的乘积告诉了孙膑。但孙膑和庞涓彼此不知道对方得到的数。第二天， 庞涓很有自信的对孙膑说：虽然我不知道这两个数是什么，但我知道你一定也不知道。随后，孙膑说：那我知道了。庞涓说：那我也知道了。这两个数是什么？\n代码示例\n#!/usr/bin/env python # -*- coding: utf-8 -*- \u0026#39;\u0026#39;\u0026#39; 第一步 庞告诉孙，已知和Sum满足有至少两种ab组合，且任意一组ab的乘积Pro都满足至少有两种ab组合，通过isPang函数将可能的ab组合放入abList_1 第二步 孙告诉庞，abList_1中的ab组合乘积得pro，该pro满足至少有两种ab组合，且所有的ab组合有且仅有一组ab组合满足isPang函数，通过isSun函数将abList中满足条件的ab组合放入abList_2，ab组合的积放入proList 第三步 庞告诉孙，abList_2中的ab组合相加得Sum，该Sum满足至少有两种ab组合，且所有的ab组合有且仅有一组ab所得乘积pro在proList中，将满足条件的ab组合放入abList，即最终答案 \u0026#39;\u0026#39;\u0026#39; # 根据给出的sum，遍历所有可能的a和b的组合 def getCombinationSum(sum): combination = [] for a in range(2, 100): for b in range(2, 100): if a + b == sum and a \u0026lt;= b: combination.append((a, b)) return combination # 根据给出的pro，遍历所有可能的a和b的组合 def getCombinationPro(pro): combination = [] for a in range(2, 100): for b in range(2, 100): if a * b == pro and a \u0026lt;= b: combination.append((a, b)) return combination def isPang(sum): \u0026#39;\u0026#39;\u0026#39; 第一步，传入的sum满足以下条件返回True，否则False: 1. 可以拆分成若干组ab的加和 2. 每一组拆分出来的ab乘积运算得pro，该pro满足有至少两组ab的乘积 \u0026#39;\u0026#39;\u0026#39; if len(getCombinationSum(sum)) \u0026lt; 2: return False else: combinationSum = getCombinationSum(sum) for i in combinationSum: status = 0 pro = i[0] * i[1] # 有其中一组ab不满足就打断循环 if len(getCombinationPro(pro)) \u0026lt; 2: status = 1 break if status == 0: return True else: return False def isSun(pro): \u0026#39;\u0026#39;\u0026#39; 第二步，传入的pro满足以下条件返回一组ab组合(元组)，否则False 1. 可以拆分成若干组ab的乘积 2. 每一组拆分出来的ab相加运算得sum，所有ab加和的sum有且仅有一个满足第一步的条件(放入isPang函数后返回True) \u0026#39;\u0026#39;\u0026#39; combination = [] combinationPro = getCombinationPro(pro) if len(combinationPro) \u0026gt; 1: for i in combinationPro: sum = i[0] + i[1] if isPang(sum): combination.append(i) if len(combination) == 1: return combination else: return False if __name__ == \u0026#39;__main__\u0026#39;: # 第一步 abList_1 = [] for sum in range(4, 198+1): if isPang(sum): abList_1 += getCombinationSum(sum) # 第二步 abList_2 = [] proList = [] for i in abList_1: pro = i[0] * i[1] if isSun(pro): abList_2.append(i) proList.append(pro) # 第三步 abList = [] for i in abList_2: sum = i[0] + i[1] n = 0 for j in getCombinationSum(sum): if j[0] * j[1] in proList: n += 1 if n == 1: abList.append(i) print(abList) 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/python-logic-problem-guiguzi/","summary":"原题如下 鬼谷子随意从 2-99 中选取了两个数。他把这两个数的和告诉了庞涓， 把这两个数的乘积告诉了孙膑。但孙膑和庞涓彼此不知道对方得到的数。第二天， 庞涓很有自信的对孙膑说：虽然我不知道这两个数是什么，但我知道你一定也不知道。随后，孙膑说：那我知道了。庞涓说：那我也知道了。这两个数是什么？ 代","title":"python | 鬼谷子数学问题"},{"content":"0 前言 挂刀是指从饰品交易平台购买游戏饰品，在 steam 市场出售以实现将人民币转换为 steam 阿根廷账号余额。\nsteam 圣诞促销活动快结束了，买了几款游戏后发现阿根廷账号余额没多少了，挂刀过程又比较繁琐，故有此文记录一下挂刀搞余额的步骤。\n1 步骤 1.1 网易 buff 账号注册及绑定 buff 账号使用手机号注册即可，绑定需要搞余额的 steam 账号，同时需要提供 steam 账号的 API key 和 交易链接，这部分 buff 有教程，或者百度，很容易找到\n1.2 挂刀油猴脚本 脚本链接\n脚本安装成功进入 网页版buff 后，简单设置一下脚本，推荐设置了一下货币转换为阿根廷比索和默认排序规则\n1.3 脚本提供的信息 每个饰品需要关注的有如下信息：\n挂刀比例 越低代表售出后可获得的余额更多 左边是 buff 售价，右边是市场售价（阿根廷比索）\nsteam 求购人数\n1.4 寻找合适饰品 以以下几个维度入手，选择合适的饰品：\n比例较低 求购人数多 价格合适，太高可能卖的慢，太低要达到自己的要求可能需要倒好几个甚至十几个饰品才够 选择好后直接购买即可，后续步骤按照 buff 教程来\n1.5 卖出饰品 脚本提供的收益只能参考，具体还是要在市场看，着急就按照最低价卖即可，别人购买后就可以愉快的买新游戏啦\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/life/steam-guadao/","summary":"0 前言 挂刀是指从饰品交易平台购买游戏饰品，在 steam 市场出售以实现将人民币转换为 steam 阿根廷账号余额。 steam 圣诞促销活动快结束了，买了几款游戏后发现阿根廷账号余额没多少了，挂刀过程又比较繁琐，故有此文记录一下挂刀搞余额的步骤。 1 步骤 1.1 网易 buff 账号注册及绑定 buff 账号使用手机号注册即可，绑定需要搞余额","title":"steam挂刀教程"},{"content":"在 /etc/prifile.d 目录下新建一个文件，用户登录系统时自动生效\nvim /etc/profile.d/history_conf.sh source /etc/profile.d/history_conf.sh # 手动生效 文件内容\nexport HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 指定命令写入文件(默认~/.bash_history) export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups 不记录“重复”的命令；连续且相同 方为“重复” ； # ignorespace 不记录所有以空格开头的命令； # ignoreboth 表示ignoredups:ignorespace ,效果相当于以上两种的组合； # erasedups 删除重复命令； export PROMPT_COMMAND=\u0026#34;history -a\u0026#34; # 设置每条命令执行完立即写入HISTFILE(默认等待退出会话写入) export HISTTIMEFORMAT=\u0026#34;`whoami` %F %T \u0026#34; # 设置命令执行时间格式，记录文件增加时间戳 shopt -s histappend # 防止会话退出时覆盖其他会话写到HISTFILE的内容； 效果如下\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-history-format/","summary":"在 /etc/prifile.d 目录下新建一个文件，用户登录系统时自动生效 vim /etc/profile.d/history_conf.sh source /etc/profile.d/history_conf.sh # 手动生效 文件内容 export HISTFILE=\u0026#34;$HOME/.bash_history\u0026#34; # 指定命令写入文件(默认~/.bash_history) export HISTSIZE=1000 # history输出记录数 export HISTFILESIZE=10000 # HISTFILE文件记录数 export HISTIGNORE=\u0026#34;cmd1:cmd2:...\u0026#34; # 忽略指定cmd1,cmd2...的命令不被记录到文件；(加参数时会记录) export HISTCONTOL=ignoredups # ignoredups","title":"linux | history 命令的格式化输出"},{"content":"流程图\n代码示例\n使用前需要登录 harbor\n确保镜像的项目名在 harbor 中已存在\n格式三类型的镜像会推送到 harbor 的 library 项目中\n#!/bin/bash # author: Amadeus Liu # date: 2022-10-11 17:02:13 # version: 1.0 harbor_url=\u0026#34;local.harbor.com\u0026#34; log_file=\u0026#34;/var/log/push-harbor.log\u0026#34; image_id=$(docker images -q | sort -u) ls ${log_file} || touch ${log_file} echo \u0026#34;############# $(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34;) #############\u0026#34; \u0026gt;\u0026gt; ${log_file} get_image_tags () { docker inspect $1 --format=\u0026#39;{{.RepoTags}}\u0026#39; | sed \u0026#39;s/\\[//g\u0026#39; | sed \u0026#39;s/\\]//g\u0026#39; } image_tag_and_push () { docker tag $1 $2 \u0026amp;\u0026amp; echo \u0026#34;docker tag $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} docker push $2 \u0026amp;\u0026amp; echo \u0026#34;docker pull $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} } for i in ${image_id}; do # 判断镜像是否有harbor仓库的标签，有则视为harbor仓库中已有 if [[ $(get_image_tags $i) =~ ${harbor_url} ]]; then echo \u0026#34;已有${harbor_url}仓库标签-----$(get_image_tags $i)\u0026#34; else # 镜像的第一个完整标签 image_tag_first=$(echo $(get_image_tags $i) | awk -F\u0026#39; \u0026#39; \u0026#39;{print $1}\u0026#39;) # 镜像的第一个完整标签并去除版本 image_tag_first_delete_ver=$(echo ${image_tag_first} | awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) # 判断标签属于哪种格式 if [[ ${image_tag_first_delete_ver} =~ \u0026#34;/\u0026#34; ]]; then # 镜像的第一个完整标签的第一部分（\u0026#39;/\u0026#39;分割后的$1） image_tag_first_repo=$(echo ${image_tag_first_delete_ver}| awk -F\u0026#39;/\u0026#39; \u0026#39;{print $1}\u0026#39;) if [[ \u0026#34;${image_tag_first_repo}\u0026#34; =~ \u0026#34;.\u0026#34; ]]; then # 格式一 image_tag_harbor=\u0026#34;${harbor_url}/$(echo ${image_tag_first} | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $2}\u0026#39;)/$(echo ${image_tag_first} | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $3}\u0026#39;)\u0026#34; echo \u0026#34;${image_tag_first} \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;tag to\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${image_tag_harbor}\u0026#34; image_tag_and_push $i ${image_tag_harbor} else # 格式二 image_tag_harbor=\u0026#34;${harbor_url}/${image_tag_first}\u0026#34; echo \u0026#34;${image_tag_first} \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;tag to\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${image_tag_harbor}\u0026#34; image_tag_and_push $i ${image_tag_harbor} fi else # 格式三 image_tag_harbor=\u0026#34;${harbor_url}/library/${image_tag_first}\u0026#34; echo \u0026#34;${image_tag_first} \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;tag to\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${image_tag_harbor}\u0026#34; image_tag_and_push $i ${image_tag_harbor} fi fi done 腾讯云搬迁声明\n我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=3ielzwnut2qsg\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-push-harbor/","summary":"流程图 代码示例 使用前需要登录 harbor 确保镜像的项目名在 harbor 中已存在 格式三类型的镜像会推送到 harbor 的 library 项目中 #!/bin/bash # author: Amadeus Liu # date: 2022-10-11 17:02:13 # version: 1.0 harbor_url=\u0026#34;local.harbor.com\u0026#34; log_file=\u0026#34;/var/log/push-harbor.log\u0026#34; image_id=$(docker images -q | sort -u) ls ${log_file} || touch ${log_file} echo \u0026#34;############# $(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34;) #############\u0026#34; \u0026gt;\u0026gt; ${log_file} get_image_tags () { docker inspect $1 --format=\u0026#39;{{.RepoTags}}\u0026#39; | sed \u0026#39;s/\\[//g\u0026#39; | sed \u0026#39;s/\\]//g\u0026#39; } image_tag_and_push () { docker tag $1 $2 \u0026amp;\u0026amp; echo \u0026#34;docker tag $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} docker push $2 \u0026amp;\u0026amp; echo \u0026#34;docker pull $1 $2\u0026#34; \u0026gt;\u0026gt; ${log_file} } for i in ${image_id}; do # 判断镜像是否有harbor","title":"shell | 将本地镜像批量推送到 harbor"},{"content":"之前本地做一些测试的时候多次修改过 hosts 文件，导致 hosts 文件出现了某些问题，按照网上很多方式自建 hosts 文件、修改编码格式、包括使用一些第三方工具修复都没有作用，记录一下成功修复 hosts 文件的步骤\n使用管理员权限打开命令提示符\n执行如下代码\nfor /f %P in (\u0026#39;dir %windir%\\WinSxS\\hosts /b /s\u0026#39;) do copy %P %windir%\\System32\\drivers\\etc \u0026amp; echo %P \u0026amp; Notepad %P 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-fix-hosts/","summary":"之前本地做一些测试的时候多次修改过 hosts 文件，导致 hosts 文件出现了某些问题，按照网上很多方式自建 hosts 文件、修改编码格式、包括使用一些第三方工具修复都没有作用，记录一下成功修复 hosts 文件的步骤 使用管理员权限打开命令提示符 执行如下代码 for /f %P in (\u0026#39;dir %windir%\\WinSxS\\hosts /b /s\u0026#39;) do copy %P %windir%\\System32\\drivers\\etc \u0026amp; echo %P \u0026amp; Notepad %P 以上","title":"windows | hosts 文件修复"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1 ConfigMap 创建 ConfigMap 后，数据实际会存储在 k8s 中的 Etcd 中，然后通过创建 pod 时引用该数据。\n应用场景：应用程序配置\npod 使用 ConfigMap 数据有两种方式：\n变量注入 数据卷挂载 可以通过读取目录或者文件快速创建 configmap\nkubectl create configmap \u0026lt;configmap-name\u0026gt; \\ --from-file=[key-name]=\u0026lt;path\u0026gt; \\ # key 不指定时使用文件名作为 key 文件内容作为 value，path 既可以文件也可以是目录 --from-env-file=\u0026lt;path\u0026gt; \\ # 文件内容应是 key=value 的形式，逐行读取 --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \\ # 通过指定的键值对创建 configmap yaml 示例\napiVersion: v1 kind: ConfigMap metadata: name: configmap-demo data: abc: \u0026#34;123\u0026#34; cde: \u0026#34;456\u0026#34; redis.properties: | port: 6379 host: 1.1.1.4 password: 123456 --- apiVersion: v1 kind: Pod metadata: name: pod-configmap spec: containers: - name: demo image: nginx:1.19 env: - name: ABCD valueFrom: configMapKeyRef: name: configmap-demo key: abc - name: CDEF valueFrom: configMapKeyRef: name: configmap-demo key: cde volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: - name: config configMap: name: configmap-demo items: - key: \u0026#34;redis.properties\u0026#34; path: \u0026#34;redis.properties\u0026#34; # 挂载文件名 容器内验证\n[root@k8s-node1 ~]# kubectl exec -it pod-configmap -- bash root@pod-configmap:/# echo $ABCD 123 root@pod-configmap:/# echo $CDEF 456 root@pod-configmap:/# cat /config/redis.properties port: 6379 host: 1.1.1.4 password: 123456 2 Secret 与 ConfigMap 类似，区别在于 Secret 主要存储敏感数据，所有的数据都会经过 base64 编码。\nSecret 支持三种数据类型：\ndocker-registry：存储镜像仓库认证信息 generic：从文件、目录或者字符串创建，例如存储用户名密码 tls：存储证书，例如 HTTPS 证书 示例\n将用户名和密码进行编码\n[root@k8s-node1 ~]# echo -n \u0026#39;admin\u0026#39; | base64 YWRtaW4= [root@k8s-node1 ~]# echo -n \u0026#39;123.com\u0026#39; | base64 MTIzLmNvbQ== secret.yaml\napiVersion: v1 kind: Secret metadata: name: db-pass type: Opaque data: username: YWRtaW4= password: MTIzLmNvbQ== pod-secret.yaml\napiVersion: v1 kind: Pod metadata: name: pod-secret-demo spec: containers: - name: demo image: nginx:1.19 env: - name: USER # 变量名 valueFrom: secretKeyRef: name: db-pass key: username - name: PASS # 变量名 valueFrom: secretKeyRef: name: db-pass key: password volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: - name: config secret: secretName: db-pass items: - key: password path: my-password # 挂载文件名 验证\n[root@k8s-node1 ~]# kubectl apply -f secret.yaml secret/db-pass created [root@k8s-node1 ~]# kubectl apply -f pod-secret.yaml pod/pod-secret-demo created [root@k8s-node1 ~]# kubectl exec -it pod-secret-demo -- bash root@pod-secret-demo:/# echo $USER admin root@pod-secret-demo:/# echo $PASS 123.com root@pod-secret-demo:/# cat /config/my-password 123.com 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-configmap-secret/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1 ConfigMap 创建 ConfigMap 后，数据实际会存储在 k8s 中的 Etcd 中，然后通过创建 pod 时引用该数据。 应用场景：应用程序配置 pod 使用 ConfigMap 数据有两种方式： 变量注入 数据卷挂载 可以通过读取目录或者文件快速创建 configmap kubectl create configmap \u0026lt;configmap-name\u0026gt; \\ --from-file=[key-name]=\u0026lt;path\u0026gt;","title":"kubernetes | configmap \u0026 secret"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1 kubernetes 安全框架 客户端要想访问 K8s 集群 API Server，一般需要 CA 证书、Token 或者用户名 + 密码 如果 Pod 访问，需要 ServiceAccount K8S 安全控制框架主要由下面 3 个阶段进行控制，每一个阶段都支持插件方式，通过 API Server 配置来启用插件。\nAuthentication（鉴权） Authorization（授权） Admission Control（准入控制） 1.1 鉴权 (Authentication) 三种客户端身份认证：\nHTTPS 证书认证：基于 CA 证书签名的数字证书认证 HTTP Token 认证：通过一个 Token 来识别用户 HTTP Base 认证：用户名 + 密码的方式认证 1.2 授权 (Authorization) RBAC（Role-Based Access Control，基于角色的访问控制）：负责完成授权（Authorization）工作。\nRBAC 根据 API 请求属性，决定允许还是拒绝。\n比较常见的授权维度：\nuser：用户名 group：用户分组 资源，例如 pod、deployment 资源操作方法：get，list，create，update，patch，watch，delete 命名空间 API 组 1.3 准入控制 (Admission Control) Adminssion Control 实际上是一个准入控制器插件列表，发送到 API Server 的请求都需要经过这个列表中的每个准入控制器插件的检查，检查不通过，则拒绝请求\n2 RBAC https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/\n2.1 基础概念 RBAC（Role-Based Access Control，基于角色的访问控制），允许通过 Kubernetes API 动态配置策略。\n角色\nRole：授权特定命名空间的访问权限 ClusterRole：授权所有命名空间的访问权限 角色绑定\nRoleBinding：将角色绑定到主体（即 subject） ClusterRoleBinding：将集群角色绑定到主体 主体（subject）\nUser：用户 Group：用户组 ServiceAccount：服务账号 2.2 示例 为 Amadeus 用户授权 default 命名空间 Pod 读取权限\n2.2.1 新建用户 新建一个 k8s 用户大概可以分为以下几步：\n签发用户证书 生成用户的证书 key 通过用户的证书 key，生成用户的证书请求 (csr) 通过 k8s api 的 ca 证书去签发用户的证书请求，生成用户的证书 (crt) 生成 kubeconfig 配置文件 kubectl config set-cluster //集群配置 kubectl config set-credentials //用户配置 kubectl config set-context //context 配置 kubectl config use-context //使用 context 使用新创建的用户 kubectl \u0026ndash;kubecofig=path // 通过参数指定 KUBECONFIG=path kubectl // 通过环境变量指定，path 可以指定多个，用 : 连接，从而将多个配置文件合并在一起使用 2.2.2 签发用户证书 可以使用 openssl 或者 cfssl 进行签发，任选一种\n[root@k8s-node1 ~]# mkdir -p /etc/kubernetes/users/Amadeus [root@k8s-node1 ~]# cd /etc/kubernetes/users/Amadeus/ openssl\n# 创建用户证书 key [root@k8s-node1 Amadeus]# openssl genrsa -out Amadeus.key 2048 # 创建用户证书请求 (csr)，-subj 指定组和用户，其中 O 是组名，CN 是用户名 [root@k8s-node1 Amadeus]# openssl req -new -key Amadeus.key -out Amadeus.csr -subj \u0026#34;/O=hello/CN=Amadeus\u0026#34; # 生成用户的证书 (crt)，使用 k8s 的 ca 签发用户证书 [root@k8s-node1 Amadeus]# openssl x509 -req -in Amadeus.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out Amadeus.crt -days 3650 [root@k8s-node1 Amadeus]# ls Amadeus.crt Amadeus.csr Amadeus.key cfssl\n下载 cfssl 工具\nwget --no-check-certificate https://github.com/cloudflare/cfssl/releases/download/1.2.0/cfssl_linux-amd64 wget --no-check-certificate https://github.com/cloudflare/cfssl/releases/download/1.2.0/cfssljson_linux-amd64 wget --no-check-certificate https://github.com/cloudflare/cfssl/releases/download/1.2.0/cfssl-certinfo_linux-amd64 chmod a+x cfssl* mv cfssl_linux-amd64 /usr/bin/cfssl mv cfssljson_linux-amd64 /usr/bin/cfssljson mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo [root@k8s-node1 ~]# cfssl version Version: 1.2.0 Revision: dev Runtime: go1.6 创建 ca-config.json 证书文件\ncat \u0026gt; ca-config.json \u0026lt;\u0026lt;EOF { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ], \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; } } } } EOF Amadeus-csr.json 证书文件\ncat \u0026gt; Amadeus-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;Amadeus\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } EOF 生成证书\n[root@k8s-node1 ~]# cfssl gencert -ca=/etc/kubernetes/pki/ca.crt -ca-key=/etc/kubernetes/pki/ca.key -config=ca-config.json -profile=kubernetes Amadeus-csr.json | cfssljson -bare Amadeus # 生成时会有警告，可以忽略，是因为提供的信息不是很全 [root@k8s-node1 ~]# ls Amadeus* # 生成如下三个文件 Amadeus.csr # csr Amadeus-key.pem # key Amadeus.pem # crt 配置 kubeconfig 配置文件\n生成 kubeconfig 文件，并将 cluster 信息添加进去\nkubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=https://1.1.1.1:6443 \\ --kubeconfig=Amadeus.kubeconfig # --embed-certs=true 表示将证书写入etcd 为 kubeconfig 配置文件添加用户配置：设置用户证书 (crt) 和证书 key\nkubectl config set-credentials Amadeus \\ --client-key=Amadeus-key.pem \\ --client-certificate=Amadeus.pem \\ --embed-certs=true \\ --kubeconfig=Amadeus.kubeconfig 为 kubeconfig 配置文件添加 context\nkubectl config set-context Amadeus@kubernetes \\ --cluster=kubernetes \\ --user=Amadeus \\ --kubeconfig=Amadeus.kubeconfig 为 kubeconfig 配置文件设置使用的 context\nkubectl config use-context Amadeus@kubernetes --kubeconfig=Amadeus.kubeconfig 查看生成的配置文件\n# KUBECONFIG=./Amadeus.kubeconfig kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://1.1.1.1:6443 name: kubernetes contexts: - context: cluster: kubernetes user: Amadeus name: Amadeus@kubernetes current-context: Amadeus@kubernetes kind: Config preferences: {} users: - name: Amadeus user: client-certificate-data: REDACTED client-key-data: REDACTED 2.2.3 创建 RBAC 权限策略 使 Amadeus 用户有权限查看 default 命名空间下的 pod\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # api组，置空为核心组 resources: [\u0026#34;pods\u0026#34;] # 资源 verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] # 对资源的操作 --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: Amadeus # 绑定的用户名 apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io 2.2.4 测试验证 [root@k8s-node1 ~]# kubectl apply -f demo-rbac.yml role.rbac.authorization.k8s.io/pod-reader created rolebinding.rbac.authorization.k8s.io/read-pods created # pod可以正常查看 [root@k8s-node1 ~]# cp /etc/kubernetes/users/Amadeus/Amadeus.kubeconfig /root/ [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get pods -n default NAME READY STATUS RESTARTS AGE bar-664fbc5498-kz4sr 1/1 Running 0 18h bar-664fbc5498-r74vl 1/1 Running 0 18h bar-664fbc5498-smqxm 1/1 Running 0 18h ...... # 其他资源都没有权限 [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \u0026#34;Amadeus\u0026#34; cannot list resource \u0026#34;nodes\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get deployments Error from server (Forbidden): deployments.apps is forbidden: User \u0026#34;Amadeus\u0026#34; cannot list resource \u0026#34;deployments\u0026#34; in API group \u0026#34;apps\u0026#34; in the namespace \u0026#34;default\u0026#34; 给该用户增加查看、创建和删除 deployment 的权限，但 pod 的权限依旧只有查看\n[root@k8s-node1 ~]# vim demo-rbac.yml # 在rbac.yaml中增加如下规则 - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] [root@k8s-node1 ~]# kubectl apply -f demo-rbac.yml role.rbac.authorization.k8s.io/pod-reader configured rolebinding.rbac.authorization.k8s.io/read-pods unchanged # 操作 deployment [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl create deployment my-dep --image=nginx:1.22.1 --replicas=3 deployment.apps/my-dep created [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl get pods -l app=my-dep NAME READY STATUS RESTARTS AGE my-dep-bc4cb798-4kbkq 1/1 Running 0 15s my-dep-bc4cb798-jdzq7 1/1 Running 0 15s my-dep-bc4cb798-lhm8p 1/1 Running 0 15s [root@k8s-node1 ~]# KUBECONFIG=/root/Amadeus.kubeconfig kubectl delete deployment my-dep deployment.apps \u0026#34;my-dep\u0026#34; deleted 3 网络策略 (Network Policy) 官方文档\n3.1 基础概念 网络策略（Network Policy），用于限制 Pod 出入流量，提供 Pod 级别和 Namespace 级别网络访问控制。\n一些应用场景：\n应用程序间的访问控制。例如微服务 A 允许访问微服务 B，微服务 C 不能访问微服务 A 开发环境命名空间不能访问测试环境命名空间 Pod 当 Pod 暴露到外部时，需要做 Pod 白名单 多租户网络环境隔离 Pod 网络入口方向隔离：\n基于 Pod 级网络隔离：只允许特定对象访问 Pod（使用标签定义），允许白名单上的 IP 地址或者 IP 段访问 Pod 基于 Namespace 级网络隔离：多个命名空间，A 和 B 命名空间 Pod 完全隔离。 Pod 网络出口方向隔离：\n拒绝某个 Namespace 上所有 Pod 访问外部 基于目的 IP 的网络隔离：只允许 Pod 访问白名单上的 IP 地址或者 IP 段 基于目标端口的网络隔离：只允许 Pod 访问白名单上的端 3.2 示例一 只允许 default 命名空间中携带 run=client1 标签的 Pod 访问 default 命名空间携带 app=web 标签的 Pod 的 80 端口，无法 ping 通\n[root@k8s-node1 ~]# kubectl create deployment web --image=nginx:1.22.1 [root@k8s-node1 ~]# kubectl run client1 --image=busybox:1.28 -- sleep 36000 [root@k8s-node1 ~]# kubectl run client2 --image=busybox:1.28 -- sleep 36000 [root@k8s-node1 ~]# kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS client1 1/1 Running 0 69s run=client1 client2 1/1 Running 0 62s run=client2 web-bc7cc9f65-5mg9d 1/1 Running 0 2m3s app=web,pod-template-hash=bc7cc9f65 networkpolicy.yaml 示例\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: app: web policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: project: default - podSelector: matchLabels: run: client1 ports: - protocol: TCP port: 80 测试验证\n[root@k8s-node1 ~]# kubectl apply -f networkpolicy.yaml [root@k8s-node1 ~]# kubectl get networkpolicy [root@k8s-node1 ~]# kubectl get pods web-bc7cc9f65-hdhr2 -o jsonpath=\u0026#39;{.metadata.annotations.cni\\.projectcalico\\.org\\/podIP}\u0026#39; 10.244.169.169/32 [root@k8s-node1 ~]# kubectl exec -it client1 -- telnet 10.244.169.169 80 Connected to 10.244.169.169 [root@k8s-node1 ~]# kubectl exec -it client2 -- telnet 10.244.169.169 80 # 超时无法联通 [root@k8s-node1 ~]# kubectl delete -f networkpolicy.yaml 3.3 示例二 ns1 命名空间下所有 pod 可以互相访问，也可以访问其他命名空间 Pod，但其他命名空间不能访问 ns1 命名空间 Pod。\n[root@k8s-node1 ~]# kubectl create ns ns1 [root@k8s-node1 ~]# kubectl run ns1-client1 --image=busybox -n ns1 -- sleep 36000 [root@k8s-node1 ~]# kubectl run ns1-client2 --image=busybox -n ns1 -- sleep 36000 [root@k8s-node1 ~]# kubectl get pods -n ns1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ns1-client1 1/1 Running 0 78s 10.244.169.168 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ns1-client2 1/1 Running 0 70s 10.244.107.212 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-node1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES client1 1/1 Running 0 51s 10.244.169.171 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; client2 1/1 Running 0 26m 10.244.107.238 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; networkpolicy.yaml\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-from-other-namespaces namespace: ns1 spec: podSelector: {} # 置空表示默认所有Pod policyTypes: - Ingress ingress: - from: - podSelector: {} # 置空表示拒绝所有 验证\n[root@k8s-node1 ~]# kubectl apply -f networkpolicy.yaml [root@k8s-node1 ~]# kubectl get networkpolicy -n ns1 # ns1命名空间内pod可以互通 [root@k8s-node1 ~]# kubectl exec -it ns1-client1 -n ns1 -- ping 10.244.107.212 # ns1-client2 PING 10.244.107.212 (10.244.107.212): 56 data bytes 64 bytes from 10.244.107.212: seq=0 ttl=62 time=0.900 ms 64 bytes from 10.244.107.212: seq=1 ttl=62 time=0.651 ms # default命名空间的pod无法访问ns1命名空间的pod [root@k8s-node1 ~]# kubectl exec -it client1 -- ping 10.244.107.212 # ns1-client2 # ns1命名空间的pod可以正常访问default命名空间的pod [root@k8s-node1 ~]# kubectl exec -it ns1-client1 -n ns1 -- ping 10.244.169.171 # client1 PING 10.244.169.171 (10.244.169.171): 56 data bytes 64 bytes from 10.244.169.171: seq=0 ttl=63 time=0.119 ms 64 bytes from 10.244.169.171: seq=1 ttl=63 time=0.067 ms [root@k8s-node1 ~]# kubectl delete -f networkpolicy.yaml 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-rbac-networkpolicy/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1 kubernetes 安全框架 客户端要想访问 K8s 集群 API Server，一般需要 CA 证书、Token 或者用户名 + 密码 如果 Pod 访问，需要 ServiceAccount K8S 安全控制框架主要由下面 3 个阶段进行控制，每一个阶段都支持插件方式，通过","title":"kubernetes | RBAC 鉴权和 NetworkPolicy"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1 service 1.1 基本概念 service 存在的意义\n服务发现：防止 Pod 失联 负载均衡：定义一组 Pod 的访问策略 service 通过 label-selector 关联 pod\nservice 的三种类型\nClusterIP：集群内部使用\n默认，分配一个稳定的 IP 地址，即 VIP，只能在集群内部访问（同 Namespace 内的 Pod） NodePort：对外暴露应用\n在每个节点上启用一个端口 (30000-32767) 来暴露服务，可以在集群外部访问。也会分配一个稳定内部集群 IP 地址。 LoadBalancer：对外暴露应用，适用公有云\n与 NodePort 类似，在每个节点上启用一个端口来暴露服务。除此之外，Kubernetes 会请求底层云平台上的负载均衡器，将每个 Node（[NodeIP]:[NodePort]）作为后端添加进去。\n示例\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: selector: app: nginx type: NodePort ports: - protocol: TCP port: 80 # service端口，内部访问端口 targetPort: 80 # 后端业务镜像实际暴露的端口 nodePort: 30002 # 内部访问端口映射到节点端口 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.22.1 ports: - containerPort: 80 1.2 代理模式 Iptables：\n灵活，功能强大 规则遍历匹配和更新，呈线性时延 IPVS：\n工作在内核态，有更好的性能 调度算法丰富：rr，wrr，lc，wlc，ip hash… 1.2.1 iptables 模式 使用 iptables 模式时，根据 iptables 的 --mode random --probability 来匹配每一条请求，每个 pod 收到的流量趋近于平衡，不是完全的轮询\n这种模式，kube-proxy 会监听 Kubernetes 对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会安装 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 任意一组 backend pod 中。对于每个 Endpoints 对象，它也会安装 iptables 规则，这个规则会选择一个 backend pod 组合。\nk8s 默认采用的代理模式是 iptables，可以通过查看 kube-proxy 组件的日志可得\n[root@k8s-node1 ~]# kubectl logs kube-proxy-8mf2l -n kube-system | grep Using I0412 02:02:29.634610 1 server_others.go:212] Using iptables Proxier. 创建一个上述示例中的 yaml ，查看 iptables 规则\n[root@k8s-node1 ~]# kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx NodePort 10.105.220.154 \u0026lt;none\u0026gt; 80:30002/TCP 4m40s # SVC当前共关联三个POD [root@k8s-node1 ~]# kubectl get pod -o wide -l app=nginx NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-55f4d8c85-l29wx 1/1 Running 0 4m57s 10.244.169.133 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-55f4d8c85-lf5dj 1/1 Running 0 4m57s 10.244.107.205 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-55f4d8c85-q4gsx 1/1 Running 0 4m57s 10.244.107.203 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-node1 ~]# iptables-save |grep -i nodeport |grep 30002 # NODEPORTS 根据端口将流量转发到 SVC 链 -A KUBE-NODEPORTS -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp --dport 30002 -j KUBE-SVC-2CMXP7HKUVJN7L6M [root@k8s-node1 ~]# iptables-save |grep KUBE-SVC-2CMXP7HKUVJN7L6M # ClusterIP 相关 -A KUBE-SERVICES -d 10.109.98.33/32 -p tcp -m comment --comment \u0026#34;default/nginx cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-2CMXP7HKUVJN7L6M # 转发到具体 POD 链，每条 POD 链都有一样的概率获取到流量 -A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment \u0026#34;default/nginx\u0026#34; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-ONLOYCYPTBL5FQH5 -A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment \u0026#34;default/nginx\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JABTJNSPARJARZOW -A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-SEP-TZBGLRHUI2CFM5CU # POD链中定义了转发到具体的POD地址， [root@k8s-node1 ~]# iptables-save |grep KUBE-SEP-ONLOYCYPTBL5FQH5 -A KUBE-SEP-ONLOYCYPTBL5FQH5 -s 10.244.107.252/32 -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-ONLOYCYPTBL5FQH5 -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp -j DNAT --to-destination 10.244.107.252:80 [root@k8s-node1 ~]# iptables-save |grep KUBE-SEP-JABTJNSPARJARZOW -A KUBE-SEP-JABTJNSPARJARZOW -s 10.244.169.135/32 -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-JABTJNSPARJARZOW -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp -j DNAT --to-destination 10.244.169.135:80 [root@k8s-node1 ~]# iptables-save |grep KUBE-SEP-TZBGLRHUI2CFM5CU -A KUBE-SEP-TZBGLRHUI2CFM5CU -s 10.244.169.136/32 -m comment --comment \u0026#34;default/nginx\u0026#34; -j KUBE-MARK-MASQ -A KUBE-SEP-TZBGLRHUI2CFM5CU -p tcp -m comment --comment \u0026#34;default/nginx\u0026#34; -m tcp -j DNAT --to-destination 10.244.169.136:80 1.2.2 ipvs 模式 ipvsadm 安装配置 (所有节点都要配置)\n[root@k8s-node1 ~]# yum install ipvsadm [root@k8s-node1 ~]# cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt; EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF [root@k8s-node1 ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules [root@k8s-node1 ~]# source /etc/sysconfig/modules/ipvs.modules 修改 service 使用的代理模式为 ipvs\n[root@k8s-node1 ~]# kubectl edit configmap kube-proxy -n kube-system mode: \u0026#34;ipvs\u0026#34; ipvs: scheduler: \u0026#34;rr\u0026#34; #rr, wrr, lc, wlc, ip hash等 # 删除所有 kube-proxy，k8s 会重新创建 [root@k8s-node1 ~]# kubectl delete pod -n kube-system -l k8s-app=kube-proxy [root@k8s-node1 ~]# kubectl logs kube-proxy-8z86w -n kube-system | grep Using I0412 08:30:21.169231 1 server_others.go:274] Using ipvs Proxier. 查看 ipvs 规则\n[root@k8s-node1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 172.17.0.1:30002 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 TCP 1.1.1.1:30002 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 TCP 10.244.36.64:30002 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 TCP 10.109.98.33:80 rr -\u0026gt; 10.244.107.252:80 Masq 1 0 0 -\u0026gt; 10.244.169.135:80 Masq 1 0 0 -\u0026gt; 10.244.169.136:80 Masq 1 0 0 2 Headless Service Headless Service 相比普通 Service 只是将 spec.clusterIP 定义为 None\nHeadless Service 几大特点：\n不分配 clusterIP\n没有负载均衡的功能 (kube-proxy 不会安装 iptables 规则)\n可以通过解析 service 的 DNS，返回所有 Pod 的 IP 和 DNS (statefulSet 部署的 Pod 才有 DNS)\n[root@k8s-node1 ~]# kubectl run -it --rm --restart=Never --image busybox:1.28 dns-test -- nslookup statefulset-nginx.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: statefulset-nginx.default.svc.cluster.local Address 1: 10.244.107.200 statefulset-nginx-1.statefulset-nginx.default.svc.cluster.local Address 2: 10.244.169.188 statefulset-nginx-0.statefulset-nginx.default.svc.cluster.local pod \u0026#34;dns-test\u0026#34; deleted Headless Services 应用场景\n自主选择权，client 可以通过查询 DNS 来获取 Real Server 的信息，自己来决定使用哪个 Real Server Headless Service 的对应的每一个 Endpoints，即每一个 Pod，都会有对应的 DNS域名，这样 Pod 之间就可以互相访问 DNS 解析名称：\npod：\u0026lt;pod-name\u0026gt;.\u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local service: \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 3 Ingress 3.1 基本概念 NodePort 的不足\n一个端口只能一个服务使用，端口需提前规划 只支持 4 层负载均衡 Ingress 是什么？\nIngress 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由。流量路由由 Ingress 资源上定义的规则控制。\n下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例：\nIngress Controller\nIngress 管理的负载均衡器，为集群提供全局的负载均衡能力。\nIngress Contronler 怎么工作的？\nIngress Contronler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个 service，生成一段 Nginx 配置，应用到管理的 Nginx 服务，然后热加载生效。\n以此来达到 Nginx 负载均衡器配置及动态更新的问题\n使用流程：\n部署 Ingress Controller 创建 Ingress 规则 Ingress Contorller 主流控制器：\ningress-nginx-controller: nginx 官方维护的控制器 Traefik： HTTP 反向代理、负载均衡工具 Istio：服务治理，控制入口流量 这里使用 Nginx 官方维护的，项目地址\n3.2 安装部署 下载 yaml 文件\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/baremetal/1.22/deploy.yaml --no-check-certificate 修改\n# 修改kind, 将原先的Deployment修改为DaemontSet，实现所有物理节点访问 kind: DaemonSet spec: template: spec: # 新增 hostNetwork, 将ingress-nginx-controller的端口直接暴露在宿主机上 hostNetwork: true containers: # 修改 image 为国内地址 image: registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.3.0 # 新增污点容忍，允许在 master 节点创建pod tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; --- kind: Job spec: template: spec: containers: # 修改 image 为国内地址 image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 --- kind: Job spec: template: spec: containers: # 修改 image 为国内地址 image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 部署\n[root@k8s-node1 ~]# kubectl apply -f deploy.yaml [root@k8s-node1 opt]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-admission-create--1-zfwrz 0/1 Completed 0 12m 10.244.169.135 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-admission-patch--1-8rhjr 0/1 Completed 0 12m 10.244.169.134 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-bb2kd 1/1 Running 0 12m 1.1.1.3 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-bp588 1/1 Running 0 12m 1.1.1.2 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-z2782 1/1 Running 0 12m 1.1.1.1 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 如果出现内部访问报错：failed calling webhook \u0026#34;validate.nginx.ingress.kubernetes.io\u0026#34; [root@k8s-node1 ~]# kubectl get ValidatingWebhookConfiguration [root@k8s-node1 ~]# kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission 3.3 测试 测试 url 跳转，创建三套 nginx 应用 : test | foo | bar\n需要注意的是，代理路径假如是 /foo 的话，后端真实路径也是 /foo\ntest 应用示例，foo 和 bar 的自行修改\napiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: test spec: replicas: 3 selector: matchLabels: app: test template: metadata: labels: app: test spec: terminationGracePeriodSeconds: 5 containers: - name: test image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www persistentVolumeClaim: claimName: pvc-test --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-test spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 2Gi --- apiVersion: v1 kind: Service metadata: name: test labels: app: test spec: selector: app: test ports: - protocol: TCP port: 80 targetPort: 80 创建 ingress\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: demo-nginx annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: test.com http: paths: - path: / pathType: Prefix backend: service: name: test port: number: 80 - path: /foo pathType: Prefix backend: service: name: foo port: number: 80 - path: /bar pathType: Prefix backend: service: name: bar port: number: 80 查看创建的 ingress\n[root@k8s-node1 ~]# kubectl describe ingress demo-nginx Name: demo-nginx Namespace: default Address: 1.1.1.1,1.1.1.2,1.1.1.3 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- test.com / test:80 (10.244.107.209:80,10.244.107.212:80,10.244.169.140:80) /foo foo:80 (10.244.107.210:80,10.244.169.138:80,10.244.169.144:80) /bar bar:80 (10.244.107.211:80,10.244.169.131:80,10.244.169.143:80) Annotations: kubernetes.io/ingress.class: nginx Events: \u0026lt;none\u0026gt; 修改 index.html\n[root@k8s-node1 ~]# echo \u0026#34;test\u0026#34; \u0026gt; /nfs/default-pvc-test-pvc-93a7df14-90f2-4466-8655-6ef42549b760/index.html [root@k8s-node1 ~]# mkdir /nfs/default-pvc-foo-pvc-75e73500-1a70-4305-8253-d1e7d8c88b49/foo [root@k8s-node1 ~]# echo \u0026#34;foo\u0026#34; \u0026gt; /nfs/default-pvc-foo-pvc-75e73500-1a70-4305-8253-d1e7d8c88b49/foo/index.html [root@k8s-node1 ~]# mkdir /nfs/default-pvc-bar-pvc-73d12b15-7c53-46ee-a1b6-d0cb2c25e7e6/bar/ [root@k8s-node1 ~]# echo \u0026#34;bar\u0026#34; \u0026gt; /nfs/default-pvc-bar-pvc-73d12b15-7c53-46ee-a1b6-d0cb2c25e7e6/bar/index.html 访问测试\n[root@k8s-node1 ~]# curl http://1.1.1.1/ -H \u0026#34;Host: test.com\u0026#34; test [root@k8s-node1 ~]# curl http://1.1.1.2/foo/ -H \u0026#34;Host: test.com\u0026#34; foo [root@k8s-node1 ~]# curl http://1.1.1.3/bar/ -H \u0026#34;Host: test.com\u0026#34; bar 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-service-ingress/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1 service 1.1 基本概念 service 存在的意义 服务发现：防止 Pod 失联 负载均衡：定义一组 Pod 的访问策略 service 通过 label-selector 关联 pod service 的三种类型 ClusterIP：集群内部使用 默认，分配一个稳定的 IP 地址，即 VIP，只能在集","title":"kubernetes | service \u0026 ingress"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n为什么需要数据卷\n启动时需要的初始数据，录入配置文件 启动过程中产生的临时数据，该临时数据需要多个容器间共享 启动过程中产生的持久化数据，例如 mysql 的 data 数据卷概述\nkubernetes 中的 volume 提供了在容器中挂载外部存储的能力 Pod 需要设置卷来源（spec.volume）和挂载点（spec.containers.volumeMounts）两个信息后才可以使用相应的 Volume 常用的数据卷：\n本地（hostPath，emptyDir） 网络（NFS，Ceph，GlusterFS） 公有云（AWS EBS） K8S 资源（configmap，secret） 1 emptyDir（临时存储卷） emptyDir 卷：是一个临时存储卷，与 Pod 生命周期绑定一起，如果 Pod 删除了卷也会被删除。\n应用场景：Pod 中容器之间数据共享\nemptyDir 的实际存储路径在 pod 所在节点的 /var/lib/kubelet/pods/\u0026lt;pod-id\u0026gt;/volumes/kubernetes.io~empty-dir 目录下\n查看 pod 的 uid\nkubectl get pod \u0026lt;pod-name\u0026gt; -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39; 示例如下\napiVersion: v1 kind: Pod metadata: name: demo-emptydir spec: terminationGracePeriodSeconds: 5 containers: - name: write image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;for i in $(seq 100); do echo $i \u0026gt;\u0026gt; /data/hello; sleep 1; done\u0026#34;] volumeMounts: - name: data mountPath: /data - name: read image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;tail -f /data/hello\u0026#34;] volumeMounts: - name: data mountPath: /data volumes: - name: data emptyDir: {} 查看日志\n[root@k8s-node1 ~]# kubectl logs -f demo-emptydir -c read 1 2 3 ... 2 hostPath（节点存储卷） hostPath 卷：挂载 Node 文件系统（Pod 所在节点）上文件或者目录到 Pod 中的容器。\n应用场景：Pod 中容器需要访问宿主机文件\n示例 yaml\napiVersion: v1 kind: Pod metadata: name: pod-hostpath spec: terminationGracePeriodSeconds: 5 containers: - name: busybox image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 36000\u0026#34;] volumeMounts: - name: data mountPath: /data volumes: - name: data hostPath: path: /tmp type: Directory 3 NFS（网络存储卷） NFS 卷提供对 NFS 挂载支持，可以自动将 NFS 共享路径挂载到 Pod 中\n配置 nfs 服务端，\nyum install nfs-utils # nfs-utils包每个节点都需安装 mkdir -p /nfs echo \u0026#34;/nfs 1.1.1.0/24(rw,async,no_root_squash)\u0026#34; \u0026gt;\u0026gt; /etc/exports # 格式：NFS共享的目录 客户端地址1(参数1,参数2,...) 客户端地址2(参数1,参数2,...)) systemctl enable --now nfs systemctl enable --now rpcbind 常用选项： ro：客户端挂载后，其权限为只读，默认选项； rw: 读写权限； sync：同时将数据写入到内存与硬盘中； async：异步，优先将数据保存到内存，然后再写入硬盘； Secure：要求请求源的端口小于 1024 用户映射： root_squash: 当 NFS 客户端使用 root 用户访问时，映射到 NFS 服务器的匿名用户； no_root_squash: 当 NFS 客户端使用 root 用户访问时，映射到 NFS 服务器的 root 用户； all_squash: 全部用户都映射为服务器端的匿名用户； anonuid=UID：将客户端登录用户映射为此处指定的用户 uid； anongid=GID：将客户端登录用户映射为此处指定的用户 gid 客户端测试\n[root@k8s-node2 ~]# mount -t nfs k8s-node1:/nfs /mnt/ [root@k8s-node2 ~]# df -hT | grep k8s-node1 k8s-node1:/nfs nfs4 44G 4.0G 41G 9% /mnt 示例 yaml\napiVersion: apps/v1 kind: Deployment metadata: name: demo-nfs labels: app: demo-nfs spec: replicas: 3 selector: matchLabels: app: demo-nfs template: metadata: labels: app: demo-nfs spec: terminationGracePeriodSeconds: 5 containers: - name: demo-nfs image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www nfs: server: k8s-node1 path: /nfs/ --- apiVersion: v1 kind: Service metadata: name: demo-nfs labels: app: demo-nfs spec: selector: app: demo-nfs ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30003 type: NodePort 验证\n[root@k8s-node1 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-nfs NodePort 10.97.209.119 \u0026lt;none\u0026gt; 80:30003/TCP 5m41s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5d2h [root@k8s-node1 ~]# echo \u0026#34;hello, nfs\u0026#34; \u0026gt; /nfs/index.html [root@k8s-node1 ~]# curl 10.97.209.119 hello, nfs 4 pv 和 pvc（持久存储卷） 4.1 基础概念 PersistentVolume（PV）：存储资源创建和使用抽象化，使得存储作为集群中的资源管理 PersistentVolumeClaim（PVC）：让用户不需要关心具体的 Volume 实现细节 pvc 如何匹配到 pv\n存储空间的请求 匹配最接近的 pv，如果没有满足条件的 pv，则 pod 处于 pending 状态 访问模式的设置 存储空间字段能否限制实际可用容量\n不能，存储空间字段只用于匹配到 pv，具体可用容量取决于网络存储 4.2 pv 生命周期 AccessModes（访问模式）：\nAccessModes 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\nReadWriteOnce（RWO）：可被一个 node 读写挂载 ReadOnlyMany（ROX）：可被多个 node 只读挂载 ReadWriteMany（RWX）：可被多个 node 读写挂载 RECLAIM POLICY（回收策略）：\n目前 PV 支持的策略有三种：\nRetain（保留）： 保留数据，需要管理员手工清理数据 Recycle（回收）：清除 PV 中的数据，效果相当于执行 rm -rf /ifs/kuberneres/* Delete（删除）：与 PV 相连的后端存储同时删除 STATUS（状态）：\n一个 PV 的生命周期中，可能会处于 4 中不同的阶段：\nAvailable（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PV 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 pv 示例\napiVersion: v1 kind: PersistentVolume metadata: name: demo-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: server: k8s-node1 path: /nfs pvc 示例\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: demo-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi deployment \u0026amp; service 示例\napiVersion: apps/v1 kind: Deployment metadata: name: demo-pvc labels: app: demo-pvc spec: replicas: 3 selector: matchLabels: app: demo-pvc template: metadata: labels: app: demo-pvc spec: terminationGracePeriodSeconds: 5 containers: - name: demo-pvc image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www-pvc mountPath: /usr/share/nginx/html/ volumes: - name: www-pvc persistentVolumeClaim: claimName: demo-pvc --- apiVersion: v1 kind: Service metadata: name: demo-pvc labels: app: demo-pvc spec: selector: app: demo-pvc ports: - protocol: TCP port: 80 targetPort: 80 验证\n[root@k8s-node1 ~]# echo \u0026#34;pvc for NFS is successful\u0026#34; \u0026gt; /nfs/index.html [root@k8s-node1 ~]# kubectl get svc -l app=demo-pvc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-pvc ClusterIP 10.97.28.93 \u0026lt;none\u0026gt; 80/TCP 102s [root@k8s-node1 ~]# curl 10.97.28.93 pvc for NFS is successful 4.3 pv 动态供给 之前的 PV 使用方式称为静态供给，需要 K8s 运维工程师提前创建一堆 PV，供开发者使用\n因此，K8s 开始支持 PV 动态供给，使用 StorageClass 对象实现。\n查看 k8s 原生支持的共享存储\n基于 NFS 实现自动创建 pv 插件\n自动创建的 pv 挂载路径为 \u0026lt;nfs-path\u0026gt;/\u0026lt;namespace\u0026gt;-\u0026lt;pvc-name\u0026gt;-\u0026lt;pv-name\u0026gt;\npvc-name：默认情况下为 yaml 中自定义的 pvc-name，使用 statefulset 控制器时 pvc 的名字为 \u0026lt;volumeClaimTemplates-name\u0026gt;-\u0026lt;pod-name\u0026gt; pv-name：pv 的名字为 pvc-\u0026lt;pvc-uid\u0026gt; k8s-1.20 版本后默认禁止使用 selfLink，需要打开一下\n修改 k8s 的 apiserver 参数，改完 apiserver 会自动重启\n[root@k8s-node1 ~]# vi /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 ··· - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --feature-gates=RemoveSelfLink=false # 添加这个配置 4.3.1 部署 NFS 插件 此组件是对 nfs-client-provisioner 的扩展，nfs-client-provisioner 已经不提供更新，且 nfs-client-provisioner 的 Github 仓库已经迁移到 NFS-Subdir-External-Provisioner 的仓库\nrbac\n创建 nfs-rbac.yml\napiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumeclaims\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;] resources: [\u0026#34;storageclasses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: kube-system roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: kube-system roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io nfs-subdir-external-provisioner\n创建 nfs-provisioner-deploy.yml\napiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate # 设置升级策略为删除再创建(默认为滚动更新) selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner #image: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.0 image: registry.cn-hangzhou.aliyuncs.com/lvbibir/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME # Provisioner的名称,以后设置的storageclass要和这个保持一致 value: nfs-client - name: NFS_SERVER # NFS服务器地址,需和valumes参数中配置的保持一致 value: 1.1.1.1 - name: NFS_PATH # NFS服务器数据存储目录,需和valumes参数中配置的保持一致 value: /nfs/kubernetes volumes: - name: nfs-client-root nfs: server: 1.1.1.1 # NFS服务器地址 path: /nfs/kubernetes # NFS服务器数据存储目录 storageClass\n创建 nfs-sc.yml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; ## 是否设置为默认的storageclass provisioner: nfs-client ## 动态卷分配者名称，必须和deployment的PROVISIONER_NAME变量中设置的Name一致 parameters: archiveOnDelete: \u0026#34;false\u0026#34; ## 设置为\u0026#34;false\u0026#34;时删除PVC不会保留数据,\u0026#34;true\u0026#34;则保留数据 mountOptions: - hard ## 指定为硬挂载方式 - nfsvers=4 ## 指定NFS版本,这个需要根据NFS Server版本号设置 创建上述资源\n[root@k8s-node1 nfs]# mkdir /nfs/kubernetes/ [root@k8s-node1 nfs]# kubectl apply -f . deployment.apps/nfs-client-provisioner created serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created storageclass.storage.k8s.io/nfs created 4.3.2 示例 apiVersion: apps/v1 kind: Deployment metadata: name: demo-auto-pv labels: app: demo-auto-pv spec: replicas: 3 selector: matchLabels: app: demo-auto-pv template: metadata: labels: app: demo-auto-pv spec: terminationGracePeriodSeconds: 5 containers: - name: demo-auto-pv image: nginx:1.22.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html/ volumes: - name: www persistentVolumeClaim: claimName: pvc-auto --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-auto spec: storageClassName: \u0026#34;nfs\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 测试验证\n[root@k8s-node1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE pod/demo-auto-pv-7c974689b4-4cwr6 1/1 Running 0 47s pod/demo-auto-pv-7c974689b4-bb9v8 1/1 Running 0 47s pod/demo-auto-pv-7c974689b4-p525n 1/1 Running 0 47s pod/nfs-client-provisioner-66d6cb77fd-47hsf 1/1 Running 0 4m15s [root@k8s-node1 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc-auto Bound pvc-22b65e10-ab97-47eb-aaa1-6c354a749a55 2Gi RWO managed-nfs-storage 47s [root@k8s-node1 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-22b65e10-ab97-47eb-aaa1-6c354a749a55 2Gi RWO Delete Bound default/pvc-auto managed-nfs-storage 47s [root@k8s-node1 ~]# ls -l /nfs/ total 4 drwxrwxrwx. 2 root root 6 Apr 11 17:37 default-pvc-auto-pvc-22b65e10-ab97-47eb-aaa1-6c354a749a55 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-storage/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 为什么需要数据卷 启动时需要的初始数据，录入配置文件 启动过程中产生的临时数据，该临时数据需要多个容器间共享 启动过程中产生的持久化数据，例如 mysql 的 data 数据卷概述 kubernetes 中的 volume 提供了在容器中挂载外","title":"kubernetes | 存储"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1 滚动升级 1.1 实现机制 滚动升级的实现机制\n两个 replicaset 控制器分别控制旧版本的 pod 和新版本 pod，replicaset2 启动一个新版版本 pod，相应的 replicaset1 停止一个旧版本 pod，从而实现滚动升级。在这过程中，无法保证业务流量完全不丢失。\n1.2 简单示例 升级\nkubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [options] # 示例 kubectl set image deployment/demo-rollout nginx=nginx:1.15 --record=true # --record=true 表示将升级的命令记录到升级记录中 回滚\n# 上次升级状态 kubectl rollout status deployment demo-rollout # 升级记录 kubectl rollout history deployment demo-rollout # 回滚至上个版本 kubectl rollout undo deployment demo-rollout # 回滚至指定版本 kubectl rollout undo deployment demo-rollout --to-revision=2 1.3 升级 在所有 work 节点先创建几个 busybox 镜像的 tag 用于升级演示\n[root@k8s-node3 ~]# for i in {1..3}; do docker tag busybox:latest busybox:v${i}; done [root@k8s-node3 ~]# docker images | grep busybox busybox latest 7cfbbec8963d 3 weeks ago 4.86MB busybox v1 7cfbbec8963d 3 weeks ago 4.86MB busybox v2 7cfbbec8963d 3 weeks ago 4.86MB busybox v3 7cfbbec8963d 3 weeks ago 4.86MB 创建 v1 版本的 deployment\napiVersion: apps/v1 kind: Deployment metadata: name: demo-rollout labels: app: demo-rollout spec: replicas: 3 selector: matchLabels: app: demo-rollout template: metadata: labels: app: demo-rollout spec: containers: - name: busybox image: busybox:v1 command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;sleep 36000\u0026#39;] 也可以使用命令创建\n[root@k8s-node1 ~]# kubectl create deployment demo-rollout --image=busybox:v1 --replicas=3 -- sleep 3600 deployment.apps/demo-rollout created [root@k8s-node1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE demo-rollout-5d847fd86c-678pr 1/1 Running 0 4s demo-rollout-5d847fd86c-9mj4v 1/1 Running 0 4s demo-rollout-5d847fd86c-xhvf7 1/1 Running 0 4s 升级至 v2 和 v3\n# 升级 [root@k8s-node1 ~]# kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true [root@k8s-node1 ~]# kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true # 查看升级状态 [root@k8s-node1 ~]# kubectl rollout status deployment demo-rollout deployment \u0026#34;demo-rollout\u0026#34; successfully rolled out [root@k8s-node1 ~]# kubectl rollout history deployment demo-rollout deployment.apps/demo-rollout REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true 3 kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true # 查看实际镜像版本 [root@k8s-node1 ~]# kubectl get deployment demo-rollout -o jsonpath=\u0026#39;{.spec.template.spec.containers}\u0026#39; [root@k8s-node1 ~]# kubectl describe deployment demo-rollout | grep -i image: Image: busybox:v3 1.4 回滚 回滚至 v1 版本\n[root@k8s-node1 ~]# kubectl rollout undo deployment/demo-rollout --to-revision=1 deployment.apps/demo-rollout rolled back [root@k8s-node1 ~]# kubectl describe deployment demo-rollout | grep -i image: Image: busybox:v1 [root@k8s-node1 ~]# kubectl rollout history deployment demo-rollout deployment.apps/demo-rollout REVISION CHANGE-CAUSE 2 kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true 3 kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true 4 \u0026lt;none\u0026gt; 可以看到 rollout history 删除了第一次的记录, 重新记录到第四条\n恢复到 v2 版本\n[root@k8s-node1 ~]# kubectl rollout undo deployment/demo-rollout --to-revision=2 deployment.apps/demo-rollout rolled back [root@k8s-node1 ~]# kubectl describe deployment demo-rollout | grep -i image: Image: busybox:v2 [root@k8s-node1 ~]# kubectl rollout history deployment demo-rollout deployment.apps/demo-rollout REVISION CHANGE-CAUSE 3 kubectl set image deployment/demo-rollout busybox=busybox:v3 --record=true 4 \u0026lt;none\u0026gt; 5 kubectl set image deployment/demo-rollout busybox=busybox:v2 --record=true 2 自动伸缩 手动扩容\nkubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME) [options] # 示例 kubectl scale deployment demo-rollout --replicas=10 自动扩容\n实现自动扩容需满足两个条件：\n运行了 metric-server pod 设置了 request 资源 Horizontal Pod Autoscaling: pod 水平扩容，k8s 中的一个 api 资源，使用 autoscale 时会创建一个 hpa 资源\nHPA 基本原理:\n查询指定的资源中所有 Pod 的资源平均使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能. HPA 自动伸缩副本时会使 POD 的资源使用率趋近于预设的 target 值 比如只有一个 POD 时, 资源使用率达到了 180%/70%, HPA 会将 POD 数量扩容到 3 个, 此时资源使用率将会是 60%/70%. 当 pod 资源使用率回到正常水平, controller-manager 会默认等待 5 分钟的时间再缩容 pod,以免再次出现突发流量. kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [options] # 基于cpu指标进行扩容 kubectl autoscale deployment demo-rollout --min=3 --max=10 --cpu-percent=10 # 查看hpa kubectl get hpa # replicaset控制器记录了pod的详细伸缩记录 kubectl get rs kubectl describe rs demo-rollout-54fdcc5676 2.1 基于 CPU 创建 deployment 资源\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-demo spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 resources: requests: memory: 50Mi cpu: 50m --- apiVersion: v1 kind: Service metadata: name: hpa-demo labels: app: nginx spec: selector: app: nginx type: NodePort ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30002 创建 hpa 资源\ncpu 使用率 = 已使用 / request\n--cpu-percent=60 代表所有 pod 的平均 cpu 使用率达到百分之 60 时触发扩容\n[root@k8s-node1 ~]# kubectl autoscale deployment hpa-demo --cpu-percent=60 --min=1 --max=10 horizontalpodautoscaler.autoscaling/hpa-demo autoscaled [root@k8s-node1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo Deployment/hpa-demo 0%/60% 1 10 1 18s 压测\n[root@k8s-node1 ~]# yum install -y httpd-tools [root@k8s-node1 ~]# ab -n 1000000 -c 200 http://1.1.1.1:30002/ hpa 自动扩容, pod 数量增加到了 10 个\n[root@k8s-node1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo Deployment/hpa-demo 160%/10% 1 10 10 50m [root@k8s-node1 ~]# kubectl describe hpa hpa-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetScale 17m (x8 over 19m) horizontal-pod-autoscaler deployments/scale.apps \u0026#34;hpa-demo\u0026#34; not found Normal SuccessfulRescale 11m horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 11m horizontal-pod-autoscaler New size: 8; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 10m horizontal-pod-autoscaler New size: 10; reason: [root@k8s-node1 ~]# kubectl describe deployment hpa-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 16m deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 1 Normal ScalingReplicaSet 10m deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 4 Normal ScalingReplicaSet 9m53s deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 8 Normal ScalingReplicaSet 9m38s deployment-controller Scaled up replica set hpa-demo-6b4467b546 to 10 压测结束后也并不会立即减少 pod 数量，会等一段时间后减少 pod 数量，防止流量再次激增。默认时间大概是 5 分钟左右\n2.2 基于内存 使用 busybox 容器测试, 另挂载一个 configMap 用于内存压力测试, 由于用到了 mount 命令, 还需要将 container 声明为特权模式.\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-mem spec: selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 36000\u0026#34;] volumeMounts: - name: increase-mem-script mountPath: /opt/ resources: requests: memory: 50Mi cpu: 50m securityContext: privileged: true volumes: - name: increase-mem-script configMap: name: increase-mem-config --- apiVersion: v1 kind: ConfigMap metadata: name: increase-mem-config data: increase-mem.sh: | #!/bin/sh mkdir /tmp/memory mount -t tmpfs -o size=40M tmpfs /tmp/memory dd if=/dev/zero of=/tmp/memory/block sleep 60 rm /tmp/memory/block umount /tmp/memory rmdir /tmp/memory 获取 hpa 的模板 yaml 文件\n[root@k8s-node1 ~]# kubectl autoscale deployment hpa-mem --min=1 --max=10 --dry-run=client -o yaml \u0026gt; hpa-mem-hpa.yml [root@k8s-node1 ~]# vim hpa-mem-hpa.yml 使用 yaml 创建 hpa, 默认使用的是 autoscaling/v1 版本的 api, 它不支持基于内存的自动扩容, 需要修改为 autoscaling/v2beta1\napiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: hpa-mem spec: maxReplicas: 10 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-mem metrics: - type: Resource resource: name: memory targetAverageUtilization: 60 执行脚本进行压测, 随着脚本执行, hpa 自动将副本数扩容到了两个\n[root@k8s-node1 ~]# kubectl exec -it hpa-mem-c6c7d4957-fpsfb -- /bin/sh /opt/increase-mem.sh [root@k8s-node1 ~]# kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-mem Deployment/hpa-mem 0%/60% 1 10 1 2m55s hpa-mem Deployment/hpa-mem 80%/60% 1 10 1 4m1s hpa-mem Deployment/hpa-mem 80%/60% 1 10 2 4m16s 脚本执行 60s 后会使内存使用率自动恢复正常, 副本数过段时间也会自动恢复\n2.3 基于自定义指标 待续……\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-auto-scale/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1 滚动升级 1.1 实现机制 滚动升级的实现机制 两个 replicaset 控制器分别控制旧版本的 pod 和新版本 pod，replicaset2 启动一个新版版本 pod，相应的 replicaset1 停止一个旧版本 pod，从而实现滚动升级。在","title":"kubernetes | 滚动升级和自动伸缩"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\ncontrollers 作用：\n管理 pod 对象 使用标签与 pod 关联 负责滚动更新、伸缩、副本管理、维持 pod 状态等 1 daemonset 2 ingress 3 statefulset 4 replicaset ReplicaSet：副本集\n协助 Deployment 做事\nPod 副本数量管理，不断对比当前 Pod 数量与期望 Pod 数量\nDeployment 每次发布都会创建一个 RS 作为记录，用于实现回滚\n5 deployment deployment 用于网站、API、微服务等，功能特性：\n管理 pod 和 replicaset 具有上线部署、副本设定、滚动升级、回滚等功能 提供声明式更新，例如只更新一个新的 image 示例\napiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19 ports: - containerPort: 80 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-controllers/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 controllers 作用： 管理 pod 对象 使用标签与 pod 关联 负责滚动更新、伸缩、副本管理、维持 pod 状态等 1 daemonset 2 ingress 3 statefulset 4 replicaset ReplicaSet：副本集 协助 Deployment 做事 Pod 副本数量管理，不断对比当前 Pod 数量与期望 Pod 数量 Deployment 每","title":"kubernetes | 控制器"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\nkubelet logs 命令的流程\nkubectl logs --请求--\u0026gt; apiserver --请求--\u0026gt; kubelet --读取--\u0026gt; container日志 k8s 日志包含两大类：\nk8s 系统的组件日志 k8s 集群中部署的应用程序的日志 标准输出 日志文件 1 组件日志 journalctl -u kubelet kubectl logs kube-proxy -n kube-system /var/log/messages 2 pod 日志 2.1 标准输出 实时查看 pod 标准输出日志\nkubectl logs [options] \u0026lt;podname\u0026gt; kubectl logs -f \u0026lt;podname\u0026gt; kubectl logs -f \u0026lt;podname\u0026gt; -c \u0026lt;containername\u0026gt; kubectl logs --previous \u0026lt;podname\u0026gt; # 查看pod上次重启的日志 k8s 会将每个 pod 中每个 container 的日志记录到 pod 所在 node 的 /var/log/pods 目录中, 日志文件其实是 docker 保存的日志文件的一个软连接.\nk8s 会为每个 pod 的每个 container 日志保留 2 份, 一份为 container 当前状态的日志, 另一份是 container 上一次生命周期的日志, 日志保留数量应该是由 k8s 的 gc 机制管控.\n# k8s 日志 /var/log/pods/\u0026lt;pod namespace\u0026gt;_\u0026lt;pod name\u0026gt;_\u0026lt;pod uid\u0026gt;/\u0026lt;容器名称\u0026gt;/容器重启次数.log # docker日志 /var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log 示例\n# 可以看到 calico-kube-controllers 这个 pod 重启了 7 次 [root@k8s-node1 ~]# kubectl get pods -n kube-system -l k8s-app=calico-kube-controllers NAME READY STATUS RESTARTS AGE calico-kube-controllers-6c76574f75-69flf 1/1 Running 7 (41h ago) 4d1h # k8s 分别保存了编号 6 和 7 两个日志文件 [root@k8s-node3 ~]# ls -l /var/log/pods/kube-system_calico-kube-controllers-6c76574f75-69flf_066e1042-97a4-4547-8d2d-6580cbad40c5/calico-kube-controllers/ lrwxrwxrwx. 1 root root 165 Apr 20 14:31 6.log -\u0026gt; /var/lib/docker/containers/8ed4865daa5d984d9b7e3412f61251ce1a5e12e295fce1f14ee341c3f79b1afe/8ed4865daa5d984d9b7e3412f61251ce1a5e12e295fce1f14ee341c3f79b1afe-json.log lrwxrwxrwx. 1 root root 165 Apr 23 10:23 7.log -\u0026gt; /var/lib/docker/containers/c30d353ee464efd853968dcd1524933aa214294303e5a7cd7828b0e86f0e94ec/c30d353ee464efd853968dcd1524933aa214294303e5a7cd7828b0e86f0e94ec-json.log # 7 号日志文件是当前生命周期的日志 [root@k8s-node1 ~]# kubectl logs --tail=1 calico-kube-controllers-6c76574f75-69flf -n kube-system 2023-04-23 03:05:54.256 [INFO][1] resources.go 350: Main client watcher loop [root@k8s-node3 calico-kube-controllers]# tail -n 1 7.log | python -m json.tool { \u0026#34;log\u0026#34;: \u0026#34;2023-04-23 03:05:54.256 [INFO][1] resources.go 350: Main client watcher loop\\n\u0026#34;, \u0026#34;stream\u0026#34;: \u0026#34;stderr\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2023-04-23T03:05:54.257447874Z\u0026#34; } # 6 号日志文件是上一次生命周期的日志 [root@k8s-node1 ~]# kubectl logs --tail=1 --previous calico-kube-controllers-6c76574f75-69flf -n kube-system 2023-04-21 08:57:20.637 [INFO][1] resources.go 350: Main client watcher loop [root@k8s-node3 calico-kube-controllers]# tail -n 1 6.log | python -m json.tool { \u0026#34;log\u0026#34;: \u0026#34;2023-04-21 08:57:20.637 [INFO][1] resources.go 350: Main client watcher loop\\n\u0026#34;, \u0026#34;stream\u0026#34;: \u0026#34;stderr\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2023-04-21T08:57:20.637780663Z\u0026#34; } 2.2 日志文件 比如 nginx 应用的日志一般保存在 accesss.log 和 error.log 日志中，这些日志是不会输出到标准输出的，可以采用如下两种方式进行采集\n2.2.1 emptyDir 数据卷 创建 pod 时挂载 emptyDIr 类型的数据卷，用以持久化自定义的日志文件\n需要先找到 pod 分配的节点\nKubectl get pods -o wide 再查看 pod 的 id\ndocker ps | grep pod-name # 或者 kubectl get pod \u0026lt;podname\u0026gt; -n \u0026lt;namespace\u0026gt; -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39; pod 日志文件路径\n/var/lib/kubelet/pods/\u0026lt;pod-id\u0026gt;/volumes/kubernetes.io~empty-dir 示例\napiVersion: v1 kind: Pod metadata: name: pod-logs-emptydir spec: containers: - name: web image: nginx volumeMounts: - name: logs mountPath: /var/log/nginx/ volumes: - name: logs emptyDir: {} 2.2.2 sidecar 边车容器 通过创建边车容器实现将应用原本的日志文件输出到标准输出\n示例：\napiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: web image: nginx:1.22.1 volumeMounts: - name: logs mountPath: /var/log/nginx/ - name: accesslog image: busybox:1.28 command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;tail -f /opt/access.log\u0026#39;] volumeMounts: - name: logs mountPath: /opt volumes: - name: logs emptyDir: {} 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-logs/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 kubelet logs 命令的流程 kubectl logs --请求--\u0026gt; apiserver --请求--\u0026gt; kubelet --读取--\u0026gt; container日志 k8s 日志包含两大类： k8s 系统的组件日志 k8s 集群中部署的应用程序的日志 标准输出 日志文","title":"kubernetes | 日志"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1 创建 pod 的工作流程 kubectl run nginx \u0026ndash;image=nginx kubectl 将创建 pod 的请求发送到 apiserver apiserver 将请求信息写入 etcd apiserver 通知 scheduler，收到请求信息后根据调度算法将 pod 分配到合适节点 scheduler 给 pod 标记调度结果，并返回给 apiserver apiserver 收到后写入 etcd 对应节点的 kubelet 收到创建 pod 的事件，从 apiserver 获取到 pod 的相关信息 kubelet 调用 docker api 创建 pod 所需的容器 创建完成之后将 pod 状态汇报给 apiserver apiserver 将收到的 pod 状态写入 apiserver kubectl get pods 即可收到相关信息 2 资源限制对 pod 调度的影响 容器资源限制：\nresources.limits.cpu resources.limits.memory 容器使用的最小资源需求，并不是实际占用，是预留资源：\nresources.requests.cpu resources.requests.memory apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: web image: nginx resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; #cpu单位也可以写浮点数，例如0.25=250m，代表四分之一核cpu limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 3 nodeSelector nodeSelector 用于将 Pod 调度到匹配 Label 的 Node 上，如果没有匹配的标签会调度失败。\n先创建 pod 后打标签，起始出于 pending 状态，打好标签后，pod 会正常分配\n给节点打标签：\nkubectl label nodes [node] key=value # 打lable, value可以是空 kubectl label nodes [node] key- # 删除label kubectl get nodes -l key=value # 根据label筛选 # 示例 kubectl label nodes k8s-node1 disktype=ssd kubectl label nodes k8s-node1 disktype- 示例\napiVersion: v1 kind: Pod metadata: name: pod-nodeselector spec: containers: - name: nginx image: nginx:1.19 nodeSelector: disktype: \u0026#34;ssd\u0026#34; 4 nodeAffinity 节点亲和性概念上类似于 nodeSelector， 它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。 节点亲和性有两种：\nrequiredDuringSchedulingIgnoredDuringExecution： 调度器只有在规则被满足的时候才能执行调度。此功能类似于 nodeSelector， 但其语法表达能力更强。 preferredDuringSchedulingIgnoredDuringExecution： 调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。 先创建 pod 后打标签起始出于 pending 状态，打好标签后，pod 会正常分配\nIgnoredDuringExecution 意味着如果节点标签在 Kubernetes 调度 Pod 后发生了变更，Pod 仍将继续运行。\n操作符：In、NotIn、Exists、DoesNotExist、Gt、Lt\n示例\napiVersion: v1 kind: Pod metadata: name: with-affinity-anti-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - windows preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: label-1 operator: In values: - key-1 - weight: 50 preference: matchExpressions: - key: label-2 operator: In values: - key-2 containers: - name: with-node-affinity image: registry.k8s.io/pause:2.0 5 Taint(污点) Taints：避免 Pod 调度到特定 Node 上\n应用场景：\n专用节点，例如配备了特殊硬件的节点 基于 Taint 的驱逐 设置污点：\nkubectl taint node [node] key=value:[effect] # 其中[effect]可取值： # - NoSchedule ：一定不能被调度。 # - PreferNoSchedule：尽量不要调度。 # - NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。 去掉污点：\nkubectl taint node [node] key:[effect]- 示例\n[root@k8s-node1 ~]# kubectl label node k8s-node2 disktype=ssd node/k8s-node2 labeled [root@k8s-node1 ~]# kubectl taint node k8s-node2 disktype=ssd:NoSchedule node/k8s-node2 tainted [root@k8s-node1 ~]# kubectl describe node k8s-node2 | grep -i taints Taints: disktype=ssd:NoSchedule Tolerations（污点容忍）\n允许 Pod 调度到持有 Taints 的 Node 上，但不是绝对分配到指定的标签，搭配 nodeSelector 或者 nodeAffinity 使用，实现将 pod 分配到特定污点的节点上\ntolerations: #设置容忍所有污点，防止节点被设置污点 - operator: \u0026#34;Exists\u0026#34; 示例\n[root@k8s-node1 ~]# kubectl describe node k8s-node2 | grep -i taints Taint Taints: disktype=ssd:NoSchedule [root@k8s-node1 ~]# kubectl apply -f pod-tolerations.yaml [root@k8s-node1 ~]# kubectl get pods pod-tolerations -o wide pod-tolerations 1/1 Running 0 13s 10.244.169.183 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yaml\napiVersion: v1 kind: Pod metadata: name: pod-tolerations spec: containers: - name: nginx image: nginx:1.19 nodeSelector: disktype: \u0026#34;ssd\u0026#34; tolerations: - key: \u0026#34;disktype\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;ssd\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 6 nodeName 指定节点名称，用于将 Pod 调度到指定的 Node 上，不经过调度器 scheduler，所以无视污点\n示例\n[root@k8s-node1 ~]# kubectl describe node k8s-node2| grep Taint Taints: disktype=ssd:NoSchedule [root@k8s-node1 ~]# kubectl apply -f pod-nodename.yaml pod/pod-nodename created [root@k8s-node1 ~]# kubectl get pod pod-nodename -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-nodename 1/1 Running 0 27s 10.244.169.184 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yaml\napiVersion: v1 kind: Pod metadata: name: pod-nodename spec: containers: - name: nginx image: nginx nodeName: k8s-node2 7 DaemonSet 控制器 DaemonSet 功能：\n在每一个 Node 上运行一个 Pod 新加入的 Node 也同样会自动运行一个 Pod 应用场景：网络插件、监控 Agent、日志 Agent，比如 k8s 的 calico-node 和 kube-proxy 组件\n示例\n[root@k8s-node1 ~]# kubectl apply -f daemonset-filebeat.yaml [root@k8s-node1 ~]# kubectl get pods -n kube-system -o wide |grep filebeat filebeat-2c6p4 1/1 Running 0 90s 10.244.107.246 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; filebeat-4ffcx 1/1 Running 0 90s 10.244.36.65 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; filebeat-h7959 1/1 Running 0 90s 10.244.169.186 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yaml\napiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: kube-system spec: selector: matchLabels: name: filebeat template: metadata: labels: name: filebeat spec: containers: - name: log image: elastic/filebeat:7.3.2 volumeMounts: - mountPath: /log/ name: log volumes: - name: log hostPath: path: /var/lib/docker/containers/ type: Directory tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-scheduler/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1 创建 pod 的工作流程 kubectl run nginx \u0026ndash;image=nginx kubectl 将创建 pod 的请求发送到 apiserver apiserver 将请求信息写入 etcd apiserver 通知 scheduler，收到请求信息后根据调度算法将 pod 分配到合适节点 scheduler 给 pod 标记调度结果，并返回给 apiserver apiserver 收到后写入","title":"kubernetes | 调度"},{"content":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n1 简介 基本概念\n最小部署单元 一组容器的集合 一个 Pod 中的容器共享网络命名空间 Pod 是短暂的 存在意义\nPod 为亲密性应用而存在。\n亲密性应用场景：\n两个应用之间发生文件交互 两个应用需要通过 127.0.0.1 或者 socket 通信（典型组合：nginx+php） 两个应用需要发生频繁的调用 2 pod 中的容器分类 Infrastructure Container：基础容器，维护整个 Pod 网络空间 InitContainers：初始化容器，先于业务容器开始执行 Containers：业务容器，并行启动 Infrastructure Container\npod 中总会多一个 pause 容器，这个容器就是实现将 pod 中的所有容器的网络命名空间进行统一，a 容器在 localhost 或者 127.0.0.1 的某个端口提供了服务，b 容器访问 localhost 或者 127.0.0.1 加端口也可以访问到\npause 容器主要为每个业务容器提供以下功能：\nPID 命名空间：Pod 中的不同应用程序可以看到其他应用程序的进程 ID。 网络命名空间：Pod 中的多个容器能够访问同一个 IP 和端口范围。 IPC 命名空间：Pod 中的多个容器能够使用 SystemV IPC 或 POSIX 消息队列进行通信。 UTS 命名空间：Pod 中的多个容器共享一个主机名；Volumes（共享存储卷）。 Init container：\n基本支持所有普通容器特征 优先普通容器执行 应用场景：\n控制普通容器启动，初始容器完成后才会启动业务容器 初始化配置，例如下载应用配置文件、注册信息等 示例\napiVersion: v1 kind: Pod metadata: name: pod-init spec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox command: [\u0026#34;wget\u0026#34;, \u0026#34;-O\u0026#34;, \u0026#34;/work-dir/index.html\u0026#34;, \u0026#34;http://www.baidu.com/index.html\u0026#34;] volumeMounts: - name: workdir mountPath: \u0026#34;/work-dir\u0026#34; volumes: - name: workdir emptyDir: {} 3 静态 pod 静态 Pod 特点：\nPod 由特定节点上的 kubelet 管理 不能使用控制器 Pod 名称标识当前节点名称 在 kubelet 配置文件启用静态 Pod：\nvi /var/lib/kubelet/config.yaml ... staticPodPath: /etc/kubernetes/manifests ... 将部署的 pod yaml 放到该目录会由 kubelet 自动创建\n4 重启策略 Pod 的 spec 中包含一个 restartPolicy 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。\nrestartPolicy 适用于 Pod 中的所有容器。\nAlways：当容器终止退出后，总是重启容器，默认策略。 OnFailure：当容器异常退出（退出状态码非 0）时，才重启容器。 Never：当容器终止退出，从不重启容器。 5 健康检查 5.1 三种探针 kubernetes 包含以下三种探针\nlivenessProbe(存活探针): 如果检查失败, 根据 Pod 的 restartPolicy 来决定是否重启 container. readinessProbe(就绪探针): 如果检查失败, 会把 Pod 暂时从 service endpoints 中剔除. startupProbe(启动探针): 如果检查失败, 根据 Pod 的 restartPolicy 来决定是否重启 container. 用于启动非常慢的应用. 需要注意的是, 如果容器未配置以上三种探针, 则视为三种探针皆为成功, liveness 和 readiness 探针的 initialDelaySeconds 配置代表 startup 探针成功后等待多少秒再去初始化 liveness 和 readiness 探针.\n5.1.1 检查方法 支持以下四种检查方法：\nhttpGet：对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 exec：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 tcpSocket：对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。 gRPC：使用 gRPC 执行一个远程过程调用。需要应用程序支持，参考 5.1.2 检查结果 Success(成功) Failure(失败) Unknown(未知): 不会执行任何操作. 5.1.3 探针配置 initialDelaySeconds: 容器启动后 (startup 探针成功) 要等待多少秒后存活和就绪探测器才被初始化, 默认是 0 秒, 最小值是 0. periodSeconds: 执行探测的时间间隔.默认是 10 秒, 最小值是 1. timeoutSeconds: 探测的超时后等待多少秒. 默认值是 1 秒. 最小值是 1. successThreshold: 探测器在失败后, 被视为成功的最小连续成功数. 默认值是 1. 存活探测的这个值必须是 1。最小值是 1. failureThreshold: 当 Pod 启动了并且探测到失败的重试次数. 存活探测情况下的放弃就意味着重新启动容器. 就绪探测情况下的放弃 Pod 会被打上未就绪的标签, 默认值是 3, 最小值是 1. 5.2 示例 5.2.1 liveness linveness 实际触发重启需要的时间 = 失败次数 * 间隔时间 + 等待容器优雅退出的宽限期 (默认 30s，docker 默认是 10s)\nfailureThreshold * periodSeconds + terminationGracePeriodSeconds\nlivenessProbe 示例\napiVersion: v1 kind: Pod metadata: name: pod-livenessprobe namespace: default spec: restartPolicy: OnFailure terminationGracePeriodSeconds: 10 containers: - name: liveness image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;touch /tmp/healthy; sleep 10; rm -rf /tmp/healthy; sleep 600\u0026#34;] livenessProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/tmp/healthy\u0026#34;] initialDelaySeconds: 5 periodSeconds: 5 failureThreshold: 2 运行结果可以看到在两分钟的时间里重启了 4 次，每次 30s\n[root@k8s-node1 opt]# kubectl get pods NAME READY STATUS RESTARTS AGE liveness-pod 1/1 Running 4 (2s ago) 2m2s POD 运行的前 10s 检查一直成功 在 POD 启动的第 15s 第一次检查失败 第 20s 第二次检查失败，给容器发送停止信号 等待 10s 后强制重启容器 5.2.2 liveness-with-startup 示例:\napiVersion: v1 kind: Pod metadata: name: pod-probes namespace: default spec: terminationGracePeriodSeconds: 10 containers: - name: liveness-with-startup image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 5; touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\u0026#34;] startupProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/tmp/healthy\u0026#34;] periodSeconds: 5 failureThreshold: 10 livenessProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/tmp/healthy\u0026#34;] initialDelaySeconds: 15 periodSeconds: 5 failureThreshold: 3 重启过程:\nPOD 启动成功,触发 startup 探针 第 10 秒 startup 探针成功 第 25 秒后初始化 liveness 探针 第 40 秒 liveness 探针第一次失败 第 50 秒 liveness 探针第三次失败, 触发重启, 等待容器优雅退出 第 60 秒强制重启 container 6 lifecycle 6.1 postStart 和 preStop 如下示例\napiVersion: v1 kind: Pod metadata: name: lifecycle-demo-pod namespace: default labels: test: lifecycle spec: containers: - name: lifecycle-demo image: nginx:1.22.1 imagePullPolicy: IfNotPresent lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello from the postStart handler\u0026#39; \u0026gt;\u0026gt; /var/log/nginx/message\u0026#34;] preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello from the preStop handler\u0026#39; \u0026gt;\u0026gt; /var/log/nginx/message\u0026#34;] volumeMounts: - name: message-log mountPath: /var/log/nginx/ readOnly: false # 读写挂载方式，默认为读写模式false initContainers: - name: init-myservice image: busybox:1.28 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello initContainers\u0026#39; \u0026gt;\u0026gt; /var/log/nginx/message\u0026#34;] volumeMounts: - name: message-log mountPath: /var/log/nginx/ readOnly: false # 读写挂载方式，默认为读写模式false volumes: - name: message-log hostPath: path: /data/volumes/nginx/log/ type: DirectoryOrCreate # 表示如果宿主机没有此目录则会自动创建 效果如下\n[root@k8s-node1 ~]# kubectl delete pod lifecycle-demo-pod [root@k8s-node2 log]# cat message Hello initContainers Hello from the postStart handler Hello from the preStop handler 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-pod/","summary":"0 前言 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0 1 简介 基本概念 最小部署单元 一组容器的集合 一个 Pod 中的容器共享网络命名空间 Pod 是短暂的 存在意义 Pod 为亲密性应用而存在。 亲密性应用场景： 两个应用之间发生文件交互 两个应用需要通过 127.0.0.1 或者 socket 通信（","title":"kubernetes | pod"},{"content":"1 kubectl 命令的自动补全 yum install bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) 2 镜像拉取策略 imagePullPolicy: Always|Never|IfNotPresent 3 修改 nodePort 范围 vim /etc/kubernetes/manifests/kube-apiserver.yaml # spec.containers.command 增加 - --service-node-port-range=1-65535 3 command 和 args containers.command 等同于 Dockerfile 中的 ENTRYPOINT\ncontainers.args 等同于 Dockerfile 中的 CMD\n如果 Dockerfile 中默认的 ENTRYPOINT 被覆盖，则默认的 CMD 指令同时也会被覆盖\n4 label 标签选择运算符 官方文档\n基于等值, 有三种运算符 = == !=\n前两种是等效的，示例\n[root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname=k8s-node1 NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname!=k8s-node1 NAME STATUS ROLES AGE VERSION k8s-node2 Ready \u0026lt;none\u0026gt; 21d v1.22.3 k8s-node3 Ready \u0026lt;none\u0026gt; 21d v1.22.3 基于集合, 同样三种运算符 in notin exists\nexists 只用于判断 key 是否存在\nhello in (foo, bar) # 所有包含了 hello 标签且值等于 foo 或者 bar 的资源 hello notin (foo, bar) # 所有包含了 hello 标签且值不等于 foo 或者 bar 的资源；以及没有 hello 标签的资源 hello # 所有包含了 hello 标签的资源；不校验值 !hello # 所有未包含 hello 标签的资源；不校验值 示例\n[root@k8s-node1 ~]# kubectl get nodes -l \u0026#34;kubernetes.io/hostname in (k8s-node1, k8s-node2)\u0026#34; NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d v1.22.3 k8s-node2 Ready \u0026lt;none\u0026gt; 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l \u0026#34;kubernetes.io/hostname notin (k8s-node1, k8s-node2)\u0026#34; NAME STATUS ROLES AGE VERSION k8s-node3 Ready \u0026lt;none\u0026gt; 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d v1.22.3 k8s-node2 Ready \u0026lt;none\u0026gt; 21d v1.22.3 k8s-node3 Ready \u0026lt;none\u0026gt; 21d v1.22.3 [root@k8s-node1 ~]# kubectl get nodes -l \\!kubernetes.io/hostname No resources found 5 常见报错 5.1 NodeNotReady 5.1.1 Image garbage collection failed once 参考地址\n报错：\n# kubectl describe node k8s-node01 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 11m kubelet Starting kubelet. Normal NodeHasSufficientMemory 11m kubelet Node k8s-node01 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 11m kubelet Node k8s-node01 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 11m kubelet Node k8s-node01 status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 11m kubelet Updated Node Allocatable limit across pods # journalctl -u kubelet | grep garbage Mar 06 09:50:33 k8s-node01 kubelet[45471]: E0306 09:50:33.106476 45471 kubelet.go:1343] \u0026#34;Image garbage collection failed once. Stats initialization may not have completed yet\u0026#34; err=\u0026#34;failed to get imageFs info: unable to find data in memory cache\u0026#34; 解决：\n未部署 CNI 组件\ndocker 镜像或容器未能正确删除导致的\ndocker system prune systemctl stop kubelet systemctl stop docker systemctl start docker systemctl start kubelet 5.2 node 无法 ping 通 pod 所有 calico 的 pod 运行都是 running 状态, 使用 calicoctl node status 看到的网卡绑定也是没问题的.\ncalico 的 pod 有如下报错\n[root@k8s-node1 ~]# kubectl logs calico-node-l66pn -n kube-system 2023-04-08 04:28:47.660 [INFO][65] felix/int_dataplane.go 1600: Received interface update msg=\u0026amp;intdataplane.ifaceUpdate{Name:\u0026#34;tunl0\u0026#34;, State:\u0026#34;down\u0026#34;, Index:4} bird: Netlink: Network is down bird: Netlink: Network is down bird: Netlink: Network is down bird: Netlink: Network is down 我这里是通过关闭 NetworkManager 解决的.关闭后 pod 日志立即就恢复正常了\n[root@k8s-node1 ~]# systemctl stop NetworkManager [root@k8s-node1 ~]# systemctl disable NetworkManager 5.3 虚拟机挂起导致 calico 网络不可用 出现在我的虚拟机测试集群上，挂起虚拟机过段时间后重新启动虚拟机，发现集群状态是正常的 (node 是 ready 状态)，然而 calico-kube-controllers metric-server nfs-provisiner 等功能组件陷入了 CrashLoopBackOff 状态，报错基本上都是无法连接到 api-server\n但是 calicoctl 看到 calico 集群是没什么问题的，之前遇到几次都是暴躁重启 docker 解决的，后面发现重启 calico 相关容器就可以了，具体原因还没找到，估计与 vmware 虚拟机挂起操作有关。\nkubectl delete pods -n kube-system -l \u0026#34;k8s-app in (calico-node, calico-kube-controllers)\u0026#34; 6 kubectl 命令 一些常用命令\n# 查看某个资源的详细信息 kubectl describe \u0026lt;type\u0026gt; \u0026lt;name\u0026gt; -n \u0026lt;namespace\u0026gt; # 查看pod的日志 kubectl logs \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; # 查看当前支持的api版本 kubectl api-versions 6.1 get options:\n-w/--watch: # 实时更新，类似tail的-f选项 -o wide: # 查看更为详细的信息，比如ip和分配的节点 -o json: # 以json格式输出 -o jsonpath=\u0026#39;{}\u0026#39; # 输出指定的json内容 -l key=vaule # 根据lable筛选 --show-lables # 显示资源的所有label 示例:\n# 查看所有支持的资源 kubectl api-resources # 查看service映射的pod的端口和ip kubectl get cp/endpoints # 查看pod kubectl get pod \u0026lt;podname\u0026gt; -n \u0026lt;namespace\u0026gt; -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39; # 查看pod的id # 查看指定pod的事件 kubectl get events --field-selector involvedObject.name=demo-probes 6.2 create kubectl create \u0026lt;resource\u0026gt; [Options] --dry-run=client: 仅尝试运行，不实际运行 -o, --output=\u0026#39;\u0026#39;: 输出为指定的格式 快速创建一系列资源\n[root@k8s-node1 ~]# kubectl create namespace test namespace/test created [root@k8s-node1 ~]# kubectl create deployment my-dep --image=nginx:1.22. --replicas=3 -n test deployment.apps/my-dep created [root@k8s-node1 ~]# kubectl expose deployment my-dep --port=80 --target-port=8080 --type=NodePort -n test service/my-dep exposed [root@k8s-node1 ~]# kubectl get pods,deployment,svc -n test NAME READY STATUS RESTARTS AGE pod/my-dep-5f8dfc8c78-7w5nz 1/1 Running 0 41s pod/my-dep-5f8dfc8c78-gt65r 1/1 Running 0 41s pod/my-dep-5f8dfc8c78-n4vjd 1/1 Running 0 41s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/my-dep 3/3 3 3 41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-dep NodePort 10.110.205.138 \u0026lt;none\u0026gt; 80:31890/TCP 17s 6.3 expose kubectl expose deployment my-dep --port=80 --target-port=8080 --type=NodePort -n test # --port 表示service暴露的端口 # --target-port 表示后端镜像实际提供服务的端口 6.4 label kubectl label nodes [node] key=value # 打lable, value可以是空 kubectl label nodes [node] key- # 删除label kubectl get nodes -l key=value # 根据label筛选 kubectl get nodes --show-labesl # 显示资源的所有标签 6.5 run kubectl run -it test --image busybox --rm -- ping 10.244.107.207 7 calicoctl 下载地址\n# 查看集群信息 DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes # 使用配置文件的方式 [root@k8s-node1 ~]# mkdir /etc/calico [root@k8s-node1 ~]# cat \u0026gt; /etc/calico/calicoctl.cfg \u0026lt;\u0026lt;EOF \u0026gt; apiVersion: projectcalico.org/v3 \u0026gt; kind: CalicoAPIConfig \u0026gt; metadata: \u0026gt; spec: \u0026gt; datastoreType: \u0026#34;kubernetes\u0026#34; \u0026gt; kubeconfig: \u0026#34;/root/.kube/config\u0026#34; \u0026gt; EOF # 查看集群信息 [root@k8s-node1 ~]# calicoctl --allow-version-mismatch get nodes NAME k8s-node1 k8s-node2 k8s-node3 [root@k8s-node1 ~]# calicoctl --allow-version-mismatch node status IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 1.1.1.2 | node-to-node mesh | up | 03:53:45 | Established | | 1.1.1.3 | node-to-node mesh | up | 03:53:51 | Established | +--------------+-------------------+-------+----------+-------------+ 8 namespace k8s 与 docker 的 namespace 不同\ndocker 中的 namespace 用于容器间的资源隔离\nk8s 中的 namespace 用于\nk8s 的抽象资源间的资源隔离，比如 pods、控制器、service 等 资源隔离后，对这一组资源进行权限控制 9 yaml 编写 通过创建资源获取 yaml\nkubectl create deployment web --image=nginx:1.19 --dry-run=client -o yaml \u0026gt; deploy.yaml 通过已有资源获取 yaml\nkubectl get deployment nginx-deployment -o yaml \u0026gt; deploy2.yaml 查看 api 中的资源及解释\nkubectl explain pods.spec.container kubectl explain deployment yaml 报错排查\nerror: error parsing pod-configmap.yaml: error converting YAML to JSON: yaml: line 19: did not find expected \u0026#39;-\u0026#39; indicator 解决\n由于 yaml 文件列表对齐不统一导致的\nyaml 文件格式要对齐，同一级别的对象要放在同一列，几个空格不重要，不要用 tab 制表符\n# 格式1 ports: - port: 80 # 格式2 ports: - port: 80 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-notes/","summary":"1 kubectl 命令的自动补全 yum install bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) 2 镜像拉取策略 imagePullPolicy: Always|Never|IfNotPresent 3 修改 nodePort 范围 vim /etc/kubernetes/manifests/kube-apiserver.yaml # spec.containers.command 增加 - --service-node-port-range=1-65535 3 command 和 args containers.command 等同于 Dockerfile 中的 ENTRYPOINT containers.args 等同于 Dockerfile 中的 CMD 如果 Dockerfile 中默认的 ENTRYPOINT 被覆盖，则默认的 CMD 指令同时也会被覆盖 4 label 标签选择运算符 官方文档 基于等值, 有三种运算符 = == != 前两种是等效的，示例 [root@k8s-node1 ~]# kubectl get nodes -l kubernetes.io/hostname=k8s-node1 NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 21d","title":"kubernetes | 杂记"},{"content":"Bash 有一个内置的 set 命令，可以用来查看、设置、取消 shell 选项\nset 设置的选项无法被继承，仅对当前的 bash 环境有效，bash 命令也可以直接使用 set 的单字符选项来开启一个自定义参数的子 bash 环境，比如执行的脚本\n查看： echo $- 和 set -o 和 echo ${SHELLOPTS} 设置： set -abefhkmnptuvxBCHP 和 set -o options-name 取消： set +abefhkmnptuvxBCHP 和 set +o options-name set - 和 set + 设置单字符选项，使用 echo $- 查看当前 shell开启的单字符选项\nset -o 和 set +o 设置多字符选项，使用 set -o 查看当前 shell 所有的多字符选项的状态 (开启或关闭)\n使用 echo ${SHELLOPTS} 查看当前 shell 开启的长格式选项\n所有的短格式选项都可以找到对应的长格式选项，长格式选项多了 emacs、history、ignoreeof、nolog、pipefail、posix、vi。详见 set 命令的 man 手册\n例如 set -B 和 set -o braceexpand 是等效的，注意这里的设置和取消有点反常识：设置用 -，关闭反而是用 +\n[root@lvbibir ~]# echo $- himBH # set + 方式去除B选项，相应的 set -o 中的 braceexpand 选项也关闭了 [root@lvbibir ~]# set +B [root@lvbibir ~]# echo $- himH [root@lvbibir ~]# set -o | grep braceexpand braceexpand off # set -o 开启 braceexpand 选项，相应的 echo $- 中的 B 选项也开启了 [root@lvbibir ~]# set -o braceexpand [root@lvbibir ~]# echo $- himBH [root@lvbibir ~]# set -o | grep braceexpand braceexpand on 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-command-set/","summary":"Bash 有一个内置的 set 命令，可以用来查看、设置、取消 shell 选项 set 设置的选项无法被继承，仅对当前的 bash 环境有效，bash 命令也可以直接使用 set 的单字符选项来开启一个自定义参数的子 bash 环境，比如执行的脚本 查看： echo $- 和 set -o 和 echo ${SHELLOPTS} 设置： set -abefhkmnptuvxBCHP 和 set -o options-name 取消： set +abefhkmnptuvxBCHP 和 set +o options-name set - 和 set + 设置单字符选项，使用 echo $- 查看","title":"linux | set 命令详解"},{"content":"0 前言 安装过程中会替换相当一部分系统内置的软件包, 不建议用于生产环境 cephadm 依赖 python3.6, 而此版本的 openeuler 内置版本为 3.7, 且不支持 platform-python, 参考 [openeuler 的 gitee 社区 issue](https: \u0026lt;//gitee.com/src-openeuler/python3/issues/I4J8RK?from=project-issue\u0026gt;)\n基础环境:\nceph: v16.2 (pacific) 操作系统: openEuler-20.03-LTS-SP3 内核版本: 4.19.90-2112.8.0.0131.oe1.x86_64 集群角色:\nip 主机名 角色 1.1.1.101 ceph-node1 cephadm, mgr, mon, osd 1.1.1.102 ceph-node2 osd, mgr, mon 1.1.1.103 ceph-node3 osd, mgr, mon 1 基础环境配置 (所有节点) 1.1 防火墙 systemctl stop firewalld systemctl disable firewalld 1.2 修改主机名 hostnamectl set-hostname ceph-node1 hostnamectl set-hostname ceph-node2 hostnamectl set-hostname ceph-node3 vi /etc/hosts # 添加 1.1.1.101 ceph-node1 1.1.1.102 ceph-node2 1.1.1.103 ceph-node3 1.3 配置 yum \u0026amp; epel rpm -e openEuler-release-20.03LTS_SP3-52.oe1.x86_64 wget -O /etc/yum.repos.d/CentOS-Base.repo https: //mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum install epel-release rm -f /etc/yum.repos.d/CentOS-Linux-* yum-config-manager --add-repo https: //mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 1.4 安装 python3.6 yum install python3-pip-wheel python3-setuptools-wheel wget http: //mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/x86_64/os/Packages/python3-libs-3.6.8-41.el8.x86_64.rpm wget http: //mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/x86_64/os/Packages/libffi-3.1-22.el8.x86_64.rpm rpm -ivh libffi-3.1-22.el8.x86_64.rpm --force cp /usr/lib64/libpython3.so /usr/lib64/libpython3.so-3.7.4 rpm -ivh python3-libs-3.6.8-41.el8.x86_64.rpm --force --nodeps mv /lib64/libpython3.so /lib64/python3.so-3.6.8 ln -s /usr/lib64/libpython3.so /lib64/libpython3.so yum install platform-python yum install python3-pip vi /usr/bin/yum # 将 #!/usr/bin/python3 改成 #!/usr/bin/python3.7 yum install python3-prettytable-0.7.2-14.el8 yum install python3-gobject-base-3.28.3-2.el8 rpm -e --nodeps firewalld-doc-0.6.6-4.oe1.noarch yum install firewalld-0.9.3-7.el8 1.5 安装 docker yum install docker-ce systemctl start docker systemctl status docker systemctl enable docker 1.6 安装 cephadm \u0026amp; ceph-common curl --silent --remote-name --location https: //github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release pacific yum install cephadm rpm -e --nodeps libicu-62.1-6.oe1.x86_64 yum install ceph-common-16.2.9-0.el8 2 ceph 集群配置 2.1 集群初始化 cephadm bootstrap --mon-ip 1.1.1.101 出现如下提示说明安装成功\n...... Generating a dashboard self-signed certificate... Creating initial admin user... Fetching dashboard port number... Ceph Dashboard is now available at: URL: https: //ceph-node1:8443/ User: admin Password: dkk08l0czz Enabling client.admin keyring and conf on hosts with \u0026#34;admin\u0026#34; label Enabling autotune for osd_memory_target You can access the Ceph CLI as following in case of multi-cluster or non-default config: sudo /usr/sbin/cephadm shell --fsid aac4d9ba-3be0-11ed-b415-000c29211f5f -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring Or, if you are only running a single cluster on this host: sudo /usr/sbin/cephadm shell Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https: //docs.ceph.com/en/pacific/mgr/telemetry/ Bootstrap complete. 访问: https://1.1.1.101:8443/\n第一次访问 dashboard 需要修改初始账号密码\n2.2 添加主机 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node3 ceph orch host add ceph-node2 1.1.1.102 --labels _admin ceph orch host add ceph-node3 1.1.1.103 --labels _admin 2.3 添加磁盘 # 单盘添加 ceph orch daemon add osd ceph-node1:/dev/vdb # 查看所有可用设备 ceph orch device ls # 自动添加所有可用设备 ceph orch apply osd --all-available-devices 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v16-cpehadm-openeuler/","summary":"0 前言 安装过程中会替换相当一部分系统内置的软件包, 不建议用于生产环境 cephadm 依赖 python3.6, 而此版本的 openeuler 内置版本为 3.7, 且不支持 platform-python, 参考 [openeuler 的 gitee 社区 issue](https: \u0026lt;//gitee.com/src-openeuler/python3/issues/I4J8RK?from=project-issue\u0026gt;) 基础环境: ceph: v16.2 (pacific) 操作系统: openEuler-20.03-LTS-SP3 内核版本: 4.19.90-2112.8.0.0131.oe1.x86_64 集群角色: ip 主机名 角色 1.1.1.101 ceph-node1 cephadm, mgr, mon, osd 1.1.1.102 ceph-node2 osd, mgr, mon 1.1.1.103 ceph-node3 osd, mgr, mon 1 基础环境配置 (所有节点) 1.1 防火墙 systemctl stop firewalld systemctl disable firewalld 1.2 修改主机名","title":"ceph | openeuler 部署 ceph-v16"},{"content":"0 前言 适用于 Centos8/openeuler + docker\n安装 cephadm、ceph-common 的过程就不赘述了，主要探讨如何实现 cephadm 离线安装 ceph v16.2.8\n1 离线包的获取 离线包主要指 rpm 包和 ceph 的 docker 镜像\n找一台有外网的测试机（尽量跟生产系统的环境一致）通过 yum 安装 cephadm、ceph-common、docker 等需要的 rpm 包，注意使用 downloadonly 参数先下载好 rpm 包和对应的依赖，然后再通过 yum localinstall 安装 使用 cephadm bootstrap 初始化单节点 ceph 集群，过程中会下载好需要的 docker 镜像 初始化完成后就可以使用 cephadm rm-cluster --force --zap-osds --fsid \u0026lt;fsid\u0026gt; 把现在的集群删除了，暂时用不到\n2 修改 docker 镜像 我们需要修改的镜像只有 quay.io/ceph/ceph:v16 这个镜像，采用 docker commit 的方式修改\n先运行一个容器用于修改文件\n[root@node-128 ~]# docker run -itd --name test quay.io/ceph/ceph:v16 520af9cf98688d1eb1f572c28c4c60db4f231e4dbf6b3594c54c3892494e5d6c [root@node-128 ~]# docker exec -it test /bin/bash # 容器操作 [root@520af9cf9868 /]# find /usr/ -name serve.py /usr/share/ceph/mgr/cephadm/serve.py /usr/lib/python3.6/site-packages/pecan/commands/serve.py [root@520af9cf9868 /]# vi /usr/share/ceph/mgr/cephadm/serve.py 如下，注释三行，大约 937 行\n如下，三处修改大约位于 1342 行\n注释 if 语句 修改 cepadm 命令的 pull 为 inspect-image 获取 container 数据改为直接写死 至此，已修改完毕，将容器提交为新的镜像\ndocker commit -m \u0026#34;修改 /usr/share/ceph/mgr/cephadm/serve.py 文件\u0026#34; -a \u0026#34;lvbibir\u0026#34; test ceph:v16 [root@node-128 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ceph v16 c654e94b4c3f 3 days ago 1.23GB quay.io/ceph/ceph v16 e8311b759ac3 3 months ago 1.23GB quay.io/ceph/ceph-grafana 8.3.5 dad864ee21e9 4 months ago 558MB quay.io/prometheus/prometheus v2.33.4 514e6a882f6e 5 months ago 204MB quay.io/prometheus/node-exporter v1.3.1 1dbe0e931976 8 months ago 20.9MB quay.io/prometheus/alertmanager v0.23.0 ba2b418f427c 11 months ago 57.5MB 然后将原先的镜像删除，将修改后的镜像改为原先的镜像 tag\ndocker rmi quay.io/ceph/ceph:v16 docker tag ceph:v16 quay.io/ceph/ceph:v16 docker rmi ceph:v16 [root@ceph-x86-node3 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/ceph/ceph v16 c654e94b4c3f 4 days ago 1.23GB quay.io/ceph/ceph-grafana 8.3.5 dad864ee21e9 4 months ago 558MB quay.io/prometheus/prometheus v2.33.4 514e6a882f6e 5 months ago 204MB quay.io/prometheus/node-exporter v1.3.1 1dbe0e931976 8 months ago 20.9MB quay.io/prometheus/alertmanager v0.23.0 ba2b418f427c 11 months ago 57.5MB 在 本博客另一篇文章 有脚本可以方便的批量导入导出镜像\n3 测试 将之前下载的 rpm 包和导出的 docker 镜像进行归档压缩，上传至无法访问外网的环境，之后就与在线部署 ceph 集群的步骤一样了\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v16-cpehadm-openuler-offline/","summary":"0 前言 适用于 Centos8/openeuler + docker 安装 cephadm、ceph-common 的过程就不赘述了，主要探讨如何实现 cephadm 离线安装 ceph v16.2.8 1 离线包的获取 离线包主要指 rpm 包和 ceph 的 docker 镜像 找一台有外网的测试机（尽量跟生产系统的环境一致）通过 yum 安装 cephadm、ceph-common、docker 等需要的 rpm 包，注意使","title":"ceph | ceph-v16 离线安装解决方案"},{"content":"0 前言 本文参考以下链接:\n搭建 PXE 服务器安装 UEFI 启动的 centos7 配置PXE环境自动安装 Linux rfc4578 在 pxe 的一般场景下，通常在只需要在 dhcp 服务中配置一个通用的 filename 来指定客户端在 tftp 服务端获取的引导程序，但是在略微复杂的场景中，比如可能有些服务器默认是 legacy 模式，而有些服务器是 UEFI 模式，这两种模式使用的引导程序是不同的，但我们又不想频繁的去修改 dhcp 配置文件。本文主要探讨的就是这个问题，如何配置 dhcp 来应对复杂的服务器环境\n难点主要有两个，一个是区分某些 dhcp 客户端是否需要 pxe 引导程序，另外一个是如何区分不同的模式和架构来去分配对应的 pxe 引导程序\n1 RFC Request For Comments（RFC），是一系列以编号排定的文件。文件收集了有关互联网相关信息，以及 UNIX 和 互联网 社区的 软件 文件。RFC 文件是由 Internet Society（ISOC）赞助发行。基本的互联网通信协议都有在 RFC 文件内详细说明。RFC 文件还额外加入许多在标准内的论题，例如对于互联网新开发的协议及发展中所有的记录。因此几乎所有的互联网标准都有收录在 RFC 文件之中。\n2 dhcp option 60 DHCP Option 60 Vendor class identifier 为厂商类标识符。这个选项作用于客户端可选地识别客户端厂商类型和配置。这个信息是 N 个 8 位编码，由 DHCP 服务端解析。厂商可能会为客户端选择定义特殊的厂商类标识符信息，以便表达特殊的配置或者其他关于客户端的信息。比如：这个标识符可能编码了客户端的硬件配置。客户端发送过来的服务器不能解析的类规范信息必须被忽略（尽管可能会有报告）。\n3 dhcp option 93 dhcp-options 的 man 手册中有提到对于架构类型在 RFC 4578 中有一套标准，可通过 if 语句判断 dhcp 客户端的 Arch 代码来提供不同的 PXE 引导程序给客户端\n# man dhcp-options option pxe-system-type uint16 [, uint16 ... ]; A list of one ore more 16-bit integers which allows a client to specify its pre-boot architecture type(s). This option is included based on RFC 4578. 下述为 RFC 4578 标准中对 arch 代码制定的标准，name 字段包含启动模式和 cpu 架构信息（自己的猜测，这里没找到对于 name 更详细的解释）\nType Architecture Name ---- ----------------- 0 Intel x86PC 1 NEC/PC98 2 EFI Itanium 3 DEC Alpha 4 Arc x86 5 Intel Lean Client 6 EFI IA32 7 EFI BC 8 EFI Xscale 9 EFI x86-64 4 抓包获取 arch 代码 通过前文描述，我们得知 arch 代码主要是由硬件厂商定义好的，配置好 pxe 服务，arch 代码的获取至关重要，去咨询硬件厂商效率太慢，这里通过更为方便的抓包获取\n抓包主要获取提供 dhcp 服务的网卡的数据包，需服务端开启 dhcp 服务，客户端通过网卡启动\nwindows 端通过 wireshark 来完成\nlinux 服务端使用 tcpdump -i \u0026lt;interface\u0026gt; -w \u0026lt;file\u0026gt; 生成到文件然后用 wireshark 分析\n以下提供几个 dhcp option 60 和 dhcp option 93 报文示例：\nAMD Ryzen 7 4800U with Radeon Graphics (x86) vmware workstation v16 平台 UEFI 模式下 这里获取到的 arch 代码为 7\nAMD Ryzen 7 4800U with Radeon Graphics (x86) vmware workstation v16 平台 legacy 模式下 这里获取到的 arch 代码为 0\nkunpeng 920 (aarch64) kvm 平台 UEFI 模式下 这里获取到的 arch 代码为 11\n以上抓包都是在网络引导的环境下进行的，在使用已安装操作系统中的网卡去发送 dhcp 请求时，整个数据包传输过程都没有 option 60 和 option 93 这两个选项的参与，我猜测这两个选项只有在网络引导的环境下才会去参与\n5 dhcp 配置文件示例 在上述论证基础之上，我们就可以通过配置 dhcp 服务来使 pxe 足以应对复杂的网络环境和硬件环境\n解决前言中提到的两个难点分别通过 option 60 和 option 93 分别解决\n# 这里应该是将 option 93 的值格式化成 16 进制，用于下面的 if 判断（猜测） option arch code 93 = unsigned integer 16; class \u0026#34;pxeclients\u0026#34; { # 这里判断 option 60 选项的值的前9个字符是否是 PXEClient match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; next-server 10.17.25.17; # 这里通过 if 判断 arch 代码来决定如何去分配对应的 pxe 引导程序 if option arch = 00:07 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else if option arch = 00:09 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else { filename \u0026#34;/pxelinux.0\u0026#34;; } } 较为详细的配置文件示例，后面有简化版\n# 启用 PXE 支持 allow booting; allow bootp; # PXE 定义命名空间 option space PXE; option PXE.mtftp-ip code 1 = ip-address; option PXE.mtftp-cport code 2 = unsigned integer 16; option PXE.mtftp-sport code 3 = unsigned integer 16; option PXE.mtftp-tmout code 4 = unsigned integer 8; option PXE.mtftp-delay code 5 = unsigned integer 8; option arch code 93 = unsigned integer 16; # RFC4578 authoritative; one-lease-per-client true; # 不使用DNS动态更新 ddns-update-style none; # 忽略客户端DNS更新 ignore client-updates; # 不使用 PXE 的网络 shared-network main { subnet 10.17.25.0 netmask 255.255.255.0 { option routers 10.17.25.254; option subnet-mask 255.255.255.0; option domain-name \u0026#34;zhijie-liu.com\u0026#34;; # 在此网络关闭PXE支持 deny bootp; pool { range 10.17.25.200 10.17.25.210; host nagios-test { hardware ethernet 00:0d:56:66:82:c3; fixed-address 10.17.25.200; } } } } # 使用 PXE 的网络 shared-network pxe { subnet 10.17.15.0 netmask 255.255.255.0 { option routers 10.17.15.254; option subnet-mask 255.255.255.0; option domain-name \u0026#34;xiyang-liu.com\u0026#34;; option domain-name-servers 10.17.26.88, 8.8.8.8; default-lease-time 86400; max-lease-time 172800; pool { range 10.17.15.1 10.17.15.20; class \u0026#34;pxeclient\u0026#34; { match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; next-server 10.17.25.17; if option arch = 00:07 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else if option arch = 00:09 { filename \u0026#34;/BOOTX64.efi\u0026#34;; } else { filename \u0026#34;/pxelinux.0\u0026#34;; } } # 根据 MAC 地址单独分配地址和指定的 PXE 引导程序 host gpxelinux { option host-name \u0026#34;gpxelinux.zhijie-liu.com\u0026#34;; hardware ethernet 00:50:56:24:0B:30; fixed-address 10.17.15.8; filename \u0026#34;/gpxelinux.0\u0026#34; } } } } 简化版（仅 kvm 平台测试通过）\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; option arch code 93 = unsigned integer 16; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; class \u0026#34;pxeclients\u0026#34; { match if substring (option vendor-class-identifier, 0, 9) = \u0026#34;PXEClient\u0026#34;; next-server 1.1.1.21; if option arch = 00:11 { filename \u0026#34;/grubaa64.efi\u0026#34;; } } } 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/pxe-dhcp-legacy-uefi-archs/","summary":"0 前言 本文参考以下链接: 搭建 PXE 服务器安装 UEFI 启动的 centos7 配置PXE环境自动安装 Linux rfc4578 在 pxe 的一般场景下，通常在只需要在 dhcp 服务中配置一个通用的 filename 来指定客户端在 tftp 服务端获取的引导程序，但是在略微复杂的场景中，比如可能有些服务器默认是 legacy 模式，而有些服务器是 UEFI 模式，这两种模式使用的引导程序是不同的","title":"pxe 如何应对复杂的服务器硬件环境"},{"content":"示例代码\nimport os # 输入文件夹地址 path = \u0026#34;C://Users//lvbibir//Desktop//lvbibir.github.io//content//posts//read//\u0026#34; files = os.listdir(path) # 输出所有文件名，只是为了确认一下 for file in files: print(file) # 获取旧名和新名 i = 0 for file in files: # 旧名称的信息 old = path + os.sep + files[i] # 新名称的信息 new = path + os.sep + file.replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # 新旧替换 print(new) os.rename(old,new) i+=1 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/python-rename-file/","summary":"示例代码 import os # 输入文件夹地址 path = \u0026#34;C://Users//lvbibir//Desktop//lvbibir.github.io//content//posts//read//\u0026#34; files = os.listdir(path) # 输出所有文件名，只是为了确认一下 for file in files: print(file) # 获取旧名和新名 i = 0 for file in files: # 旧名称的信息 old = path + os.sep + files[i] # 新名称的信息 new = path + os.sep + file.replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # 新旧替换 print(new) os.rename(old,new) i+=1 以上","title":"python | 批量修改目录下文件名"},{"content":"0 前言 书名《微习惯》，作者斯提芬·盖斯 [美]，江西人民出版社，译者桂君\n微习惯是一种非常微小的积极行为，你需要每天强迫自己完成它。微习惯太小，小到不可能失败。正是因为这个特性，它不会给你造成任何负担，而且具有超强的“欺骗性”，它因此成了极具优势的习惯养成策略。\n微习惯策略的科学原理表明了人们无法长期坚持大多数主流成长策略的原因，也解释了人们长期坚持微习惯策略的可能性。人们无法让改变的效果持久时，往往认为原因在于自己，但其实有问题的并不是他们本身，而是他们采用的策略。当你开始用微习惯策略教你的方法按照大脑的规律做事情时，持久改变其实很容易。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n1 微习惯是什么 千里之行，始于足下。 ——老子\n欢迎来到微习惯的世界，首先陈述两个事实：\n哪怕是一点点行动，也比毫不作为强无数倍（在数学意义上如此，实际生活中也是如此）。 相比某一天做很多事，每天做一点事的影响力会更大。 几乎每个人都经历过瓶颈期，竭尽全力想提升自己却最终失败，然后无数次尝试并遭遇失败后很久不敢重新开始。\n很多时候，我们没能实施行动，也没能实现计划，但有没有可能这并不是我们的错，而是我们采用并认可的策略出了问题呢？\n只为培养好习惯\n微习惯不会直接帮你戒烟、戒酒或者控制赌瘾，微习惯策略只会帮你培养你认可的好习惯，给你的生活增添积极行为，持续丰富你的生活。消除坏习惯和建立好习惯有着共同的目标——用更好的行为方式取代原有的行为方式。\n如果你有好习惯，你改变自己的主要动力是靠近这些积极的东西；如果你有坏习惯，你改变自己的主要动力是远离这些消极的东西。\n微习惯简介\n如果你想培养一个新习惯，微习惯基本就是它经过大幅缩减的版本——把“每天做 100 个俯卧撑”缩减成每天 1 个，把“每天写 3000 字”缩减成每天写 50 字，把“始终保持积极思考”缩减成每天想两件好事。\n微习惯体系的基础在于“微步骤”，那些“小到不可思议的一小步”。\n一家银行可能因为规模太大而不至于失败，而微习惯是因为太小而不至于无法完成，因为，你不会有机会体验未完成目标导致的常见消极情绪，比如愧疚和挫败感。\n习惯还与压力有关\n现在试想一下：如果坏习惯让你压力过大，你会怎么做。压力是负反馈循环的绝佳导火索，它会触发一个坏习惯，坏习惯又会触发内疚感、内心的焦虑和更多压力，这些消极因素会再次触发这个坏习惯。\n但是，如果习惯本身就能缓解压力会怎样？拿锻炼来说，你的压力会把你拽到健身房，锻炼会帮你缓解焦虑。\n养成新习惯需要多长时间\n不是 21 天，也不是 30 天。“21 天”谬论可能源自整形外科医生麦克斯韦尔·马尔茨（Maxwell Maltz）。据说马尔茨医生发现接受截肢手术的患者需要大约 21 天来适应肢体残缺的事实，因此，他认为 21 天是人们适应任何生活变化所需要的时间长度。\n不同行为所需要的时间差别很大，从 18 天到 254 天不等，甚至在某些案例中，这些时间可能惊人的长。\n2 大脑的工作原理 大脑是我的一切，华生。身体只是附件而已。 ——阿瑟·柯南·道尔，《福尔摩斯探案集》\n大脑是变化缓慢且状态稳定的\n人类大脑有一套对外部世界做出反应的固定体系。有时我们觉得不易改变的大脑令人感到沮丧，但总体来说，好处还是相当多的\n一旦成功养成健康的新习惯，一切都会变得轻松起来。我们无需跟大脑持久战斗就可以自然的执行这些习惯。\n大脑中的两个核心角色\n愚蠢的重复者——基底神经节 聪明的管理者——前额皮层 基底神经节是愚蠢的，你抽烟的时候，它不会考虑到肺癌的可能性；你锻炼的时候，它也不会幻想健康身体的好处。但是它可以高效率地重复模式，节省精力，它的工作几乎无需我们消耗额外的意志力或者动力即可完成。\n前额皮层则相当聪明，是个可以理解长远利益和结果的管理者，它拥有抑制基底神经节的能力，同时它还负责处理短期思维和决策。\n前额皮层的功能这么强大，所以会消耗相当的精力，从而使你感到疲劳。这个时候，掌管重复部分的基底神经节就会接管大脑。\n让大脑的其他部分喜欢上前额皮层想要的东西，是建立新习惯的唯一方式\n3 动力 v.s. 意志力 情绪要么顺服你，要么支配你，这要看谁说了算。 ——吉米·罗恩\n当动力处于峰值时，意志力消耗量为 0 或可以忽略不计，这是因为你无需强迫自己做你本来就愿意做的事情，可以当动力降为 0 时，强烈的内心抵触意味着我们必须消耗非常大的意志力\n做事缺乏动力，意志力的消耗猛涨，这种方式很难维持一个行为并将其培养成习惯\n“激发动力”策略的诸多问题\n激发动力有效果吗？答案并不是那么确定。偶尔我们可以激发强烈的动力做某件事，比如锻炼身体，比如阅读，比如学习某项技能，但是扪心自问我们无法确保下次是否还有如此强烈的动力。\n动力是一种能带来诸多好处的重要感觉，但是当它出现时，请把它看作一个额外的奖励，一件美好的事物。我们可以享受它带来的好处，但不要尝试去依赖动力。\n动力并不可靠 动力之所以不可靠，是因为它是以人的感受为基础的，而人类的感受容易改变且无法预测已经是几百年来公认的事实了。几乎所有东西都能改变你的感受，所以我们不要把希望放在如此不稳定的东西上。任何事物能成为基础的第一原则就是它必须牢固可靠。\n我们无法做到每次都愿意激发动力 问题在于，动力是很难或者说几乎不可能按需培养的。我们只有在精力充沛、思维模式健康、没有受到其他强烈诱惑的时候，我们才能依靠动力成功。\n你根本不想让自己想让自己想锻炼。很多时候，你积聚动力只是为了让自己有动力激发动力而已。生活中总有那么几次，你不愿意为了激发动力而激发动力。\n“热情递减法则” “热情递减法则”不是一条真正的法则，是作者创造的术语。它比对应的“边际效用递减法则”更好理解。这条经济法则认为，吃第五块披萨的时候愉悦感略低于吃第四块的时候，吃第四块的时候又略低于吃第三块的时候。可能下面这个例子更形象，一瓶三块钱的冰可乐第一口至少值两块五。甚至我觉得感情生活中的新鲜感同样适用这个法则，新鲜感就是一种动力，然而大部分情侣新鲜感也只能维持几个月而已。\n习惯是一个我们选择做一件事而做一件事的行为，行动开始前和结束后不会出现剧烈波动。有热情是好事，但我们应该把这种动力看作一种额外奖励，而不是实施行动的信号。即表现更稳定和自动的基底神经节掌握控制权。\n为什么意志力能打败动力？\n有必要重申一遍，动力是好东西，只是不可靠而已。借助意志力，动力会变得更加可靠；而且如果先采取行动，继续行动的动力会被迅速激发。\n意志力很可靠 意志力可以被强化 意志力可以通过计划执行 意志力的工作原理\n做决定也会消耗意志力 在同一天里做过艰难决定的人在后来面对诱惑时屈服的可能性更高，这体现了自控力的下降。重大决定和意志力似乎需要消耗同样的能量。比如你上午强迫自己学习了几个小时，在吃晚饭时在炸鸡和更为健康的饮食之间会非常偏向前者，前提是学习和控制饮食你都没有养成习惯的前提下。\n意志力损耗的五个最重要的因素 元分析是从指定主题的相关文献中提取出重要结论的过程。\n2010 年的一项针对自我损耗的元分析中发现了引起自我损耗的五个最重要的因素：努力程度、感知难度、消极情绪、主观疲劳和血糖水平。\n总结一下上述的本章内容\n我们是用动力或者意志力开启新的行为的（非习惯性）。 动力不可靠，所以不能充当建立习惯的策略。 意志力可靠，但前提是你没有把它耗尽。 引发意志力损耗的五大重要因素：努力程度、感知难度、消极情绪、主观疲劳和血糖水平。 如果我们能克服这五项障碍，我们就应该能走向成功。 4 微习惯策略 塑造你生活的不是你偶尔做的一两件事，而是你一贯坚持做的事。 ——安东尼·罗宾\n以微习惯方式运用意志力\n微习惯是怎样有效消除意志力的五大威胁的\n努力程度 微习惯需要非常少的实际努力，自我损耗极少。\n感知难度 微习惯的本质决定它几乎不会让你在还没做的时候就感受到困难。一旦你开始做且能随心所欲地继续下去，“已经开始”带来的心理影响会让感知难度明显降低。正如从物理学角度来看，物体的惯性在运动开始时最大，一旦物体处于运动状态，因为存在动量，一切都会变得简单。\n很多时候我们无法坚持做某件事的时候都是因为在一开始就感受到了很大的难度，所以有了这样的想法：如果最终做不到，我们宁可不开始。\n消极情绪 即使微习惯占用了一件本应使你快乐的事情的时间，你要做的努力也非常少，所以几乎感受到消极情绪。何况通常情况下，我们都会用有益的行为取代浪费时间的行为，这个过程本身就会带来积极情绪。\n主观疲劳 这个因素很有意思，不是“疲劳”，而是“主观疲劳”，就是说我们在评估自己的疲劳程度时并不是完全客观的。通常更难的任务在开始前就会感受到很大的压迫感。\n采用微习惯策略的结果：主管疲劳无法彻底消除，但是微习惯可以有效缓解主观疲劳。\n血糖水平 葡萄糖是人体首要的能量来源。如果血液中葡萄糖的含量变低，你会感觉疲惫。\n采用微习惯策略，你无需动用前额皮层去做一些重大决定，或者消耗很多意志力，这有助于保持我们的血糖水平。\n微习惯如何拓宽你的舒适区\n你现在有一个心理舒适区（comfort zone），把它想象成一个圆圈。圆圈内是当下的我们，圆圈外则是我们想要达到的目标，也许是身材变好，读完了几本书，学会了某项技能。但是这些目标都要经历一些不太舒适的过程才能实现（因为脱离了基底神经节目前的模式）。\n通常我们采取“只要能成功怎么做都行”，然后开始大量行动，我们全力冲刺到舒适区外边，拼命挣扎想要留在那里，此时我们的潜意识：“有意思，但是这么剧烈的变化让我很不舒服”，当我们的动力和意志力不足以支撑时，我们会被拽回到舒适圈内。\n而微习惯就像是走到圆圈的边缘，轻轻往外走一小步，我们完全可以走一小步后退回到舒适圈，潜意识不会对这么微小的改变的做出太大反应，但是长此以往，舒适圈就会被我们扩大。\n我们偶尔会超额完成目标，可以用基础物理学知识来解释。牛顿第一运动定律的内容包括：\n除非受到外力作用，否则静止的物体总保持静止状态 除非受到外力作用。否则出于运动状态中的物体的速度不会改变。 我们可以得到一个新等式：一小步 + 想做的事 = 较高的进一步行动的可能性\n5 微习惯的独特之处 是故胜兵先胜而后求战，败兵先战而后求胜。 ——孙子，《孙子兵法》\n微习惯能与现有习惯一较高下\n培养一个新习惯也是对之前我们养成的一些习惯的挑战，我们需要摒弃一些曾经不好的习惯，以让更好的行为代替它。大脑会抗拒大幅度的改变，所以我们要以极其微小的行为做出一点点改变，潜移默化的影响我们的大脑，让新的行为成为基底神经节的一种模式。\n微习惯没有截止时间\n很多将心理学、行为学或者其他尝试帮助你养成好习惯的书籍都是基于“习惯是 21 天或者 30 天养成的”这个理论，而微习惯没有明确的截止时间，它要求尽可能一天都不能落下，长久地坚持。但是不像其他方法要求你每天健身一小时或者读一个小时的书，我们要做的只是很小很小的一部分，例如读 2 页书就好，只是这样。\n微习惯可以提升自我效能感\n大多数人都成尝试过把一个良好的行为养成习惯，然后由于各种各样的原因没有坚持下来，这会使我们缺乏基本的自我效能感。\n微习惯正是重新开始的完美方法。你不会再被巨大的目标打垮，也不会因为目标未实现带来的内疚感感到焦虑煎熬。这一次，你每天都能成功。这些胜利也许微不足道，但是对于一颗心灰意冷的心来说是至关重要的。\n微习惯帮你培养正念和意志力\n正念是一个非常重要的技能，它指的是我们对于自己的思维和行动有清醒的认知。正念是目标清晰地活着和敷衍活着之间的区别。如果你的微习惯是每天起床后喝一杯水，那么你就会对自己总共喝了多少水有所认知，如果是每天看两页书，那么你就会时常想自己已经看完了多少书。\n前文提过意志力是一项非常宝贵的资源，而微习惯是一项频繁重复小任务的行为，这是锻炼意志力的一个绝佳方法。\n6 彻底改变只需要八步 一个得不到执行的念头只会消亡 ——罗杰·冯·欧克\n第一步：选择适合自己的微习惯和计划\n选择适合自己的微习惯\n可以是一个每天都要做的事情，也可以是一个时间段的弹性计划，比如一周跑步三次\n把习惯变成一个小到不可思议的一步\n比如把每天做 20 个俯卧撑改成 1 个\n第二步：挖掘每个微习惯的内在价值\n我们很多时候无法养成习惯的原因在于我们想做一件事，但为要不要做这件事而苦恼。可以反思一下我们在第一步制定的习惯，一般都是我们长期以来潜意识里觉得正确的事情，却一直缺乏动力或者不那么理解这项行为能给我们带来什么。\n用“为什么钻头”来挖掘一下：\n我想每天读两页书。为什么？ 因为读书几乎一直以来都是成功人士的标配。为什么？ 因为读书是人们汲取知识、拓宽视野极好的途径。为什么？ 因为读书可以练就腹有诗书气自华和沉稳的气质，而这两点正是目前的我极其渴求的。 第三步：明确习惯依据，将其纳入日程\n培养习惯的常见依据有两个：时间和行为方式。对于朝九晚五时间比较规律的人群，更推荐根据时间方式作为依据，日程比较灵活的可以使用行为方式作为依据。\n时间：每周一三五的下午 3 点锻炼 行为方式：吃完晚饭后半个小时开始读书 也有第三种自由度更高的方式，我们在当天任意时间完成都可以，最低限度是睡觉前。\n第四步：建立回报机制，以奖励提升成就感\n一个有趣的现象：一个申请假释的犯人，假释听证会的最佳时间是在假释官吃完东西、结束休息之后，因为研究发现假释官在吃饱睡好之后做出的判决对被告更有利（大概是因为他们更愿意倾听）。\n拿锻炼来举例，锻炼可以让你获得健康的体魄，良好的身材。但是你刚开始锻炼的时候，锻炼结束后回到家里你收获到的回报是什么？汗水？于此同时，你的大脑却现在就想吃炸鸡，因为糖会刺激味蕾并激活大脑的回报中心，所以炸鸡是一种感官（首要）回报，而锻炼带来的是抽象（次要）回报，比如拥有好身材在沙滩上漫步、对付出的努力感到满意。次级回报需要更长的时间才能在大脑站稳脚跟。\n在一开始锻炼产生的内啡肽和期望产生的回报差距过大时，我们可以给自己一些奖励策略，比如挑选一个想买的东西加入购物车，在跑步一个月后把它买下来、如果能坚持到两个月就给自己买一块专业的运动手表（比如我给自己买的高驰 pace2）。\n这像是教小孩骑自行车，一开始我们需要向孩子保证会扶着自行车，可是在某个时候我们把手松开后，孩子不需要扶持也能继续骑车了\n第五步：记录与追踪情况\n遗忘是人类的天性。在一项习惯的前期阶段，遗忘也是一个阻力，我们偶尔会忘记我们给自己制定的计划。所以采取一些策略来提醒我们还是很有必要的。\n方式不重要，重要的是可以有效地提醒我们。如果你有看日历的习惯，就把要做的事情写到便签上放到日历旁边；如果你每天都使用电脑，可以把便签粘到显示器下面。手机闹钟、一些带有提醒功能的 APP 等等都可以。\n第六步：微量开始，超额完成\n我们在完成微习惯时消耗的是意志力，但是我们在达成目标后继续努力时动力就会开始起作用了。\n当你一旦开始，就会希望多完成一些。到那个时候，继续做和停下来一样容易。\n第七步：服从计划安排，拜托高期待值\n在第一步中，我们已经把习惯的难度定的非常低了，所以超额完成是很平常的事情，正如之前说到的，请把它作为一个奖励，不要把超额完成的部分作为你今后每天的目标，这是一个很危险的行为。读完两页书就是成功，句号！\n举个例子：你连续半个月每天都读了 30 页书，而不是 2 页书，于是理所当然的把微习惯改为了每天读 30 页书，但是一旦某天状态不佳或者因为一些其他因素导致没有完成 30 页书的目标，你建立起来的自信会受挫。不要嘴上说着读 2 页书就好，心里却把 30 页书作为目标。不要忘记你是如何做到读 30 页书的（是从每天 2 页书开始的）。\n坚持做一件小事，比偶尔做一件大事能从根本上改变更多。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n微习惯策略的所有益处、力量和优势都取决与你在纸面上和心里始终都将目标保持在微小状态的能力。\n第八步：习惯养成的标志\n代表行为已经成为习惯的信号：\n没有抵触情绪：该行为似乎做起来容易，不做反而更难 身份：你打心底认同该行为，可以信心十足地说“我是个跑者” 行动时无需考虑：不需要做任何决定，自然而然地去做。 你不再担心：你知道你会一直做这件事。 常态化：习惯是非情绪化的。你开始你的微习惯时没有任何情绪波动，而不会因为你正在做这件事而“激动不已” 它很无聊：好的习惯并不会让人很兴奋，它只是对你有好处而已。你会因为它们对生活更有激情，但不是对行为本身。 7 微习惯策略的八大规则 1. 绝对不要自欺欺人\n比如觉得某项微习惯太小，觉得偶尔一天不做也没什么；或者给自己制定的微习惯是每天一个俯卧撑，却在心里偷偷要求自己完成更多。\n2. 满意每一个进步\n对小小的进步感到满意和标准低不是一回事。李小龙有一句名言可以很好地总结这一点：“要满意，但别满足”。\n微习惯策略的核心是一个很简单的大脑错觉，同时也是一种重视开始的生活哲理，一种认为行动优于动力的生活哲理，一种相信将每一小步积累起来便能让量变转为质变的生活哲理。\n3. 经常回报自己，尤其在完成微习惯之后\n哪怕在完成微习惯之后对自己说“你很棒”这一点小小的激励，最终都会建立一个正反馈循环。\n4. 保持头脑清醒\n可能坚持几个月的微习惯后你能看到比较大的变化，进而过度兴奋，但别让这种兴奋成为你实施行动的原动力。变得依赖动力或情绪正是导致很多习惯没有养成的原因。\n在完成目标的过程中，无聊才是常态。使用冷静的头脑分析你的行为。\n5. 感到强烈抵触时，后退并缩小目标\n常识告诉我们，突破才能获得进步，然而这只适用于短期目标，比如项目的 deadline，你需要逼一逼自己才能完成。但是对于养成一个习惯来说，保证我们可以长时间的坚持才是最重要的。\n如果你给自己制定的计划让你感到很痛苦，你需要考虑是这项行为本身的问题还是目标设立的太大了。养成习惯过程中有抵触情绪是正常的，但采用微习惯策略时假如你能感受到明显的抵触情绪，那一定要缩小目标。\n6. 提醒自己这件事很轻松\n在微习惯策略中，你对实施行动的抵触行为很多时候都是因为考虑的太宽泛，比如健身，这是一个听起来就比较有压力的行为。但是你想想你今天要做的仅仅是做一个俯卧撑，自然而然会感受到轻松。\n7. 绝不要小看微步骤\n每一个大的工程都是由无数个小步骤做成的。持续做一件很小的小事，坚持一段时间，反正又花不了你多长时间，大部分微习惯两分钟之内就能完成，你会慢慢看到效果的。\n8. 用多余精力超额完成任务，而不是制定更大的目标\n大目标在纸面上看着漂亮，但只有行动才算数。\n目标渺小、结果丰满。你是想要这样的结果，还是反过来？\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/read/wei-xi-guan/","summary":"0 前言 书名《微习惯》，作者斯提芬·盖斯 [美]，江西人民出版社，译者桂君 微习惯是一种非常微小的积极行为，你需要每天强迫自己完成它。微习惯太小，小到不可能失败。正是因为这个特性，它不会给你造成任何负担，而且具有超强的“欺骗性”，它因此成了极具优势的习惯养成策略。 微习惯策略的科学原理表","title":"《微习惯》"},{"content":"0 前言 安装过程中会替换相当一部分系统内置的软件包, 不建议用于生产环境 cephadm 依赖 python3.6, 而此版本的 openeuler 内置版本为 3.7, 且不支持 platform-python, 参考: [openeuler 的 gitee 社区 issue](https: \u0026lt;//gitee.com/src-openeuler/python3/issues/I4J8RK?from=project-issue\u0026gt;)\n基础环境:\nceph: v16.2（pacific） 操作系统: icloudos_v1.0_aarch64（openEuler-20.03-LTS-aarch64） 内核版本: 4.19.90-2003.4.0.0037.aarch64 集群角色:\nip 主机名 角色 192.168.47.133 ceph-aarch64-node1 cephadm, mgr, mon, osd 192.168.47.135 ceph-aarch64-node2 osd 192.168.47.130 ceph-aarch64-node3 osd 1 基础环境配置 (所有节点) 1.1 关闭 node_exporter systemctl stop node_exporter systemctl disable node_exporter 1.2 修改主机名 hostnamectl set-hostname ceph-aarch64-node1 hostnamectl set-hostname ceph-aarch64-node2 hostnamectl set-hostname ceph-aarch64-node3 vi /etc/hosts # 添加 192.168.47.133 ceph-aarch64-node1 192.168.47.135 ceph-aarch64-node2 192.168.47.130 ceph-aarch64-node3 1.3 添加 yum 源 wget -O /etc/yum.repos.d/CentOS-Base.repo https: //mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum-config-manager --add-repo https: //mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/docker-ce.repo 1.4 添加 epel 源 yum install epel-release # 修改 $releasever sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-modular.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-playground.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-testing-modular.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-testing.repo 1.5 修改 /etc/os-release sed -i \u0026#39;s/ID=\u0026#34;isoft\u0026#34;/ID=\u0026#34;centos\u0026#34;/g\u0026#39; /etc/os-release sed -i \u0026#39;s/VERSION_ID=\u0026#34;1.0\u0026#34;/VERSION_ID=\u0026#34;8.0\u0026#34;/g\u0026#39; /etc/os-release 1.6 安装 python3.6 yum install python3-pip-wheel python3-setuptools-wheel wget http: //mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/aarch64/os/Packages/python3-libs-3.6.8-41.el8.aarch64.rpm wget http: //mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/aarch64/os/Packages/libffi-3.1-22.el8.aarch64.rpm rpm -ivh libffi-3.1-22.el8.aarch64.rpm --force cp /usr/lib64/libpython3.so /usr/lib64/libpython3.so-3.7.4 rpm -ivh python3-libs-3.6.8-41.el8.aarch64.rpm --force --nodeps mv /lib64/libpython3.so /lib64/python3.so-3.6.8 ln -s /usr/lib64/libpython3.so /lib64/libpython3.so yum install platform-python yum install python3-pip-9.0.3-20.el8.noarch vim /usr/bin/yum # 将 #!/usr/bin/python3 改成 #!/usr/bin/python3.7 yum install python3-prettytable-0.7.2-14.el8 yum install python3-gobject-base-3.28.3-2.el8.aarch64 yum install firewalld-0.9.3-7.el8 1.7 安装 docker yum install docker-ce systemctl start docker systemctl status docker systemctl enable docker 1.8 安装 cephadm \u0026amp; ceph-common curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release pacific yum install cephadm yum install ceph-common-16.2.9-0.el8 2 ceph 集群配置 2.1 集群初始化 cephadm bootstrap --mon-ip 192.168.47.133 访问: https://192.168.47.133:8443/\n第一次访问 dashboard 需要修改初始账号密码\n2.2 添加主机 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-aarch64-node2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-aarch64-node3 ceph orch host add ceph-aarch64-node2 192.168.47.135 --labels _admin ceph orch host add ceph-aarch64-node3 192.168.47.130 --labels _admin 2.3 添加磁盘 # 单盘添加 ceph orch daemon add osd ceph-aarch64-node1: /dev/vdb # 查看所有可用设备 ceph orch device ls # 自动添加所有可用设备 ceph orch apply osd --all-available-devices 3 其他 3.1 清除 ceph 集群 # 暂停集群, 避免部署新的 ceph 守护进程 ceph orch pause # 验证集群 fsid ceph fsid # 清除集群所有主机的 ceph 守护进程 cephadm rm-cluster --force --zap-osds --fsid \u0026lt;fsid\u0026gt; 3.2 no active mgr cephadm ls cephadm run --name mgr.ceph-aarch64-node3.ipgtzj --fsid 17136806-0735-11ed-9c4f-52546f3387f3 ceph orch apply mgr label: _admin 3.3 osd 误删除 https://blog.csdn.net/cjfcxf010101/article/details/100411984\n3.4 cephadm_failed_daemon [删除 osd 后引起的 cephadm_failed_daemon 错误](https: \u0026lt;//www.cnblogs.com/st2021/p/15026526.html\u0026gt;)\n3.5 禁用自动添加 osd ceph orch apply osd --all-available-devices --unmanaged=true 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v16-cpehadm-openeuler-aarch64/","summary":"0 前言 安装过程中会替换相当一部分系统内置的软件包, 不建议用于生产环境 cephadm 依赖 python3.6, 而此版本的 openeuler 内置版本为 3.7, 且不支持 platform-python, 参考: [openeuler 的 gitee 社区 issue](https: \u0026lt;//gitee.com/src-openeuler/python3/issues/I4J8RK?from=project-issue\u0026gt;) 基础环境: ceph: v16.2（pacific） 操作系统: icloudos_v1.0_aarch64（openEuler-20.03-LTS-aarch64）","title":"ceph | openeuler (aarch64) 部署 ceph-v16"},{"content":"1 起因 在使用 cephadm 安装 ceph v16.2 时升级了 python，系统默认版本是 3.7.4 ，升级后版本是 3.8.5，glibc 作为依赖同时进行了升级，系统默认版本是 2.28 ，升级后版本是 2.31，幸好记录及时，截图留存了软件包升级信息，如下\n在没有十分把握的情况下不要用 yum install -y，使用 yum install 先判断好依赖安装带来的影响\n升级过程未出任何问题，便没在意，可是后续 openssh 由于 glibc 的升级导致连接失败，一番 baidu 加 google 未解决 openssh 连接问题，于是便着手开始降级 glibc 至系统默认版本，从系统镜像中找到 glibc 相关的三个软件包\n由于是版本降级，脑子一热便采用 rpm -Uvh --nodeps glibc* 方式强制安装，至此，系统崩溃\n系统几乎所有命令都无法使用，报错如下\n出现这个问题的原因大致是因为强制安装并未完全成功，lib64 一些相关的库文件软链接丢失\n[root@localhost ~]# ls -l /lib64/libc.so.6 lrwxrwxrwx 1 root root 12 7月 14 14:43 /lib64/libc.so.6 -\u0026gt; libc-2.28.so # 恢复前这里是 libc-2.31.so 在强制安装 glibc-2.28 时， libc-2.31.so 已经被替换成了 libc-2.28.so ，由于安装失败 libc.so.6 链接到的还是 libc-2.31.so，自然会报错 no such file\n2 恢复 系统绝大部分命令都是依赖 libc.so.6 的，我们可以通过 export LD_PRELOAD=\u0026quot;库文件路径\u0026quot; 设置优先使用的库\nexport LD_PRELOAD=/lib64/libc-2.28.so 此时 ls 、cd、mv 等基础命令以及最重要的 ln 链接命令已经可以使用了，接下来就是恢复软链接\nrm -f /lib64/libc.so.6 ln -s /lib64/libc-2.28.so /lib64/libc.so.6 但是 yum 命令依赖的几个库软链接还没有恢复，按照报错提示跟上述步骤一样，先删除掉依赖的库文件，再重新软链接过去\n之后就是重新 yum localinstall 安装一下未安装成功的 glic ，之前强制安装时已经将高版本的 glibc 清理掉了，这里重新安装很顺利\n也许之前使用 yum localinstall 安装可能就不会出现这个问题了，rpm \u0026ndash;nodeps 也要少用~\nyum localinstall glibc* 软件包安装过程中没有报错，经测试系统一切正常，openssh 也可以正常连接了\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/troubleshooting-glibc-wu-sheng-ji/","summary":"1 起因 在使用 cephadm 安装 ceph v16.2 时升级了 python，系统默认版本是 3.7.4 ，升级后版本是 3.8.5，glibc 作为依赖同时进行了升级，系统默认版本是 2.28 ，升级后版本是 2.31，幸好记录及时，截图留存了软件包升级信息，如下 在没有十分把握的情况下不要用 yum install -y，使用 yum install 先判断好依赖安装带来的影响 升级","title":"troubleshooting | glibc 误升级后修复"},{"content":"0 前言 本文参考以下链接\nopenEuler 官方文档 PXE 自动化安装 CentOS 8 PXE 网络引导系统之服务器 arm64 测试环境：\nx86_64（amd ryzen 7 4800u）：vmware workstation V16.1.2\naarch64（kunpeng 920）： kvm-2.12\n注意测试的网络环境中不要存在其他的 dhcp 服务\n注意测试虚拟机内存尽量大于 4G，否则会报错 no space left 或者测试机直接黑屏\n注意 ks.cfg 尽量在当前环境先手动安装一台模板机，使用模板机生成的 ks 文件来进行修改，否则可能会有一些清理磁盘分区的破坏性操作，基本只需要将安装方式从 cdrom 修改成 install 和 url --url=http://……\n1 服务端配置 1.1 基础环境 系统版本：iSoft-ServerOS-V6.0-rc1\nip 地址：1.1.1.21\n网卡选择 nat 模式，注意关闭一下 workstation 自带的 dhcp，也可使用自定义的 lan区段\n1.2 关闭防火墙及 selinux iptables -F systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config 1.3 安装相关的软件包 这里由于 HW 行动的原因，外网 yum 源暂不可用，使用本地 yum 源安装相关软件包\nmount -o loop /root/iSoft-Taiji-Server-OS-6.0-x86_64-rc1-202112311623.iso /mnt mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/isoft* /etc/yum.repos.d/bak/ cat \u0026gt; /etc/yum.repos.d/local.repo \u0026lt;\u0026lt;EOF [local] name=local baseurl=file:///mnt gpgcheck=0 enabled=1 EOF dnf clean all dnf makecache cenots8 安装 syslinux 时需要加 \u0026ndash;nonlinux 后缀，centos7 则不需要\ndnf install dhcp-server tftp-server httpd syslinux-nonlinux 1.4 http 服务配置 mkdir /var/www/html/ks/ chmod 755 -R /var/www/html/ systemctl start httpd systemctl enable httpd 能访问到 httpd 即可\n1.5 tftp 服务配置 systemctl start tftp systemctl enable tftp 1.6 dhcp 服务配置 x86_64 架构和 aarch64 架构的 dhcp 的配置略有不同，按照下文分别配置\nsystemctl enable dhcpd 2 x86_64 2.1 服务端配置 2.1.1 dhcp 服务配置 vim /etc/dhcp/dhcpd.conf\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; next-server 1.1.1.21; # 本机ip（tftpserver的ip） filename \u0026#34;pxelinux.0\u0026#34;; } systemctl restart dhcpd 2.2 isoft_4.2_x86 2.2.1 http 服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_4.2/isos/x86_64/ # 挂载镜像文件 mount -o loop /root/iSoft-Server-OS-4.2-x86_64-201907051149.iso /var/www/html/isoft_4.2/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-isoft-4.2-x86.cfg chmod -R 755 /var/www/html ks.cfg 文件内容\n#version=DEVEL # System authorization information auth --enableshadow --passalgo=sha512 # Use CDROM installation media install url --url=http://1.1.1.21/isoft_4.2/isos/x86_64/ # Use graphical install graphical # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda # Keyboard layouts keyboard --vckeymap=cn --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=dhcp --device=ens32 --onboot=off --ipv6=auto --no-activate network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$9yXT2.jd8oofY89W$q1nVQ4rRfAE937KeG5bHCAP3iI3GgyVJJF/MN5Ipe9omdXIEjelaTQSPplr9E9aFOGG17F3GkzIzNnifvjdO20 # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc # X Window System configuration information xconfig --startxonboot # System bootloader configuration bootloader --append=\u0026#34; crashkernel=auto\u0026#34; --location=mbr --boot-drive=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel %packages @^gnome-desktop-environment @base @core @desktop-debugging @dial-up @directory-client @fonts @gnome-desktop @guest-agents @guest-desktop-agents @input-methods @internet-browser @java-platform @multimedia @network-file-system-client @networkmanager-submodules @print-client @x11 chrony kexec-tools %end %addon com_redhat_kdump --enable --reserve-mb=\u0026#39;auto\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot 2.2.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/icloud_1.0/isos/x86_64/Packages/syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm /root/ rpm2cpio syslinux-4.05-15.el7.isoft.x86_64.rpm | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-4.05-15.el7.isoft.x86_64.rpm | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/isoft_4.2/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/ks/ks-isoft-6.0-x86.cfg 2.3 isoft_6.0-rc1_x86 2.3.1 http 服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_6.0/isos/x86_64/ # 挂载镜像文件 mount -o loop /root/iSoft-Taiji-Server-OS-6.0-x86_64-rc1-202112311623.iso /var/www/html/isoft_6.0/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-isoft-6.0-x86.cfg chmod -R 755 /var/www/html ks.cfg 文件内容\n# Use graphical install graphical install url --url=http://1.1.1.21/isoft_6.0/isos/x86_64/ %packages @^graphical-server-environment %end # Keyboard layouts keyboard --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=ens33 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # System timezone timezone Asia/Shanghai --isUtc # Root password rootpw --iscrypted $6$w6X5WYQDyMeAizfs$TFKls9Kuj4Jv6PNKcMZ2BmB1Z/dvRCRkGD9uzm0n8te2UwDgdPCPGkUxCPvExKGenCMINTMcjSH55bCWYDiHx. %addon com_redhat_kdump --disable --reserve-mb=\u0026#39;128\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot 2.3.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/isoft_6.0/isos/x86_64/Packages/syslinux-nonlinux-6.04-7.oe1.isoft.noarch /root/ rpm2cpio syslinux-nonlinux-6.04-7.oe1.isoft.noarch | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-nonlinux-6.04-7.oe1.isoft.noarch | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/ks/ks-isoft-6.0-x86.cfg 2.4 icloud_1.0_x86 2.4.1 http 服务配置 mkdir -p /var/www/html/icloud_1.0/isos/x86_64/ # 挂载镜像 mount -o loop /root/i-CloudOS-1.0-x86_64-202108131137.iso /var/www/html/icloud_1.0/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-icloud-1.0-x86.cfg chmod -R 755 /var/www/html ks-icloud-1.0-x86.cfg 文件内容\n#version=RHEL8 ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/icloud_1.0/isos/x86_64/ # Keyboard layouts keyboard --vckeymap=us --xlayouts=\u0026#39;\u0026#39; # System language lang zh_CN.UTF-8 # Root password rootpw --iscrypted 123.com # Run the Setup Agent on first boot firstboot --enable # Do not configure the X Window System skipx # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^vmserver-compute-node %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot 2.4.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/icloud_1.0/isos/x86_64/Packages/syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm /root/ rpm2cpio syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-nonlinux-6.04-4.el8.isoft.noarch.rpm | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default # 下面这三个文件centos7可以不要，centos8对于这三个文件有一定依赖性 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault menu.c32 timeout 30 menu title i-CloudOS 1.0 label linux menu label ^Install i-CloudOS 1.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/icloud_1.0/isos/x86_64/ks-icloud-1.0-x86.cfg 2.5 openeuler_20.03-LTS-SP1_x86 2.5.1 http 服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/ # 挂载镜像文件 mount -o loop /root/iSoft-Taiji-Server-OS-6.0-x86_64-rc1-202112311623.iso /var/www/html/isoft_6.0/isos/x86_64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-openeuler-20.03-LTS-x86.cfg chmod -R 755 /var/www/html /var/www/html/ks/ks-openeuler-20.03-LTS-x86.cfg 文件内容\n# Use graphical install graphical install url --url=http://1.1.1.21/openeuler_20.03-LTS-SP1/isos/x86_64/ %packages @^minimal-environment %end # Keyboard layouts keyboard --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=ens33 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # System timezone timezone Asia/Shanghai --isUtc # Root password rootpw --iscrypted $6$w6X5WYQDyMeAizfs$TFKls9Kuj4Jv6PNKcMZ2BmB1Z/dvRCRkGD9uzm0n8te2UwDgdPCPGkUxCPvExKGenCMINTMcjSH55bCWYDiHx. %addon com_redhat_kdump --disable --reserve-mb=\u0026#39;128\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot 2.5.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* rm -rf /root/usr mkdir /var/lib/tftpboot/pxelinux.cfg # 提取 menu.c32 和 pxelinux.0 cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/Packages/syslinux-nonlinux-6.04-5.oe1.noarch.rpm /root/ rpm2cpio syslinux-nonlinux-6.04-5.oe1.noarch.rpm | cpio -idv ./usr/share/syslinux/menu.c32 rpm2cpio syslinux-nonlinux-6.04-5.oe1.noarch.rpm | cpio -idv ./usr/share/syslinux/pxelinux.0 cp /root/usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /root/usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ # 拷贝内核启动文件 cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS-SP1/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/ks/ks-isoft-6.0-x86.cfg 3 aarch64 3.1 服务端配置 3.1.1 dhcp 服务配置 vim /etc/dhcp/dhcpd.conf\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; next-server 1.1.1.21; # 本机ip（tftpserver的ip） filename \u0026#34;grubaa64.efi\u0026#34;; } systemctl restart dhcpd 3.2 isoft_6.0_aarch64 3.2.1 http 服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_6.0/isos/aarch64/ # 挂载镜像文件 mount -o loop /root/iSoft-Taiji-Server-OS-6.0-aarch64-202201240952.iso /var/www/html/isoft_6.0/isos/aarch64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-isoft-6.0-aarch64.cfg chmod -R 755 /var/www/html ks-isoft-6.0-aarch64.cfg 文件内容\n#version=DEVEL ignoredisk --only-use=vda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/isoft_6.0/isos/aarch64 # Keyboard layouts keyboard --vckeymap=cn --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=enp3s0 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$x94MGsfCoFdE/G4O$MEakgOwtq0O5i4pRIVzXntKQuMJVh9CJ3anhZKl8YZhZDtSXhzuMk5mpDr3wu..rDareWgy5tjsepCaGiPK3g/ # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^mate-desktop-environment %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot 3.2.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* cp /var/www/html/isoft_6.0/isos/aarch64/EFI/BOOT/grub.cfg /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/aarch64/EFI/BOOT/grubaa64.efi /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/aarch64/images/pxeboot/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/aarch64/images/pxeboot/initrd.img /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/grub.cfg\nset default=\u0026#34;1\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=6 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iSoft-Taiji-Server-OS-6.0\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iSoft-Taiji-Server-OS 6.0 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { set root=(tftp,1.1.1.21) linux /vmlinuz ro inst.geoloc=0 console=ttyAMA0 console=tty0 rd.iscsi.waitnet=0 inst.repo=http://1.1.1.21/isoft_6.0/isos/aarch64/ inst.ks=http://1.1.1.21/ks/ks-isoft-6.0-aarch64.cfg initrd /initrd.img } } 3.3 icloud_1.0_aarch64 这里 iso 没有直接挂载到 apache 目录，是因为该 iso 文件 Packages 目录中有个别软件包没有读取权限，直接挂载无法修改权限\n3.3.1 http 服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/icloud_1.0/isos/aarch64/ # 挂载镜像文件 mount -o loop /root/iCloudOS-1.0-aarch64-2021-0805-1423-test-1.iso /mnt/ cp -r /mnt/* /var/www/html/icloud_1.0/isos/aarch64/ # 上传 ks.cfg 应答文件 vim /var/www/html/ks/ks-icloud-1.0-aarch64.cfg chmod -R 755 /var/www/html ks-icloud-1.0-aarch64.cfg 文件内容\n#version=RHEL8 ignoredisk --only-use=vda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/icloud_1.0/isos/aarch64/ # Keyboard layouts keyboard --vckeymap=us --xlayouts=\u0026#39;\u0026#39; # System language lang zh_CN.UTF-8 # Root password rootpw --iscrypted 123.com # Run the Setup Agent on first boot firstboot --enable # Do not configure the X Window System skipx # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^vmserver-compute-node %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot 3.3.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* cp /var/www/html/icloud_1.0/isos/aarch64/EFI/BOOT/grub.cfg /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/aarch64/EFI/BOOT/grubaa64.efi /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/aarch64/images/pxeboot/vmlinuz /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/aarch64/images/pxeboot/initrd.img /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/grub.cfg\nset default=\u0026#34;1\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=6 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iCloudOS-1.0-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iCloudOS 1.0 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { set root=(tftp,1.1.1.21) linux /vmlinuz ro inst.geoloc=0 console=ttyAMA0 console=tty0 rd.iscsi.waitnet=0 inst.repo=http://1.1.1.21/icloud_1.0/isos/aarch64 inst.ks=http://1.1.1.21/ks/ks-icloud-1.0-aarch64.cfg initrd /initrd.img } 3.4 openeuler_20.03-LTS_aarch64 3.4.1 http 服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/openeuler_20.03-LTS/isos/aarch64/ # 挂载镜像文件 mount -o loop /root/openEuler-20.03-LTS-aarch64-dvd.iso /var/www/html/openeuler_20.03-LTS/isos/aarch64/ # 创建ks.cfg应答文件 vim /var/www/html/ks/ks-openeuler-20.03-LTS-aarch64.cfg chmod -R 755 /var/www/html ks-openeuler-20.03-LTS-aarch64.cfg 文件内容\n#version=DEVEL ignoredisk --only-use=vda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/openeuler_20.03-LTS/isos/aarch64 # Keyboard layouts keyboard --vckeymap=cn --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=enp3s0 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$x94MGsfCoFdE/G4O$MEakgOwtq0O5i4pRIVzXntKQuMJVh9CJ3anhZKl8YZhZDtSXhzuMk5mpDr3wu..rDareWgy5tjsepCaGiPK3g/ # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^minimal-environment %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot 3.4.2 tftp 服务配置 rm -rf /var/lib/tftpboot/* cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/EFI/BOOT/grub.cfg /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/EFI/BOOT/grubaa64.efi /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/images/pxeboot/vmlinuz /var/lib/tftpboot/ cp /var/www/html/openeuler_20.03-LTS/isos/aarch64/images/pxeboot/initrd.img /var/lib/tftpboot/ chmod -R 755 /var/lib/tftpboot/* systemctl restart tftp vim /var/lib/tftpboot/grub.cfg\nset default=\u0026#34;1\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=60 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;openEuler-20.03-LTS-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install openEuler 20.03 LTS\u0026#39; --class red --class gnu-linux --class gnu --class os { set root=(tftp,1.1.1.21) linux /vmlinuz ro inst.geoloc=0 console=ttyAMA0 console=tty0 rd.iscsi.waitnet=0 inst.repo=http://1.1.1.21/openeuler_20.03-LTS/isos/aarch64/ inst.ks=http://1.1.1.21/ks/ks-openeuler-20.03-LTS-aarch64.cfg initrd /initrd.img } } 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/pxe-install-scripts/","summary":"0 前言 本文参考以下链接 openEuler 官方文档 PXE 自动化安装 CentOS 8 PXE 网络引导系统之服务器 arm64 测试环境： x86_64（amd ryzen 7 4800u）：vmware workstation V16.1.2 aarch64（kunpeng 920）： kvm-2.12 注意测试的网络环境中不要存在其他的 dhcp 服务 注意测试虚拟机内存尽量大于 4G，否则会报错 no space left 或者测试机直接","title":"pxe 安装配置大全"},{"content":"0 前言 书名《人间失格》，北京燕山出版社，译者高艳\n1 语句摘录 人是不可能一边笑还一边紧紧攥着拳头的，只有猴子才会这样\n女人如果突然哭起来，只要让她们吃些好吃的东西，她们就会立刻好转\n越是对人感到恐惧的人，反倒越希望亲眼看到狰狞恐怖的怪物；越是胆小怯懦、神经兮兮的人，越是期盼暴风雨来的更猛烈一些\n世上所有人的说话方式，都喜欢这样绕圈子，不明说，也不说破，带着想逃避责任的心理，复杂又微妙\n我对死倒是不在乎，但如果因受伤变成残疾人，我是接受不了的\n“你喝太多酒了。”\n“不喝了！从明天起，我滴酒不沾了！”\n“真的？”\n“真的，我一定戒。假如我戒了，良子肯嫁给我吗？” 说要娶她的事，其实是一句玩笑话。\n“当然了。”\n“好，那我们就一言为定。我肯定戒酒”\n可第二天，我又照样从中午起便捏起酒盅来。傍晚时分，我摇摇晃晃走出酒馆，站在由子家的铺子前。\n“良子，对不起，我又喝酒了。”\n“哎呀，真讨厌，故意装成一副喝醉的样子。”\n我被她的话吓了一跳，酒一下子醒了许多。\n“不，是真的。我真喝酒了，不是故意装成喝醉的样子。”\n“别捉弄我，你真坏。” 她对我丝毫没有疑心。\n“你一看不就明白了？我今天又从中午开始喝酒了。原谅我！”\n“你演戏演得真像。”\n“不是演戏，你这个傻丫头！当心我亲你哦。”\n“亲呀！”\n“不，我没有资格亲你。要你嫁给我的事，就此作罢吧。你看我的脸，通红通红的是吧？我确实喝了。”\n“那是因为夕阳照在脸上的缘故，你骗我也没用的。因为我们昨天说定了，你不可能去喝酒的，你承诺过我，你却说自己喝酒了，肯定是在骗人、骗人、骗人！”\n","permalink":"https://www.lvbibir.cn/en/posts/read/ren-jian-shi-ge/","summary":"0 前言 书名《人间失格》，北京燕山出版社，译者高艳 1 语句摘录 人是不可能一边笑还一边紧紧攥着拳头的，只有猴子才会这样 女人如果突然哭起来，只要让她们吃些好吃的东西，她们就会立刻好转 越是对人感到恐惧的人，反倒越希望亲眼看到狰狞恐怖的怪物；越是胆小怯懦、神经兮兮的人，越是期盼暴风雨来的更猛","title":"《人间失格》"},{"content":" 标题说明: 月份 _ 当月跑步次数 _ 当月跑步距离\n2023 12_05_40 11_05_40 10_06_62 09_18_140 08_05_25 01-07_15_64 2022 12_01_5 11_08_51 10_16_100 09_13_123 0917 第一次半马 08_15_124 0827 第一次 15km 07_15_78 0728 第一次 10km 0706 第一次 5km 06_10_26 ","permalink":"https://www.lvbibir.cn/en/posts/life/running/","summary":"标题说明: 月份 _ 当月跑步次数 _ 当月跑步距离 2023 12_05_40 11_05_40 10_06_62 09_18_140 08_05_25 01-07_15_64 2022 12_01_5 11_08_51 10_16_100 09_13_123 0917 第一次半马 08_15_124 0827 第一次 15km 07_15_78 0728 第一次 10km 0706 第一次 5km 06_10_26","title":"跑步日常"},{"content":"0 前言 cve 官网或者工信部会发布一些 cve 漏洞，可以看到该漏洞在某次 commit 提交代码后修复的。\n可以通过检索 kernel.org 中所有内核版本的 ChangeLog 文件中是否包含该 commit 来判断漏洞影响的内核版本（仅针对 linux 的 kernel 相关的漏洞）\n1 脚本 #!/bin/bash # author: lvbibir # date: 2022-06-23 # 检索 kernel.org 下的所有 ChangeLog 文件，是否包含某项特定的 commit 号 commit=\u0026#39;520778042ccca019f3ffa136dd0ca565c486cedd\u0026#39; version=4 number=0 curl -ks https://cdn.kernel.org/pub/linux/kernel/v$version\\.x/ \u0026gt; list_$version cat list_$version | grep Change | grep -v sign | awk -F\\\u0026#34; \u0026#39;{print $2}\u0026#39; \u0026gt; list_$version\\_cut total=`wc -l list_$version\\_cut | awk \u0026#39;{print $1}\u0026#39;` while read line; do let \u0026#39;number+=1\u0026#39; url=\u0026#34;https://cdn.kernel.org/pub/linux/kernel/v$version.x/$line\u0026#34; echo -e \u0026#34;\\033[31m---------------------正在检索$url----------------第$number 个文件，共$total 个文件\\033[0m\u0026#34; curl -ks $url | grep $commit if [ $? -eq 0 ]; then echo $url \u0026gt;\u0026gt; ./result_$version fi done \u0026lt; ./list_$version\\_cut echo -e \u0026#34;\\033[32m脚本执行完成，结果已保存至当前目录的 result_$version \\033[0m\u0026#34; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-search-url-files/","summary":"0 前言 cve 官网或者工信部会发布一些 cve 漏洞，可以看到该漏洞在某次 commit 提交代码后修复的。 可以通过检索 kernel.org 中所有内核版本的 ChangeLog 文件中是否包含该 commit 来判断漏洞影响的内核版本（仅针对 linux 的 kernel 相关的漏洞） 1 脚本 #!/bin/bash # author: lvbibir # date: 2022-06-23 # 检索 kernel.org 下的所有 ChangeLog 文件，是否包含某项特定的 commit 号 commit=\u0026#39;520778042ccca019f3ffa136dd0ca565c486cedd\u0026#39; version=4 number=0 curl -ks https://cdn.kernel.org/pub/linux/kernel/v$version\\.x/ \u0026gt; list_$version cat list_$version | grep Change | grep -v sign","title":"shell | 检索某 url 中所有文件的内容"},{"content":"1 git 1.1 submodule 当 clone 一个含有子模块的 git 仓库时可以使用如下命令安装所有子模块\ngit submodule init git submodule update 1.2 branch 管理 查看分支\ngit branch -a 创建分支\n# 以当前分支为模板创建并切换分支 git checkout -b dev # 以 master 为模板创建并切换分支, master 可以是哈希值或者 origin/master 这种远程地址 git checkout -b dev master # 推送分支, 如远端不存在则自动创建 git checkout dev git push origin dev 删除分支\n# 本地删除 git checkout master git branch -d dev # 如果分支包含未合并的更改，使用 `-D` 强制删除 git branch -D dev # 远端删除 git push origin --delete dev # 或 git push origin :dev 2 git 配置 查看 git 设置\n# 当前仓库 git config --list # 全局配置 git config --global --list 2.1 设置代理 设置全局代理，使用 http 代理\ngit config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 取消 github.com 代理\ngit config --global --unset http.https://github.com.proxy git config --global --unset https.https://github.com.proxy 设置全局代理，使用 socks5 代理\ngit config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 取消全局代理\ngit config --global --unset http.proxy git config --global --unset https.proxy 只对 github.com 使用代理\ngit config --global http.https://github.com.proxy http://127.0.0.1:7890 git config --global https.https://github.com.proxy http://127.0.0.1:7890 2.2 CRLF 和 LF # 提交时转换为LF，检出时转换为CRLF git config --global core.autocrlf true # 提交时转换为LF，检出时不转换 git config --global core.autocrlf input # 提交检出均不转换 git config --global core.autocrlf false # 拒绝提交包含混合换行符的文件 git config --global core.safecrlf true # 允许提交包含混合换行符的文件 git config --global core.safecrlf false # 提交包含混合换行符的文件时给出警告 git config --global core.safecrlf warn 3 常见问题 3.1 git clone 报错 fatal: early EOF\nfatal: fetch-pack: invalid index-pack output\n解决\ngit config --global http.sslVerify \u0026#34;false\u0026#34; git config --global core.compression -1 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/git/","summary":"1 git 1.1 submodule 当 clone 一个含有子模块的 git 仓库时可以使用如下命令安装所有子模块 git submodule init git submodule update 1.2 branch 管理 查看分支 git branch -a 创建分支 # 以当前分支为模板创建并切换分支 git checkout -b dev # 以 master 为模板创建并切换分支, master 可以是哈希值或者 origin/master 这种远程地址 git checkout -b dev master # 推送分支, 如远端不存在则自动创建 git checkout dev git push origin dev 删除分支 # 本地","title":"git"},{"content":"0 前言 本文参考以下链接:\n详解 shell 中 source、sh、bash、./ 执行脚本的区别 1 不同的执行方式 shell 脚本通常有 sh filename、bash filename、./filename、source filename 这四种执行方式\nsource filename 可以使用 . filename 代替，在当前的 bash 环境下读取并执行脚本文件中的命令，且脚本文件的变量，在脚本执行完成后会保存下来 ./filename 和 sh filename 或者 bash filename 是等效的，都是开启一个子 shell 来运行脚本文件，脚本中设置的变量执行完毕后不会保存 除 ./filename 外，source filename 、. filename 、sh filename 、bash filename 都是不需要执行权限的\n变量和权限问题示例\n# 设置临时变量，仅在当前 bash 环境生效 [root@lvbibir ~]# name=lvbibir [root@lvbibir ~]# echo $name lvbibir [root@lvbibir ~]# [root@lvbibir ~]# cat test.sh #!/bin/bash echo $name # source 或者 . 可以获取到父 bash 环境的变量 [root@lvbibir ~]# source test.sh lvbibir [root@lvbibir ~]# . test.sh lvbibir # sh、bash、./三种方式都使用了子 bash 环境，所以无法获取父 bash 环境的变量 # ./ 方式需要脚本有执行权限 [root@lvbibir ~]# sh test.sh [root@lvbibir ~]# bash test.sh [root@lvbibir ~]# ./test.sh -bash: ./test.sh: Permission denied [root@lvbibir ~]# chmod a+x test.sh [root@lvbibir ~]# ./test.sh 同理，使用 source 或者 . 也可以在 bash 环境中获取到脚本中设置的变量\n[root@lvbibir ~]# cat \u0026gt; test.sh \u0026lt;\u0026lt; EOF \u0026gt; #!/bin/bash \u0026gt; number=22 \u0026gt; \u0026gt; EOF [root@lvbibir ~]# echo $number # sh bash ./ 三种方式无法获取脚本中的变量 [root@lvbibir ~]# [root@lvbibir ~]# sh test.sh [root@lvbibir ~]# echo $number [root@lvbibir ~]# bash test.sh [root@lvbibir ~]# echo $number [root@lvbibir ~]# ./test.sh [root@lvbibir ~]# echo $number # source 方式可以获取脚本中的变量 [root@lvbibir ~]# source test.sh [root@lvbibir ~]# echo $number 22 [root@lvbibir ~]# 2 其他问题 关于是否在子 bash 环境运行的区别出了变量问题还会存在一些其他影响，如下测试\n已知目前存在一个 mysqld 进程，其 pid 为 29426 ，写一个监控 pid 的脚本\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | awk \u0026#39;{print $4}\u0026#39;) echo $pid 两种方式分别运行一下\n[root@lvbibir ~]# sh test.sh mysqld 27038 27039 29426 [root@lvbibir ~]# bash test.sh mysqld 27047 27048 29426 [root@lvbibir ~]# ./test.sh mysqld 27056 27057 29426 [root@lvbibir ~]# [root@lvbibir ~]# source test.sh mysqld 29426 [root@lvbibir ~]# . test.sh mysqld 29426 [root@lvbibir ~]# 问题出现了，由于某种原因导致子 bash 环境中执行的脚本监控到多个 pid ，给脚本添加个 sleep 来看下\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | awk \u0026#39;{print $4}\u0026#39;) echo $pid sleep 30 [root@lvbibir ~]# ./test.sh mysqld 27396 27397 29426 新开一个终端，查看进程\n第一个 pid 是在子 shell 中执行监控脚本的进程号 第二个 pid 不太清楚哪里来的，也 grep 不到这个进程号，应该是脚本执行一瞬间就释放掉了 第三个 pid 是 mysql 实际运行中的进程号 实际中脚本的 pid 和 mysqld 的 pid 顺序不太一样，取决于 pid 的大小\n在脚本再添加个 grep 过滤掉脚本本身的进程来规避这个问题\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | grep -v bash | awk \u0026#39;{print $4}\u0026#39;) echo $pid [root@lvbibir ~]# ./test.sh mysqld 29426 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-different-execution-mode/","summary":"0 前言 本文参考以下链接: 详解 shell 中 source、sh、bash、./ 执行脚本的区别 1 不同的执行方式 shell 脚本通常有 sh filename、bash filename、./filename、source filename 这四种执行方式 source filename 可以使用 . filename 代替，在当前的 bash 环境下读取并执行脚本文件中的命令，且脚本文件","title":"shell | 不同执行方式的区别"},{"content":"0 前言 本文参考以下链接:\nBash 脚本中的 set -euxo pipefail shell 脚本是没有 debug 模式的，不过可以通过 set 指令实现简单的 debug 功能\nshell 脚本中默认每条指令都会从上到下依次执行，但是当某行指令报错时，我们大多数情况下是不希望继续执行后续指令的\n这时可以使用 shell 脚本中 set 指令的四个参数：-e、-u、-x、-o pipefail\n命令报错即返回值（$?）不为 0\n1 set -e set -e 选项可以在脚本出现异常的时候立即退出，后续命令不再执行，相当于打上了一个断点\nif 判断条件里出现异常也会直接退出，如果不希望退出可以在判断语句后面加上 || true 来阻止退出\n1.1 before 脚本内容\nfoo 是一个不存在的命令，用于模拟命令报错\n#!/bin/bash foo echo \u0026#34;hello\u0026#34; 执行结果\n./test.sh: line 3: foo: command not found hello 1.2 after 脚本内容\n#!/bin/bash set -e foo echo \u0026#34;hello\u0026#34; 执行结果\n./test.sh: line 5: foo: command not found 1.3 阻止立即退出的例子 #!/bin/bash set -e foo || true echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found hello 2 set -o pipefail 默认情况下 bash 只会检查管道（pipelie）操作的最后一个命令的返回值，即最后一个命令返回值为 0 则判断整条管道语句是正确的\n如下\nset -o pipefail 的作用就是管道中只要有一个命令失败，则整个管道视为失败\n2.1 before #!/bin/bash set -e foo | echo \u0026#34;a\u0026#34; echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found a hello 2.2 after #!/bin/bash set -eo pipefail foo | echo \u0026#34;a\u0026#34; echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found a 3 set -u set -u 的作用是将所有未定义的变量视为错误，默认情况下 bash 会将未定义的变量视为空\n3.1 before #!/bin/bash set -eo pipefail echo $a echo \u0026#34;hello\u0026#34; hello 3.2 after #!/bin/bash set -euo pipefail echo $a echo \u0026#34;hello\u0026#34; ./test.sh: line 5: a: unbound variable 4 set -x set -x 可以让 bash 把每个命令在执行前先打印出来，好处显而易见，可以快速方便的找到出问题的脚本位置，坏处就是 bash 的 log 会格外的乱\n另外，它在打印的时候会先把变量解析出来\n纵然 log 可能会乱一些，但也比 debug 的时候掉头发强\n#!/bin/bash set -euox pipefail a=2 echo $a echo \u0026#34;hello\u0026#34; + a=2 + echo 2 # 这里已经将变量 a 解析为 2 了 2 + echo hello hello 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-enable-debug-mode/","summary":"0 前言 本文参考以下链接: Bash 脚本中的 set -euxo pipefail shell 脚本是没有 debug 模式的，不过可以通过 set 指令实现简单的 debug 功能 shell 脚本中默认每条指令都会从上到下依次执行，但是当某行指令报错时，我们大多数情况下是不希望继续执行后续指令的 这时可以使用 shell 脚本中 set 指令的四个参数：-e、-u、-x、-o pipefail 命令报错即返回值","title":"shell | 开启 debug 模式"},{"content":"1 remote ssh 使用密钥 remote ssh 远程服务器时每次都要求输入密码, 可以通过密钥实现免密登录\n修改 ssh 配置文件, 一般在 C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\config, 配置文件的路径取决于 remote ssh 使用的配置文件, 在文件内添加私钥的路径即可, 如下\nHost lvbibir.cn HostName lvbibir.cn User root IdentityFile C:\\Users\\lvbibir\\.ssh\\id_rsa 2 右键菜单问题 前端时间突然发现文件和目录的右键菜单中的 在 vscode 中打开 消失, 可以将下述代码保存为 .reg 文件并以管理员运行, ==记得将目录修改为正确的 vscode 安装路径==\nWindows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\*\\shell\\VSCode] @=\u0026#34;Open with Code\u0026#34; \u0026#34;Icon\u0026#34;=\u0026#34;D:\\\\software\\\\Vscode\\\\Code.exe\u0026#34; [HKEY_CLASSES_ROOT\\*\\shell\\VSCode\\command] @=\u0026#34;\\\u0026#34;D:\\\\software\\\\Vscode\\\\Code.exe\\\u0026#34; \\\u0026#34;%1\\\u0026#34;\u0026#34; Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\Directory\\shell\\VSCode] @=\u0026#34;Open with Code\u0026#34; \u0026#34;Icon\u0026#34;=\u0026#34;D:\\\\software\\\\Vscode\\\\Code.exe\u0026#34; [HKEY_CLASSES_ROOT\\Directory\\shell\\VSCode\\command] @=\u0026#34;\\\u0026#34;D:\\\\software\\\\Vscode\\\\Code.exe\\\u0026#34; \\\u0026#34;%V\\\u0026#34;\u0026#34; Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\VSCode] @=\u0026#34;Open with Code\u0026#34; \u0026#34;Icon\u0026#34;=\u0026#34;D:\\\\software\\\\Vscode\\\\Code.exe\u0026#34; [HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\VSCode\\command] @=\u0026#34;\\\u0026#34;D:\\\\software\\\\Vscode\\\\Code.exe\\\u0026#34; \\\u0026#34;%V\\\u0026#34;\u0026#34; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/vscode-common-problems/","summary":"1 remote ssh 使用密钥 remote ssh 远程服务器时每次都要求输入密码, 可以通过密钥实现免密登录 修改 ssh 配置文件, 一般在 C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\config, 配置文件的路径取决于 remote ssh 使用的配置文件, 在文件内添加私钥的路径即可, 如下 Host lvbibir.cn HostName lvbibir.cn User root IdentityFile C:\\Users\\lvbibir\\.ssh\\id_rsa 2 右键菜单问题 前端时间突然发现文件和目录的右键菜单中的 在 vscode 中打开 消失, 可以将下述代码保存为 .reg 文","title":"vscode | 常见问题"},{"content":"0 前言 在工作中需要连接公司内网（有线，不可联网），访问外网时需要连接无线\n同时接入这两个网络时，内网访问正常，外网无法访问。\n此时可以通过调整网络优先级及配置路由实现内外网同时访问\n一般来说，内网的网段数量较少，我们可以配置使默认路由走外网，走内网时通过配置的静态路由\n1 centos8 在 linux 系统中网络优先级是通过 metric 控制的，值越小，优先级越高，通过 route -n 查看路由\n可以通过修改配置文件实现，在网卡配置文件中添加或者修改 IPV4_ROUTE_METRIC=100 参数实现，之后重启网络服务\n# network systemctl restart network # NetworkManager nmcli c reload nmcli c down enp3s0 nmcli c up enp3s0 route -n 1.1 添加路由 临时添加静态路由命令如下（重启服务器或者重启网络服务后消失）\nroute add -net 192.168.45.0 netmask 255.255.255.0 dev enp4s0 metric 3 永久添加静态路由\n参照 /etc/init.d/network 中对 /etc/sysconfig/static-routes 是如何处理的\n/etc/sysconfig/static-routes 文件不存在的话，创建一个即可\n# Add non interface-specific static-routes. if [ -f /etc/sysconfig/static-routes ]; then if [ -x /sbin/route ]; then grep \u0026#34;^any\u0026#34; /etc/sysconfig/static-routes | while read ignore args ; do /sbin/route add -$args done else net_log $\u0026#34;Legacy static-route support not available: /sbin/route not found\u0026#34; fi fi 则，如果添加一条静态路由的路由如下\nroute add -net 192.168.45.0 netmask 255.255.255.0 dev enp4s0 metric 3 那么，在 /etc/sysconfig/static-routes 中对应的则应该写为\nany -net 192.168.45.0 netmask 255.255.255.0 dev enp4s0 metric 3 2 win10 2.1 调整网络优先级 查看默认路由\nroute print 0.0.0.0 这两个路由分别是内网和外网的默认路由，绝大部分情况网络都是走的默认路由，但这里有两条默认路由，默认路由的优先级是按照跃点数的多少决定的，跃点数越少，优先级越高\n将外网无线的跃点数调小\nroute print 可以看到跃点数修改成功了，此时外网无线的跃点数更小，优先级更高\n2.2 配置路由 配置路由需要以管理员权限运行 powershell 或者 cmd\n配置路由后，内网访问也没有问题了\nroute add 172.16.2.0 mask 255.255.255.0 172.30.4.254 metric 3 route add 172.16.3.0 mask 255.255.255.0 172.30.4.254 metric 3 route add 172.16.4.0 mask 255.255.255.0 172.30.4.254 metric 3 这里配置的路由重启系统后会消失，加 -p 选项设置为永久路由\nroute add -p 172.16.2.0 mask 255.255.255.0 172.30.4.254 metric 3 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/network-priority/","summary":"0 前言 在工作中需要连接公司内网（有线，不可联网），访问外网时需要连接无线 同时接入这两个网络时，内网访问正常，外网无法访问。 此时可以通过调整网络优先级及配置路由实现内外网同时访问 一般来说，内网的网段数量较少，我们可以配置使默认路由走外网，走内网时通过配置的静态路由 1 centos8 在 linux 系统中网络","title":"windows \u0026 linux 多网卡时设置默认路由以及添加静态路由"},{"content":"0 前言 本文参考以下链接:\n【MySQL】主从复制实现原理详解 基于 centos-7.9 mysql-5.7.42\nmysql 安装参考 mysql 系列文章\n1 主从复制原理 主从复制涉及到 3 个线程, 4 个文件\n1.1 线程 master:\nlog dump : 当 slave 连接 master 时, 主节点会为其创建一个 log dump 线程, 用于发送和读取 binlog 的内容. 在读取 binlog 中的操作时, log dump 线程会对主节点上的 binlog 加锁, 当读取完成, 在发送给从节点之前, 锁会被释放.\n主节点会为自己的每一个从节点创建一个 log dump 线程 .\nslave:\nIO : 接受 master 发送的 binlog 文件位置的副本. 然后将数据的更新记录到 relaylog 中. SQL : 负责读取 relaylog 中的内容, 解析成具体的操作并执行, 最终保证主从数据的一致性 1.2 文件 master:\nbinlog : 二进制文件, 记录库中的信息 slave:\nrelaylog : 中继日志, 用来同步 master 的 binlog relaylog.info : 记录文件复制的进度. master.info : 存放 master 信息, 以及上次读取到的 master 同步过来的 binlog 的位置 1.3 详细步骤 slave 执行 change master to 命令 ( master 的连接信息 + 复制的起点), 将以上信息记录到 master.info 文件 slave 执行 start slave 命令,开启 IO 线程和 SQL 线程 slave 的 IO 线程读取 master.info 文件中的信息, 获取到 IP, PORT, User, Pass, binlog 的位置信息 slave 的 IO 线程请求连接 master, master 提供一个 log dump 线程, 负责和 IO 线程交互 IO 线程根据 binlog 的位置信息 (mysql-bin.000004 , 444), 请求 master 新的 binlog master 通过 log dump 线程将最新的 binlog 传输给 slave 的 IO 线程 IO 线程接收到新的 binlog 日志, 存储到 TCP/IP 缓存, 立即返回 ACK 给 master , 并更新 master.info IO 线程将 TCP/IP 缓存中的数据, 转储到磁盘 relaylog 中 SQL 线程读取 relaylog.info 中的信息, 获取到上次已经应用过的 relaylog 的位置信息 SQL 线程会按照上次的位置点回放最新的 relaylog, 再次更新 relaylog.info 信息 slave 会自动 purge 应用过的 relaylog 进行定期清理 2 主从复制模式 mysql 默认一般为异步同步数据\n2.1 全同步 当 mster 执行完一个事务, 然后所有的 slave 都复制了该事务并成功执行完才返回成功信息给客户端. 因为需要等待所有 slave 执行完该事务才能返回成功信息, 所以全同步复制的性能必然会收到严重的影响.\n2.2 半同步 介于异步复制和全同步复制之间, master 在执行完客户端提交的事务后不是立刻返回给客户端, 而是等待至少一个 slave 接收到并写到 relaylog 中才返回成功信息给客户端 (只能保证 master 的 binlog 至少传输到了一个 slave 上, 但并不能保证 slave 将此事务执行更新到 db 中), 否则需要等待直到超时时间然后切换成异步模式再提交. 相对于异步复制, 半同步复制提高了数据的安全性, 一定程度的保证了数据能成功备份到 slave, 同时它也造成了一定程度的延迟, 但是比全同步模式延迟要低, 这个延迟最少是一个 TCP/IP 往返的时间. 所以, 半同步复制最好在低延时的网络中使用.\n2.3 异步 master 不会主动推送 binlog 到 slave, master 在执行完客户端提交的事务后会立即将结果返给给客户端, 并不关心 slave 是否已经接收并处理, 这样就会有一个问题, master 如果崩溃掉了, 此时 master 上已经提交的事务可能并没有传到 slave 上, 如果此时, 强行将 slave 提升为 master, 可能导致新 master 节点上的数据不完整.\n3 主从复制方式 MySQL 主从复制有三种方式: 基于 SQL 语句的复制 (statement-based replication, SBR), 基于行的复制 (row-based replication, RBR), 混合模式复制 (mixed-based replication, MBR). 对应的 bin-log 文件的格式也有三种: STATEMENT, ROW, MIXED\n可通过如下命令查看 binlog 格式\nSHOW VARIABLES LIKE \u0026#34;binlog_format\u0026#34;; 3.1 SBR 就是记录 sql 语句在 bin-log 中, Mysql 5.1.4 及之前的版本都是使用的这种复制格式. 优点是只需要记录会修改数据的 sql 语句到 bin-log 中, 减少了 bin-log 日质量, 节约 I/O, 提高性能. 缺点是在某些情况下, 会导致主从节点中数据不一致 (比如 sleep(), now() 等).\n3.2 RBR mysql master 将 SQL 语句分解为基于 Row 更改的语句并记录在 bin-log 中, 也就是只记录哪条数据被修改了, 修改成什么样. 优点是不会出现某些特定情况下的存储过程、或者函数、或者 trigger 的调用或者触发无法被正确复制的问题. 缺点是会产生大量的日志, 尤其是修改 table 的时候会让日志暴增,同时增加 bin-log 同步时间. 也不能通过 bin-log 解析获取执行过的 sql 语句, 只能看到发生的 data 变更.\n3.3 MBR MySQL NDB cluster 7.3 和 7.4 使用的 MBR. 是以上两种模式的混合, 对于一般的复制使用 STATEMENT 模式保存到 bin-log, 对于 STATEMENT 模式无法复制的操作则使用 ROW 模式来保存, MySQL 会根据执行的 SQL 语句选择日志保存方式.\n4 GTID 复制 在原来基于日志的复制中, slave 需要告知 master 要从哪个偏移量进行增量同步, 如果指定错误会造成数据的遗漏, 从而造成数据的不一致.\n而基于 GTID 的复制中, slave 会告知 master 已经执行的事务的 GTID 的值, 然后 master 会将所有未执行的事务的 GTID 的列表返回给 slave. 并且可以保证同一个事务只在指定的 slave 执行一次. 通过全局的事务 ID 确定 slave 要执行的事务的方式代替了以前需要用 binlog 和 pos 点确定 slave 要执行的事务的方式.\nGTID 是由 server_uuid 和事物 id 组成, 格式为: GTID=server_uuid:transaction_id. server_uuid 是在数据库启动过程中自动生成, 每台机器的 server-uuid 不一样. uuid 存放在数据目录的 auto.cnf 文件中，而 transaction_id 就是事务提交时系统顺序分配的一个不会重复的序列号\nmaster 更新数据时, 会在事务前产生 GTID, 一起记录到 binlog 日志中. slave 的 IO 线程将变更的 binlog 写入到本地的 relaylog 中. SQL 线程从 relaylog 中获取 GTID, 然后对比本地 binlog 是否有记录 (所以 slave 必须要开启 binlog, 并且将 log_slave_updates 设置为 ON). 如果有记录，说明该 GTID 的事务已经执行, slave 会忽略. 如果没有记录, slave 就会从 relaylog 中执行该 GTID 的事务, 并记录到 binlog. 在解析过程中会判断是否有主键, 如果没有就用二级索引, 如果有就用全部扫描.\n5 并行复制 master 大多数情况下都是多线程多客户端去写, 而 slave 只有一个 SQL 线程进行写, 无法避免地会出现主从复制的延迟问题, 并行复制可以指定线程数量, 从而提高 slave 写的速度.\n在 mysql 5.6 版本之后引入了并行复制的概念\n通过上图我们可以发现其实所谓的并行复制, 就是在中间添加了一个分发的环节, 也就是说原来的 SQL 线程变成了现在的 coordinator 组件, 当 relaylog 日志更新后, coordinator 负责读取日志信息以及分发事务, 真正的执行过程是放在了 worker 线程上, 由多个线程并行的去执行.\n# 查看并行的slave的线程的个数，默认是0.表示单线程 show global variables like \u0026#39;slave_parallel_workers\u0026#39;; # 根据实际情况保证开启多少线程 set global slave_parallel_workers = 4; # 设置并发复制的方式，默认是一个线程处理一个库，值为database show global variables like \u0026#39;%slave_parallel_type%\u0026#39;; # 停止slave stop slave; # 设置属性值 set global slave_parallel_type=\u0026#39;logical_check\u0026#39;; # 开启slave start slave # 查看线程数 show full processlist; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/mysql-2-master-slave/","summary":"0 前言 本文参考以下链接: 【MySQL】主从复制实现原理详解 基于 centos-7.9 mysql-5.7.42 mysql 安装参考 mysql 系列文章 1 主从复制原理 主从复制涉及到 3 个线程, 4 个文件 1.1 线程 master: log dump : 当 slave 连接 master 时, 主节点会为其创建一个 log dump 线程, 用于发送和读取 binlog 的内容. 在读取 binlog 中的操作时, log dump 线程会对主节点上的 binlog 加锁, 当读取完成, 在发","title":"mysql (二) 主从复制原理 GTID 并行复制"},{"content":"0 前言 基于 centos-7.9 mysql-5.7.42\n1 基础环境 配置 hostname\nhostnamectl set-hostname master bash 关闭防火墙及 selinux\niptables -F systemctl disable firewalld systemctl stop firewalld sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 配置 yum 源\nmkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo # curl http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel.repo yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion ntpdate timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com 卸载自带的 mariadb\nyum remove -y $(rpm -qa | grep mariadb) 2 配置 yum 源 提供清华源和官方源两种方式, 任选其一, 前者速度稍快一些\n2.1 清华源 这里使用的是清华的源\ncat \u0026gt; /etc/yum.repos.d/mysql-community.repo \u0026lt;\u0026lt; EOF [mysql-connectors-community] name=MySQL Connectors Community baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-connectors-community-el7-\\$basearch/ enabled=1 gpgcheck=1 gpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql [mysql-tools-community] name=MySQL Tools Community baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7-\\$basearch/ enabled=1 gpgcheck=1 gpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql [mysql-5.7-community] name=MySQL 5.7 Community Server baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-5.7-community-el7-\\$basearch/ enabled=1 gpgcheck=1 gpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql EOF 2.2 官方源 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm yum localinstall mysql57-community-release-el7-8.noarch.rpm 3 安装 mysql 执行安装\nrpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022 yum install mysql-community-server 启动服务\nsystemctl enable --now mysqld 获取默认密码\n# grep \u0026#34;password\u0026#34; /var/log/mysqld.log 2023-05-07T02:31:54.375626Z 1 [Note] A temporary password is generated for root@localhost: QZFuIayXk0:l 如果没有返回，找不到 root 密码，解决方案如下\n# 删除原来安装过的mysql残留的数据 rm -rf /var/lib/mysql # 重启 mysqld 服务, 会重新初始化数据 systemctl restart mysqld # 再去找临时密码 grep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log 登录并修改密码\nmysql -uroot -p # 输入默认密码 ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;\u0026lt;pass\u0026gt;\u0026#39;; # 修改密码, 需要有大小写和特殊符号 exit; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/mysql-1-deploy/","summary":"0 前言 基于 centos-7.9 mysql-5.7.42 1 基础环境 配置 hostname hostnamectl set-hostname master bash 关闭防火墙及 selinux iptables -F systemctl disable firewalld systemctl stop firewalld sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 配置 yum 源 mkdir /etc/yum.repos.d/bak || true mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ || true curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo # curl http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel.repo yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion ntpdate timedatectl set-timezone Asia/Shanghai ntpdate time.windows.com 卸载自带的 mariadb yum remove -y $(rpm -qa | grep mariadb) 2 配置 yum 源 提供清华源和官方源两种方式, 任选其一, 前者速度稍快一些 2.1 清华源 这里使用的","title":"mysql (一) 部署"},{"content":"0 前言 基于 centos-7.9 mysql-5.7.42\n","permalink":"https://www.lvbibir.cn/en/posts/tech/mysql-0-notes/","summary":"0 前言 基于 centos-7.9 mysql-5.7.42","title":"mysql | 杂记"},{"content":"0 前言 基础环境\n系统：Centos 7.9.2009 minimal 配置：4 cpus / 24G mem / 50G disk 网卡：1.1.1.4/24 我这里采用的是 all-in-one 的配置，即所有操作都在一台主机上，如资源充足可以将 jenkins 和 gitlab 与后续项目容器分开部署\n1 系统配置 防火墙、selinux、yum\nsed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo yum clean all yum makecache fast yum install -y wget net-tools vim bash-completion unzip mkdir /mydata 2 docker 先安装 docker-compose\nwget https://github.com/docker/compose/releases/download/v2.16.0/docker-compose-linux-x86_64 -O /usr/local/bin/docker-compose chmod a+x /usr/local/bin/docker-compose ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose docker-compose version 安装 docker\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo yum -y install docker-ce mkdir -p /etc/docker # 镜像加速器 tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34;] } EOF 允许 docker 守护进程的 tcp 访问，为了后续 jenkins 构建时调用，以生成 docker 镜像\n[root@localhost ~]# vim /usr/lib/systemd/system/docker.service # 修改如下内容 # ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock systemctl daemon-reload systemctl restart docker \u0026amp;\u0026amp; systemctl enable docker 查看端口，确保修改正确\n[root@localhost ~]# ss -tnlp | grep 2375 LISTEN 0 128 [::]:2375 [::]:* users:((\u0026#34;dockerd\u0026#34;,pid=1124,fd=4)) 安装一系列后续需要的镜像，镜像文件比较大，这步比较耗时\ndocker pull jenkins/jenkins:lts docker pull gitlab/gitlab-ce:latest docker pull mysql:5.7 docker pull redis:7 docker pull nginx:1.22 docker pull rabbitmq:3.9-management docker pull elasticsearch:7.17.3 docker pull logstash:7.17.3 docker pull kibana:7.17.3 docker pull mongo:4 docker pull nacos/nacos-server:v2.1.0 3 jenkins 3.1 启动容器 docker run -d --restart=always \\ -p 8080:8080 -p 50000:5000 \\ --name jenkins -u root \\ -v /mydata/jenkins_home:/var/jenkins_home \\ jenkins/jenkins:lts # 获取初始管理员密码 [root@localhost ~]# cat /mydata/jenkins_home/secrets/initialAdminPassword bd5b64c7c8c8467985a0faa6fbe1848f 3.2 跳过在线验证 启动成功访问 http://1.1.1.4:8080 ，等出现密码界面后输入密码应该会进入一个离线页面，如下\n❗ 这个界面不要关，新开一个窗口访问 http://1.1.1.4:8080/pluginManager/advanced\n将 update site 的 url 修改为 http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json，这步是为了加速插件安装\n接下来跳过 jenkins 的在线验证，在终端再执行\ndocker exec -it jenkins /bin/sh -c \u0026#34;echo 127.0.0.1 www.google.com \u0026gt;\u0026gt; /etc/hosts\u0026#34; docker exec -it jenkins cat /etc/hosts 然后回到第一个离线页面刷新一下，应该可以看到离线状态消除了，这里是因为 jenkins 在 /mydata/jenkins_home/updates/default.json 中定义了通过访问 google 来判断 jenkins 节点是否是在线状态\n之后选择安装推荐的插件，进入插件安装界面，这个过程耗时会比较长，如果有插件安装失败可以重试\n之后创建管理员用户，一路确定后到主页\n3.3 插件配置 dashboard -\u0026gt; 系统管理 -\u0026gt; 插件管理中安装 ssh 插件和 Role-based Authorization Strategy 插件，安装完成后重启 jenkins\n新增 ssh 凭据\n新增 ssh 配置，配置好之后右下角测试一下，连接正常后保存\n新增 maven 配置\n3.4 权限配置 我们可以使用 Jenkins 的角色管理插件来管理 Jenkins 的用户，比如我们可以给管理员赋予所有权限，运维人员赋予执行任务的相关权限，其他人员只赋予查看权限。\n在系统管理 -\u0026gt; 全局安全配置中启用基于角色的权限管理：\n关闭代理，保存\n分配管理员、运维和 other 三个角色，分别配置对应权限\n将用户和角色绑定\n4 gitlab 4.1 启动容器 docker run --detach --restart=always\\ -p 10443:443 -p 1080:80 -p 1022:22 \\ --name gitlab \\ --restart always \\ --volume /mydata/gitlab/config:/etc/gitlab \\ --volume /mydata/gitlab/logs:/var/log/gitlab \\ --volume /mydata/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest # 获取密码 docker exec -it gitlab grep \u0026#39;Password:\u0026#39; /etc/gitlab/initial_root_password 访问 http://1.1.1.4:1080/, 默认用户为 root\n4.2 配置 配置中文，修改完后刷新网页即可\n修改默认密码\n4.3 上传项目 新建空白项目\n新建 mall-swarm 项目\nclone github 上的原项目，我是 windows 系统，所以这里用的是 git-bash\ngit clone https://github.com/macrozheng/mall-swarm.git cd mall-swarm # 重命名github远端仓库 git remote rename origin github # 添加gitlab仓库 git remote add gitlab http://1.1.1.4:1080/root/mall-swarm.git git remote -v 修改一下 docker.host 变量\n新建 commit 并提交到 gitlab 仓库，初次提交需要输入 gitlab 的用户名密码\ngit add . git commit -m \u0026#34;change docker.host -\u0026gt; 1.1.1.4\u0026#34; git push gitlab master 默认配置不合理，修改 docker-compose-env.yml 中 nginx 的配置文件挂载\n- /data/nginx/nginx.conf:/etc/nginx/nginx.conf #配置文件挂载 上传到 gitlab\ngit add . git commit -m \u0026#34;update nginx volume config in document/docker/docker-compose.env.yml\u0026#34; git push gitlab master 5 依赖服务部署 需要上传到服务器的配置文件准备，如下图所示，为了方便可以将整个 document 目录传到服务器\n5.1 前期配置 Elasticsearch\n设置内核参数，否则会因为内存不足无法启动 sysctl -w vm.max_map_count=262144 sysctl -p 创建数据目录并设置权限，否则会报权限错误 mkdir -p /mydata/elasticsearch/data/ chmod 777 /mydata/elasticsearch/data/ Nginx\n创建目录，上传配置文件 mkdir -p /mydata/nginx/conf/ cp /mydata/document/docker/nginx.conf /mydata/nginx/conf/ Logstash\n创建目录上传配置文件 mkdir /mydata/logstash cp /mydata/document/elk/logstash.conf /mydata/logstash/ 5.2 启动服务 docker-compose -f /mydata/document/docker/docker-compose-env.yml up -d docker-compose 会自动创建一个 docker_default 网络，所有容器都在这个网络下\n启动完成后 rabbitmq 由于权限问题未能正常启动，给 log 目录设置权限，再执行 docker-compose 启动异常的容器\nchmod 777 /mydata/rabbitmq/log/ docker-compose -f /mydata/document/docker/docker-compose-env.yml up -d 确保所有容器正常启动\ndocker ps | grep -v \u0026#34;Up\u0026#34; 5.3 服务配置 mysql\n需要创建 mall 数据库并授权给 reader 用户\n将 sql 文件拷贝到容器 docker cp /mydata/document/sql/mall.sql mysql:/ 进入 mysql 容器执行如下操作 # 进入mysql容器 docker exec -it mysql /bin/bash # 连接到mysql服务 mysql -uroot -proot --default-character-set=utf8 # 创建远程访问用户 grant all privileges on *.* to \u0026#39;reader\u0026#39; @\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; # 创建mall数据库 create database mall character set utf8; # 使用mall数据库 use mall; # 导入mall.sql脚本 source /mall.sql; # 退出数据库 exit # 退出容器 ctrl + d Elasticsearch\n需要安装中文分词器 IKAnalyzer 下载地址 注意版本需要与 elasticsearch 的版本一致\n上传到服务器并解压到 plugins 目录 mkdir /mydata/elasticsearch/plugins/analysis-ik unzip /mydata/elasticsearch-analysis-ik-7.17.3.zip -d /mydata/elasticsearch/plugins/analysis-ik/ 重启容器 docker restart elasticsearch Logstash\n安装 json_lines 插件并重启 docker exec -it logstash /bin/bash logstash-plugin install logstash-codec-json_lines docker restart logstash rabbitmq\n需要创建一个 mall 用户并设置虚拟 host 为/mall\n访问管理页面: http://1.1.1.4:15672/ 默认账户密码: guest / guest 创建管理员用户: mall / mall 创建一个新的虚拟 host 为 /mall 点击 mall 用户进入用户配置界面 给 mall 账户配置虚拟 host /mall 的权限 nacos\n由于我们使用 Nacos 作为配置中心，统一管理配置，所以我们需要将项目 config 目录下的所有配置都添加到 Nacos 中 Nacos 访问地址：http://1.1.1.4:8848/nacos/ 账号密码：nacos / nacos\n需要上传的配置\n上传配置 全部上传完成 6 jenkins 手动发布项目 6.1 脚本配置 Jenkins 自动化部署是需要依赖 Linux 执行脚本的\n添加执行权限\nchmod a+x /mydata/document/sh/*.sh 之前使用的是 Docker Compose 启动所有依赖服务，会默认创建一个网络，所有的依赖服务都会在此网络之中，不同网络内的服务无法互相访问。所以需要指定 sh 脚本中服务运行的的网络，否则启动的应用服务会无法连接到依赖服务。\n修改脚本内容，为每个脚本添加 --network docker_default \\\nsed -i \u0026#39;/^docker run/ a\\--network docker_default \\\\\u0026#39; /mydata/document/sh/*.sh 确认修改是否成功\n6.2 jenkins 配置 6.2.1 mall-admin 工程配置 由于各个模块执行任务的创建都大同小异，下面将详细讲解 mall-admin 模块任务的创建，其他模块将简略讲解。\n源码管理\n创建一个构建，构建 mall-swarm 项目中的依赖模块，否则当构建可运行的服务模块时会因为无法找到这些模块而构建失败\n# 只install mall-common,mall-mbg两个模块 clean install -pl mall-common,mall-mbg -am 创建一个构建，单独构建并打包 mall-admin 模块\nclean package ${WORKSPACE}/mall-admin/pom.xml 再创建一个构建，通过 SSH 去执行 sh 脚本，这里执行的是 mall-admin 的运行脚本：\n6.2.2 其他模块工程配置 以 mall-gateway 为例\n输入任务名称，直接复制 mall-admin 工程配置\n修改第二步构建中的 pom 文件位置和第三步构建中的 sh 文件位置\n6.3 开始构建 单击开始构建即可开始构建任务，可以实时看到任务的控制台输出\n由于作为注册中心和配置中心的 Nacos 已经启动了，其他模块基本没有启动顺序的限制，但是最好还是按照下面的顺序启动。\n推荐启动顺序：\nmall-auth mall-gateway mall-monitor mall-admin mall-portal mall-search 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/cicd-mall-swarm/","summary":"0 前言 基础环境 系统：Centos 7.9.2009 minimal 配置：4 cpus / 24G mem / 50G disk 网卡：1.1.1.4/24 我这里采用的是 all-in-one 的配置，即所有操作都在一台主机上，如资源充足可以将 jenkins 和 gitlab 与后续项目容器分开部署 1 系统配置 防火墙、selinux、yum sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ curl http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/Centos-Base.repo sed -i \u0026#39;/aliyuncs.com/d\u0026#39; /etc/yum.repos.d/Centos-Base.repo yum clean all yum","title":"cicd | jenkins 部署 mall-swarm 项目"},{"content":"系统版本：isoft-serveros-v4.2（centos7）\n源码下载链接：\nhttps://dlcdn.apache.org//apr/apr-1.7.0.tar.bz2 https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 https://dlcdn.apache.org//httpd/httpd-2.4.52.tar.bz2 安装依赖\nyum install -y wget gcc rpm-build yum install -y autoconf zlib-devel libselinux-devel libuuid-devel apr-devel apr-util-devel pcre-devel openldap-devel lua-devel libxml2-devel openssl-devel yum install -y libtool doxygen yum install -y postgresql-devel mysql-devel sqlite-devel unixODBC-devel nss-devel libdb4-devel 依赖需要使用 epel 源安装，这里使用阿里的 epel 源\n# 添加阿里yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 手动修改repo文件中的系统版本，因为本系统检测到的版本号是4 sed -i \u0026#39;s/$releasever/7/g\u0026#39; /etc/yum.repos.d/CentOS-Base.repo # 安装epel源 yum install -y epel-release # 安装libdb4-devel yum install -y libdb4-devel 编译准备\n[root@localhost ~]# mkdir -p /root/rpmbuild/{SPECS,SOURCES} [root@localhost ~]# cd /root/rpmbuild/SOURCES/ [root@localhost SOURCES]# wget --no-check-certificate https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 [root@localhost SOURCES]# tar jxf apr-1.7.0.tar.bz2 [root@localhost SOURCES]# tar jxf apr-util-1.6.1.tar.bz2 [root@localhost SOURCES]# tar jxf httpd-2.4.52.tar.bz2 [root@localhost SOURCES]# cp apr-1.7.0/apr.spec ../SPECS/ [root@localhost SOURCES]# cp apr-util-1.6.1/apr-util.spec ../SPECS/ [root@localhost SOURCES]# cp httpd-2.4.52/httpd.spec ../SPECS/ 开始编译\n[root@localhost SOURCES]# cd ../SPECS/ # 修改spec文件 [root@localhost SPECS]# vim apr.spec Release: 1%dist [root@localhost SPECS]# vim apr-util.spec Release: 1%dist [root@localhost SPECS]# vim httpd.spec Release: 1%dist [root@localhost SPECS]# rpmbuild -ba apr.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/apr-* [root@localhost SPECS]# rpmbuild -ba apr-util.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/apr-util-* [root@localhost SPECS]# rpmbuild -ba httpd.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/httpd-* [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/mod_* # 打包所有的软件包 [root@localhost ~]# tar zcf httpd-2.4.25.tar.gz x86_64/ 这里修改%dist 是为了修改编译后生成的软件包的名字，dist 具体代表什么可以在/etc/rpm/macros.dist 文件中看到\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-build-httpd/","summary":"系统版本：isoft-serveros-v4.2（centos7） 源码下载链接： https://dlcdn.apache.org//apr/apr-1.7.0.tar.bz2 https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 https://dlcdn.apache.org//httpd/httpd-2.4.52.tar.bz2 安装依赖 yum install -y wget gcc rpm-build yum install -y autoconf zlib-devel libselinux-devel libuuid-devel apr-devel apr-util-devel pcre-devel openldap-devel lua-devel libxml2-devel openssl-devel yum install -y libtool doxygen yum install -y postgresql-devel mysql-devel sqlite-devel unixODBC-devel nss-devel libdb4-devel 依赖需要使用 epel 源安装，这里使用阿里的 epel 源 # 添加阿里yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 手动修改repo文件中的系统版本，因为本系统检测到","title":"httpd 源码打包编译成 rpm 包"},{"content":"1 环境 iSoftserver-v4.2(Centos-7)\nopenssl version：1.0.2k\n2 编译 从 github 上看到的编译脚本，本地修改后：\n#!/bin/bash set -e set -v mkdir ~/openssl \u0026amp;\u0026amp; cd ~/openssl yum -y install \\ curl \\ which \\ make \\ gcc \\ perl \\ perl-WWW-Curl \\ rpm-build # Get openssl tarball cp /root/openssl-1.1.1m.tar.gz ./ # SPEC file cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; ~/openssl/openssl.spec Summary: OpenSSL 1.1.1m for Centos Name: openssl Version: %{?version}%{!?version:1.1.1m} Release: 1%{?dist} Obsoletes: %{name} \u0026lt;= %{version} Provides: %{name} = %{version} URL: https://www.openssl.org/ License: GPLv2+ Source: https://www.openssl.org/source/%{name}-%{version}.tar.gz BuildRequires: make gcc perl perl-WWW-Curl BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %global openssldir /usr/openssl %description OpenSSL RPM for version 1.1.1m on Centos %package devel Summary: Development files for programs which will use the openssl library Group: Development/Libraries Requires: %{name} = %{version}-%{release} %description devel OpenSSL RPM for version 1.1.1m on Centos (development package) %prep %setup -q %build ./config --prefix=%{openssldir} --openssldir=%{openssldir} make %install [ \u0026#34;%{buildroot}\u0026#34; != \u0026#34;/\u0026#34; ] \u0026amp;\u0026amp; %{__rm} -rf %{buildroot} %make_install mkdir -p %{buildroot}%{_bindir} mkdir -p %{buildroot}%{_libdir} ln -sf %{openssldir}/lib/libssl.so.1.1 %{buildroot}%{_libdir} ln -sf %{openssldir}/lib/libcrypto.so.1.1 %{buildroot}%{_libdir} ln -sf %{openssldir}/bin/openssl %{buildroot}%{_bindir} %clean [ \u0026#34;%{buildroot}\u0026#34; != \u0026#34;/\u0026#34; ] \u0026amp;\u0026amp; %{__rm} -rf %{buildroot} %files %{openssldir} %defattr(-,root,root) /usr/bin/openssl /usr/lib64/libcrypto.so.1.1 /usr/lib64/libssl.so.1.1 %files devel %{openssldir}/include/* %defattr(-,root,root) %post -p /sbin/ldconfig %postun -p /sbin/ldconfig EOF mkdir -p /root/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} cp ~/openssl/openssl.spec /root/rpmbuild/SPECS/openssl.spec mv openssl-1.1.1m.tar.gz /root/rpmbuild/SOURCES cd /root/rpmbuild/SPECS \u0026amp;\u0026amp; \\ rpmbuild \\ -D \u0026#34;version 1.1.1m\u0026#34; \\ -ba openssl.spec # Before Uninstall Openssl : rpm -qa openssl # Uninstall Current Openssl Vesion : yum -y remove openssl # For install: rpm -ivvh /root/rpmbuild/RPMS/x86_64/openssl-1.1.1m-1.el7.x86_64.rpm --nodeps # Verify install: rpm -qa openssl # openssl version 运行脚本\nchmod 755 install_openssl-1.1.1m.sh ./isntall_openssl-1.1.1m.sh tree rpmbuild/*RPMS 3 升级 rpm -e openssl --nodeps rpm -ivh openssl-1.1.1m-1.el7.isoft.x86_64.rpm --nodeps openssl version 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-build-openssl/","summary":"1 环境 iSoftserver-v4.2(Centos-7) openssl version：1.0.2k 2 编译 从 github 上看到的编译脚本，本地修改后： #!/bin/bash set -e set -v mkdir ~/openssl \u0026amp;\u0026amp; cd ~/openssl yum -y install \\ curl \\ which \\ make \\ gcc \\ perl \\ perl-WWW-Curl \\ rpm-build # Get openssl tarball cp /root/openssl-1.1.1m.tar.gz ./ # SPEC file cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; ~/openssl/openssl.spec Summary: OpenSSL 1.1.1m for Centos Name: openssl Version: %{?version}%{!?version:1.1.1m} Release: 1%{?dist} Obsoletes: %{name} \u0026lt;= %{version} Provides: %{name} = %{version} URL: https://www.openssl.org/ License: GPLv2+ Source: https://www.openssl.org/source/%{name}-%{version}.tar.gz BuildRequires: make gcc perl perl-WWW-Curl BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %global openssldir /usr/openssl %description OpenSSL RPM for version 1.1.1m on Centos %package devel Summary: Development files for programs which will use the openssl library Group: Development/Libraries Requires:","title":"openssl 源码打包编译成 rpm 包"},{"content":"0 前言 在使用 kolla-ansible 部署多节点 openstack 时，所有节点的外网网卡名称和管理网卡名称需要一样，其中两台是型号相同的物理机，网卡名无需变动，第三台是虚拟机，默认是 ens* 形式的网卡，需要改成 enp*s*f* 的格式\n本文参考以下链接:\nHow to change a network interface name on CentOS 7 1 修改配置文件 vim /etc/sysconfig/network-scripts/ifcfg-ens32 3 配置网络规则命名文件 vim /etc/udev/rules.d/70-persistent-ipoib.rules # 添加如下行，mac 地址自行修改 SUBSYSTEM==\u0026#34;net\u0026#34;, ACTION==\u0026#34;add\u0026#34;, DRIVERS==\u0026#34;?*\u0026#34;, ATTR{address}==\u0026#34;00:0c:29:bc:1e:01\u0026#34;, ATTR{type}==\u0026#34;1\u0026#34;, KERNEL==\u0026#34;eth*\u0026#34;, NAME=\u0026#34;enp11s0f0\u0026#34; 4 配置 grub 并重启 vim /etc/default/grub # 修改如下行 GRUB_CMDLINE_LINUX=\u0026#34;crashkernel=auto rd.lvm.lv=centos/root net.ifnames=0 rd.lvm.lv=centos/swap rhgb quiet\u0026#34; grub2-mkconfig -o /boot/grub2/grub.cfg 之后直接 reboot 重启系统\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-change-network-card-name/","summary":"0 前言 在使用 kolla-ansible 部署多节点 openstack 时，所有节点的外网网卡名称和管理网卡名称需要一样，其中两台是型号相同的物理机，网卡名无需变动，第三台是虚拟机，默认是 ens* 形式的网卡，需要改成 enp*s*f* 的格式 本文参考以下链接: How to change a network interface name on CentOS 7 1 修改配置文件 vim /etc/sysconfig/network-scripts/ifcfg-ens32 3 配置网络规则命名文件 vim /etc/udev/rules.d/70-persistent-ipoib.rules # 添加如下行，mac 地址自","title":"centos7 | 修改网卡名称"},{"content":"0 前言 本文参考以下链接\nCeph 存储池 pg_num 配置详解 1 pg_num 用此命令创建存储池时：\nceph osd pool create {pool-name} pg_num 确定 pg_num 取值是强制性的，因为不能自动计算。常用的较为通用的取值：\n少于 5 个 osd，pg_num 设置为 128 osd 数量在 5 到 10 个时，pg_num 设置为 512 osd 数量在 10 到 50 个时，pg_num = 4096 osd 数量大于 50 是，需要理解 ceph 的权衡算法，自己计算 pg_num 取值 自行计算 pg_num 取值时可使用 ceph 配套的 pg_num 取值工具 pgcalc ","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-pg-num-config-create-pool/","summary":"0 前言 本文参考以下链接 Ceph 存储池 pg_num 配置详解 1 pg_num 用此命令创建存储池时： ceph osd pool create {pool-name} pg_num 确定 pg_num 取值是强制性的，因为不能自动计算。常用的较为通用的取值： 少于 5 个 osd，pg_num 设置为 128 osd 数量在 5 到 10 个时，pg_num 设置为 512 osd 数量在 10 到 50 个时，pg_num = 4096 osd 数量大于 50 是，需要理解 ceph 的","title":"ceph | pool pg_num 配置"},{"content":"0 前言 本文参考以下链接:\n小工具: 批量导入导出主机上的 docker 镜像 1 python 批量导出，运行后所有 tar 包都在当前目录下\n# encoding: utf-8 import re import os import subprocess if __name__ == \u0026#34;__main__\u0026#34;: p = subprocess.Popen(\u0026#39;docker images\u0026#39;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines(): # 此处的正则表达式是为了匹配镜像名以kolla为开头的镜像 # 实际使用中根据需要自行调整 m = re.match(r\u0026#39;(^kolla[^\\s]*\\s*)\\s([^\\s]*\\s)\u0026#39;, line) if not m: continue # 镜像名 iname = m.group(1).strip() # tag itag = m.group(2).strip() # tar包的名字 if iname.find(\u0026#39;/\u0026#39;): tarname = iname.split(\u0026#39;/\u0026#39;)[0] + \u0026#39;_\u0026#39; + iname.split(\u0026#39;/\u0026#39;)[-1] + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; else: tarname = iname + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; print tarname ifull = iname + \u0026#39;:\u0026#39; + itag #save cmd = \u0026#39;docker save -o \u0026#39; + tarname + \u0026#39; \u0026#39; + ifull print os.system(cmd) retval = p.wait() 批量导入，同理导入当前目录下的所有的 tar 包\nimport os images = os.listdir(os.getcwd()) for imagename in images: if imagename.endswith(\u0026#39;.tar\u0026#39;): print(imagename) os.system(\u0026#39;docker load -i %s\u0026#39;%imagename) 2 bash 2.1 导出 #!/bin/bash docker images \u0026gt; images.txt awk \u0026#39;{print $1}\u0026#39; images.txt \u0026gt; images_cut.txt sed -i \u0026#39;1d\u0026#39; images_cut.txt while read LINE do docker save $LINE \u0026gt; ${LINE//\\//_}.train.tar echo ok done \u0026lt; images_cut.txt echo finish 2.2 导入 #!/bin/bash while read LINE do docker load -i $LINE echo ok done \u0026lt; tarname.txt echo finish 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-import-export-image/","summary":"0 前言 本文参考以下链接: 小工具: 批量导入导出主机上的 docker 镜像 1 python 批量导出，运行后所有 tar 包都在当前目录下 # encoding: utf-8 import re import os import subprocess if __name__ == \u0026#34;__main__\u0026#34;: p = subprocess.Popen(\u0026#39;docker images\u0026#39;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines(): # 此处的正则表达式是为了匹配镜像名以kolla为开头的镜像 # 实际使用中根据需要自行调整 m = re.match(r\u0026#39;(^kolla[^\\s]*\\s*)\\s([^\\s]*\\s)\u0026#39;, line) if not m: continue # 镜像名 iname = m.group(1).strip() # tag itag = m.group(2).strip() # ta","title":"docker | 脚本方式批量导出/导入镜像"},{"content":"pam 模块\npam：Pluggable Authentication Modules 可插拔的认证模块，linux 中的认证方式，“可插拔的”说明可以按需对认证内容进行变更。与 nsswitch 一样，也是一个通用框架。只不过是提供认证功能的。\n查看密码失败次数\npam_tally2 -u root # 或者 faillock --user root 重置密码失败次数\npam_tally2 -r -u root # 或者 faillock --user root --reset 具体取决于在规则文件中使用的是 pam_faillock.so 模块还是 pam_tally2.so 模块\n例：\ncat /etc/pam.d/system-auth ","permalink":"https://www.lvbibir.cn/en/posts/tech/centos-too-many-password-attempts/","summary":"pam 模块 pam：Pluggable Authentication Modules 可插拔的认证模块，linux 中的认证方式，“可插拔的”说明可以按需对认证内容进行变更。与 nsswitch 一样，也是一个通用框架。只不过是提供认证功能的。 查看密码失败次数 pam_tally2 -u root # 或者 faillock --user root 重置密码失败次数 pam_tally2 -r -u root # 或者 faillock --user root --reset 具体取决于在规则文件中使用的是 pam_faillock.so 模","title":"centos 密码尝试次数过多问题处理"},{"content":"0 前言 内核版本介绍：\nlt longterm 的缩写 长期维护版 ml mainline 的缩写 最新稳定版 本文参考以下链接:\nLinux 离线升级内核 elrepo kernel rpm packge directory 1 升级内核 查看内核版本\n[dpl@test1 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo) 使用 wget 命令下载内核 RPM 包\n[dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-5.10.81-1.el7.x86_64.rpm [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 安装内核\nyum localinstall -y kernel-ml-5.10.81-1.el7.x86_64.rpm kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 查看所有可用内核启动项\n[dpl@test1 ~] awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg 0 : CentOS Linux (5.10.81-1.el7.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.21.1.el7.x86_64) 7 (Core) 2 : CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) 3 : CentOS Linux (0-rescue-9a4efd5deb094f5d8a9a259066ff4f3d) 7 (Core) 记下 5.10.81 内核前面的序号，修改启动项需要\n修改默认启动项\n默认启动项由 /etc/default/grub 中的 GRUB_DEFAULT 控制，如果 GRUB_DEFAULT=saved，则该参数将存在 /boot/grub2/grubenv\n输入 grub2-editenv list 命令查看默认启动项\n[root@localhost ~]# grub2-editenv list saved_entry=CentOS Linux (3.10.0-1060.el7.x86_64) 7 (Core) 输入 grub2-set-default 命令修改默认启动项，0 表示 5.10.81 内核的序号\n[dpl@test1 ~]# grub2-set-default 0 再次查看默认启动项，发现默认启动项已经改成了 0\n[dpl@test1 ~]# grub2-editenv list saved_entry=0 重启后再次查看内核版本\n[dpl@test1 ~]# uname -r 5.10.81-1.el7.elrepo.x86_64 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-update-kernel-to-5.10/","summary":"0 前言 内核版本介绍： lt longterm 的缩写 长期维护版 ml mainline 的缩写 最新稳定版 本文参考以下链接: Linux 离线升级内核 elrepo kernel rpm packge directory 1 升级内核 查看内核版本 [dpl@test1 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo) 使用 wget 命令下载内核 RPM 包 [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-5.10.81-1.el7.x86_64.rpm [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 安装内核 yum localinstall -y kernel-ml-5.10.81-1.el7.x86_64.rpm kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 查看所有可用内核启动项 [dpl@test1 ~] awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg 0 : CentOS Linux (5.10.81-1.el7.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.21.1.el7.x86_64) 7 (Core)","title":"centos7 | 升级内核至 5.10"},{"content":"0 前言 本文参考以下链接:\nBug 1647621 - Xorg listening on port 6000 by default in 7.6 基于 CVE-1999-0526 漏洞的披露，对系统 X 服务的 6000 端口进行关闭\n有三种方式：\n修改系统/usr/bin/X 内容，增加 nolisten 参数 开启系统防火墙，关闭 6000 端口的对外访问 禁用桌面 (runlevel-5)，开机进入字符界面 (runlevel-3) 1 修改 /usr/bin/X 脚本 1.1 关闭 rm -f /usr/bin/X vim /usr/bin/X ################### 添加如下内容 #!/bin/bash exec /usr/bin/Xorg \u0026#34;$@\u0026#34; -nolisten tcp exit 0 #################### chmod 777 /usr/bin/X kill -9 进程号 # ps -elf |grep X 显示的进程号 1.2 恢复 rm -f /usr/bin/X ln -s /usr/bin/Xorg /usr/bin/X kill -9 进程号 # pe -elf | grep Xorg 显示的进程号 在测试过程中出现过杀死 X 服务进程后没有自启的情况，可尝试使用 init 3 \u0026amp;\u0026amp; init 5 尝试重新启动 X 服务\n2 修改防火墙方式 # 开启除6000端口以外的所有端口(6000端口无法访问) systemctl start firewalld firewall-cmd --permanent --zone=public --add-port=1-65535/udp firewall-cmd --permanent --zone=public --add-port=1-5999/tcp firewall-cmd --permanent --zone=public --add-port=6001-65535/tcp firewall-cmd --reload firewall-cmd --list-all # 恢复（6000端口可以访问） firewall-cmd --permanent --zone=public --add-port=6000/tcp firewall-cmd --reload firewall-cmd --list-all 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/cve-1999-0526/","summary":"0 前言 本文参考以下链接: Bug 1647621 - Xorg listening on port 6000 by default in 7.6 基于 CVE-1999-0526 漏洞的披露，对系统 X 服务的 6000 端口进行关闭 有三种方式： 修改系统/usr/bin/X 内容，增加 nolisten 参数 开启系统防火墙，关闭 6000 端口的对外访问 禁用桌面 (runlevel-5)，开机进入字符界面 (runlevel-3) 1 修改 /usr/bin/X 脚本 1.1 关闭 rm -f /usr/bin/X vim /usr/bin/X ################### 添加如下内容","title":"CVE-1999-0526"},{"content":"0 前言 本文参考以下链接:\nCeph-deploy 快速部署 Ceph 分布式存储 centos 7 下 Ceph 配置安装 ceph luminous 新功能之内置 dashboard 之 mgr 功能模块配置 ceph dashboard rgw 管理功能的开启 1 基本环境 物理环境：Vmware Workstaion 系统版本：Centos-7.9-Minimal 两个 osd 节点添加一块虚拟磁盘，建议不小于 20G ip hostname services 192.168.150.101 ceph-admin(ceph-deploy) mds1、mon1、mon_mgr、ntp-server 192.168.150.102 ceph-node1 osd1 192.168.150.103 ceph-node2 osd2 2 前期配置 以下操作所有节点都需执行\n修改主机名\nhostnamectl set-hostname ceph-admin bash hostnamectl set-hostname ceph-node1 bash hostnamectl set-hostname ceph-node2 bash 修改 hosts 文件\nvim /etc/hosts 192.168.150.101 ceph-admin 192.168.150.102 ceph-node1 192.168.150.103 ceph-node2 关闭防火墙和 selinux、修改 yum 源及安装一些常用工具\n这里提供了一个简单的系统初始化脚本用来做上述操作，适用于 Centos7\nchmod 777 init.sh ./init.sh init.sh 文件内容\n#!/bin/bash echo \u0026#34;========start=============\u0026#34; sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld echo \u0026#34;====dowload wget=========\u0026#34; yum install -y wget echo \u0026#34;====backup repo===========\u0026#34; mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ echo \u0026#34;====dowload aliyum-repo====\u0026#34; wget http://mirrors.aliyun.com/repo/Centos-7.repo -O /etc/yum.repos.d/Centos-Base.repo wget http://mirrors.aliyun.com/repo/epel-7.repo -O /etc/yum.repos.d/epel.repo echo \u0026#34;====upgrade yum============\u0026#34; yum clean all yum makecache fast echo \u0026#34;====dowload tools=========\u0026#34; yum install -y net-tools vim bash-completion echo \u0026#34;=========finish============\u0026#34; 每个节点安装和配置 NTP (官方推荐的是集群的所有节点全部安装并配置 NTP，需要保证各节点的系统时间一致。没有自己部署 ntp 服务器，就在线同步 NTP)\nyum install chrony -y systemctl start chronyd systemctl enable chronyd ceph-admin\nvim /etc/chrony.conf systemctl restart chronyd chronyc sources 这里使用阿里云的 ntp 服务器\nceph-node1、ceph-node2\nvim /etc/chrony.conf systemctl restart chronyd chronyc sources 这里指定 ceph-admin 节点的 ip 即可\n添加 ceph 源\nyum -y install epel-release rpm --import http://mirrors.163.com/ceph/keys/release.asc rpm -Uvh --replacepkgs http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm 安装完后应有如下内容的 repo 文件\n[Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-nautilus/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-nautilus/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-nautilus/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc 3 磁盘准备 以下操作在 osd 节点（ceph-node1、ceph-node2）执行\n# 检查磁盘 [root@ceph-node1 ~]# fdisk -l /dev/sdb # 格式化磁盘 [root@ceph-node1 ~]# parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100% [root@ceph-node1 ~]# mkfs.xfs /dev/sdb -f # 查看磁盘格式 [root@ceph-node1 ~]# blkid -o value -s TYPE /dev/sdb xfs 4 安装 ceph 集群 配置 ssh 免密\n[root@ceph-admin ~]# ssh-keygen # 一路回车 [root@ceph-admin ~]# ssh-copy-id root@ceph-node1 [root@ceph-admin ~]# ssh-copy-id root@ceph-node2 安装 ceph-deploy\n[root@ceph-admin ~]# yum install -y python2-pip [root@ceph-admin ~]# yum install -y ceph-deploy 创建文件夹用户存放集群文件\n[root@ceph-admin ~]# mkdir /root/my-ceph [root@ceph-admin ~]# cd /root/my-ceph/ 创建集群, 后面填写 monit 节点的主机名，这里 monit 节点和管理节点是同一台机器，即 ceph-admin\n[root@ceph-admin my-ceph]# ceph-deploy new ceph-admin [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph-admin [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] func : \u0026lt;function new at 0x7f2217df3de8\u0026gt; [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f221756e4d0\u0026gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : [\u0026#39;ceph-admin\u0026#39;] [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-admin][DEBUG ] connected to host: ceph-admin [ceph-admin][DEBUG ] detect platform information from remote host [ceph-admin][DEBUG ] detect machine type [ceph-admin][DEBUG ] find the location of an executable [ceph-admin][INFO ] Running command: /usr/sbin/ip link show [ceph-admin][INFO ] Running command: /usr/sbin/ip addr show [ceph-admin][DEBUG ] IP addresses found: [u\u0026#39;192.168.150.101\u0026#39;] [ceph_deploy.new][DEBUG ] Resolving host ceph-admin [ceph_deploy.new][DEBUG ] Monitor ceph-admin at 192.168.150.101 [ceph_deploy.new][DEBUG ] Monitor initial members are [\u0026#39;ceph-admin\u0026#39;] [ceph_deploy.new][DEBUG ] Monitor addrs are [\u0026#39;192.168.150.101\u0026#39;] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... 修改集群配置文件\n注意：mon_host 必须和 public network 网络是同网段内\n[root@ceph-admin my-ceph]# vim ceph.conf # 添加如下两行内容 ...... public_network = 192.168.150.0/24 osd_pool_default_size = 2 开始安装\n[root@ceph-admin my-ceph]# ceph-deploy install --release nautilus ceph-admin ceph-node1 ceph-node2 # 出现以下提示说明安装成功 [ceph-node2][DEBUG ] Complete! [ceph-node2][INFO ] Running command: ceph --version [ceph-node2][DEBUG ] ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) nautilus (stable) 初始化 monit 监控节点，并收集所有密钥\n[root@ceph-admin my-ceph]# ceph-deploy mon create-initial [root@ceph-admin my-ceph]# ceph-deploy gatherkeys ceph-admin 检查 OSD 节点上所有可用的磁盘\n[root@ceph-admin my-ceph]# ceph-deploy disk list ceph-node1 ceph-node2 删除所有 osd 节点上的分区、准备 osd 及激活 osd\n主机上有多块磁盘要作为 osd 时新增 --data 即可, 每个 --data 后跟一块磁盘\n[root@ceph-admin my-ceph]# ceph-deploy osd create ceph-node1 --data /dev/sdb [root@ceph-admin my-ceph]# ceph-deploy osd create ceph-node2 --data /dev/sdb 在两个 osd 节点上通过命令已显示磁盘已成功 mount\n[root@ceph-node1 ~]# lsblk 查看 osd\n[root@ceph-admin my-ceph]# ceph-deploy disk list ceph-node1 ceph-node2 ...... ...... [ceph-node1][INFO ] Disk /dev/mapper/ceph--2bb0ec8d--547c--42c2--9858--08ccfd043bd4-osd--block--33e8dba4--6dfc--4753--b9ba--0d0c54166f0c: 21.5 GB, 21470642176 bytes, 41934848 sectors ...... ...... [ceph-node2][INFO ] Disk /dev/mapper/ceph--f9a95e6c--fc7b--46b4--a835--dd997c0d6335-osd--block--db903124--4c01--40d7--8a58--b26e17c1db29: 21.5 GB, 21470642176 bytes, 41934848 sectors 同步集群文件，这样就可以在所有节点执行 ceph 命令了\n[root@ceph-admin my-ceph]# ceph-deploy admin ceph-admin ceph-node1 ceph-node2 在其他节点查看 osd 的目录树\n[root@ceph-node1 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.03897 root default -3 0.01949 host ceph-node1 0 hdd 0.01949 osd.0 up 1.00000 1.00000 -5 0.01949 host ceph-node2 1 hdd 0.01949 osd.1 up 1.00000 1.00000 配置 mgr\n[root@ceph-admin my-ceph]# ceph-deploy mgr create ceph-admin 查看集群状态和集群 service 状态\n此时是 HEALTH_WARN 状态，是由于启用了不安全模式\n[root@ceph-admin my-ceph]# ceph health HEALTH_WARN mon is allowing insecure global_id reclaim [root@ceph-admin my-ceph]# ceph -s cluster: id: fd816347-598c-4ed6-b356-591a618a0bdc health: HEALTH_WARN mon is allowing insecure global_id reclaim services: mon: 1 daemons, quorum ceph-admin (age 3h) mgr: mon_mgr(active, since 17s) osd: 2 osds: 2 up (since 3m), 2 in (since 3m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2.0 GiB used, 38 GiB / 40 GiB avail pgs: 禁用不安全模式\n[root@ceph-admin my-ceph]# ceph config set mon auth_allow_insecure_global_id_reclaim false [root@ceph-admin my-ceph]# ceph health HEALTH_OK 5 开启 dashboard [root@ceph-admin my-ceph]# yum install -y ceph-mgr-dashboard [root@ceph-admin my-ceph]# ceph mgr module enable dashboard # 创建自签证书 [root@ceph-admin my-ceph]# ceph dashboard create-self-signed-cert # 创建密码文件 [root@ceph-admin my-ceph]# echo abc123 \u0026gt; ./dashboard_user_pw # 创建dashboard的登录用户 [root@ceph-admin my-ceph]# ceph dashboard ac-user-create admin -i ./dashboard_user_pw administrator {\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;lastUpdate\u0026#34;: 1646037503, \u0026#34;name\u0026#34;: null, \u0026#34;roles\u0026#34;: [\u0026#34;administrator\u0026#34;], \u0026#34;password\u0026#34;: \u0026#34;$2b$12$jGsvau8jFMb4pDwLU/t8KO1sKvmBMcNUYycbXusmgkvTQzlzrMyKi\u0026#34;, \u0026#34;email\u0026#34;: null} [root@ceph-admin my-ceph]# ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;https://ceph-admin:8443/\u0026#34; } 测试访问\n上图中测试环境是 win10+chrome，同事反应 mac+chrome 会出现无法访问的情况，原因是我们使用的自签证书，浏览器并不信任此证书，可以通过以下两种方式解决\n关闭 dashboard 的 ssl 访问 下载证书配置浏览器信任证书 5.1 关闭 ssl [root@ceph-admin my-ceph]# ceph config set mgr mgr/dashboard/ssl false [root@ceph-admin my-ceph]# ceph mgr module disable dashboard [root@ceph-admin my-ceph]# ceph mgr module enable dashboard [root@ceph-admin my-ceph]# ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;http://ceph-admin:8080/\u0026#34; } 如果出现 Module 'dashboard' has failed: IOError(\u0026quot;Port 8443 not free on '::'\u0026quot;,) 这种报错，需要重启下 mgr：systemctl restart ceph-mgr@ceph-admin\n测试访问\n5.2 开启 rgw 默认 object gateway 功能没有开启\n创建 rgw 实例\nceph-deploy rgw create ceph-admin 默认运行端口是 7480\n创建 rgw 用户\n[root@ceph-admin my-ceph]# radosgw-admin user create --uid=rgw --display-name=rgw --system 提供 dashboard 证书\n[root@ceph-admin my-ceph]# echo UI2T50HNZUCVVYYZNDHP \u0026gt; rgw_user_access_key [root@ceph-admin my-ceph]# echo 11rg0WbXuh2Svexck3vJKs19u1UQINixDWIpN5Dq \u0026gt; rgw_user_secret_key [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-access-key -i rgw_user_access_key Option RGW_API_ACCESS_KEY updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-secret-key -i rgw_user_secret_key Option RGW_API_SECRET_KEY updated 禁用 ssl\n[root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-ssl-verify False Option RGW_API_SSL_VERIFY updated 启用 rgw\n[root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-host 192.168.150.101 Option RGW_API_HOST updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-port 7480 Option RGW_API_PORT updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-scheme http Option RGW_API_SCHEME updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-user-id rgw Option RGW_API_USER_ID updated [root@ceph-admin my-ceph]# systemctl restart ceph-radosgw.target 验证\n目前 object gateway 功能已成功开启\n6 其他 6.1 清除 ceph 集群 清除安装包\n[root@ceph-admin ~]# ceph-deploy purge ceph-admin ceph-node1 ceph-node2 清除配置信息\n[root@ceph-admin ~]# ceph-deploy purgedata ceph-admin ceph-node1 ceph-node2 [root@ceph-admin ~]# ceph-deploy forgetkeys 每个节点删除残留的配置文件\nrm -rf /var/lib/ceph/osd/* rm -rf /var/lib/ceph/mon/* rm -rf /var/lib/ceph/mds/* rm -rf /var/lib/ceph/bootstrap-mds/* rm -rf /var/lib/ceph/bootstrap-osd/* rm -rf /var/lib/ceph/bootstrap-mon/* rm -rf /var/lib/ceph/tmp/* rm -rf /etc/ceph/* rm -rf /var/run/ceph/* 清理磁盘设备 (/dev/mapper/ceph*)\nls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % 6.2 dashboard 无法访问的问题 在关闭 dashboard 的 https 后，出现了一个很奇怪的问题，使用 chrome 浏览器无法访问 dashboard 了，edge 或者使用 chrome 无痕模式可以正常访问，期间尝试了各种方法包括重新配置 dashboard 和清理 chrome 浏览器的缓存和 cookie 等方式都没有解决问题，结果第二天起来打开环境一看自己好了 ＞﹏＜\n问题情况见下图\n日志报错：\n6.3 同步配置文件 ceph-deploy --overwrite-conf config push ceph-node{1,2,3,4} 6.4 添加 mon 节点和 mgr 节点 ceph-deploy mon create ceph-node{1,2,3,4} ceph-deploy mgr create ceph-node{1,2,3,4} 记得修改配置文件\n之后同步配置文件\nceph-deploy --overwrite-conf config push ceph-node{1,2,3,4} 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/ceph-v12-nautilus-cephdeploy/","summary":"0 前言 本文参考以下链接: Ceph-deploy 快速部署 Ceph 分布式存储 centos 7 下 Ceph 配置安装 ceph luminous 新功能之内置 dashboard 之 mgr 功能模块配置 ceph dashboard rgw 管理功能的开启 1 基本环境 物理环境：Vmware Workstaion 系统版本：Centos-7.9-Minimal 两个 osd 节点添加一块虚拟磁盘，建议不小于 20G ip hostname services 192.168.150.101 ceph-admin(ceph-deploy) mds1、mon1、mon_mgr","title":"ceph | centos7 部署 ceph-v12"},{"content":"1 清单 地址 中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple 清华 https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣 http://pypi.douban.com/simple 华中理工大学 http://pypi.hustunique.com/simple 山东理工大学 http://pypi.sdutlinux.org/simple 阿里云 https://mirrors.aliyun.com/pypi/simple/ 2 linux 环境 mkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF 3 windows 环境 打开 cmd 使用 dos 命令 set 找到 userprofile 路径，在该路径下创建 pip 文件夹，在 pip 文件夹下创建 pip.ini\npip.ini 具体配置\n[global] timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/python-change-pip-repo/","summary":"1 清单 地址 中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple 清华 https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣 http://pypi.douban.com/simple 华中理工大学 http://pypi.hustunique.com/simple 山东理工大学 http://pypi.sdutlinux.org/simple 阿里云 https://mirrors.aliyun.com/pypi/simple/ 2 linux 环境 mkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF 3 windows 环境 打开 cmd 使用 dos 命令 set 找到 userprofile 路径，在该路径下创建 pip 文件夹，在 pip 文件夹下创建 pip.ini pip.ini 具体配置 [global] timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn 以上","title":"python | 修改 pip 源"},{"content":"0 前言 在 openEuler20.03 (LTS-SP1) 系统上进行一些测试，发现某个东西会自动修改 ssh 配置文件导致系统无法通过密码登录，最后排查是由于安装了 cloud-init 导致的。\n排查思路 出现这个问题前做的操作是安装了一些项目组同事指定的包，问题就应该出在这些包上\nyum install -y telnet rsync ntpdate zip unzip libaio dos2unix sos vim vim-enhanced net-tools man ftp lrzsz psmisc gzip network-scripts cloud-init cloud-utils-growpart tar libnsl authselect-compat 大致看了下，除了 cloud-Init 和 cloud-utils-growpart 这两个包其他包基本不可能去修改 ssh 的配置\n直接检索这两个包的所有文件中的配置，是否与 PasswordAuthentication 有关\n[root@localhost ~]# grep -nr PasswordAuthentication `rpm -ql cloud-utils-growpart` [root@localhost ~]# grep -nr PasswordAuthentication `rpm -ql cloud-init` 找到了修改这个参数代码的具体实现\n查看该文件\n[root@localhost ~]# vim +98 /usr/lib/python3.7/site-packages/cloudinit/config/cc_set_passwords.py 具体的判断操作和修改操作\n修改操作就不去深究了，主要看下判断操作，可以看到判断操作是使用了 util.is_true() ，该 util 模块也在该文件中引用了\n再去找这个 util 模块的具体实现\npython 引用的模块路径如下，否则会抛出错误\n文件的同级路径下 sys.path 路径下 并没有在同级目录下\n[root@localhost ~]# ll /usr/lib/python3.7/site-packages/cloudinit/config/ | grep cloudinit sys.path 路径不知道可以用 python 终端输出下\n在/usr/lib/python3.7/site-packages 路径下找到了 cloudinit 模块的 util 子模块\n查看 util.is_true 和 util.is_false 具体的函数实现\n逻辑很简单，判断 val 参数是否为 bool 值，否则对 val 参数的值进行处理后再查看是否在 check_set 中\n再回头看之前的 /usr/lib/python3.7/site-packages/cloudinit/config/cc_set_passwords.py 文件是怎样对 util.is_true 和 util.is_false 传参的\n可以看到是由 handle_ssh_pwauth() 函数传进来的\n再继续找哪个文件调用了这个函数\n还是这个文件，第 230 行\n这里参数 pw_auth 传的值是 cfg.get(\u0026lsquo;ssh_pwauth\u0026rsquo;)\ncfg.get() 这个函数 get 的东西是 /etc/cloud/cloud.cfg 配置文件下的 ssh_pwauth 的值\n到这里，就可以回头再看整个逻辑了\n调用 handle_ssh_pwauth() 函数，传了一个参数 pw_auth=0 调用 util.is_true() 和 util.is_false 函数，传了同一个参数 val=0 上述两个函数执行完后 cfg_val 的值最终为 no 调用 update_ssh_config({cfg_name: cfg_val}) 函数，cfg_name=PasswordAuthentication，cfg_val=no 即将 sshd 的配置文件的 PasswordAuthentication 值改为 no 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/troubleshooting-cloud-init-change-ssh-config/","summary":"0 前言 在 openEuler20.03 (LTS-SP1) 系统上进行一些测试，发现某个东西会自动修改 ssh 配置文件导致系统无法通过密码登录，最后排查是由于安装了 cloud-init 导致的。 排查思路 出现这个问题前做的操作是安装了一些项目组同事指定的包，问题就应该出在这些包上 yum install -y telnet rsync ntpdate zip unzip libaio dos2unix sos vim vim-enhanced net-tools man ftp lrzsz psmisc gzip network-scripts cloud-init cloud-utils-growpart tar libnsl authselect-compat 大致看了下，除了 cloud-Init 和 cloud-utils-growpart 这两","title":"troubleshooting | 安装 cloud-init 后导致 ssh 连接失败"},{"content":"0 前言 基于 autohotkey v1 版本\n使用方法: 安装 autohotkey 后, 将下述代码保存为 .ahk 文件, 双击执行即可\n如需开机自启, 在 运行 中执行 shell:startup, 将 .ahk 文件放到自启动目录即可\n1 prtsc 改为 shift insert 我的机械键盘是 80 配列, 没有 insert, shift+insert 几乎是所有软件都支持的粘贴方式, 遂将很不常用的 prtsc 键改为 shfit+insert 的组合键\nPrintScreen::+Insert 2 typora 快捷修改字体颜色 实现 alt + 数字键快速将光标选中的文本改为对应的颜色\n; 分号以及分号后的内容代表注释，以下为代码解释 #IfWinActive ahk_exe Typora.exe { ; alt+0 黑色 !0::addFontColor(\u0026#34;black\u0026#34;) ; alt+1 珊瑚色 !1::addFontColor(\u0026#34;coral\u0026#34;) ; alt+2 红色 !2::addFontColor(\u0026#34;red\u0026#34;) ; alt+3 黄色 !3::addFontColor(\u0026#34;yellow\u0026#34;) ; alt+4 绿色 !4::addFontColor(\u0026#34;green\u0026#34;) ; alt+5 浅蓝色 !5::addFontColor(\u0026#34;cornflowerblue\u0026#34;) ; alt+6 青色 !6::addFontColor(\u0026#34;cyan\u0026#34;) ; alt+7 紫色 !7::addFontColor(\u0026#34;purple\u0026#34;) } ; 快捷增加字体颜色 addFontColor(color){ clipboard := \u0026#34;\u0026#34; ; 清空剪切板 Send {ctrl down}c{ctrl up} ; 复制 ; SendInput {Text} ; 解决中文输入法问题 SendInput {TEXT}\u0026lt;font color=\u0026#39;%color%\u0026#39;\u0026gt; SendInput {ctrl down}v{ctrl up} ; 粘贴 If(clipboard = \u0026#34;\u0026#34;){ SendInput {TEXT}\u0026lt;/font\u0026gt; ; Typora 在这不会自动补充 }else{ SendInput {TEXT}\u0026lt;/ ; Typora中自动补全标签 } } 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/windows-autohotkey-scripts/","summary":"0 前言 基于 autohotkey v1 版本 使用方法: 安装 autohotkey 后, 将下述代码保存为 .ahk 文件, 双击执行即可 如需开机自启, 在 运行 中执行 shell:startup, 将 .ahk 文件放到自启动目录即可 1 prtsc 改为 shift insert 我的机械键盘是 80 配列, 没有 insert, shift+insert 几乎是所有软件都支持的粘贴方式, 遂将很不常用的 prtsc 键改为 shfit+insert 的组合键 PrintScreen::+Insert 2 typora 快捷修改字体颜色 实现 alt + 数字键快速将光标","title":"windows | autohotkey 常用脚本"},{"content":"0 前言 本文参考以下链接:\n修改 rpm 中的文件重新打包 要修改 rpm 包中的文件，对于自己编译的 rpm 包，只需要在源码中修改好然后重新编译即可。而对于并不是自己编译的 rpm 包，且不熟悉编译环境的情况下，可以使用 rpm-build 和 rpm-rebuild 工具反编译来修改 rpm 中的文件\n这里使用 ceph-mgr 软件包进行演示\n1 安装 rpm-build\u0026amp;rpmrebuild rpmrebuild 官网 rpmrebuild 下载地址\n解压 rpmrebuild\n[root@localhost ~]# mkdir -p /data/rpmbuild [root@localhost ~]# tar zxf rpmrebuild-2.15.tar.gz -C /data/rpmbuild/ [root@localhost ~]# ll /opt/rpmrebuild/ rpm-build 直接使用 yum 安装即可\n[root@localhost ~]# yum install -y rpm-build 2 反编译\u0026amp;修改\u0026amp;重新编译 安装准备重新打包的 rpm\n[root@localhost ~]# rpm -ivh ceph-mgr-12.2.13-0.el7.x86_64.rpm 查看 rpm 的安装名称\n[root@localhost ~]# rpm -qa |grep mgr ceph-mgr-12.2.13-0.el7.x86_64 配置 rpm 编译目录\nvim ~/.rpmmacros %_topdir /data/rpmbuild 创建目录\nmkdir /data/rpmbuild/BUILDROOT mkdir /data/rpmbuild/SPECS 执行脚本\n[root@localhost ~]# cd /data/rpmbuild/ [root@localhost rpmbuild]# ./rpmrebuild.sh -s SPECS/abc.spec ceph-mgr [root@localhost rpmbuild]# cd 解压原版 RPM 包\n[root@localhost ~]# rpm2cpio ceph-mgr-12.2.13-0.el7.x86_64.rpm |cpio -idv 这里软件包解压后是两个目录\n根据需求替换修改解压后的文件，这里我替换两个文件 /root/usr/lib64/ceph/mgr/dashboard/static/Ceph_Logo_Standard_RGB_White_120411_fa.png 和 /root/usr/lib64/ceph/mgr/dashboard/static/logo-mini.png，并给原先的文件做一个备份\n[root@localhost static]# mv logo-mini.png logo-mini.png.bak [root@localhost static]# mv Ceph_Logo_Standard_RGB_White_120411_fa.png Ceph_Logo_Standard_RGB_White_120411_fa.png.bak [root@localhost static]# cp kubernetes-logo.svg logo-mini.png [root@localhost static]# cp kubernetes-logo.svg Ceph_Logo_Standard_RGB_White_120411_fa.png 修改 abc.spec 文件\n找到原文件所在的行，添加备份文件\n[root@localhost ~]# vim /data/rpmbuild/SPECS/abc.spec 这里创建的 bbb 目录是临时使用，编译过程肯定会报错，因为路径不对，根据报错修改路径\n[root@localhost ~]# mkdir -p /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# mv ./usr/ /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# mv ./var/ /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# rpmbuild -ba /data/rpmbuild/SPECS/abc.spec 这里可以看到他请求的路径\n修改目录名\n[root@localhost ~]# mv /data/rpmbuild/BUILDROOT/bbb/ /data/rpmbuild/BUILDROOT/ceph-mgr-12.2.13-0.el7.x86_64 再次编译\n[root@localhost ~]# rpmbuild -ba /data/rpmbuild/SPECS/abc.spec 生成的 rpm 位置在/data/rpmbuild/RPMS/\n查看原 rpm 包的文件\n[root@localhost ~]# cd /usr/lib64/ceph/mgr/dashboard/static [root@localhost static]# ll total 16 drwxr-xr-x 5 root root 117 Dec 6 03:11 AdminLTE-2.3.7 -rw-r--r-- 1 root root 4801 Jan 30 2020 Ceph_Logo_Standard_RGB_White_120411_fa.png -rw-r--r-- 1 root root 1150 Jan 30 2020 favicon.ico drwxr-xr-x 7 root root 94 Dec 6 03:11 libs -rw-r--r-- 1 root root 1811 Jan 30 2020 logo-mini.png 安装新 rpm 包，查看文件\n[root@localhost ~]# cd /data/rpmbuild/RPMS/x86_64 [root@localhost x86_64]# rpm -e --nodeps ceph-mgr [root@localhost x86_64]# rpm -ivh ceph-mgr-12.2.13-0.el7.x86_64.rpm [root@localhost x86_64]# cd /usr/lib64/ceph/mgr/dashboard/static [root@localhost static]# ll total 24 drwxr-xr-x 5 root root 117 Dec 6 03:53 AdminLTE-2.3.7 -rw-r--r-- 1 root root 1877 Dec 6 03:44 Ceph_Logo_Standard_RGB_White_120411_fa.png -rw-r--r-- 1 root root 4801 Dec 6 03:41 Ceph_Logo_Standard_RGB_White_120411_fa.png.bak -rw-r--r-- 1 root root 1150 Dec 6 03:41 favicon.ico drwxr-xr-x 7 root root 94 Dec 6 03:53 libs -rw-r--r-- 1 root root 1877 Dec 6 03:44 logo-mini.png -rw-r--r-- 1 root root 1811 Dec 6 03:41 logo-mini.png.bak 至此，rpm 包中的文件修改以及重新打包的所有步骤都已完成\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-change-file/","summary":"0 前言 本文参考以下链接: 修改 rpm 中的文件重新打包 要修改 rpm 包中的文件，对于自己编译的 rpm 包，只需要在源码中修改好然后重新编译即可。而对于并不是自己编译的 rpm 包，且不熟悉编译环境的情况下，可以使用 rpm-build 和 rpm-rebuild 工具反编译来修改 rpm 中的文件 这里使用 ceph-mgr 软件包进行演示 1 安装 rpm-build\u0026amp;rpmrebuild rpmrebuild 官网 rpmrebuild 下载地址 解压 rpmrebuild [root@localhost ~]# mkdir -p","title":"通过反编译修改 rpm 包内的文件"},{"content":"0 前言 基于 isoft-serveros-v4.2\n本文参考以下链接:\n安装Ambari 2.7.5 + HDP3.1.5（附安装包） 1 前期准备 1.1 安装包准备 Ambari-2.7.5 HDP-3.1.5 libtirpc-devel:\n链接：https://pan.baidu.com/s/1eteZ2jGkSq4Pz5YFfHyJgQ\n提取码：6hq3\n1.2 服务器配置 主机名 cpu 内存 硬盘 系统版本 ip 地址 node001 4c 10g 50g isoft-serveros-4.2 192.168.150.106 node002 2c 4g 20g isoft-serveros-4.2 192.168.150.107 1.3 修改系统版本文件 (allnode) sed -i \u0026#39;s/4/7/g\u0026#39; /etc/redhat-release sed -i \u0026#39;s/4/7/g\u0026#39; /etc/os-release 1.4 配置主机名 (allnode) 2 台服务器的 hosts 都需要做如下修改\n修改主机名 hostnamectl set-hostname node001 bash 修改 hosts 文件 vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.150.106 node001 192.168.150.107 node002 1.5 关闭防火墙及 selinux (allnode) 2 台服务器上分别执行以下操作，关闭防火墙并配置开机不自动启动\nsystemctl stop firewalld systemctl disable firewalld setenforce 0 为了重启后依然关闭，配置如下文件\nvim /etc/sysconfig/selinux # 修改 SELINUX=disabled 1.6 配置 ssh 互信 (allnode) 方法一\n在每台服务器上执行如下操作，一直回车即可\nssh-keygen -t rsa ssh-copy-id -i /root/.ssh/id_rsa.pub node001 ssh-copy-id -i /root/.ssh/id_rsa.pub node002 方法二\n在每台服务器上执行如下操作，一直回车即可\nssh-keygen -t rsa 在服务器 1 上将公钥（名为 id_rsa.pub 文件）追加到认证文件（名为 authorized_keys 文件）中:\ncat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 去其他服务器节点将 ~/.ssh/id_rsa.pub 中的内容拷贝到服务器 1 的 ~/.ssh/authorized_keys 中,查看文件中的内容\ncat ~/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/eA09X4s5RIYvuYNxVvtOo6unY1mgipsFyoz/hy/Gwk0onfZvBi/Sl3TVRZO5aqcHccAGlLF7OPTKH1qUuKVtnUOQik0TouL5VKsOBDMHHRT9D5UwqaIE8tYDC8V6uwieFgscZcBjhrsJ/Iramo9ce7N9RTO3otRMRQxOs+Wd1F/ZOmpRtMGU2N4RH4i2quRU6m2lt/eJKpNupSHKoztTQRsEanilHVASnikAXH8JpG70iO7RXR/hLz+/Of3ISUrOMSO4/ZIIu4xnYN3jvsXOdK/qIhP/PI2s+uF22IvVE6xZYVadQFa4zAuhQmCBWkE7vMyI1UJkxP7OQYj72LUH root@node001 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnz8wHoytR2Xlnl04rQq4I2vgUVWbkKjv30pj+Toz4719ah4cY9pvZj0JsfhVzaaCsR14BLFVLkqKUhCWK3K6muT4iHb+N0WirpbwfJkztmQeco7Ha9xrPQ8v/I4xZujFoMVA0tkb/32zRTxOkPv9AUgB8V6Lin6LnB/AcnhnmoIs5PdbAdh/kBGpQGKIZkbyCUOYz9/PZuGJoJBblqfWiqzxYYLN9+cYMkmPnB1HdDewAepIsIC18U3ujE+1Su2UlmISPvvr1zG4XR4ZZoKQsOOJq3XRMGVkDvmFhl03JHZpd6BW0796CeYVZ41UomWXTOduQql+tYWUbegzGLmRZ root@node002 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8AFoGJHp2M45xLYNzXUHLWzRwHsgRPHjeErStq0tEy9bQv4OkN41j0FrxVAYJiGHdHGturriVgUEtL59RjcrJH6bAvhP54nM5YiQlNnWnSUR27Zuaodz4nhYUFq/Co5eDN6lTfL8pgYiEdpBOvE5t1w3bisdblP7YGQ2lF1zzCEGfQ79QbntEbyGNoR9sGHm11x9fOH+fape8TjQJrEAO4d1tAhMqVygQKwqwAPKeqhEum6BaLli83TsXzd7gyz9H7AAc1m04NaLB26xfynW6MVuk1j94awXKlGXjrbNTC/Kg6M8bd5PT/k3DOkx4b+nEs8xZ5x1j4D2OaO1X6rZx root@node003 设置认证文件的权限：\nchmod 600 ~/.ssh/authorized_keys 将 ~/.ssh/authorized_keys 同步到其他节点\nscp ~/.ssh/authorized_keys node002:~/.ssh/authorized_keys 注意：这里第一次使用同步还需要密码，之后就不需要了\n验证免密是否配置成功\nssh 到不同服务器\nssh node002 1.7 配置 ntp 时钟同步 选择一台服务器作为 NTP Server，这里选择 node001\n将如下配置 vim /etc/ntp.conf\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 127.127.1.0 fudge 127.127.1.0 stratum 10 node002 节点做如下配置\nvim /etc/ntp.conf 将\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 192.168.150.106 在每台服务器上启动 ntpd 服务，并配置服务开机自启动\nsystemctl restart ntpd systemctl enable ntpd 1.8 设置 swap(allnode) echo vm.swappiness = 1 \u0026gt;\u0026gt; /etc/sysctl.conf sysctl vm.swappiness=1 sysctl -p 1.9 关闭透明大页面 (allnode) 由于透明超大页面已知会导致意外的节点重新启动并导致 RAC 出现性能问题，因此 Oracle 强烈建议禁用\necho never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled 1.10 安装 http 服务 (node001) 安装 apache 的 httpd 服务主要用于搭建 OS. Ambari 和 hdp 的 yum 源。在集群服务器中选择一台服务器来安装 httpd 服务，命令如下：\nyum -y install httpd systemctl start httpd systemctl enable httpd.service 验证，在浏览器输入http://192.168.150.106看到如下截图则说明启动成功。\n1.11 安装 Java(allnode) 下载地址：https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\ntar -zxvf jdk-8u271-linux-x64.tar.gz mkdir /usr/local/java mv jdk1.8.0_271/* /usr/local/java 配置环境变量\nvim /root/.bashrc 添加如下配置\nexport JAVA_HOME=/usr/local/java export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export JRE_HOME=$JAVA_HOME/jre 激活配置\nsource /root/.bashrc java -version 1.12 安装 maven3.6(node001) 下载解压\ntar -zxvf apache-maven-3.6.3-bin.tar.gz mkdir -p /opt/src/maven mv apache-maven-3.6.3/* /opt/src/maven 配置 maven 环境变量\nvim /root/.bashrc # set maven home export PATH=$PATH:/opt/src/maven/bin 激活\nsource /root/.bashrc 2 安装 Ambari \u0026amp; HDP 2.1 配置本地源 解压\ntar -zxvf ambari-2.7.5.0-centos7.tar.gz -C /var/www/html/ tar -zxvf HDP-3.1.5.0-centos7-rpm.tar.gz -C /var/www/html/ tar -zxvf HDP-GPL-3.1.5.0-centos7-gpl.tar.gz -C /var/www/html/ tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /var/www/html/ ll /var/www/html/ 总用量 0 drwxr-xr-x. 3 root root 21 11月 23 22:31 ambari drwxr-xr-x. 3 1001 users 21 12月 18 2019 HDP drwxr-xr-x. 3 1001 users 21 12月 18 2019 HDP-GPL drwxr-xr-x. 3 1001 users 21 8月 13 2018 HDP-UTILS 设置设置用户组和授权\nchown -R root:root /var/www/html/HDP chown -R root:root /var/www/html/HDP-GPL chown -R root:root /var/www/html/HDP-UTILS chmod -R 755 /var/www/html/HDP chmod -R 755 /var/www/html/HDP-GPL chmod -R 755 /var/www/html/HDP-UTILS 创建 libtirpc-devel 本地源\nmkdir /var/www/html/libtirpc mv /root/libtirpc-* /var/www/html/libtirpc/ cd /var/www/html/libtirpc createrepo . 制作本地源\n配置 ambari.repo\nvim /etc/yum.repos.d/ambari.repo [Ambari-2.7.5.0] name=Ambari-2.7.5.0 baseurl=http://192.168.150.106/ambari/centos7/2.7.5.0-72/ gpgcheck=0 enabled=1 priority=1 配置 HDP 和 HDP-TILS\nvim /etc/yum.repos.d/HDP.repo [HDP-3.1.5.0] name=HDP Version - HDP-3.1.5.0 baseurl=http://192.168.150.106/HDP/centos7/3.1.5.0-152/ gpgcheck=0 enabled=1 priority=1 [HDP-UTILS-1.1.0.22] name=HDP-UTILS Version - HDP-UTILS-1.1.0.22 baseurl=http://192.168.150.106/HDP-UTILS/centos7/1.1.0.22/ gpgcheck=0 enabled=1 priority=1 [HDP-GPL-3.1.5.0] name=HDP-GPL Version - HDP-GPL-3.1.5.0 baseurl=http://192.168.150.106/HDP-GPL/centos7/3.1.5.0-152 gpgcheck=0 enabled=1 priority=1 配置 libtirpc.repo\nvim /etc/yum.repos.d/libtirpc.repo [libtirpc_repo] name=libtirpc-0.2.4-0.16 baseurl=http://192.168.150.106/libtirpc/ gpgcheck=0 enabled=1 priority=1 拷贝到其他节点\nscp /etc/yum.repos.d/* node002:/etc/yum.repos.d/ 查看源\nyum clean all yum repolist 2.2 安装 mariadb(node001) 安装 MariaDB 服务器\nyum install mariadb-server -y 启动并设置开机启动\nsystemctl enable mariadb systemctl start mariadb 初始化\n/usr/bin/mysql_secure_installation [...] Enter current password for root (enter for none): OK, successfully used password, moving on... [...] Set root password? [Y/n] Y New password:123456 Re-enter new password:123456 [...] Remove anonymous users? [Y/n] Y [...] Disallow root login remotely? [Y/n] N [...] Remove test database and access to it [Y/n] Y [...] Reload privilege tables now? [Y/n] Y [...] All done! If you\u0026#39;ve completed all of the above steps, your MariaDB 18 installation should now be secure. Thanks for using MariaDB! 为 MariaDB 安装 MySQL JDBC 驱动程序\ntar zxf mysql-connector-java-5.1.40.tar.gz mv mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/share/java/mysql-connector-java.jar 创建需要的数据库\n如果需要 ranger，编辑以下⽂件： vim /etc/my.cnf 并添加以下⾏：\nlog_bin_trust_function_creators = 1 重启数据库并登录\nsystemctl restart mariadb mysql -u root -p123456 2.3 安装和配置 ambari-server (node001) 安装 ambari-server\nyum -y install ambari-server 复制 mysql jdbc 驱动到 /var/lib/ambari-server/resources/\ncp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/ 配置 /etc/ambari-server/conf/ambari.properties，添加如下行\nvim /etc/ambari-server/conf/ambari.properties server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar 执行\nambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 初始化 ambari-server\nambari-server setup 1） 提示是否自定义设置。输入：y Customize user account for ambari-server daemon [y/n] (n)? y （2）ambari-server 账号。 Enter user account for ambari-server daemon (root): 如果直接回车就是默认选择root用户 如果输入已经创建的用户就会显示： Enter user account for ambari-server daemon (root):ambari Adjusting ambari-server permissions and ownership... （3）设置JDK。输入：2 Checking JDK... Do you want to change Oracle JDK [y/n] (n)? y [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Custom JDK ============================================================================== Enter choice (1): 2 如果上面选择3自定义JDK,则需要设置JAVA_HOME。输入：/usr/local/java WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: /usr/local/java Validating JDK on Ambari Server...done. Completing setup... （4）安装GPL，选择：y Checking GPL software agreement... GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html Enable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? y （5）数据库配置。选择：y Configuring database... Enter advanced database configuration [y/n] (n)? y （6）选择数据库类型。输入：3 Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL/ MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere ============================================================================== Enter choice (3): 3 （7）设置数据库的具体配置信息，根据实际情况输入，如果和括号内相同，则可以直接回车。如果想重命名，就输入。 Hostname (localhost):node001 Port (3306): 3306 Database name (ambari): ambari Username (ambari): ambari Enter Database Password (bigdata):ambari123 Re-Enter password: ambari123 （8）将Ambari数据库脚本导入到数据库 WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql 这个sql后面会用到，导入数据库 Proceed with configuring remote database connection properties [y/n] (y)? y 登录 mariadb 创建 ambari 安装所需要的库\n设置的账号后面配置 ambari-server 的时候会用到\nmysql -uroot -p123456 CREATE DATABASE ambari; use ambari; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;node001\u0026#39;; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql show tables; use mysql; select host,user from user where user=\u0026#39;ambari\u0026#39;; CREATE DATABASE hive; use hive; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;node001\u0026#39;; CREATE DATABASE oozie; use oozie; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;node001\u0026#39;; FLUSH PRIVILEGES; 2.4 安装 ambari-agent(allnode) pssh -h /node.list -i \u0026#39;yum -y install ambari-agent\u0026#39; pssh -h /node.list -i \u0026#39;systemctl start ambari-agent\u0026#39; 2.5 安装 libtirpc-devel(allnode) pssh -h /node.list -i \u0026#39;yum -y install libtirpc-devel\u0026#39; 2.6 启动 ambari 服务 ambari-server start 3 部署集群 3.1 登录界面 http://192.168.150.106:8080\n默认管理员账户登录， 账户：admin 密码：admin\n3.2 选择版本，配置 yum 源 1）选择 Launch Install Wizard\n2）配置集群名称\n3）选择版本并修改本地源地址\n选 HDP-3.1(Default Version Definition);\n选 Use Local Repository;\n选 redhat7:\nHDP-3.1：http://node001/HDP/centos7/3.1.5.0-152/\nHDP-3.1-GPL: http://node001/HDP-GPL/centos7/3.1.5.0-152/\nHDP-UTILS-1.1.0.22: http://node001/HDP-UTILS/centos7/1.1.0.22/\n3.3 配置节点和密钥 下载主节点的 /root/.ssh/id_rsa，并上传！点击下一步，进入确认主机界面\n也可直接 cat /root/.ssh/id_rsa 粘贴即可\n验证通过\n3.4 勾选需要安装的服务 由于资源有限，这里并没有选择所有服务\n3.5 分配服务 master 3.6 分配服务 slaves 设置相关服务的密码\nGrafana Admin: 123456\nHive Database: hive\nActivity Explorer’s Admin: admin\n3.7 连接数据库 3.8 编辑配置，默认即可 3.9 开始部署 3.10 安装成功 右上角两个警告是磁盘使用率警告，虚机分配的磁盘较小\n4 其他 4.1 添加其他系统支持 HDP 默认不支持安装到 isoft-serverosv4.2，需手动添加支持\nvim /usr/lib/ambari-server/lib/ambari_commons/resources/os_family.json 添加如下两行，注意缩进和逗号\n4.2 YARN Registry DNS 服务启动失败 lsof -i:53 kill -9 4.3 设置初始检测的系统版本 vim /etc/ambari-server/conf/ambari.properties server.os_family=redhat7 server.os_type=redhat7 ","permalink":"https://www.lvbibir.cn/en/posts/tech/centos-deploy-ambari-2.7.5-and-hdp-3.1.5/","summary":"0 前言 基于 isoft-serveros-v4.2 本文参考以下链接: 安装Ambari 2.7.5 + HDP3.1.5（附安装包） 1 前期准备 1.1 安装包准备 Ambari-2.7.5 HDP-3.1.5 libtirpc-devel: 链接：https://pan.baidu.com/s/1eteZ2jGkSq4Pz5YFfHyJgQ 提取码：6hq3 1.2 服务器配置 主机名 cpu 内存 硬盘 系统版本 ip 地址 node001 4c 10g 50g isoft-serveros-4.2 192.168.150.106 node002 2c","title":"部署 Ambari 2.7.5 + HDP 3.1.5"},{"content":"\n🏡 关于本站 本博客主要记录一些学习生活，和一些个人觉得值得记录的问题及其解决办法。如果本博客能有哪些内容帮助到了你，那也是极好的。\n👦🏻 博主是谁 网络时代的素质教育漏网之鱼 | 晚睡协会常任理事 | 国家级抬杠运动员 | 中国驰名窝里横 | 国宝级老污龟 | 超水平怼人大师 | 一秒入戏准影帝\n精通CSS、JavaScript、PHP、C、C＋＋、C#、java、Ruby、Perl、Lisp、python等单词的拼写；\n熟悉windows、Linux、Mac、Android、IOS等系统的开关机；\n🏹 兴趣爱好 🏃‍♂️ 跑步 | 🎮️ 游戏 | 🎧 音乐 | 📺 动漫 | 🛌 摆烂\n📈 博客变更记录 2022年9月8日 本博客正式加入 十年之约\n今夕何夕，人生能有几个十年\n2022年7月16日 迁移之前发布在 csdn 的文章，将图片外链全部转为七牛图床\n2022年7月4日 hugo 站点试运行，域名：https://www.lvbibir.cn\n2021年8月15日 将阿里云轻量服务器自带的 wordpress 应用改为 docker 应用，wordpress 站点改为 docker-compose 部署\n2021年7月13日 wordpress 博客站点开始运行，域名：https://lvbibir.cn\n","permalink":"https://www.lvbibir.cn/en/about/","summary":"🏡 关于本站 本博客主要记录一些学习生活，和一些个人觉得值得记录的问题及其解决办法。如果本博客能有哪些内容帮助到了你，那也是极好的。 👦🏻 博主是谁 网络时代的素质教育漏网之鱼 | 晚睡协会常任理事 | 国家级抬杠运动员 | 中国驰名窝里横 | 国宝级老污龟 | 超水平怼人大师 | 一秒入戏准影帝 精通CSS、Ja","title":"🙋🏻‍♂️ 关于"},{"content":"👉友链为随机顺序 cuikx\u0026#39;s blog cuikx\u0026#39;s blog Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 陈桂林博客 成功最有效的方法就是向有经验的人学习！ 黄忠德的博客 DevOps,SRE,Python,Golang程序员,开源爱好者 阿虚同学的储物间 收集了很多实用网站 老生杂谈的 IT 人 老生杂谈，后继有人。 Yunyi’s Blog Little squirrel Hopping around Yuin’s blog The world is your oyster 👉友链格式 名称: lvbibir's Blog\n网址: https://www.lvbibir.cn\n图标: https://www.lvbibir.cn/img/avatar.gif\n描述: life is a fucking movie\n👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告\n","permalink":"https://www.lvbibir.cn/en/links/","summary":"👉友链为随机顺序 cuikx\u0026#39;s blog cuikx\u0026#39;s blog Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 陈桂林博客 成功最有效的方法就是向有经验的人学习！ 黄忠德的博客 DevOps,SRE,Python,Golang程序员,开源爱好者 阿虚同学的储物间 收集了很多实用网站 老生杂谈的 IT 人 老生杂谈，后继有人。 Yunyi’s Blog Little squirrel Hopping around Yu","title":"🤝 友链"},{"content":"0 前言 本文参考以下链接:\n官方文档 kolla-ansible 部署 all-in-one 单节点 openstack kolla-ansible 添加新节点 (nova 和 cinder 服务) kolla ansible 部署 openstack 高可用集群 1 kolla ansible 简介 kolla 的使命是为 openstack 云平台提供生产级别的、开箱即用的交付能力。kolla 的基本思想是一切皆容器，将所有服务基于 Docker 运行，并且保证一个容器只跑一个服务（进程），做到最小粒度的运行 docker。\nkolla 要实现 openetack 部署总体上分为两步，第一步是制作 docker 镜像，第二步是编排部署。因此，kolla 项目又被分为两个小项目：kolla、kolla-ansible 。\nkolla-ansible 项目 kolla 项目 dockerhub 镜像地址\n2 部署 openstack 集群 2.1 安装环境准备 官方部署文档\n本次部署 train 版 all-in-one 单节点，使用一台 centos7.8 minimal 节点进行部署，该节点同时作为控制节点、计算节点、网络节点和 cinder 存储节点使用，同时也是 kolla ansible 的部署节点。\nkolla 安装节点要求：\n2 network interfaces 8GB main memory 40GB disk space\n如果是 vmware workstation 环境，勾选处理器选项的虚拟化引擎相关功能，否则后面需要配置 nova_compute_virt_type=qemu 参数，这里选择勾选，跳过以下步骤。\ncat /etc/kolla/globals.yml nova_compute_virt_type: \u0026#34;qemu\u0026#34; #或者部署完成后手动调整 [root@kolla ~]# cat /etc/kolla/nova-compute/nova.conf |grep virt_type #virt_type = kvm virt_type = qemu [root@kolla ~]# docker restart nova_compute kolla 的安装要求目标机器至少两块网卡，本次安装使用 2 块网卡对应管理网络和外部网络两个网络平面，在 vmware workstation 虚拟机新增一块网卡 ens34：\nens32，NAT 模式，管理网络，正常配置静态 IP 即可。租户网络与该网络复用，租户 vm 网络不单独创建网卡 ens34，桥接模式，外部网络，无需配置 IP 地址，这个其实是让 neutron 的 br-ex 绑定使用，虚拟机通过这块网卡访问外网。\nens34 网卡 配置参考\ncat \u0026gt; /etc/sysconfig/network-scripts/ifcfg-ens34 \u0026lt;\u0026lt;EOF NAME=ens34 DEVICE=ens34 TYPE=Ethernet ONBOOT=\u0026#34;yes\u0026#34; BOOTPROTO=\u0026#34;none\u0026#34; EOF #重新加载ens34网卡设备 nmcli con reload \u0026amp;\u0026amp; nmcli con up ens34 如果启用 cinder 还需要额外添加磁盘，这里以添加一块/dev/sdb 磁盘为例，创建为物理卷并加入卷组。\npvcreate /dev/sdb vgcreate cinder-volumes /dev/sdb 注意卷组名称为 cinder-volumes，默认与后面的 globals.yml 中定义一致。\n[root@kolla ~]# cat /etc/kolla/globals.yml | grep cinder_volume_group #cinder_volume_group: \u0026#34;cinder-volumes\u0026#34; 2.2 部署 kolla ansible 配置主机名,kolla 预检查时 rabbitmq 可能需要能够进行主机名解析\nhostnamectl set-hostname kolla 安装依赖\nyum install -y python-devel libffi-devel gcc openssl-devel libselinux-python python2-pip python-pbr epel-release ansible 配置阿里云 pip 源，否则 pip 安装时会很慢\nmkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF 安装 kolla-ansible\nkolla 版本与 openstack 版本对应关系\npip install setuptools==22.0.5 pip install pip==20.3.4 pip install wheel pip install kolla-ansible==9.1.0 --ignore-installed PyYAML 复制 kolla-ansible 配置文件到当前环境\nmkdir -p /etc/kolla chown $USER:$USER /etc/kolla cp -r /usr/share/kolla-ansible/etc_examples/kolla/* /etc/kolla cp /usr/share/kolla-ansible/ansible/inventory/* . 修改 ansible 配置文件\ncat \u0026lt;\u0026lt; EOF | sed -i \u0026#39;/^\\[defaults\\]$/ r /dev/stdin\u0026#39; /etc/ansible/ansible.cfg host_key_checking=False pipelining=True forks=100 EOF 默认有 all-in-one 和 multinode 两个 inventory 文件，这里使用 all-in-one，来规划集群角色，配置默认即可\n[root@kolla ~]# cat all-in-one | more 检查 inventory 配置是否正确，执行：\nansible -i all-in-one all -m ping 生成 openstack 组件用到的密码，该操作会填充/etc/kolla/passwords.yml，该文件中默认参数为空。\nkolla-genpwd 修改 keystone_admin_password，可以修改为自定义的密码方便后续 horizon 登录，这里改为 kolla。\n$ sed -i \u0026#39;s#keystone_admin_password:.*#keystone_admin_password: kolla#g\u0026#39; /etc/kolla/passwords.yml $ cat /etc/kolla/passwords.yml | grep keystone_admin_password keystone_admin_password: kolla 修改全局配置文件 globals.yml，该文件用来控制安装哪些组件，以及如何配置组件，由于全部是注释，这里直接追加进去，也可以逐个找到对应项进行修改。\ncp /etc/kolla/globals.yml{,.bak} cat \u0026gt;\u0026gt; /etc/kolla/globals.yml \u0026lt;\u0026lt;EOF # Kolla options kolla_base_distro: \u0026#34;centos\u0026#34; kolla_install_type: \u0026#34;binary\u0026#34; openstack_release: \u0026#34;train\u0026#34; kolla_internal_vip_address: \u0026#34;192.168.150.155\u0026#34; # Docker options # docker_registry: \u0026#34;registry.cn-beijing.aliyuncs.com\u0026#34; # docker_namespace: \u0026#34;kollaimage\u0026#34; # Neutron - Networking Options network_interface: \u0026#34;ens32\u0026#34; neutron_external_interface: \u0026#34;ens34\u0026#34; neutron_plugin_agent: \u0026#34;openvswitch\u0026#34; enable_neutron_provider_networks: \u0026#34;yes\u0026#34; # OpenStack services enable_cinder: \u0026#34;yes\u0026#34; enable_cinder_backend_lvm: \u0026#34;yes\u0026#34; EOF 参数说明：\nkolla_base_distro: kolla 镜像基于不同 linux 发型版构建，主机使用 centos 这里对应使用 centos 类型的 docker 镜像即可。 kolla_install_type: kolla 镜像基于 binary 二进制和 source 源码两种类型构建，实际部署使用 binary 即可。 openstack_release: openstack 版本可自定义，会从 dockerhub 拉取对应版本的镜像 kolla_internal_vip_address: 单节点部署 kolla 也会启用 haproxy 和 keepalived，方便后续扩容为高可用集群，该地址是 ens32 网卡网络中的一个可用 IP。 docker_registry: 默认从 dockerhub 拉取镜像，也可以本地搭建仓库，提前推送镜像上去。 docker_namespace: 阿里云 kolla 镜像仓库所在的命名空间，dockerhub 官网默认是 kolla。 network_interface: 管理网络的网卡 neutron_external_interface: 外部网络的网卡 neutron_plugin_agent: 默认启用 openvswitch enable_neutron_provider_networks: 启用外部网络 enable_cinder: 启用 cinder enable_cinder_backend_lvm: 指定 cinder 后端存储为 lvm 2.3 部署 openstack 组件 部署 openstack\n# 预配置，安装docker、docker sdk、关闭防火墙、配置时间同步等 kolla-ansible -i ./all-in-one bootstrap-servers # 部署前环境检查，可能会报docker版本的错，可以忽略 kolla-ansible -i ./all-in-one prechecks # 拉取镜像，也可省略该步骤，默认会自动拉取 kolla-ansible -i ./all-in-one pull # 执行实际部署，拉取镜像，运行对应组件容器 kolla-ansible -i ./all-in-one deploy # 生成openrc文件 kolla-ansible post-deploy 以上部署没有报错中断说明部署成功，所有 openstack 组件以容器方式运行，查看容器\n[root@kolla ~]# docker ps -a 确认没有 Exited 等异常状态的容器\n[root@kolla ~]# docker ps -a | grep -v Up 本次部署运行了 38 个容器\n[root@localhost kolla-env]# docker ps -a | wc -l 39 查看拉取的镜像\n[root@kolla ~]# docker images | wc -l 39 [root@kolla ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE kolla/centos-binary-heat-api train b97df3444b35 10 months ago 1.11GB kolla/centos-binary-heat-engine train e19de6feec32 10 months ago 1.11GB ...... 查看 cinder 使用的卷，自动创建了 lvm\n[root@kolla ~]# lsblk | grep cinder ├─cinder--volumes-cinder--volumes--pool_tmeta 253:3 0 20M 0 lvm │ └─cinder--volumes-cinder--volumes--pool 253:5 0 19G 0 lvm └─cinder--volumes-cinder--volumes--pool_tdata 253:4 0 19G 0 lvm └─cinder--volumes-cinder--volumes--pool 253:5 0 19G 0 lvm [root@kolla ~]# lvs | grep cinder cinder-volumes-pool cinder-volumes twi-a-tz-- 19.00g 0.00 10.55 另外需要注意，不要在该节点安装 libvirt 等工具，这些工具安装后可能会启用 libvirtd 和 iscsid.sock 等服务，kolla 已经在容器中运行了这些服务，这些服务会调用节点上的 sock 文件，如果节点上也启用这些服务去抢占这些文件，会导致容器异常。默认 kolla 在预配置时也会主动禁用节点上的相关服务。\n2.4 安装 openStack 客户端 可以直接安装到服务器上或者使用 docker 安装容器\n推荐使用 docker 容器方式运行客户端\n使用 docker 容器作为客户端\ndocker run -d --name client \\ --restart always \\ -v /etc/kolla/admin-openrc.sh:/admin-openrc.sh:ro \\ -v /usr/share/kolla-ansible/init-runonce:/init-runonce:rw \\ kolla/centos-binary-openstack-base:train sleep infinity docker exec -it client bash source /admin-openrc.sh openstack service list yum 安装 openstack 客户端\n#启用openstack存储库 yum install -y centos-release-openstack-train #安装openstack客户端 yum install -y python-openstackclient #启用selinux,安装openstack-selinux软件包以自动管理OpenStack服务的安全策略 yum install -y openstack-selinux #报错处理 pip uninstall urllib3 yum install -y python2-urllib3 2.5 运行 cirros 实例 kolla ansible 提供了一个快速创建 cirros demo 实例的脚本/usr/share/kolla-ansible/init-runonce。\n脚本需要 cirros 镜像，如果网络较慢可以使用浏览器下载放在/opt/cache/files 目录下：\nwget https://github.com/cirros-dev/cirros/releases/download/0.4.0/cirros-0.4.0-x86_64-disk.img mkdir -p /opt/cache/files/ mv cirros-0.4.0-x86_64-disk.img /opt/cache/files/ 定义 init-runonce 示例脚本外部网络配置：\n#定义init-runonce示例脚本外部网络配置 vim /usr/share/kolla-ansible/init-runonce EXT_NET_CIDR=${EXT_NET_CIDR:-\u0026#39;192.168.35/24\u0026#39;} EXT_NET_RANGE=${EXT_NET_RANGE:-\u0026#39;start=192.168.35.150,end=192.168.35.188\u0026#39;} EXT_NET_GATEWAY=${EXT_NET_GATEWAY:-\u0026#39;192.168.35.1\u0026#39;} #执行脚本，上传镜像到glance，创建内部网络、外部网络、flavor、ssh key，并运行一个实例 source /etc/kolla/admin-openrc.sh /usr/share/kolla-ansible/init-runonce 参数说明：\nEXT_NET_CIDR 指定外部网络，由于使用桥接模式，直接桥接到了电脑的无线网卡，所以这里网络就是无线网卡的网段。 EXT_NET_RANGE 指定从外部网络取出一个地址范围，作为外部网络的地址池 EXT_NET_GATEWAY 外部网络网关，这里与 wifi 网络使用的网关一致\n根据最终提示运行实例\nopenstack server create \\ --image cirros \\ --flavor m1.tiny \\ --key-name mykey \\ --network demo-net \\ demo1 2.6 访问 openstack horizon 访问 openstack horizon 需要使用 vip 地址，节点上可以看到由 keepalived 容器生成的 vip\n[root@kolla ~]# ip a |grep ens32 2: ens32: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 192.168.150.101/24 brd 192.168.150.255 scope global noprefixroute dynamic ens32 inet 192.168.150.155/32 scope global ens32 浏览器直接访问该地址即可登录到 horizon\nhttp://192.168.150.155\n我这里的用户名密码为 admin/kolla，信息可以从 admin-openrc.sh 中获取\n[root@kolla ~]# cat /etc/kolla/admin-openrc.sh # Clear any old environment that may conflict. for key in $( set | awk \u0026#39;{FS=\u0026#34;=\u0026#34;} /^OS_/ {print $1}\u0026#39; ); do unset $key ; done export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=kolla export OS_AUTH_URL=http://192.168.150.155:35357/v3 export OS_INTERFACE=internal export OS_ENDPOINT_TYPE=internalURL export OS_IDENTITY_API_VERSION=3 export OS_REGION_NAME=RegionOne export OS_AUTH_PLUGIN=password 默认登录后如下\n在 horizion 查看创建的网络和实例\n登录实例控制台，验证实例与外网的连通性，cirros 用户密码在初次登录时有提示：\n为实例绑定浮动 IP 地址，方便从外部 ssh 远程连接到实例\n点击 + 随机分配一个浮动 IP\n在实例界面可以看到绑定的浮动 ip\n在 kolla 节点上或者在集群外部使用 SecureCRT 等 ssh 工具连接到实例。cirros 镜像默认用户密码为 cirros/gocubsgo，该镜像信息官网有 介绍\n[root@kolla ~]# ssh cirros@192.168.35.183 cirros@192.168.35.183\u0026#39;s password: 2.7 运行 centos 实例 centos 官方维护有相关 cloud image，如果不需要进行定制，可以直接下载来运行实例 参考\nCentOS 官方维护的镜像 下载地址\n也可以使用命令直接下载镜像，但是下载可能较慢，建议下载好在进行上传。以 centos7.8 为例：\nwget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2c 下载完成后上传镜像到 openstack，直接在 horizon 上传即可。也可以使用命令上传。\n注意：默认该镜像运行的实例只能使用 ssh key 以 centos 用户身份登录，如果需要使用 root 远程 ssh 连接到实例需要在上传前为镜像配置 root 免密并开启 ssh 访问, 参考\n另外我们的命令客户端在容器中，所有这里有些不方便，首先要将镜像复制到容器中，然后使用 openstack 命令上传。\n这里复制到 client 容器的根目录下。\n[root@kolla ~]# docker cp CentOS-7-x86_64-GenericCloud-2003.qcow2c client:/ [root@kolla ~]# docker exec -it client bash ()[root@f11a103c5ade /]# ()[root@f11a103c5ade /]# source /admin-openrc.sh ()[root@f11a103c5ade /]# ls | grep CentOS CentOS-7-x86_64-GenericCloud-2003.qcow2c 执行以下 openstack 命令上传镜像\nopenstack image create \u0026#34;CentOS78-image\u0026#34; \\ --file CentOS-7-x86_64-GenericCloud-2003.qcow2c \\ --disk-format qcow2 --container-format bare \\ --public 创建实例\nopenstack server create \\ --image CentOS78-image \\ --flavor m1.small \\ --key-name mykey \\ --network demo-net \\ demo-centos 创建完成后为实例绑定浮动 IP。\n如果实例创建失败可以查看相关组件报错日志\n[root@kolla ~]# tail -100f /var/log/kolla/nova/nova-compute.log 如果没有提前定制镜像修改 root 密码，只能使用 centos 用户及 sshkey 登录，由于是在容器中运行的 demo 示例，ssh 私钥也保存在容器的默认目录下，在容器中连接实例浮动 IP 测试\n[root@kolla ~]# docker exec -it client bash ()[root@b86f87f7f101 ~]# ssh -i /root/.ssh/id_rsa centos@192.168.35.186 Last login: Fri Oct 29 08:10:42 2021 from 192.168.35.188 [centos@demo-centos ~]$ sudo -i [root@demo-centos ~]# 2.8 运行 ubuntu 实例 下载镜像\nwget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img docker cp bionic-server-cloudimg-amd64.img client:/ 上传镜像\nopenstack image create \u0026#34;Ubuntu1804\u0026#34; \\ --file bionic-server-cloudimg-amd64.img \\ --disk-format qcow2 --container-format bare \\ --public 创建实例\nopenstack server create \\ --image Ubuntu1804 \\ --flavor m1.small \\ --key-name mykey \\ --network demo-net \\ demo-ubuntu 绑定浮动 ip\nubuntu 镜像默认用户为 ubuntu，首次登陆使用 sshkey 方式\n3 调整集群配置 3.1 新增 magnum \u0026amp; ironic 组件 magnum 和 ironic 默认状态下是没有安装的，在 /etc/kolla/globals.yml 可以看到默认配置\n#enable_magnum: \u0026#34;no\u0026#34; #enable_ironic: \u0026#34;no\u0026#34; 在 /etc/kolla/globals.yml 之前的配置下面新增如下，参数的具体含义查看 官方文档\n# ironic enable_ironic: true ironic_dnsmasq_interface: \u0026#34;enp11s0f1\u0026#34; ironic_dnsmasq_dhcp_range: \u0026#34;192.168.45.200,192.168.45.210\u0026#34; ironic_dnsmasq_default_gateway: 192.168.45.1 ironic_cleaning_network: \u0026#34;public1\u0026#34; ironic_dnsmasq_boot_file: pxelinux.0 # magnum enable_magnum: true ironic 组件还需要一些其他操作\nmkdir -p /etc/kolla/config/ironic/ curl https://tarballs.openstack.org/ironic-python-agent/dib/files/ipa-centos7-master.kernel -o /etc/kolla/config/ironic/ironic-agent.kernel curl https://tarballs.openstack.org/ironic-python-agent/dib/files/ipa-centos7-master.initramfs -o /etc/kolla/config/ironic/ironic-agent.initramfs 在现有集群中新增组件\nkolla-ansible -i all-in-one deploy --tags horizon,magnum,ironic 3.2 修改组件配置 集群部署完成后需要开启新的组件或者扩容，可以修改/etc/kolla/global.yml 调整参数。\n或者在/etc/kolla/config 目录下创建自定义配置文件，例如\n# mkdir -p /etc/kolla/config/nova # vim /etc/kolla/config/nova/nova.conf [DEFAULT] block_device_allocate_retries = 300 block_device_allocate_retries_interval = 3 重新配置 openstack，kolla 会自动重建配置变动的容器组件。\nkolla-ansible -i all-in-one reconfigure -t nova 3.3 kolla 配置和日志文件 各个组件配置文件目录： /etc/kolla/ 各个组件日志文件目录：/var/log/kolla/ 3.4 清理 kolla ansilbe 集群 kolla-ansible destroy --include-images --yes-i-really-really-mean-it # 或者 [root@kolla ~]# cd /usr/share/kolla-ansible/tools/ [root@all tools]# ./cleanup-containers [root@all tools]# ./cleanup-host #重置cinder卷，谨慎操作 vgremove cinder-volumes 3.5 重新部署 kolla ansible 集群 ## 清除操作 先关闭所有运行的实例，再进行下面操作 [root@kolla ~]# cd /usr/share/kolla-ansible/tools/ [root@all tools]# ./cleanup-containers vgremove cinder-volumes ## 重建操作 pvcreate /dev/sdb vgcreate cinder-volumes /dev/sdb kolla-ansible -i ./all-in-one deploy kolla-ansible post-deploy 4 可能遇到的问题 4.1 虚拟 ip 分配失败 这种情况多半是由于虚拟 ip 没有分配到，并不是端口问题\n解决方法 1 在全局的配置中添加/修改这个 id 值，必须是 0-255 之间的数字，并且确保在整个二层网络中是唯一的\nvim /etc/kolla/globals.yml keepalived_virtual_router_id: \u0026#34;199\u0026#34; 解决方法 2 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/openstack-kolla-ansible-allinone-train/","summary":"0 前言 本文参考以下链接: 官方文档 kolla-ansible 部署 all-in-one 单节点 openstack kolla-ansible 添加新节点 (nova 和 cinder 服务) kolla ansible 部署 openstack 高可用集群 1 kolla ansible 简介 kolla 的使命是为 openstack 云平台提供生产级别的、开箱即用的交付能力。kolla 的基本思想是一切皆容器，将所有服务基于 Docker 运行，并且保证一个容器只跑一个服务（进程），做到最小粒度的运行 docke","title":"kolla-ansible 部署 openstack (Train) (all-in-one)"},{"content":"1 Kubernetes 概述 kubernetes 是什么\nkubernetes 是 Google 在 2014 年开源的一个容器集群管理平台，kubernetes 简称 k8s k8s 用于容器化应用程序的部署，扩展和管理。 k8s 提供了容器的编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能 kubernetes 目标是让部署容器化应用简单高效 Kubernetes 特性\n自我修复 在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端请求，确保线上服务不中断。 伸缩性 使用命令、UI 或者基于 CPU 使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行服务。 自动部署和回滚 K8S 采用滚动更新策略更新应用，一次更新一个 Pod，而不是同时删除所有 Pod，如果更新过程中出现问题，将回滚更改，确保升级不受影响业务。 服务发现和负载均衡 K8S 为多个容器提供一个统一访问入口（内部 IP 地址和一个 DNS 名称），并且负载均衡关联的所有容器，使得用户无需考虑容器 IP 问题。 机密和配置管理 管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在 K8S 中，方便应用程序使用。 存储编排 挂载外部存储系统，无论是来自本地存储，公有云（如 AWS），还是网络存储（如 NFS、GlusterFS、Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。 批处理 提供一次性任务，定时任务；满足批量数据处理和分析的场景。 Kubeadm 概述\nkubeadm 是 Kubernetes 项目自带的及集群构建工具，负责执行构建一个最小化的可用集群以及将其启动等的必要基本步骤，kubeadm 是 Kubernetes 集群全生命周期的管理工具，可用于实现集群的部署、升级、降级及拆除。kubeadm 部署 Kubernetes 集群是将大部分资源以 pod 的方式运行，例如（kube-proxy、kube-controller-manager、kube-scheduler、kube-apiserver、flannel) 都是以 pod 方式运行。 Kubeadm 仅关心如何初始化并启动集群，余下的其他操作，例如安装 Kubernetes Dashboard、监控系统、日志系统等必要的附加组件则不在其考虑范围之内，需要管理员自行部署。 Kubeadm 集成了 Kubeadm init 和 kubeadm join 等工具程序，其中 kubeadm init 用于集群的快速初始化，其核心功能是部署 Master 节点的各个组件，而 kubeadm join 则用于将节点快速加入到指定集群中，它们是创建 Kubernetes 集群最佳实践的“快速路径”。另外，kubeadm token 可于集群构建后管理用于加入集群时使用的认证令牌（token)，而 kubeadm reset 命令的功能则是删除集群构建过程中生成的文件以重置回初始状态。 2 环境准备 基于 centos7.9，docker-ce-20.10.18，kubelet-1.22.3-0\n部署 Kubernetes 集群需要满足每个节点至少满足 2 核 CPU、2G 内存和 30GB 硬盘且都可以访问外网\n角色 IP k8s-node1 1.1.1.1 k8s-node2 1.1.1.2 k8s-node3 1.1.1.3 2.1 基础配置 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭selinux sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭swap swapoff -a # 临时 vim /etc/fstab # 永久, 注释掉swap分区相关行 # 设置主机名 hostnamectl set-hostname \u0026lt;hostname\u0026gt; # 添加hosts cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 1.1.1.1 k8s-node1 1.1.1.2 k8s-node2 1.1.1.3 k8s-node3 EOF # 将桥接的IPv4流量传递到iptables的链 cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 生效 # 时间同步 timedatectl set-timezone Asia/Shanghai yum install ntpdate -y ntpdate time.windows.com 2.2 安装 Docker Kubernetes 默认 CRI（容器运行时）为 Docker，因此先安装 Docker。\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo yum list docker-ce --show-duplicates yum install docker-ce-20.10.23-3.el7.x86_64 配置镜像下载加速器，同时修改 docker 的 cgroupdriver 为 systemd\nmkdir /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34; ], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF systemctl daemon-reload systemctl enable docker \u0026amp;\u0026amp; systemctl start docker docker info 2.3 kubeadm/kubelet/kubectl 添加阿里云 YUM 软件源\ncat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 这里指定版本号部署\nyum install -y kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3 systemctl enable kubelet systemctl start kubelet 3 部署 Kubernetes Master 官方文档 1 官方文档 2\n在 1.1.1.1（Master）执行\nkubeadm init \\ --apiserver-advertise-address=1.1.1.1 \\ --kubernetes-version v1.22.3 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all \\ --image-repository registry.aliyuncs.com/google_containers \u0026ndash;apiserver-advertise-address 集群通告地址 \u0026ndash;kubernetes-version K8s 版本，与上面安装的一致 \u0026ndash;service-cidr 集群内部虚拟网络，Pod 统一访问入口 \u0026ndash;pod-network-cidr Pod 网络，与下面部署的 CNI 网络组件 yaml 中保持一致 \u0026ndash;ignore-preflight-errors=all，跳过一些错误 \u0026ndash;image-repository 由于默认拉取镜像地址 k8s.gcr.io 国内无法访问，这里指定阿里云镜像仓库地址 或者使用配置文件引导：\ncat \u0026gt; kubeadm.conf \u0026lt;\u0026lt; EOF apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.22.3 imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12 EOF kubeadm init --config kubeadm.conf --ignore-preflight-errors=all 拷贝 kubectl 使用的连接 k8s 认证文件到默认路径：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看 k8s 集群状态\nkubectl get cs NAME STATUS MESSAGE ERROR scheduler Unhealthy Get \u0026#34;http://127.0.0.1:10251/healthz\u0026#34;: dial tcp 127.0.0.1:10251: connect: connection refused controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} vim /etc/kubernetes/manifests/kube-scheduler.yaml # 注释掉 --port=0 ，scheduler会自动重启，稍等一小会状态变为正常 kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} 4 加入 Kubernetes Node 官方文档\n在 192.168.150.102/103（Node）执行。\n向集群添加新节点，执行在 kubeadm init 输出的 kubeadm join 命令：\nkubeadm join 1.1.1.1:6443 --token esce21.q6hetwm8si29qxwn \\ --discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5 默认 token 有效期为 24 小时，当过期之后，该 token 就不可用了。这时就需要重新创建 token，操作如下：\nkubeadm token create kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; 63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 kubeadm join 1.1.1.1:6443 --token nuja6n.o3jrhsffiqs9swnu --discovery-token-ca-cert-hash sha256:63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 或者直接命令快捷生成: kubeadm token create --print-join-command\n5 部署容器网络 (cni) Calico 是一个纯三层的数据中心网络方案，Calico 支持广泛的平台，包括 Kubernetes、OpenStack 等。\nCalico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。\n此外，Calico 项目还实现了 Kubernetes 网络策略，提供 ACL 功能。\nquickstart\n版本对照表，在此页面可以看到 calico 每个版本支持的 kubernetes 的版本\n安装 calico\nwget --no-check-certificate https://docs.tigera.io/archive/v3.23/manifests/calico.yaml 修改 Pod 网络和网卡识别参数，Pod 网络与前面 kubeadm init 指定的一样\n[root@k8s-node1 ~]# vim calico.yaml # 修改位置：DaemonSet.spec.template.spec.containers.env # 新增如下四行 - name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.244.0.0/16\u0026#34; - name: IP_AUTODETECTION_METHOD value: interface=bond*,ens* #网卡名根据实际情况修改 kubectl apply -f calico.yaml kubectl get pods -n kube-system # 所有Pod起来后，节点状态应该都是Ready状态了 [root@k8s-node1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node1 Ready control-plane,master 153m v1.22.3 k8s-node2 Ready \u0026lt;none\u0026gt; 151m v1.22.3 k8s-node3 Ready \u0026lt;none\u0026gt; 151m v1.22.3 6 metric-server cadvisor 负责提供数据，已集成到 k8s 中\nMetrics-server 负责数据汇总，需额外安装\n下载 yaml\nwget --no-check-certificate https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.0/components.yaml mv components.yaml metrics-server.yaml 修改 yaml\ncontainers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP # 第一处修改 - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # 第二处修改 image: registry.aliyuncs.com/google_containers/metrics-server:v0.6.0 # 第三处修改 imagePullPolicy: IfNotPresent --kubelet-insecure-tls 不验证 kubelet 自签的证书 `\u0026ndash;kubelet-preferred-address-types=InternalIP Metrics-server 连接 cadvisor 默认通过主机名即 node 的名称进行连接，而 Metric-server 作为 pod 运行在集群中默认是无法解析的，所以这里修改成通过节点 ip 连接 部署 metrics-server\n[root@k8s-node1 ~]# kubectl apply -f metrics-server.yaml [root@k8s-node1 ~]# kubectl get pods -n kube-system -l k8s-app=metrics-server NAME READY STATUS RESTARTS AGE metrics-server-7f66b69ff6-bkfqg 1/1 Running 0 59s [root@k8s-node1 ~]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-node1 226m 11% 2004Mi 54% k8s-node2 97m 4% 1047Mi 28% k8s-node3 98m 4% 1096Mi 29% 7 测试 kubernetes 集群 验证 Pod 工作 验证 Pod 网络通信 验证 DNS 解析 在 Kubernetes 集群中创建一个 pod，验证是否正常运行：\nkubectl create deployment nginx --image=nginx kubectl expose deployment nginx --type=NodePort --port=80 --target-port=80 [root@k8s-node1 ~]# kubectl get pod,deploy,svc NAME READY STATUS RESTARTS AGE pod/nginx-6799fc88d8-57bqd 1/1 Running 0 10m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1/1 1 1 10m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 170m service/nginx NodePort 10.102.188.108 \u0026lt;none\u0026gt; 80:30954/TCP 2m31s 访问地址：http://1.1.1.1:30954，端口是固定的，ip 可以是集群内任一节点的 ip\n8 部署 Dashboard wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml 默认 Dashboard 只能集群内部访问，修改 Service 为 NodePort 类型，暴露到外部：\nvi recommended.yaml ... kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard type: NodePort ... kubectl apply -f recommended.yaml kubectl get pods -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-6b4884c9d5-gl8nr 1/1 Running 0 13m kubernetes-dashboard-7f99b75bf4-89cds 1/1 Running 0 13m 访问地址：https://NodeIP:30001\n创建 service account 并绑定默认 cluster-admin 管理员集群角色：\n# 创建用户 kubectl create serviceaccount dashboard-admin -n kube-system # 用户授权 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取用户Token kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 使用输出的 token 登录 Dashboard。\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/kubernetes-deploy-v1.22.3/","summary":"1 Kubernetes 概述 kubernetes 是什么 kubernetes 是 Google 在 2014 年开源的一个容器集群管理平台，kubernetes 简称 k8s k8s 用于容器化应用程序的部署，扩展和管理。 k8s 提供了容器的编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能 kubernetes 目标是让部署容器化应用简单高效 Kubernetes 特性 自我修复 在节点故障时重新启动失败的容器，替换和重新","title":"kubernetes | kubeadm 搭建 K8s 集群 v1.22.3"},{"content":"0 前言 本文参考以下链接:\nsystemd 和 sysv 的服务管理 systemd-sysv-generator 中文手册 openssh 编译 rpm 1 openssh-9.5p1 1.1 编译环境 编译平台: 华为私有云 系统版本: 麒麟 v7.4 系统内核: 3.10.0-693.43.1.el7.x86_64 软件版本:\nopenssh-9.5p1.tar.gz openssl-3.0.11.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 编译脚本项目地址\n1.2 编译步骤 添加 yum 源略, 云平台自带 yum 源\n创建工作目录\nmkdir -p /opt/openssh-9.5/openssh-9.5p1-update-script/resource 下载 编译脚本, 上传至服务器 /opt/openssh-9.5 目录并解压\ncd /opt/openssh-9.5; unzip openssh-rpms-main.zip yum 安装依赖工具\nyum clean all; yum makecache yum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts --downloadonly --downloaddir=/opt/openssh-9.5/openssh-9.5p1-update-script/resource rpm -Uvh /opt/openssh-9.5/openssh-9.5p1-update-script/resource/*.rpm 默认 openssh 源码中是没有 ssh-copy-id 相关参数的，如果直接编译安装，会发现安装后没有 ssh-copy-id 命令，因此如果需要用到该命令，需要修改编译参数控制文件 openssh.spec, 本次安装使用的操作系统是 el7 系列的\nvim /opt/openssh-9.5/openssh-rpms-main/el7/SPECS/openssh.spec 如下位置添加一行\ninstall -m755 contrib/ssh-copy-id $RPM_BUILD_ROOT/usr/bin/ssh-copy-id 第二处添加\n%attr(0755,root,root) %{_bindir}/ssh-copy-id 下载各源码包上传至服务器 /opt/openssh-9.5/openssh-rpms-main/downloads\nopenssh openssl x11-ssh-askpass 执行编译打包脚本\ncd /opt/openssh-9.5/openssh-rpms-main/; bash compile.sh 脚本应正常运行, 查看编译后的 rpm 包\ncd /opt/openssh-9.5/openssh-rpms-main/el7/RPMS/x86_64/; ls 1.3 升级脚本 拷贝编译后的包到 resource 目录\ncp /opt/openssh-9.5/openssh-rpms-main/el7/RPMS/x86_64/*.rpm /opt/openssh-9.5/openssh-9.5p1-update-script/resource/ 编写升级脚本\ncat \u0026gt; /opt/openssh-9.5/openssh-9.5p1-update-script/run.sh \u0026lt;\u0026lt;EOF #!/bin/bash # # Author : lvbibir # Email : lvbibir@gmail.com # Version : V1.0 # Time : 2023-11-14 16:48:30 # Desc : A script for update ssh set -e ssh -V /bin/cp /etc/pam.d/sshd /etc/pam.d/sshd_bak /bin/cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak yum localinstall -y resource/openssh-*.rpm /bin/cp /etc/pam.d/sshd_bak /etc/pam.d/sshd /bin/cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd ssh -V EOF chmod 755 /opt/openssh-9.5/openssh-9.5p1-update-script/run.sh 打包\ncd /opt/openssh-9.5/ tar zcf openssh-9.5p1-update-script.tar.gz openssh-9.5p1-update-script 上传压缩包至服务器 /opt/ 目录\ncd /opt; tar zxf openssh-9.5p1-update-script.tar.gz cd /opt/openssh-9.5p1-update-script/; sh run.sh 2 openssh-8.7p1 2.1 编译环境 编译平台：\tvmware workstation 系统版本：\t普华服务器操作系统 v4.2 系统内核：\t3.10.0-327.el7.isoft.x86_64 软件版本：\nopenssh-8.7p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 2.2 编译步骤 yum 安装依赖工具\nyum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载 openssh 编译包和 x11-ssh-askpass 依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.7p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.7p1.tar.gz cp openssh-8.7p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 操作验证\ncd /root/rpmbuild/RPMS/x86_64/ vim run.sh #!/bin/bash set -e cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd chmod 755 run.sh ./run.sh ssh -V 2.3 升级脚本 [root@localhost ~]# cd /root/rpmbuild/RPMS/x86_64/ [root@localhost x86_64]# ls openssh-8.7p1-1.el7.isoft.x86_64.rpm openssh-askpass-8.7p1-1.el7.isoft.x86_64.rpm openssh-askpass-gnome-8.7p1-1.el7.isoft.x86_64.rpm openssh-clients-8.7p1-1.el7.isoft.x86_64.rpm openssh-debuginfo-8.7p1-1.el7.isoft.x86_64.rpm openssh-server-8.7p1-1.el7.isoft.x86_64.rpm run.sh [root@localhost x86_64]# vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd [root@localhost x86_64]# tar zcvf openssh-8.7p1.rpm.x86_64.tar.gz ./* [root@localhost x86_64]# mv openssh-8.7p1.rpm.x86_64.tar.gz /root 使用\ntar zxf openssh-8.7p1.rpm.x86_64.tar.gz ./run.sh 3 openssh-9.0p1 3.1 编译环境 编译平台：\tvmware workstation\n系统版本：\t普华服务器操作系统 v3.0\n系统内核：\n2.6.32-279.el6.isoft.x86_64 2.6.32-504.el6.isoft.x86_64 软件版本：\nopenssh-9.0p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 这两个内核版本步骤基本一样，区别在于 279 内核需要升级 openssl\n3.2 编译步骤 添加阿里云 yum 源和本地 yum 源\n# 阿里yum源 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-6.10.repo # 本地yum源 mount /dev/sr0 /mnt/ cat \u0026gt; /etc/yum.repos.d/local.repo \u0026lt;\u0026lt;EOF [local] name=local baseurl=file:///mnt gpgcheck=0 enabled=1 EOF yum 安装依赖工具\nyum clean all yum makecache yum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载 openssh 编译包和 x11-ssh-askpass 依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://mirrors.aliyun.com/pub/OpenBSD/OpenSSH/portable/openssh-9.0p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz --no-check-certificate tar -zxf openssh-9.0p1.tar.gz cp openssh-9.0p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 添加缺少的文件\ncd /root/rpmbuild/SOURCES/openssh-9.0p1/contrib/redhat cp sshd.init sshd.init.old cp sshd.pam sshd.pam.old 重新打包，否则会报错找不到 sshd.pam.old 和 sshd.init.old\ncd /root/rpmbuild/SOURCES/ tar zcf openssh-9.0p1.tar.gz openssh-9.0p1 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 注意，从这步开始两个内核版本的后续操作不太相同\n3.2.1 2.6.32-279.el6.isoft.x86_64 准备目录\nmkdir -pv /root/openssh-9.0p1-rpms/openssl-1.0.1e-rpms/ cp /root/rpmbuild/RPMS/x86_64/* /root/openssh-9.0p1-rpms/ 下载 openssl-1.0.1e 离线包\n这步由于之前安装编译的依赖的时候已经安装过，可以用全新的系统重新下载 openssl-1.0.1e 的依赖\nyum install -y yum-plugin-downloadonly yum install openssl openssl-devel --downloadonly --downloaddir=/root/openssh-9.0p1-rpms/openssl-1.0.1e-rpms/ 编写升级脚本\ncat \u0026gt; /root/openssh-9.0p1-rpms/run.sh \u0026lt;\u0026lt;EOF #!/bin/bash set -e cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -e --nodeps libsepol-2.0.41-4.el6.isoft.x86_64 rpm -Uvh ./openssl-1.0.1e-rpms/*.rpm rpm -Uvh ./*.rpm cp /etc/pam.d/sshd_bak /etc/pam.d/sshd cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key service sshd restart ssh -V EOF chmod 755 /root/openssh-9.0p1-rpms/run.sh 打包\ntar zcf /root/openssh-9.0p1-rpms.tar.gz /root/openssh-9.0p1-rpms 3.2.2 2.6.32-504.el6.isoft.x86_64 准备目录\nmkdir /root/openssh-9.0p1-rpms/ cp /root/rpmbuild/RPMS/x86_64/* /root/openssh-9.0p1-rpms/ 编写升级脚本\ncat \u0026gt; /root/openssh-9.0p1-rpms/run.sh \u0026lt;\u0026lt;EOF #!/bin/bash set -e ssh -V /bin/cp /etc/pam.d/sshd /etc/pam.d/sshd_bak /bin/cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm /bin/cp /etc/pam.d/sshd_bak /etc/pam.d/sshd /bin/cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key service sshd restart ssh -V EOF chmod 755 /root/openssh-9.0p1-rpms/run.sh 打包\ntar zcf /root/openssh-9.0p1-rpms.tar.gz /root/openssh-9.0p1-rpms 3.3 使用 tar zxf openssh-9.0p1-rpms.tar.gz cd openssh-9.0p1-rpms sh run.sh 4 openssh-8.6p1-aarch64 4.1 编译环境 系统版本：普华服务器操作系统 openeuler 版 系统内核：4.19.90-2003.4.0.0036.oe1.aarch64 软件版本： openssh-8.6p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 4.2 编译步骤 dnf 安装依赖工具\ndnf install gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载 openssh 编译包和 x11-ssh-askpass 依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.6p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.6p1.tar.gz cp openssh-8.6p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 修改下面两行 %attr(4711,root,root) %{_libexecdir}/openssh/ssh-sk-helper %attr(0644,root,root) %{_mandir}/man8/ssh-sk-helper.8.gz 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 操作验证\ncd /root/rpmbuild/RPMS/aarch64 vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd chmod 755 run.sh ./run.sh ssh -V OpenSSH_8.6p1, OpenSSL 1.1.1d 10 Sep 2019 从版本看，ssh 已经升级成功。但是每次重启服务都会提示 sshd 的 unit 文件发生改变，需要执行 systemctl daemon-reload。执行完 reload 后重启 sshd 依旧报错 Warning: The unit file, source configuration file or drop-ins of sshd.service changed on disk. Run \u0026lsquo;systemctl daemon-reload\u0026rsquo; to reload units.\n先不管这个问题，测试下 sshd 服务是否正常。\n用终端连接试试\n一切正常，如果出现 PAM unable to dlopen(/usr/lib64/security/pam_stack.so): /usr/lib64/security/pam_stack.so: cannot open shared object file: No such file or directory 类似报错，需要还原原先的/etc/pam.d/sshd 文件\n继续看之前那个报错，一般这种错误为服务的配置文件或者 unit 文件发生改变，需要执行 daemon-reload 重新加载一下，逐个排查\n查看配置文件\n查看 unit 文件\n没有找到 sshd.service 的 unit 文件，find 查找一下\n第一个文件是老版本 ssh 的残留的自启的 unit 链接文件，已经失效了。第三个和第四个文件都是第二个文件的链接文件。\n不知为何我们自己编译的 ssh 安装后 unit 文件会放到这个位置，后续再研究，尝试自己写一份 unit 文件，试试能不能恢复 sshd。\n备份 unit 文件\n[root@localhost ~]# cp /run/systemd/generator.late/sshd.service /root/sshd.service-20210702 查看 unit 文件中的控制参数和 pid 文件位置等\n自建一个 unit 文件，放到/usr/lib/systemd/system 目录\n[root@localhost ~]# vim /usr/lib/systemd/system/sshd.service [UNIT] Description=OpenSSH server daemon After=network.target sshd-keygen.target Wants=sshd-keygen.target [Service] Type=forking ExecStart=/etc/rc.d/init.d/sshd start ExecReload=/etc/rc.d/init.d/sshd restart ExecStop=/etc/rc.d/init.d/sshd stop PrivateTmp=True [Install] WantedBy=multi-user.target [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart sshd [root@localhost ~]# systemctl status sshd [root@localhost ~]# ssh -V OpenSSH_8.6p1, OpenSSL 1.1.1d 10 Sep 2019 打包归档\n[root@localhost ~]# cp /usr/lib/systemd/system/sshd.service /root/rpmbuild/RPMS/aarch64/ [root@localhost ~]# cd /root/rpmbuild/RPMS/aarch64/ [root@localhost aarch64]# ls openssh-8.6p1-1.isoft.isoft.aarch64.rpm openssh-debugsource-8.6p1-1.isoft.isoft.aarch64.rpm openssh-askpass-8.6p1-1.isoft.isoft.aarch64.rpm openssh-server-8.6p1-1.isoft.isoft.aarch64.rpm openssh-askpass-gnome-8.6p1-1.isoft.isoft.aarch64.rpm run.sh openssh-clients-8.6p1-1.isoft.isoft.aarch64.rpm sshd.service openssh-debuginfo-8.6p1-1.isoft.isoft.aarch64.rpm [root@localhost aarch64]# vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config cp ./sshd.service /usr/lib/systemd/system/sshd.service rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd systemctl enable sshd [root@localhost aarch64]# tar zcvf openssh-8.6p1-rpm-aarch64.tar.gz ./* [root@localhost aarch64]# mv openssh-8.6p1-rpm-aarch64.tar.gz /root 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/rpm-build-openssh/","summary":"0 前言 本文参考以下链接: systemd 和 sysv 的服务管理 systemd-sysv-generator 中文手册 openssh 编译 rpm 1 openssh-9.5p1 1.1 编译环境 编译平台: 华为私有云 系统版本: 麒麟 v7.4 系统内核: 3.10.0-693.43.1.el7.x86_64 软件版本: openssh-9.5p1.tar.gz openssl-3.0.11.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 编译脚本项目地址 1.2 编译步骤 添加 yum 源略, 云平台自带 yum 源 创建工作目录 mkdir -p /opt/openssh-9.5/openssh-9.5p1-update-script/resource 下载 编译脚本, 上传至服务器 /opt/openssh-9.5 目录并解压 cd /opt/openssh-9.5; unzip openssh-rpms-main.zip yum 安装依赖工具 yum clean all; yum makecache yum install","title":"openssh 源码打包编译成 rpm 包"},{"content":"代码如下\n#!/bin/bash #参数定义 date=`date +\u0026#34;%Y-%m-%d-%H:%M:%S\u0026#34;` centosVersion=$(awk \u0026#39;{print $(NF-1)}\u0026#39; /etc/redhat-release) VERSION=`date +%F` #日志相关 LOGPATH=\u0026#34;/tmp/awr\u0026#34; [ -e $LOGPATH ] || mkdir -p $LOGPATH RESULTFILE=\u0026#34;$LOGPATH/HostCheck-`hostname`-`date +%Y%m%d`.txt\u0026#34; #调用函数库 [ -f /etc/init.d/functions ] \u0026amp;\u0026amp; source /etc/init.d/functions export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile #root用户执行脚本 [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34;请用root用户执行此脚本！\u0026#34; \u0026amp;\u0026amp; exit 1 function version(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;[${date}] \u0026gt;\u0026gt;\u0026gt; `hostname -s` 主机巡检\u0026#34; } function getSystemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m****************************************************系统检查****************************************************\\033[0m\u0026#34; if [ -e /etc/sysconfig/i18n ];then default_LANG=\u0026#34;$(grep \u0026#34;LANG=\u0026#34; /etc/sysconfig/i18n | grep -v \u0026#34;^#\u0026#34; | awk -F \u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39;)\u0026#34; else default_LANG=$LANG fi export LANG=\u0026#34;en_US.UTF-8\u0026#34; Release=$(cat /etc/redhat-release 2\u0026gt;/dev/null) Kernel=$(uname -r) OS=$(uname -o) Hostname=$(uname -n) SELinux=$(/usr/sbin/sestatus | grep \u0026#34;SELinux status: \u0026#34; | awk \u0026#39;{print $3}\u0026#39;) LastReboot=$(who -b | awk \u0026#39;{print $3,$4}\u0026#39;) uptime=$(uptime | sed \u0026#39;s/.*up \\([^,]*\\), .*/\\1/\u0026#39;) echo \u0026#34; 系统：$OS\u0026#34; echo \u0026#34; 发行版本：$Release\u0026#34; echo \u0026#34; 内核：$Kernel\u0026#34; echo \u0026#34; 主机名：$Hostname\u0026#34; echo \u0026#34; SELinux：$SELinux\u0026#34; echo \u0026#34;语言/编码：$default_LANG\u0026#34; echo \u0026#34; 当前时间：$(date +\u0026#39;%F %T\u0026#39;)\u0026#34; echo \u0026#34; 最后启动：$LastReboot\u0026#34; echo \u0026#34; 运行时间：$uptime\u0026#34; export LANG=\u0026#34;$default_LANG\u0026#34; } function getCpuStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m****************************************************CPU检查*****************************************************\\033[0m\u0026#34; Physical_CPUs=$(grep \u0026#34;physical id\u0026#34; /proc/cpuinfo| sort | uniq | wc -l) Virt_CPUs=$(grep \u0026#34;processor\u0026#34; /proc/cpuinfo | wc -l) CPU_Kernels=$(grep \u0026#34;cores\u0026#34; /proc/cpuinfo|uniq| awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39;) CPU_Type=$(grep \u0026#34;model name\u0026#34; /proc/cpuinfo | awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39; | sort | uniq) CPU_Arch=$(uname -m) echo \u0026#34;物理CPU个数:$Physical_CPUs\u0026#34; echo \u0026#34;逻辑CPU个数:$Virt_CPUs\u0026#34; echo \u0026#34;每CPU核心数:$CPU_Kernels\u0026#34; echo \u0026#34; CPU型号:$CPU_Type\u0026#34; echo \u0026#34; CPU架构:$CPU_Arch\u0026#34; } function getMemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************内存检查*****************************************************\\033[0m\u0026#34; if [[ $centosVersion \u0026lt; 7 ]];then free -mo else free -h fi #报表信息 MemTotal=$(grep MemTotal /proc/meminfo| awk \u0026#39;{print $2}\u0026#39;) #KB MemFree=$(grep MemFree /proc/meminfo| awk \u0026#39;{print $2}\u0026#39;) #KB let MemUsed=MemTotal-MemFree MemPercent=$(awk \u0026#34;BEGIN {if($MemTotal==0){printf 100}else{printf \\\u0026#34;%.2f\\\u0026#34;,$MemUsed*100/$MemTotal}}\u0026#34;) } function getDiskStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************磁盘检查******************************************************\\033[0m\u0026#34; df -hiP | sed \u0026#39;s/Mounted on/Mounted/\u0026#39;\u0026gt; /tmp/inode df -hTP | sed \u0026#39;s/Mounted on/Mounted/\u0026#39;\u0026gt; /tmp/disk join /tmp/disk /tmp/inode | awk \u0026#39;{print $1,$2,\u0026#34;|\u0026#34;,$3,$4,$5,$6,\u0026#34;|\u0026#34;,$8,$9,$10,$11,\u0026#34;|\u0026#34;,$12}\u0026#39;| column -t #报表信息 diskdata=$(df -TP | sed \u0026#39;1d\u0026#39; | awk \u0026#39;$2!=\u0026#34;tmpfs\u0026#34;{print}\u0026#39;) #KB disktotal=$(echo \u0026#34;$diskdata\u0026#34; | awk \u0026#39;{total+=$3}END{print total}\u0026#39;) #KB diskused=$(echo \u0026#34;$diskdata\u0026#34; | awk \u0026#39;{total+=$4}END{print total}\u0026#39;) #KB diskfree=$((disktotal-diskused)) #KB diskusedpercent=$(echo $disktotal $diskused | awk \u0026#39;{if($1==0){printf 100}else{printf \u0026#34;%.2f\u0026#34;,$2*100/$1}}\u0026#39;) inodedata=$(df -iTP | sed \u0026#39;1d\u0026#39; | awk \u0026#39;$2!=\u0026#34;tmpfs\u0026#34;{print}\u0026#39;) inodetotal=$(echo \u0026#34;$inodedata\u0026#34; | awk \u0026#39;{total+=$3}END{print total}\u0026#39;) inodeused=$(echo \u0026#34;$inodedata\u0026#34; | awk \u0026#39;{total+=$4}END{print total}\u0026#39;) inodefree=$((inodetotal-inodeused)) inodeusedpercent=$(echo $inodetotal $inodeused | awk \u0026#39;{if($1==0){printf 100}else{printf \u0026#34;%.2f\u0026#34;,$2*100/$1}}\u0026#39;) } function get_resource(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************资源消耗统计**************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m*************带宽资源消耗统计*************\\033[0m\u0026#34; #用数组存放网卡名 nic=(`ifconfig | grep ^[a-z] | grep -vE \u0026#39;lo|docker0\u0026#39;| awk -F: \u0026#39;{print $1}\u0026#39;`) time=`date \u0026#34;+%Y-%m-%d %k:%M\u0026#34;` num=0 for ((i=0;i\u0026lt;${#nic[@]};i++)) do #循环五次，避免看到的是偶然的数据 while (( $num\u0026lt;5 )) do rx_before=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $2}\u0026#39;) tx_before=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $10}\u0026#39;) sleep 2 #用sed先获取第7列,再用awk获取第2列，再cut切割,从第7个到最后，即只切割网卡流量数字部分 rx_after=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $2}\u0026#39;) tx_after=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $10}\u0026#39;) #注意下面截取的相差2秒的两个时刻的累计和发送的bytes(即累计传送和接收的位) rx_result=$[(rx_after-rx_before)/1024/1024/2*8] tx_result=$[(tx_after-tx_before)/1024/1024/2*8] echo \u0026#34;$time Now_In_Speed: $rx_result Mbps Now_OUt_Speed: $tx_result Mbps\u0026#34; \u0026gt;\u0026gt; /tmp/network.txt let \u0026#34;num++\u0026#34; done #注意下面grep后面的$time变量要用双引号括起来 rx_result=$(cat /tmp/network.txt|grep \u0026#34;$time\u0026#34;|awk \u0026#39;{In+=$4}END{print In}\u0026#39;) tx_result=$(cat /tmp/network.txt|grep \u0026#34;$time\u0026#34;|awk \u0026#39;{Out+=$7}END{print Out}\u0026#39;) In_Speed=$(echo \u0026#34;scale=2;$rx_result/5\u0026#34;|bc) Out_Speed=$(echo \u0026#34;scale=2;$tx_result/5\u0026#34;|bc) echo -e \u0026#34;\\033[32m In_Speed_average: $In_Speed Mbps Out_Speed_average: $Out_Speed Mbps! \\033[0m\u0026#34; done echo -e \u0026#34;\\033[36m*************CPU资源消耗统计*************\\033[0m\u0026#34; #使用vmstat 1 5命令统计5秒内的使用情况，再计算每秒使用情况 total=`vmstat 1 5|awk \u0026#39;{x+=$13;y+=$14}END{print x+y}\u0026#39;` cpu_average=$(echo \u0026#34;scale=2;$total/5\u0026#34;|bc) #判断CPU使用率（浮点数与整数比较） if [ `echo \u0026#34;${cpu_average} \u0026gt; 70\u0026#34; | bc` -eq 1 ];then echo -e \u0026#34;\\033[31m Total CPU is already use: ${cpu_average}%,请及时处理！\\033[0m\u0026#34; else echo -e \u0026#34;\\033[32m Total CPU is already use: ${cpu_average}%! \\033[0m\u0026#34; fi echo -e \u0026#34;\\033[36m*************磁盘资源消耗统计*************\\033[0m\u0026#34; #磁盘使用情况(注意：需要用sed先进行格式化才能进行累加处理) disk_used=$(df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{used+=$3} END{print used}\u0026#39;) disk_totalSpace=$(df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{totalSpace+=$2} END{print totalSpace}\u0026#39;) disk_all=$(echo \u0026#34;scale=4;$disk_used/$disk_totalSpace\u0026#34; | bc) disk_percent1=$(echo $disk_all | cut -c 2-3) disk_percent2=$(echo $disk_all | cut -c 4-5) disk_warning=`df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{if ($5\u0026gt;85) print $6 \u0026#34;目录使用率：\u0026#34; $5;} \u0026#39;` echo -e \u0026#34;\\033[32m Total disk has used: $disk_percent1.$disk_percent2% \\033[0m\u0026#34; #echo -e \u0026#34;\\t\\t..\u0026#34; 表示换行 if [ -n \u0026#34;$disk_warning\u0026#34; ];then echo -e \u0026#34;\\033[31m${disk_warning} \\n [Error]以上目录使用率超过85%，请及时处理！\\033[0m\u0026#34; fi echo -e \u0026#34;\\033[36m*************内存资源消耗统计*************\\033[0m\u0026#34; #获得系统总内存 memery_all=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $2}\u0026#39;) #获得占用内存（操作系统 角度） system_memery_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) #获得buffer、cache占用内存，当内存不够时会及时回收，所以这两部分可用于可用内存的计算 buffer_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $6}\u0026#39;) cache_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $7}\u0026#39;) #获得被使用内存，所以这部分可用于可用内存的计算，注意计算方法 actual_used_all=$[memery_all-(free+buffer_used+cache_used)] #获得实际占用的内存 actual_used_all=`expr $memery_all - $free + $buffer_used + $cache_used ` memery_percent=$(echo \u0026#34;scale=4;$system_memery_used / $memery_all\u0026#34; | bc) memery_percent2=$(echo \u0026#34;scale=4; $actual_used_all / $memery_all\u0026#34; | bc) percent_part1=$(echo $memery_percent | cut -c 2-3) percent_part2=$(echo $memery_percent | cut -c 4-5) percent_part11=$(echo $memery_percent2 | cut -c 2-3) percent_part22=$(echo $memery_percent2 | cut -c 4-5) #获得占用内存（操作系统角度） echo -e \u0026#34;\\033[32m system memery is already use: $percent_part1.$percent_part2% \\033[0m\u0026#34; #获得实际内存占用率 echo -e \u0026#34;\\033[32m actual memery is already use: $percent_part11.$percent_part22% \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m buffer is already used : $buffer_used M \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m cache is already used : $cache_used M \\033[0m\u0026#34; } function getServiceStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************服务检查*******************************************************\\033[0m\u0026#34; echo \u0026#34;\u0026#34; if [[ $centosVersion \u0026gt; 7 ]];then conf=$(systemctl list-unit-files --type=service --state=enabled --no-pager | grep \u0026#34;enabled\u0026#34;) process=$(systemctl list-units --type=service --state=running --no-pager | grep \u0026#34;.service\u0026#34;) else conf=$(/sbin/chkconfig | grep -E \u0026#34;:on|:启用\u0026#34;) process=$(/sbin/service --status-all 2\u0026gt;/dev/null | grep -E \u0026#34;is running|正在运行\u0026#34;) fi echo -e \u0026#34;\\033[36m******************服务配置******************\\033[0m\u0026#34; echo \u0026#34;$conf\u0026#34; | column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m**************正在运行的服务****************\\033[0m\u0026#34; echo \u0026#34;$process\u0026#34; } function getAutoStartStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************自启动检查*******************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m****************自启动命令*****************\\033[0m\u0026#34; conf=$(grep -v \u0026#34;^#\u0026#34; /etc/rc.d/rc.local| sed \u0026#39;/^$/d\u0026#39;) echo \u0026#34;$conf\u0026#34; } function getLoginStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************登录检查********************************************************\\033[0m\u0026#34; last | head } function getNetworkStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************网络检查********************************************************\\033[0m\u0026#34; if [[ $centosVersion \u0026lt; 7 ]];then /sbin/ifconfig -a | grep -v packets | grep -v collisions | grep -v i net6 else #ip a for i in $(ip link | grep BROADCAST | awk -F: \u0026#39;{print $2}\u0026#39;);do ip add show $i | grep -E \u0026#34;BROADCAST|global\u0026#34;| awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; ;echo \u0026#34;\u0026#34; ;done fi GATEWAY=$(ip route | grep default | awk \u0026#39;{print $3}\u0026#39;) DNS=$(grep nameserver /etc/resolv.conf| grep -v \u0026#34;#\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;\u0026#34; echo \u0026#34;网关：$GATEWAY \u0026#34; echo \u0026#34;DNS：$DNS\u0026#34; #报表信息 IP=$(ip -f inet addr | grep -v 127.0.0.1 | grep inet | awk \u0026#39;{print $NF,$2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) MAC=$(ip link | grep -v \u0026#34;LOOPBACK\\|loopback\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sed \u0026#39;N;s/\\n//\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;\u0026#34; ping -c 4 www.baidu.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ];then echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[32m网络连接：正常！\\033[0m\u0026#34; else echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[31m网络连接：异常！\\033[0m\u0026#34; fi } function getListenStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************监听检查********************************************************\\033[0m\u0026#34; TCPListen=$(ss -ntul | column -t) echo \u0026#34;$TCPListen\u0026#34; } function getCronStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**********************************************计划任务检查******************************************************\\033[0m\u0026#34; Crontab=0 for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(grep \u0026#34;$shell\u0026#34; /etc/passwd| awk -F: \u0026#39;{print $1}\u0026#39;);do crontab -l -u $user \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 status=$? if [ $status -eq 0 ];then echo -e \u0026#34;\\033[36m************$user用户的定时任务**************\\033[0m\u0026#34; crontab -l -u $user let Crontab=Crontab+$(crontab -l -u $user | wc -l) echo \u0026#34;\u0026#34; fi done done #计划任务 #find /etc/cron* -type f | xargs -i ls -l {} | column -t #let Crontab=Crontab+$(find /etc/cron* -type f | wc -l) } function getHowLongAgo(){ # 计算一个时间戳离现在有多久了 datetime=\u0026#34;$*\u0026#34; [ -z \u0026#34;$datetime\u0026#34; ] \u0026amp;\u0026amp; echo `stat /etc/passwd|awk \u0026#34;NR==6\u0026#34;` Timestamp=$(date +%s -d \u0026#34;$datetime\u0026#34;) Now_Timestamp=$(date +%s) Difference_Timestamp=$(($Now_Timestamp-$Timestamp)) days=0;hours=0;minutes=0; sec_in_day=$((60*60*24)); sec_in_hour=$((60*60)); sec_in_minute=60 while (( $(($Difference_Timestamp-$sec_in_day)) \u0026gt; 1 )) do let Difference_Timestamp=Difference_Timestamp-sec_in_day let days++ done while (( $(($Difference_Timestamp-$sec_in_hour)) \u0026gt; 1 )) do let Difference_Timestamp=Difference_Timestamp-sec_in_hour let hours++ done echo \u0026#34;$days 天 $hours 小时前\u0026#34; } function getUserLastLogin(){ # 获取用户最近一次登录的时间，含年份 # 很遗憾last命令不支持显示年份，只有\u0026#34;last -t YYYYMMDDHHMMSS\u0026#34;表示某个时间之间的登录，我 # 们只能用最笨的方法了，对比今天之前和今年元旦之前（或者去年之前和前年之前……）某个用户 # 登录次数，如果登录统计次数有变化，则说明最近一次登录是今年。 username=$1 : ${username:=\u0026#34;`whoami`\u0026#34;} thisYear=$(date +%Y) oldesYear=$(last | tail -n1 | awk \u0026#39;{print $NF}\u0026#39;) while(( $thisYear \u0026gt;= $oldesYear));do loginBeforeToday=$(last $username | grep $username | wc -l) loginBeforeNewYearsDayOfThisYear=$(last $username -t $thisYear\u0026#34;0101000000\u0026#34; | grep $username | wc -l) if [ $loginBeforeToday -eq 0 ];then echo \u0026#34;从未登录过\u0026#34; break elif [ $loginBeforeToday -gt $loginBeforeNewYearsDayOfThisYear ];then lastDateTime=$(last -i $username | head -n1 | awk \u0026#39;{for(i=4;i\u0026lt;(NF-2);i++)printf\u0026#34;%s \u0026#34;,$i}\u0026#39;)\u0026#34; $thisYear\u0026#34; lastDateTime=$(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34; -d \u0026#34;$lastDateTime\u0026#34;) echo \u0026#34;$lastDateTime\u0026#34; break else thisYear=$((thisYear-1)) fi done } function getUserStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************用户检查*******************************************************\\033[0m\u0026#34; #/etc/passwd 最后修改时间 pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; Modify=$(stat /etc/passwd | grep Modify | tr \u0026#39;.\u0026#39; \u0026#39; \u0026#39; | awk \u0026#39;{print $2,$3}\u0026#39;) echo \u0026#34;/etc/passwd: $Modify ($(getHowLongAgo $Modify))\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************特权用户******************\\033[0m\u0026#34; RootUser=\u0026#34;\u0026#34; for user in $(echo \u0026#34;$pwdfile\u0026#34; | awk -F: \u0026#39;{print $1}\u0026#39;);do if [ $(id -u $user) -eq 0 ];then echo \u0026#34;$user\u0026#34; RootUser=\u0026#34;$RootUser,$user\u0026#34; fi done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************用户列表******************\\033[0m\u0026#34; USERs=0 echo \u0026#34;$( echo \u0026#34;用户名 UID GID HOME SHELL 最后一次登录\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for username in $(grep \u0026#34;$shell\u0026#34; /etc/passwd| awk -F: \u0026#39;{print $1}\u0026#39;);do userLastLogin=\u0026#34;$(getUserLastLogin $username)\u0026#34; echo \u0026#34;$pwdfile\u0026#34; | grep -w \u0026#34;$username\u0026#34; |grep -w \u0026#34;$shell\u0026#34;| awk -F: -v lastlogin=\u0026#34;$(echo \u0026#34;$userLastLogin\u0026#34; | tr \u0026#39; \u0026#39; \u0026#39;_\u0026#39;)\u0026#34; \u0026#39;{print $1,$3,$4,$6,$7,lastlogin}\u0026#39; done let USERs=USERs+$(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34;| wc -l) done )\u0026#34; | column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************空密码用户****************\\033[0m\u0026#34; USEREmptyPassword=\u0026#34;\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34; | cut -d: -f1);do r=$(awk -F: \u0026#39;$2==\u0026#34;!!\u0026#34;{print $1}\u0026#39; /etc/shadow | grep -w $user) if [ ! -z $r ];then echo $r USEREmptyPassword=\u0026#34;$USEREmptyPassword,\u0026#34;$r fi done done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*****************相同ID用户*****************\\033[0m\u0026#34; USERTheSameUID=\u0026#34;\u0026#34; UIDs=$(cut -d: -f3 /etc/passwd | sort | uniq -c | awk \u0026#39;$1\u0026gt;1{print $2}\u0026#39;) for uid in $UIDs;do echo -n \u0026#34;$uid\u0026#34;; USERTheSameUID=\u0026#34;$uid\u0026#34; r=$(awk -F: \u0026#39;ORS=\u0026#34;\u0026#34;;$3==\u0026#39;\u0026#34;$uid\u0026#34;\u0026#39;{print \u0026#34;:\u0026#34;,$1}\u0026#39; /etc/passwd) echo \u0026#34;$r\u0026#34; echo \u0026#34;\u0026#34; USERTheSameUID=\u0026#34;$USERTheSameUID $r,\u0026#34; done } function getPasswordStatus { echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************密码检查*******************************************************\\033[0m\u0026#34; pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************密码过期检查****************\\033[0m\u0026#34; result=\u0026#34;\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34; | cut -d: -f1);do get_expiry_date=$(/usr/bin/chage -l $user | grep \u0026#39;Password expires\u0026#39; | cut -d: -f2) if [[ $get_expiry_date = \u0026#39; never\u0026#39; || $get_expiry_date = \u0026#39;never\u0026#39; ]];then printf \u0026#34;%-15s 永不过期\\n\u0026#34; $user result=\u0026#34;$result,$user:never\u0026#34; else password_expiry_date=$(date -d \u0026#34;$get_expiry_date\u0026#34; \u0026#34;+%s\u0026#34;) current_date=$(date \u0026#34;+%s\u0026#34;) diff=$(($password_expiry_date-$current_date)) let DAYS=$(($diff/(60*60*24))) printf \u0026#34;%-15s %s天后过期\\n\u0026#34; $user $DAYS result=\u0026#34;$result,$user:$DAYS days\u0026#34; fi done done report_PasswordExpiry=$(echo $result | sed \u0026#39;s/^,//\u0026#39;) echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************密码策略检查****************\\033[0m\u0026#34; grep -v \u0026#34;#\u0026#34; /etc/login.defs | grep -E \u0026#34;PASS_MAX_DAYS|PASS_MIN_DAYS|PASS_MIN_LEN|PASS_WARN_AGE\u0026#34; } function getSudoersStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**********************************************Sudoers检查*******************************************************\\033[0m\u0026#34; conf=$(grep -v \u0026#34;^#\u0026#34; /etc/sudoers| grep -v \u0026#34;^Defaults\u0026#34; | sed \u0026#39;/^$/d\u0026#39;) echo \u0026#34;$conf\u0026#34; echo \u0026#34;\u0026#34; } function getInstalledStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************软件检查*******************************************************\\033[0m\u0026#34; rpm -qa --last | head | column -t } function getProcessStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************进程检查*******************************************************\\033[0m\u0026#34; if [ $(ps -ef | grep defunct | grep -v grep | wc -l) -ge 1 ];then echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m***************僵尸进程***************\\033[0m\u0026#34; ps -ef | head -n1 ps -ef | grep defunct | grep -v grep fi echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m************CPU占用TOP 10进程*************\\033[0m\u0026#34; echo -e \u0026#34;用户 进程ID %CPU 命令 $(ps aux | awk \u0026#39;{print $1, $2, $3, $11}\u0026#39; | sort -k3rn | head -n 10 )\u0026#34;| column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m************内存占用TOP 10进程*************\\033[0m\u0026#34; echo -e \u0026#34;用户 进程ID %MEM 虚拟内存 常驻内存 命令 $(ps aux | awk \u0026#39;{print $1, $2, $4, $5, $6, $11}\u0026#39; | sort -k3rn | head -n 10 )\u0026#34;| column -t #echo \u0026#34;\u0026#34; #echo -e \u0026#34;\\033[36m************SWAP占用TOP 10进程*************\\033[0m\u0026#34; #awk: fatal: cannot open file `/proc/18713/smaps\u0026#39; for reading (No such file or directory) #for i in `cd /proc;ls |grep \u0026#34;^[0-9]\u0026#34;|awk \u0026#39; $0 \u0026gt;100\u0026#39;`;do awk \u0026#39;{if (-f /proc/$i/smaps) print \u0026#34;$i file is not exist\u0026#34;; else print \u0026#34;$i\u0026#34;}\u0026#39;;done # for i in `cd /proc;ls |grep \u0026#34;^[0-9]\u0026#34;|awk \u0026#39; $0 \u0026gt;100\u0026#39;` ;do awk \u0026#39;/Swap:/{a=a+$2}END{print \u0026#39;\u0026#34;$i\u0026#34;\u0026#39;,a/1024\u0026#34;M\u0026#34;}\u0026#39; /proc/$i/smaps ;done |sort -k2nr \u0026gt; /tmp/swap.txt #echo -e \u0026#34;进程ID SWAP使用 $(cat /tmp/swap.txt|grep -v awk | head -n 10)\u0026#34;| column -t } function getSyslogStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************syslog检查*******************************************************\\033[0m\u0026#34; echo \u0026#34;SYSLOG服务状态：$(getState rsyslog)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m***************rsyslog配置******************\\033[0m\u0026#34; cat /etc/rsyslog.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | grep -v \u0026#34;^\\\\$\u0026#34; | sed \u0026#39;/^$/d\u0026#39; | column -t } function getFirewallStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************防火墙检查*******************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m****************防火墙状态******************\\033[0m\u0026#34; if [[ $centosVersion = 7 ]];then systemctl status firewalld \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 status=$? if [ $status -eq 0 ];then s=\u0026#34;active\u0026#34; elif [ $status -eq 3 ];then s=\u0026#34;inactive\u0026#34; elif [ $status -eq 4 ];then s=\u0026#34;permission denied\u0026#34; else s=\u0026#34;unknown\u0026#34; fi else s=\u0026#34;$(getState iptables)\u0026#34; fi echo \u0026#34;firewalld: $s\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************防火墙配置******************\\033[0m\u0026#34; cat /etc/sysconfig/firewalld 2\u0026gt;/dev/null } function getSNMPStatus(){ #SNMP服务状态，配置等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************SNMP检查*********************************************************\\033[0m\u0026#34; status=\u0026#34;$(getState snmpd)\u0026#34; echo \u0026#34;SNMP服务状态：$status\u0026#34; echo \u0026#34;\u0026#34; if [ -e /etc/snmp/snmpd.conf ];then echo \u0026#34;/etc/snmp/snmpd.conf\u0026#34; echo \u0026#34;--------------------\u0026#34; cat /etc/snmp/snmpd.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; fi } function getState(){ if [[ $centosVersion \u0026lt; 7 ]];then if [ -e \u0026#34;/etc/init.d/$1\u0026#34; ];then if [ `/etc/init.d/$1 status 2\u0026gt;/dev/null | grep -E \u0026#34;is running|正在运行\u0026#34; | wc -l` -ge 1 ];then r=\u0026#34;active\u0026#34; else r=\u0026#34;inactive\u0026#34; fi else r=\u0026#34;unknown\u0026#34; fi else #CentOS 7+ r=\u0026#34;$(systemctl is-active $1 2\u0026gt;\u0026amp;1)\u0026#34; fi echo \u0026#34;$r\u0026#34; } function getSSHStatus(){ #SSHD服务状态，配置,受信任主机等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************SSH检查*********************************************************\\033[0m\u0026#34; #检查受信任主机 pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; echo \u0026#34;SSH服务状态：$(getState sshd)\u0026#34; Protocol_Version=$(cat /etc/ssh/sshd_config | grep Protocol | awk \u0026#39;{print $2}\u0026#39;) echo \u0026#34;SSH协议版本：$Protocol_Version\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************信任主机******************\\033[0m\u0026#34; authorized=0 for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep /bin/bash | awk -F: \u0026#39;{print $1}\u0026#39;);do authorize_file=$(echo \u0026#34;$pwdfile\u0026#34; | grep -w $user | awk -F: \u0026#39;{printf $6\u0026#34;/.ssh/authorized_keys\u0026#34;}\u0026#39;) authorized_host=$(cat $authorize_file 2\u0026gt;/dev/null | awk \u0026#39;{print $3}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) if [ ! -z $authorized_host ];then echo \u0026#34;$user 授权 \\\u0026#34;$authorized_host\\\u0026#34; 无密码访问\u0026#34; fi let authorized=authorized+$(cat $authorize_file 2\u0026gt;/dev/null | awk \u0026#39;{print $3}\u0026#39;|wc -l) done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*******是否允许ROOT远程登录***************\\033[0m\u0026#34; config=$(cat /etc/ssh/sshd_config | grep PermitRootLogin) firstChar=${config:0:1} if [ $firstChar == \u0026#34;#\u0026#34; ];then PermitRootLogin=\u0026#34;yes\u0026#34; else PermitRootLogin=$(echo $config | awk \u0026#39;{print $2}\u0026#39;) fi echo \u0026#34;PermitRootLogin $PermitRootLogin\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*************ssh服务配置******************\\033[0m\u0026#34; cat /etc/ssh/sshd_config | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; } function getNTPStatus(){ #NTP服务状态，当前时间，配置等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************NTP检查**********************************************************\\033[0m\u0026#34; if [ -e /etc/ntp.conf ];then echo \u0026#34;NTP服务状态：$(getState ntpd)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*************NTP服务配置******************\\033[0m\u0026#34; cat /etc/ntp.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; fi } function check(){ version getSystemStatus get_resource getCpuStatus getMemStatus getDiskStatus getNetworkStatus getListenStatus getProcessStatus getServiceStatus getAutoStartStatus getLoginStatus getCronStatus getUserStatus getPasswordStatus getSudoersStatus getFirewallStatus getSSHStatus getSyslogStatus getSNMPStatus getNTPStatus getInstalledStatus } #执行检查并保存检查结果 check \u0026gt; $RESULTFILE echo -e \u0026#34;\\033[44;37m 主机巡检结果存放在：$RESULTFILE \\033[0m\u0026#34; #上传检查结果的文件 #curl -F \u0026#34;filename=@$RESULTFILE\u0026#34; \u0026#34;$uploadHostDailyCheckApi\u0026#34; 2\u0026gt;/dev/null cat $RESULTFILE 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-server-inspection/","summary":"代码如下 #!/bin/bash #参数定义 date=`date +\u0026#34;%Y-%m-%d-%H:%M:%S\u0026#34;` centosVersion=$(awk \u0026#39;{print $(NF-1)}\u0026#39; /etc/redhat-release) VERSION=`date +%F` #日志相关 LOGPATH=\u0026#34;/tmp/awr\u0026#34; [ -e $LOGPATH ] || mkdir -p $LOGPATH RESULTFILE=\u0026#34;$LOGPATH/HostCheck-`hostname`-`date +%Y%m%d`.txt\u0026#34; #调用函数库 [ -f /etc/init.d/functions ] \u0026amp;\u0026amp; source /etc/init.d/functions export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile #root用户执行脚本 [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34;请用root用户执行此脚本！\u0026#34; \u0026amp;\u0026amp; exit 1 function version(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;[${date}] \u0026gt;\u0026gt;\u0026gt; `hostname -s` 主机巡检\u0026#34; } function getSystemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***","title":"shell | 服务器巡检脚本"},{"content":"0 前言 有需求需要在 openeuler 的操作系统上测试一个 C 程序，做了一个简化版的程序，程序很简单，循环读取一个文件并打印文件内容，在程序执行过程中使用 echo 手动向文件中追加内容，程序要能读取到，效果如下：\n测试程序代码如下：\n#include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0) { buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;read:%s\\n\u0026#34;,buffer); } else { printf(\u0026#34;noread\\n\u0026#34;); } sleep(2); } return 0; } 在 Rhel-7.5 上测试一切正常，开始在 openeuler 上进行测试，结果发现后续追加的内容没有输出：\n1 故障排查 考虑到影响程序执行结果的几个因素：程序本身，内核版本，gcc 版本，glibc 版本。\n程序本身应该是没问题的，内核版本一般对 C 语言程序的影响也不会很大，还是优先看 gcc 版本和 glibc 版本。\n按照思路进行了一些测试，测试结果：\n可行： centos7.5（gcc-4.8.5，kernel-3.10，glibc\u0026lt;=2.28） centos7.5（gcc-7.3.0，kernel-3.10，glibc\u0026lt;=2.28） centos7.5（gcc-7.3.0，kernel-5.12，glibc\u0026lt;=2.28） 不可行： isoft-server-6.0（gcc-7.3.0，4.19.90，glibc\u0026gt;=2.28） centos8（gcc-8.4.0，kernel-4.18.0，glibc\u0026gt;=2.28） openeuler-20.03-LTS-SP1（gcc-7.3.0，kernel-4.19.90，glibc\u0026gt;=2.28） 按照测试结果，似乎 gcc 版本和内核版本对程序没什么影响，大概率应该是 glibc 版本导致的。由于程序很简单，只是以 rb 方式 fopen 打开文件循环读取文件内容，求证 (google) 起来也比较轻松，很快就找到了问题在哪：glibc 2.28 修复了 fread 的行为\n这个 glibc 的 bug 是 05 年提的，到 18 年才修复，也是担心 break 之前大量的代码。https://sourceware.org/bugzilla/show_bug.cgi?id=1190\n现在再修改一下代码：\n#include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0) { buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;read:%s\\n\u0026#34;,buffer); } else { if (feof (f)) { printf(\u0026#34;Read error, clear error flag to retry...\\n\u0026#34;); clearerr (f); } } sleep(2); } return 0; } 添加了一块清除标记的片段，在 glibc\u0026gt;=2.28 的系统上程序也可以正常运行了\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/troubleshooting-glibc-fread/","summary":"0 前言 有需求需要在 openeuler 的操作系统上测试一个 C 程序，做了一个简化版的程序，程序很简单，循环读取一个文件并打印文件内容，在程序执行过程中使用 echo 手动向文件中追加内容，程序要能读取到，效果如下： 测试程序代码如下： #include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt;","title":"troubleshooting | glibc 的 fread 问题"},{"content":" ","permalink":"https://www.lvbibir.cn/en/talk/","summary":"","title":"💬 说说"},{"content":"0 前言 BBR 简介：（Bottleneck Bandwidth and RTT）是一种新的拥塞控制算法，由 Google 开发。有了 BBR，Linux 服务器可以显着提高吞吐量并减少连接延迟\n本文参考以下链接:\n介绍在 CentOS7 上部署 BBR 的详细过程 1 更新内核 查看当前内核版本\nuname -r 显示当前内核为 3.10.0，因此我们需要更新内核\n使用 ELRepo RPM 仓库升级内核\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org //无返回内容 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 使用 ELRepo repo 更新安装 5.12.3 内核\nyum --enablerepo=elrepo-kernel install kernel-ml -y 更新完成后，执行如下命令，确认更新结果\nrpm -qa | grep kernel # kernel-ml-5.12.3-1.el7.elrepo.x86_64 通过设置默认引导为 grub2 ，来启用 5.12.3 内核\negrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \u0026#39;\\\u0026#39; 根据显示结果得知 5.12.3 内核处于行首，对应行号为 0 执行以下命令将其设置为默认引导项\ngrub2-set-default 0 重启系统并确认内核版本\nreboot uname -r 至此完成内核更新与默认引导设置\n2 启用 BBR 执行命令查看当前拥塞控制算法\nsysctl -n net.ipv4.tcp_congestion_control 启用 BBR 算法，需要对 sysctl.conf 配置文件进行修改，依次执行以下每行命令\necho \u0026#39;net.core.default_qdisc=fq\u0026#39; | tee -a /etc/sysctl.conf echo \u0026#39;net.ipv4.tcp_congestion_control=bbr\u0026#39; | tee -a /etc/sysctl.conf sysctl -p 进行 BBR 的启用验证\nsysctl net.ipv4.tcp_available_congestion_control sysctl -n net.ipv4.tcp_congestion_control 最后检查 BBR 模块是否已经加载\nlsmod | grep bbr 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-open-bbr/","summary":"0 前言 BBR 简介：（Bottleneck Bandwidth and RTT）是一种新的拥塞控制算法，由 Google 开发。有了 BBR，Linux 服务器可以显着提高吞吐量并减少连接延迟 本文参考以下链接: 介绍在 CentOS7 上部署 BBR 的详细过程 1 更新内核 查看当前内核版本 uname -r 显示当前内核为 3.10.0，因此我们需要更新内核 使用 ELRepo RPM 仓库升级","title":"centos7 | 开启 bbr 算法"},{"content":" 进入 bios 修改启动模式，将 UEFI 改为 Legacy bios 重启服务器，ctrl + r 进入 lsi 阵列卡管理 选择对应阵列卡 配置逻辑盘 配置完逻辑盘后可以选择从某一块逻辑盘启动 Ctrl-P 进入到 ctrl mgmt. -\u0026gt; TAB 切换到 boot device\n回车后可以看到当前的逻辑盘，上下选择要引导的逻辑盘即可。\nApply 保存退出完成。\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/h3c-server-config-raid/","summary":"进入 bios 修改启动模式，将 UEFI 改为 Legacy bios 重启服务器，ctrl + r 进入 lsi 阵列卡管理 选择对应阵列卡 配置逻辑盘 配置完逻辑盘后可以选择从某一块逻辑盘启动 Ctrl-P 进入到 ctrl mgmt. -\u0026gt; TAB 切换到 boot device 回车后可以看到当前的逻辑盘，上下选择要引导的逻辑盘即可。 Apply 保存退出完成。 以上","title":"H3C 服务器配置 raid"},{"content":"0 前言 本文参考以下链接:\nCentOS 上 free 命令详解 1 CentOS6 在 CentOS6 及以前的版本中，free 命令输出是这样的：\n[root@wordpress ~]# free -m total used free shared buffers cached Mem: 1002 769 233 0 62 421 -/+ buffers/cache: 286 716 Swap: 1153 0 1153 第一行：\n系统内存主要分为五部分：total(系统内存总量)，used(程序已使用内存)，free(空闲内存)，buffers(buffer cache)，cached(Page cache)。 系统总内存 total = used + free； buffers 和 cached 被算在 used 里，因此第一行系统已使用内存 used = buffers + cached + 第二行系统已使用内存 used 由于 buffers 和 cached 在系统需要时可以被回收使用，因此系统可用内存 = free + buffers + cached； shared 为程序共享的内存空间，往往为 0。 第二行：\n正因为 buffers 和 cached 中的一部分内存容量在系统需要时可以被回收使用，因此 buffer 和 cached 中有部分内存其实可以算作可用内存，因此： 系统已使用内存，即第二行的 used = total - 第二行 free 系统可用内存，即第二行的 free = 第一行的 free + buffers + cached 第三行：\n- swap 内存交换空间使用情况\n2 CentOS7 CentOS7 及以后 free 命令的输出如下：\n[root@wordpress ~]# free -m total used free shared buff/cache available Mem: 1839 866 74 97 897 695 Swap: 0 0 0 buffer 和 cached 被合成一组，加入了一个 available，关于此 available，文档上的说明如下：\nMemAvailable: An estimate of how much memory is available for starting new applications, without swapping.\n即系统可用内存，之前说过由于 buffer 和 cache 可以在需要时被释放回收，系统可用内存即 free + buffer + cache，在 CentOS7 之后这种说法并不准确，因为并不是所有的 buffer/cache 空间都可以被回收。\n即 available = free + buffer/cache - 不可被回收内存 (共享内存段、tmpfs、ramfs 等)。\n因此在 CentOS7 之后，用户不需要去计算 buffer/cache，即可以看到还有多少内存可用，更加简单直观。\n3 相关介绍 3.1 buffer/cache buffer 和 cache 是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义。在 Linux 的内存管理中，这里的 buffer 指 Linux 内存的： Buffer cache 。这里的 cache 指 Linux 内存中的： Page cache 。翻译成中文可以叫做缓冲区缓存和页面缓存。在历史上，它们一个（ buffer ）被用来当成对 io 设备写的缓存，而另一个（ cache ）被用来当作对 io 设备的读缓存，这里的 io 设备，主要指的是块设备文件和文件系统上的普通文件。但是现在，它们的意义已经不一样了。在当前的内核中， page cache 顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以 page 进行分配管理的，都可以使用 page cache 作为其缓存来管理使用。当然，不是所有的内存都是以页（ page ）进行管理的，也有很多是针对块（ block ）进行管理的，这部分内存使用如果要用到 cache 功能，则都集中到 buffer cache 中来使用。（从这个角度出发，是不是 buffer cache 改名叫做 block cache 更好？）然而，也不是所有块（ block ）都有固定长度，系统上块的长度主要是根据所使用的块设备决定的，而页长度在 X86 上无论是 32 位还是 64 位都是 4k 。\n明白了这两套缓存系统的区别，就可以理解它们究竟都可以用来做什么了。\n3.2 page cache Page cache 主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有 read ／ write 操作的时候。如果你仔细想想的话，作为可以映射文件到内存的系统调用： mmap 是不是很自然的也应该用到 page cache ？在当前的系统实现里， page cache 也被作为其它文件类型的缓存设备来用，所以事实上 page cache 也负责了大部分的块设备文件的缓存工作。\n3.3 buffer cache Buffer cache 则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。这意味着某些对块的操作会使用 buffer cache 进行缓存，比如我们在格式化文件系统的时候。一般情况下两个缓存系统是一起配合使用的，比如当我们对一个文件进行写操作的时候， page cache 的内容会被改变，而 buffer cache 则可以用来将 page 标记为不同的缓冲区，并记录是哪一个缓冲区被修改了。这样，内核在后续执行脏数据的回写（ writeback ）时，就不用将整个 page 写回，而只需要写回修改的部分即可。\n3.4 回收 cache Linux 内核会在内存将要耗尽的时候，触发内存回收的工作，以便释放出内存给急需内存的进程使用。一般情况下，这个操作中主要的内存释放都来自于对 buffer ／ cache 的释放。尤其是被使用更多的 cache 空间。既然它主要用来做缓存，只是在内存够用的时候加快进程对文件的读写速度，那么在内存压力较大的情况下，当然有必要清空释放 cache ，作为 free 空间分给相关进程使用。所以一般情况下，我们认为 buffer/cache 空间可以被释放，这个理解是正确的。\n但是这种清缓存的工作也并不是没有成本。理解 cache 是干什么的就可以明白清缓存必须保证 cache 中的数据跟对应文件中的数据一致，才能对 cache 进行释放。所以伴随着 cache 清除的行为的，一般都是系统 IO 飙高。因为内核要对比 cache 中的数据和对应硬盘文件上的数据是否一致，如果不一致需要写回，之后才能回收。\n在系统中除了内存将被耗尽的时候可以清缓存以外，我们还可以使用下面这个文件来人工触发缓存清除的操作\n[root@tencent64 ~]# cat /proc/sys/vm/drop_caches 方法是：\necho 3 \u0026gt; /proc/sys/vm/drop_caches 当然，这个文件可以设置的值分别为 1 、 2 、 3 。它们所表示的含义为：\n表示清除 pagecache echo 1 \u0026gt; /proc/sys/vm/drop_caches 表示清除回收 slab 分配器中的对象（包括目录项缓存和 inode 缓存）。 slab 分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的 pagecache echo 2 \u0026gt; /proc/sys/vm/drop_caches 表示清除 pagecache 和 slab 分配器中的缓存对象。 echo 3 \u0026gt; /proc/sys/vm/drop_caches 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-command-free/","summary":"0 前言 本文参考以下链接: CentOS 上 free 命令详解 1 CentOS6 在 CentOS6 及以前的版本中，free 命令输出是这样的： [root@wordpress ~]# free -m total used free shared buffers cached Mem: 1002 769 233 0 62 421 -/+ buffers/cache: 286 716 Swap: 1153 0 1153 第一行： 系统内存主要分为五部分：total(系统内存总量)，used(程序已使用内存)，free(空闲内存)，buffers(buffer ca","title":"linux | free 命令详解"},{"content":"1 pxe 环境 dhcp+tftp+http\npxe-server：isoft-serveros-v4.2（3.10.0-957.el7.isoft.x86_64）\n引导的 iso：isoft-serveros-aarch64-oe1-v5.1（4.19.90-2003.4.0.0036.oe1.aarch64）\n物理服务器：浪潮 Inspur\n2 dhcpd.conf 配置 [root@localhost isoft-5.1-arm]# vim /etc/dhcp/dhcpd.conf default-lease-time 43200; max-lease-time 345600; option space PXE; option arch code 93 = unsigned integer 16; option routers 192.168.1.1; option subnet-mask 255.255.255.0; option broadcast-address 192.168.1.255; option time-offset -18000; ddns-update-style none; allow client-updates; allow booting; allow bootp; next-server 192.168.1.1; if option arch = 00:07 or arch = 00:09 { filename \u0026#34;x86/bootx64.efi\u0026#34;; } else { filename \u0026#34;arm/grubaa64.efi\u0026#34;; } shared-network works { subnet 192.168.1.0 netmask 255.255.255.0 { range dynamic-bootp 192.168.1.221 192.168.1.253; } } 3 grub.cfg 配置 [root@localhost tftpboot]# vim arm/grub.cfg set default=\u0026#34;0\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=60 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iSoftServerOS-5.1-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iSoftServerOS 5.1 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { # linux /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=iSoftServerOS-5.1-aarch64 ro inst.geoloc=0 selinux=0 # initrd /images/pxeboot/initrd.img linux /arm51/vmlinuz ip=dhcp method=http://192.168.1.1/isoft-5.1-arm ks=http://192.168.1.1/isoft-5.1-arm/anaconda-ks.cfg initrd /arm51/initrd.img } #menuentry \u0026#39;Install iSoftServerOS 5.1 for ZF with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { # linux /arm51-zf/vmlinuz ip=dhcp method=http://192.168.1.1/isoft-5.1-zfarm # initrd /arm51-zf/initrd.img #} 4 ks.cfg 配置 [root@localhost isoft-5.1-arm]# vim anaconda-ks.cfg lang zh_CN.UTF-8 # Network information network --bootproto=dhcp --device=eno1 --ipv6=auto --no-activate network --bootproto=dhcp --device=eno2 --ipv6=auto network --bootproto=dhcp --device=eno3 --ipv6=auto network --bootproto=dhcp --device=eno4 --ipv6=auto network --bootproto=dhcp --device=enp22s0f0 --ipv6=auto network --bootproto=dhcp --device=enp22s0f1 --ipv6=auto network --bootproto=dhcp --device=enp22s0f2 --ipv6=auto network --bootproto=dhcp --device=enp22s0f3 --ipv6=auto network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$afv9h6qEnQTq3WSl$GHtOmvLkHrBin8vTWLbRaa2r.Ur9mUQR7XypWRoEWZYCwwJ2MnuMPxpNiNLSG1vSa5qBODHJcqIUUWkHm0IVl. # SELinux configuration selinux --disabled # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc user --groups=wheel --name=testuser --password=$6$9SyzoTjQU2syj2Bk$SQ4WZAV/go3KeX6rJN3cieNpY4l7aU2wHxad75yWlbKBh.ithhrU/jfA09JUq7cb10D0QTCwtClmItfg/N47t. --iscrypted --gecos=\u0026#34;testuser\u0026#34; # Disk partitioning information part /boot/efi --fstype=\u0026#34;efi\u0026#34; --ondisk=sda --size=200 --fsoptions=\u0026#34;umask=0077,shortname=winnt\u0026#34; part pv.521 --fstype=\u0026#34;lvmpv\u0026#34; --ondisk=sda --size=913974 part /boot --fstype=\u0026#34;ext4\u0026#34; --ondisk=sda --size=1024 volgroup isoftserveros --pesize=4096 pv.521 logvol /home --fstype=\u0026#34;xfs\u0026#34; --size=756272 --name=home --vgname=isoftserveros logvol swap --fstype=\u0026#34;swap\u0026#34; --size=4096 --name=swap --vgname=isoftserveros logvol / --fstype=\u0026#34;xfs\u0026#34; --size=153600 --name=root --vgname=isoftserveros %packages @^mate-desktop-environment @additional-devel @development @file-server @headless-management @legacy-unix @network-server @network-tools @scientific @security-tools @system-tools @virtual-tools %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/pxe-inspur-isoft5.1-aarch64/","summary":"1 pxe 环境 dhcp+tftp+http pxe-server：isoft-serveros-v4.2（3.10.0-957.el7.isoft.x86_64） 引导的 iso：isoft-serveros-aarch64-oe1-v5.1（4.19.90-2003.4.0.0036.oe1.aarch64） 物理服","title":"pxe 安装 isoft-5.1(aarch64)"},{"content":"前端时间在国家信息中心的一个项目上需要在 H3C 服务器上安装操作系统然后配置一套 spring boot 项目，结果在装操作系统过程中就遇到了问题：安装完操作系统后无法自动引导，只能通过重启服务器按 F7 进入引导选项，选择对应的逻辑盘才能正常引导\n服务器有 7 块物理磁盘，前两块是 600 GB 的机械盘，后五块是 1T 的机械盘，前两块 600GB 的盘做了 raid1 ，剩下的 5 块盘，选择 n+2 做 raid6 。\n规划是这样的，操作系统安装在 raid6 上，raid1 那块逻辑磁盘等系统安装完后再进行挂载，用作业务的数据备份。\n安装完之后却发现有很多台系统引导不起来，必须手动引导，只有一台可以重启后直接进入系统。为了快速解决问题，还是第一时间联系了 H3C 的售后开工单解决，结果不言而喻，业务水平堪忧，并没有解决。不过也给我提供了一些思路。\n整理一下思路：\n出现问题之后更换安装介质重新安装了两次，问题都是一样的 系统安装这块操作肯定没问题，那问题就出在硬件上面了 开始寻找硬件上面的问题，服务器都是全新的，只是做了 raid 。询问了下做 raid 的同事，看可以正常引导的服务器和非正常引导的服务器之间 raid 配置有何不同\n问题估计找到了：正常服务器是先创建的 raid6 ，剩下的都是先创建的 raid1。\n解决方案：\n系统需要重装：删除原先已经创建好的 raid，先创建系统使用的 raid6. 系统无需重装：删除掉 raid1 ，保存后重新创建 raid1。这时，raid6 的顺位会比 raid1 高，系统就可以正常启动了 最终我们这边采取的是第二种方案\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/troubleshooting-h3c-server-can-not-boot-system/","summary":"前端时间在国家信息中心的一个项目上需要在 H3C 服务器上安装操作系统然后配置一套 spring boot 项目，结果在装操作系统过程中就遇到了问题：安装完操作系统后无法自动引导，只能通过重启服务器按 F7 进入引导选项，选择对应的逻辑盘才能正常引导 服务器有 7 块物理磁盘，前两块是 600 GB 的机械盘，后五块是 1T 的机械盘，前","title":"troubleshooting | H3C 服务器装完系统无法引导"},{"content":"0 前言 本文参考以下链接:\n从能用到好用-快速搭建高性能WordPress指南 前段时间着手开始搭建自己的 wordpress 博客，刚开始图方便直接买了阿里云的轻量应用服务器，它是一套预先搭建好的 lamp 架构，并已经做了一些初始化配置，直接访问 ip 就可以进行 wordpress 的安装和配置了。\n这套 wordpress 的一个非常好的优点就是可以在阿里云的控制台一键配置 https 证书，当然仅限在阿里云购买的 ssl 证书\n后续还是决定将 wordpress 整体迁移到 docker 中，全部服务都用 docker 跑。这样只要数据做好持久化，使用 docker 的灵活性会好很多，做全站备份和迁移也很方便。\n1 备份\u0026amp;迁移 wordpress 迁移起来还是比较方便的，需要备份的内容大概有这些：插件、主题、uploads、数据库\n备份插件：UpdraftPlus，这是一款个人使用过一款比较优秀的备份/迁移插件，免费版的功能基本满足大部分人需求，支持手动备份和定时备份、备份和恢复都支持部分备份，比如只备份数据库，只恢复数据库的某一张表。\n免费版的并不支持 wordpress 迁移，但我们可以通过导入导出备份文件的方式实现站点迁移，前提是做好测试。\n备份步骤：\n在备份插件中手动备份一次 下载备份文件 迁移步骤：\n准备好系统环境和 docker 环境（docker-compose） 启动 docker 容器 http 访问 wordpress 地址初始化安装 安装备份插件和 ssl 插件（really simple ssl） 上传备份文件并进行恢复操作（不恢复 wp-options 表） 为 nginx 反代服务器配置 ssl 证书，开启 https 访问 在 really simple ssl 中为 wordpress 启用 https 恢复 wp-options 表 1.1 手动备份\u0026amp;下载备份文件 备份完之后可以直接从 web 端下载，但是建议从 web 端下载一份，通过 ssh 或者 ftp 等方式再下载一份，避免备份文件出现问题\n备份的文件在 wordpress目录/wp-content/updraft 目录中\n通过 scp 下载到本地\n1.2 准备系统环境 安装好 docker 和 docker-compose 即可，docker 的安装和使用教程在本博客中 docker 分类有\n1.3 docker-compose 一键启动 wordpress 环境 这里我提供了一键部署的 docker-compose 文件和各服务进行了优化的配置文件，可以直接拿来用 下载链接\n注意：\n使用前建议修改数据库相关信息\n建议不要随意改动 ip\n所有的数据文件和配置文件默认都在当前的目录下\n如果前面不加 nginx 反代，记得把注释掉的端口映射改成自己想要的\n所有的配置文件都在 nginx 目录下，已经预先定义好，可以自行进行修改\n内置的 wordpress 目录权限用户和组是 33:tape\nversion: \u0026#39;3.1\u0026#39; services: proxy: image: superng6/nginx:debian-stable-1.18.0 container_name: nginx-proxy restart: always networks: wordpress_net: ipv4_address: 172.19.0.6 ports: - 80:80 - 443:443 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/proxy/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/proxy/default.conf:/etc/nginx/conf.d/default.conf - $PWD/ssl:/etc/nginx/ssl - $PWD/logs/proxy:/var/log/nginx depends_on: - web web: image: superng6/nginx:debian-stable-1.18.0 container_name: wordpress-nginx restart: always networks: wordpress_net: ipv4_address: 172.19.0.5 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/nginx/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/nginx/default.conf:/etc/nginx/conf.d/default.conf - $PWD/conf/fastcgi.conf:/etc/nginx/fastcgi.conf - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/wordpress:/var/www/html - $PWD/logs/nginx:/var/log/nginx depends_on: - wordpress wordpress: image: wordpress:5-fpm container_name: wordpress-php restart: always networks: wordpress_net: ipv4_address: 172.19.0.4 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/wordpress:/var/www/html - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/conf/uploads.ini:/usr/local/etc/php/php.ini depends_on: - redis - db redis: image: redis:5 container_name: wordpress-redis restart: always networks: wordpress_net: ipv4_address: 172.19.0.3 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/redis-data:/data depends_on: - db db: image: mysql:5.7 container_name: wordpress-mysql restart: always networks: wordpress_net: ipv4_address: 172.19.0.2 environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;1\u0026#39; volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/mysql-data:/var/lib/mysql - $PWD/conf/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf networks: wordpress_net: driver: bridge ipam: config: - subnet: 172.19.0.0/16 进入到 wordpress-blog 目录下使用 docker-compose up -d 启动 docker 容器\n1.4 配置 nginx 反向代理 配置 80 和 443 端口的反代\n把域名、证书路径以及后端服务器等信息换成自己的\n免费 ssl 证书的申请我在 阿里云wordpress配置免费ssl证书 中介绍过，直接下载 nginx 版的证书放到 wordpress-blog/ssl/目录下即可\n[root@lvbibir ~]# vim wordpress-blog/conf/proxy/default.conf server { listen 80; listen [::]:80; server_name lvbibir.cn; # return 301 https://$host$request_uri; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Real-Port $remote_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $host; proxy_set_header X-NginX-Proxy true; } } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;gzip\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } [root@lvbibir ~]# docker exec -i nginx-proxy nginx -s reload 1.5 安装 wordpress 现在已经可以通过 http 访问 nginx 反代的 80 端口访问 wordpress 了\n安装信息跟之前站点设置一样即可\n1.6 恢复备份 安装好之后启用插件，把备份文件上传到备份目录\n记得修改权限\n[root@lvbibir ~]# chown -R 33:tape wordpress-blog/wordpress/wp-content/ 恢复备份\n注：如果站点之前开启了 https，在这步不要恢复 wp-options 表，不然会导致后台访问不了\n点击恢复即可\n1.7 配置 ssl 启用 really simple ssl 插件，因为之前在 nginx 反代配置了 ssl 证书，虽然我们没有通过 https 访问，但是这个插件已经检测到了证书，可以一键为 wordpress 配置 ssl\n这里我们已经可以通过 https 访问我们的 wordpress 了\n站点路径该插件也会自动修改，之前不恢复 wp-options 表的原因就在这，在我们没有配置好 ssl 之前，直接覆盖 wordpress 的各项设置会导致站点访问不了，重定向循环等各种各样的问题。\n1.8 恢复 wp-options 表 开启了 ssl 之后，通过备份插件再恢复一次，可以只恢复一张 wp-options 表，也可以再全量恢复下数据库，至此，站点迁移工作基本完成了。\n2 后续优化 2.1 开启 https 强制跳转 开启 https 强制跳转后，所有使用 http 访问我们站点的请求都会转到 https，提高站点安全性\n[root@lvbibir ~]# vim /etc/nginx/nginx.conf server { listen 80; listen [::]:80; server_name lvbibir.cn; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;gzip\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } [root@lvbibir ~]# docker exec -i nginx-proxy nginx -s reload 2.2 开启 redis 缓存 wordpress搭配redis加速网站访问速度\n2.3 搭配 jsdelivr-CDN 实现全站 cdn WordPress+jsDelivr开启伪全站CDN\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-to-docker/","summary":"0 前言 本文参考以下链接: 从能用到好用-快速搭建高性能WordPress指南 前段时间着手开始搭建自己的 wordpress 博客，刚开始图方便直接买了阿里云的轻量应用服务器，它是一套预先搭建好的 lamp 架构，并已经做了一些初始化配置，直接访问 ip 就可以进行 wordpress 的安装和配置了。 这套 wordpress 的一个非常好的优点就是可以在阿","title":"wordpress | 迁移到 docker"},{"content":"0 前言 本文参考以下链接\ncentos6 源码编译 openvpn 并打包成 rpm openvpn 源码下载地址 centos6 搭建 openvpn centos6 做端口映射/端口转发 1 实验环境 3 台 centos6.5，1 台 win10，openvpn-2.4.7，easy-rsa-3.0.5\n2 拓扑结构 Win10 安装 openvpn-gui，三台 centos6.5 为 vmware 虚拟机，分为 client、vpnserver、proxy\n三台 centos6.5 的 eth0 网卡均为内网 (lan 区段) 地址 1.1.1.0/24 网段，proxy 额外添加一块 eth1 网卡设置 nat 模式模拟外网 ip\n3 实验目的 win10 访问 proxy 的外网 ip 对应端口连接到 vpnserver，分配到内网 ip 后可以访问到 client\n4 实验思路 proxy 配置 ipv4 转发，将访问到本机 eth1 网卡相对应的端口上的流量转发给 vpnserver 的 vpn 服务端口\nvpnserver 为 win10 分配 ip 实现访问内网\n5 实施步骤 5.1 初始化环境 配置 ip 节点 ip client： 1.1.1.1/24 vpnserver： 1.1.1.2/24 proxy： 1.1.1.3/24 192.168.150.114/24 win10： 192.168.150.1/24 环境初始化（client 和 vpnserver 关闭 iptables 和 selinux，proxy 仅关闭 selinux） sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 5.2 安装 vpnserver \u0026amp; easy-rsa vpnserver 安装 openvpn 由于 centos6 的所有官方源已失效，使用 此链接 中的方法将源码编译成 rpm 包。\nopenvpn 版本：2.4.7\n下载 easy-rsa 下载地址 5.3 创建目录，配置 vars 解压 easy-rsa 目录 [root@vpnserver ~]# mkdir openvpn [root@vpnserver ~]# unzip easy-rsa-3.0.5.zip [root@vpnserver ~]# mv easy-rsa-3.0.5 easy-rsa [root@vpnserver ~]# mkdir -p /etc/openvpn [root@vpnserver ~]# cp -a easy-rsa /etc/openvpn 配置/etc/openvpn 目录 [root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# cp vars.example vars [root@vpnserver easyrsa3]# vim vars 添加如下变量\nset_var EASYRSA_REQ_COUNTRY \u0026#34;CN\u0026#34; set_var EASYRSA_REQ_PROVINCE \u0026#34;Beijing\u0026#34; set_var EASYRSA_REQ_CITY \u0026#34;Beijing\u0026#34; set_var EASYRSA_REQ_ORG \u0026#34;lvbibir\u0026#34; set_var EASYRSA_REQ_EMAIL \u0026#34;lvbibir@163.com\u0026#34; set_var EASYRSA_REQ_OU \u0026#34;My OpenVPN\u0026#34; 5.4 创建服务端证书及 key 创建服务端证书及 key 初始化\n[root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa init-pki 创建根证书\n[root@vpnserver easyrsa3]# ./easyrsa build-ca 注意：在上述部分需要输入 PEM 密码 PEM pass phrase，输入两次，此密码必须记住，不然以后不能为证书签名。还需要输入 common name 通用名，这个你自己随便设置个独一无二的\n创建服务器端证书\n[root@vpnserver easyrsa3]# ./easyrsa gen-req server nopass 该过程中需要输入 common name，随意但是不要跟之前的根证书的一样\n签约服务端证书\n[root@vpnserver easyrsa3]# ./easyrsa sign server server 需要手动输入 yes 确认，还需要提供创建 ca 证书时的密码\n创建 Diffie-Hellman，确保 key 穿越不安全网络的命令\n[root@vpnserver easyrsa3]# ./easyrsa gen-dh 5.5 创建客户端证书及 key 创建客户端证书 初始化\n[root@vpnserver ~]# mkdir client [root@vpnserver ~]# cd client/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa init-pki 需输入 yes 确认\n创建客户端 key 及生成证书\n[root@vpnserver easyrsa3]# ./easyrsa gen-req zhijie.liu 名字自己自定义，该密码是用户使用该 key 登录时输入的密码，可以加 nopass 参数在客户端登录时无需输入密码\n导入 req 证书\n[root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa import-req /root/client/easy-rsa/easyrsa3/pki/reqs/zhijie.liu.req zhijie.liu 签约证书\n[root@vpnserver easyrsa3]# ./easyrsa sign client zhijie.liu 这里生成 client，名字要与之前导入名字一致\n签约证书期间需要输入 yes 确认，期间需要输入 CA 的密码\n5.6 归置证书 把服务器端必要文件放到/etc/openvpn 下（ca 证书、服务端证书、密钥） [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/private/server.key /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/server.crt /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/dh.pem /etc/openvpn/ 把客户端必要文件放到/root/client 目录下（客户端的证书、密钥） [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /root/client [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/zhijie.liu.crt /root/client/ [root@vpnserver ~]# cp /root/client/easy-rsa/easyrsa3/pki/private/zhijie.liu.key /root/client 5.7 server.conf 配置 为服务器端编写配置文件 安装好配置文件后他会提供一个 server 配置的文件案例，将该文件放到/etc/openvpn 下\n[root@vpnserver ~]# rpm -ql openvpn | grep server.conf [root@vpnserver ~]# cp /usr/share/doc/openvpn-2.4.7/sample/sample-config-files/server.conf /etc/openvpn/ 修改配置文件 [root@vpnserver ~]# vim /etc/openvpn/server.conf [root@vpnserver ~]# grep \u0026#39;^[^#|;]\u0026#39; /etc/openvpn/server.conf local 0.0.0.0 #监听地址 port 1194 #监听端口 proto tcp #监听协议 dev tun #采用路由隧道模式 ca /etc/openvpn/ca.crt #ca证书路径 cert /etc/openvpn/server.crt #服务器证书 key /etc/openvpn/server.key # This file should be kept secret 服务器秘钥 dh /etc/openvpn/dh.pem #密钥交换协议文件 server 10.8.0.0 255.255.255.0 #给客户端分配地址池，注意：不能和VPN服务器内网网段有相同 ifconfig-pool-persist ipp.txt push \u0026#34;route 1.1.1.0 255.255.255.0\u0026#34;\t#推送内网地址 client-to-client #客户端之间互相通信 keepalive 10 120 #存活时间，10秒ping一次,120 如未收到响应则视为断线 comp-lzo #传输数据压缩 max-clients 100 #最多允许 100 客户端连接 user openvpn #用户 group openvpn #用户组 persist-key persist-tun status /var/log/openvpn/openvpn-status.log log /var/log/openvpn/openvpn.log verb 3 5.8 其他设置 用户配置 [root@vpnserver ~]# mkdir /var/log/openvpn/ [root@vpnserver ~]# useradd openvpn -s /sbin/nologin [root@vpnserver ~]# chown -R openvpn.openvpn /var/log/openvpn/ [root@vpnserver ~]# chown -R openvpn.openvpn /etc/openvpn/* iptables 设置 nat 规则和打开路由转发 [root@vpnserver ~]# iptables -A INPUT -p tcp --dport 1194 -j ACCEPT [root@vpnserver ~]# iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE [root@vpnserver ~]# iptables -vnL -t nat Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * * 10.8.0.0/24 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination [root@vpnserver ~]# vim /etc/sysctl.conf net.ipv4.ip_forward = 1 [root@vpnserver ~]# sysctl -p 开启 openvpn 服务 [root@vpnserver ~]# openvpn --daemon --config /etc/openvpn/server.conf [root@vpnserver ~]# netstat -anput | grep 1194 proxy 开启端口转发/映射 [root@along ~]# vim /etc/sysctl.conf //打开路由转发 net.ipv4.ip_forward = 1 [root@proxy ~]# sysctl -p [root@proxy ~]# iptables -t nat -A PREROUTING -d 192.168.150.114 -p tcp --dport 1194 -j DNAT --to-destination 1.1.1.2:1194 [root@proxy ~]# iptables -t nat -A POSTROUTING -d 1.1.1.2 -p tcp --dport 1194 -j SNAT --to 1.1.1.3 [root@proxy ~]# iptables -A FORWARD -o eth0 -d 1.1.1.2 -p tcp --dport 1194 -j ACCEPT [root@proxy ~]# iptables -A FORWARD -i eth0 -s 1.1.1.2 -p tcp --sport 1194 -j ACCEPT [root@proxy ~]# iptables -A INPUT -p tcp --dport 1194 -j ACCEPT [root@proxy ~]# service iptables save [root@proxy ~]# service iptables reload [root@proxy ~]# iptables -L -n 6 客户段连接测试 6.1 client 配置文件 [root@vpnserver ~]# rpm -ql openvpn | grep client.ovpn /usr/share/doc/openvpn-2.4.7/sample/sample-plugins/keying-material-exporter-demo/client.ovpn [root@vpnserver ~]# cp /usr/share/doc/openvpn-2.4.7/sample/sample-plugins/keying-material-exporter-demo/client.ovpn /root/client [root@vpnserver ~]# vim /root/client/client.ovpn client dev tun proto tcp remote 192.168.150.114 1194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert client.crt key client.key comp-lzo verb 3 6.2 拷贝客户端证书及配置文件 vpnserver 没装 vmtools 所以先将所有文件放到 proxy 上然后通过 scp 下载\n[root@vpnserver openvpn]# scp /root/client/ca.crt root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/zhijie.liu.crt root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/zhijie.liu.key root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/client.ovpn root@1.1.1.3:/root/ 将这四个文件放到 win10 的 C:\\Users\\lvbibir\\OpenVPN\\config 目录下\n然后点击连接\n6.3 ping 测试 ping client 的内网 ip 1.1.1.1\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos6-deploy-openvpn/","summary":"0 前言 本文参考以下链接 centos6 源码编译 openvpn 并打包成 rpm openvpn 源码下载地址 centos6 搭建 openvpn centos6 做端口映射/端口转发 1 实验环境 3 台 centos6.5，1 台 win10，openvpn-2.4.7，easy-rsa-3.0.5 2 拓扑结构 Win10 安装 openvpn-gui，三台 centos6.5 为 vmware 虚拟机，分为 client、vpnse","title":"centos6 部署 openvpn"},{"content":"0 介绍 项目地址\n这个项目准备打造一个安全基线检查平台，期望能够以最简单的方式在需要进行检查的服务器上运行。能够达到这么一种效果：基线检查脚本 (以后称之为 agent) 可以单独在目标服务器上运行，并展示出相应不符合基线的地方，并且可以将检查时搜集到的信息以 json 串的形式上传到后端处理服务器上，后端服务器可以进行统计并进行可视化展示。\nAgent 用到的技术：\nShell 脚本 Powershell 脚本 后端服务器用到的技术：\npython django bootstrap html 存储所用：\nsqlite3 1 前端页面部署 1.1 环境 系统 centos7.8(最小化安装) 前端：192.168.150.101 client 端：192.168.150.102 1.2 安装 python3.6 源码包下载地址\nyum install gcc gcc-c++ zlib-devel sqlite-devel mariadb-server mariadb-devel openssl-devel tcl-devel tk-devel tree libffi-devel -y tar -xf Python-3.6.10.tgz ./configure --enable-optimizations make make install python3 -V 1.3 安装 pip3+django 源码包下载地址\ntar zxvf pip-21.0.1.tar.gz cd pip-21.0.1/ python3 setup.py build python3 setup.py install pip3 install django==2.2.15 1.4 clone 项目到本地 yum install -y git git clone https://github.com/chroblert/assetmanage.git 1.5 部署 server 端 cd assetManage # 使用 python3 安装依赖包 python3 -m pip install -r requirements.txt python3 manage.py makemigrations python3 manage.py migrate python3 manage.py runserver 0.0.0.0:8888 # 假定该服务器的 IP 为 112.112.112.112 访问测试：http://192.168.150.101:8888/\n2 客户端进行检查 将项目目录中的 Agent 目录 copy 到需要进行基线检查的客户端 scp -r assetmanage/Agent/ 192.168.150.102:/root/ cd Agent/ chmod a+x ./*.sh 修改 linux_baseline_check.sh 文件的最后一行，配置前端 django 项目的 ip 和端口 运行脚本即可，终端会有检查结果的输出，前端页面相应也会有数据 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/centos7-deploy-benchmark/","summary":"0 介绍 项目地址 这个项目准备打造一个安全基线检查平台，期望能够以最简单的方式在需要进行检查的服务器上运行。能够达到这么一种效果：基线检查脚本 (以后称之为 agent) 可以单独在目标服务器上运行，并展示出相应不符合基线的地方，并且可以将检查时搜集到的信息以 json 串的形式上传到后端处理服务器上，后端服","title":"centos7 | benchmark 平台部署"},{"content":"0 前言 本文参考以下链接:\n网卡的 HWADDR 和 MACADDR 的区别 LINUX 系统中 HWaddr 和 macaddr 什么区别 Linux 永久生效 MAC 1 详解 环境：centos7.8\n在 centos 中可以在如下文件中查看一个 NIC 的配置 ： /etc/sysconfig/network-scripts/ifcfg-N\nHWADDR=, 其中 以 AA:BB:CC:DD:EE:FF 形式的以太网设备的硬件地址.在有多个网卡设备的机器上，这个字段是非常有用的，它保证设备接口被分配了正确的设备名 ，而不考虑每个网卡模块被配置的加载顺序.这个字段不能和 MACADDR 一起使用.\nMACADDR=, 其中 以 AA:BB:CC:DD:EE:FF 形式的以太网设备的硬件地址.在有多个网卡设备的机器上.这个字段用于给一个接口分配一个 MAC 地址，覆盖物理分配的 MAC 地址 . 这个字段不能和 HWADDR 一起使用.\n简单总结一下：\nMACADDR 是系统的网卡物理地址，因为在接收数据包时需要根据这个值来做包过滤。 HWADDR 是网卡的硬件物理地址，只有厂家才能修改 可以用 MACADDR 来覆盖 HWADDR，但这两个参数不能同时使用 ifconfig 和 nmcli 等网络命令中显示的物理地址其实是 MACADDR 的值，虽然显示的名称写的是 HWADDR(ether)。 修改网卡的 mac 地址\n#sudo vim /etc/sysconfig/network-scripts/ifcfg-ens32 注释其中的\u0026#34;HWADDR=xx:xx:xx:xx:xx:xx\u0026#34; 添加或者修改\u0026#34;MACADDR=xx:xx:xx:xx:xx:xx\u0026#34; 如果没有删除或者注释掉HWADDR，当HWADDR与MACADDR地地不同时，启动不了网络服务的提示：　“Bringing up interface eth0: Device eth0 has different MAC address than expected,ignoring.” 故正确的操作是将HWADDR删除或注释掉，改成MACADDR 查看系统初始的 mac 地址即 HWADDR 把配置文件中的 MACADDR 注释或者删除掉，不用配置 HWADDR，重启网络服务后用命令查看到的 mac 地址就是网卡的 HWADDR\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/linux-hwaddr-macaddr-different/","summary":"0 前言 本文参考以下链接: 网卡的 HWADDR 和 MACADDR 的区别 LINUX 系统中 HWaddr 和 macaddr 什么区别 Linux 永久生效 MAC 1 详解 环境：centos7.8 在 centos 中可以在如下文件中查看一个 NIC 的配置 ： /etc/sysconfig/network-scripts/ifcfg-N HWADDR=, 其中 以 AA:BB:CC:DD:EE:FF 形式的以太网设备的硬件地址.在有多个网卡设备的机器上，这个字段是非常有用的，它保证设备接口被分配了正确的设备名 ，而不考虑每","title":"linux | hwaddr 和 macaddr 的区别"},{"content":"1 七牛云配置 1.1 注册七牛云，新建存储空间 七牛云新用户有 10G 的免费空间，作为个人博客来说基本足够了\n1.2 为存储空间配置加速域名 1.3 配置 https 证书 1.3.1 购买免费证书 1.3.2 补全域名信息 1.3.3 域名验证 根据在域名提供商处新建解析\ndns 配置好之后等待 CA 机构审核后颁发证书就可以了\n1.4.4 开启 https 2 PicGo 配置 2.1 下载安装 下载链接：https://github.com/Molunerfinn/PicGo/releases/\n建议下载稳定版\n2.2 配置七牛云图床 ak 和 sk 在七牛云→个人中心→密钥管理中查看\n在 picgo 端配置各项信息，注意网址要改成 https\n3 typora 测试图片上传 下载地址：https://www.typora.io/\n在文件→偏好设置→图像中配置图片上传，选择安装好的 PicGo 的应用程序\n点击验证图片上传\n到七牛云存储空间看是否有这两个文件\ntypora 可以实现自动的图片上传，并将本地连接自动转换为外链地址\n4 可能的报错 一般报错原因都可在 picgo 的日志文件找到，路径：C:\\Users\\username\\AppData\\Roaming\\picgo\n4.1 failed to fetch 日志报错如下\n问题在于端口冲突，如果你打开了多个 picgo 程序，就会端口冲突，picgo 自动帮你把 36677 端口改为 366771 端口，导致错误。\n重新验证\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/typora-picgo-qiniu-upload-image/","summary":"1 七牛云配置 1.1 注册七牛云，新建存储空间 七牛云新用户有 10G 的免费空间，作为个人博客来说基本足够了 1.2 为存储空间配置加速域名 1.3 配置 https 证书 1.3.1 购买免费证书 1.3.2 补全域名信息 1.3.3 域名验证 根据在域名提供商处新建解析 dns 配置好之后等待 CA 机构审核后颁发证书就可以了 1.4.4 开启 https 2 PicGo 配置 2.1 下载安装 下载链接：http","title":"markdown 图片存储方案 | typora + picgo + 七牛云"},{"content":"1 现象 博客加载不出来我在七牛云的图片资源 使用浏览器直接访问图片 url 却是可以成功的 我将之前 csdn 的博客迁移到了 wordpress，图片外链地址就是 csdn 的，都可以正常加载。 使用浏览器直接访问图片 url 却是可以成功的\n我将之前 csdn 的博客迁移到了 wordpress，图片外链地址就是 csdn 的，都可以正常加载。\n2 排查 1、由于浏览器直接访问七牛云图床的 url 地址是可以访问的，证明地址并没错，有没有可能是 referer 防盗链的配置问题\n查看防盗链配置，并没有开\n2、wordpress 可以加载出来 csdn 的外链图片，期间也试了其他图床都是没问题的。\n3、看看七牛的图片外链和 csdn 的有何区别\n注意到七牛的图片外链是 http，当时嫌麻烦并没有配置 https，看来问题是出在这了\n因为我的网站配置了 ssl 证书，可能由于安全问题浏览器不予加载 http 项目，用 http 访问站点测试下图片是否可以加载\n访问成功了！\n3 解决 给图床服务器安装 ssl 证书，开启 https 访问，参考：typora-picgo-qiniu-upload-image\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-load-image-failed/","summary":"1 现象 博客加载不出来我在七牛云的图片资源 使用浏览器直接访问图片 url 却是可以成功的 我将之前 csdn 的博客迁移到了 wordpress，图片外链地址就是 csdn 的，都可以正常加载。 使用浏览器直接访问图片 url 却是可以成功的 我将之前 csdn 的博客迁移到了 wordpress，图片外链地址就是 csdn 的，都可以正常加载。","title":"wordpress | 加载图片失败"},{"content":"默认主题下在后台设置里修改即可\ndux 主题修改方式：在后台管理→dux 主题编辑器→网站底部信息中添加\n\u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;京ICP备2021023168号-1\u0026lt;/a\u0026gt; 通用修改方式\n在主题目录的 footer.php 文件中的 \u0026lt;footer\u0026gt;\u0026lt;/footer\u0026gt; 下添加代码\n\u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;你的备案号\u0026lt;/a\u0026gt; 以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-add-icp/","summary":"默认主题下在后台设置里修改即可 dux 主题修改方式：在后台管理→dux 主题编辑器→网站底部信息中添加 \u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;京ICP备2021023168号-1\u0026lt;/a\u0026gt; 通用修改方式 在主题目录的 footer.php 文件中的 \u0026lt;footer\u0026gt;\u0026lt;/footer\u0026gt; 下添加代码 \u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=","title":"wordpress | 添加 icp 备案号"},{"content":"0 前言 本文参考以下链接:\n在 Apache 服务器上安装 SSL 证书 WordPress 强制跳转 https 教程 1 配置 ssl 证书 1、登录阿里云，选择产品中的 ssl 证书\n如果域名是阿里的他会自动创建 dns 解析，如果是其他厂商需要按照图片配置，等待几分钟进行验证\n点击审核，等待签发\n签发后根据需求下载所需证书\n我的 wordpress 是直接买的阿里轻量应用服务器，打开轻量应用服务器的控制台配置域名\n选择刚申请好的 ssl 证书\n在 wordpress 后台修改地址\n大功告成\n2 配置 https 强制跳转 一般站点需要在 httpd.conf 中的 \u0026lt;VirtualHost *:80\u0026gt; \u0026lt;/VirtualHost\u0026gt; 中配置重定向\nwordpress 不同，需要在伪静态文件（.htaccess）中配置重定向，无需在 httpd.conf 中配置\n2.1 修改伪静态文件（.htaccess） 伪静态文件一般在网页根目录，是一个隐藏文件\n在 #END Wordpress 前添加如下重定向代码，记得把域名修改成自己的\nRewriteEngine On RewriteCond %{HTTPS} !on RewriteRule ^(.*)$ https://lvbibir.cn/%{REQUEST_URI} [L,R=301] 图中两段重定向代码略有不同\n第一段代码重定向触发器：当访问的端口不是 443 时进行重定向重定向规则：重定向到：https://{原域名}/{原 url 资源} 第二段代码重定向触发器：当访问的协议不是 TLS/SLL（https）时进行重定向重定向规则：重定向到：https://lvbibir.cn/{原 url 资源} 第一段代码使用端口判断，第二段代码通过访问方式判断，建议使用访问方式判断，这样服务改了端口也可以正常跳转 第一段代码重定向的原先的域名，第二段代码可以把 ip 地址重定向到指定域名 2.2 测试 curl -I http://lvbibir.cn 使用 http 访问站点的 80 端口成功通过 301 跳转到了 https\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/blog/wordpress-ssl/","summary":"0 前言 本文参考以下链接: 在 Apache 服务器上安装 SSL 证书 WordPress 强制跳转 https 教程 1 配置 ssl 证书 1、登录阿里云，选择产品中的 ssl 证书 如果域名是阿里的他会自动创建 dns 解析，如果是其他厂商需要按照图片配置，等待几分钟进行验证 点击审核，等待签发 签发后根据需求下载所需证书 我的 wordpress 是直接买的阿里轻量应用服务器，打开轻","title":"wordpress | 配置免费 ssl 证书和 https 强制跳转"},{"content":"0 前言 本文参考以下链接:\ndocker 三剑客之 machine 1 Docker Machine 简介 Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker 环境。\nDocker Machine 支持在常规 Linux 操作系统、虚拟化平台、openstack、公有云等不同环境下安装配置 dockerhost。\nDocker Machine 项目基于 Go 语言实现，目前在 Github 上的 维护地址\n2 Docker Machine 实践 2.1 环境准备 三台 centos7，两台新系统，一台装有 docker machine：192.168.1.101 host1:192.168.1.127 host2:192.168.1.180 保证三台 centos7 可以连接到外网 2.2 安装 machine base=https://github.com/docker/machine/releases/download/v0.14.0 \u0026amp;\u0026amp; curl -L $base/docker-machine-$(uname -s)-$(uname -m) \u0026gt;/tmp/docker-machine \u0026amp;\u0026amp; sudo install /tmp/docker-machine /usr/local/bin/docker-machine\t下载并安装 doker-machine，路径在 /usr/local/bin 下\n2.3 创建 machine machine 指的是 docker daemon 主机，其实就是在 host 上安装和部署 docker。\n创建流程：\n安装 docker 软件包 ssh 免密登陆远程主机 复制证书 配置 docker daemon 启动 docker 2.4 免密登录 ssh-keygen ssh-copy-id 目标ip [root@server5 ~]# ssh-keygen [root@server5 ~]# ssh-copy-id 192.168.1.127 [root@server5 ~]# ssh-copy-id 192.168.1.180 测试：\nssh 192.168.1.127 ssh 192.168.1.180 2.4.1 创建主机 docker-machine create --driver generic --generic-ip-address=192.168.1.127 host1 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-machine/","summary":"0 前言 本文参考以下链接: docker 三剑客之 machine 1 Docker Machine 简介 Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker 环境。 Docker Machine 支持在常规 Linux 操作系统、虚拟化平台、openstack、公有云等不同环境下安装配置 dockerhost。 Docker Machine 项目基于 Go 语言实现，目前在 Github 上的 维护","title":"docker | 三剑客之 machine"},{"content":"0 前言 本文参考以下链接:\nDocker三剑客——Swarm docker swarm 删除节点（解散集群） 截取已创建好的 swarm 集群的 token 1 部署 1.1 环境准备 准备 3 台 Ubuntu 系统主机 (即用于搭建集群的 3 个 Docker 机器)，每台机器上都需要安装 Docker 并且可以连接网络，同时要求 Docker 版本必须是 1.12 及以上，因为老版本不支持 Docker Swarm 集群管理节点 Docker 机器的 IP 地址必须固定，集群中所有节点都能够访问该管理节点。 集群节点之间必须使用相应的协议并保证其以下端口可用： 用于集群管理通信的 TCP 端口 2377； TCP 和 UDP 端口 7946，用于节点间的通信； UDP 端口 4789，用于覆盖网络流量 为了进行本实例演示，此处按照要求安装了 3 台使用 centos7.4 系统的机器，这三台机器的主机名称分别为 manager1(作为管理节点)，worker1(作为工作节点)，worker2(作为工作节点),其 IP 地址分别如下：\n主机名 IP 地址 manager 192.168.0.101 worker-1 192.168.0.102 worker-2 192.168.0.103 1.2 创建集群 在 manager 上创建 swarm 集群\n[root@node-1 ~]# docker swarm init --advertise-addr 192.168.0.101 使用 docker node ls 查看集群节点信息\n[root@manager ~]# docker node ls 1.3 添加工作节点 在 worker1 和 worker2 中执行，加入 swarm 集群\ndocker swarm join --token SWMTKN-1-2zhqxsklcroivbpjzzntn5snsim79o5z7xzj4hzexk9phsz68q-d0seaxjgxpjebk8fdqt6d6yz5 192.168.0.101:2377 在管理节点上，使用 docker node ls 查看集群节点信息\n[root@manager ~]# docker node ls 1.4 部署服务 在向 docker swarm 集群中部署服务时，既可以使用 docker hub 上自带的镜像来启动服务，也可以自己通过 dockerfile 的镜像来启动服务，如果使用自己的 dockerfile 构建的镜像来启动服务，那么必须先将镜像推送到 docker hub 中心仓库\n这里，我们使用 docker hub 上自带的 alpine 镜像为例来部署集群服务\n[root@manager ~]# docker service create --replicas 1 --name helloworld alpine ping docker.com 1.5 查看服务 当服务部署完成后，在管理节点上可以通过 docker service ls 查看当前集群中的服务列表信息\n[root@manager ~]# docker service ls 使用 docker service inspect 查看部署的服务具体详情\n[root@manager ~]# docker service inspect helloworl 使用 docker service ps 查看指定服务在集群节点上的分配和运行情况\n[root@manager ~]# docker service ps helloworld 1.6 更改副本数量 在集群中部署的服务，如果只运行一个副本，就无法体现出集群的优势，并且一旦该机器或副本崩溃，该服务将无法访问，所以通常一个服务会启动多个副本\n[root@manager ~]# docker service scale helloworld=5 更改完成后，就可以谈过 docker service ps 查看这五个服务副本在 3 个节点上的具体分布和运行情况\n[root@manager ~]# docker service ps helloworld 1.7 删除服务 对于不需要的服务，我们可以进行删除\n[root@manager ~]# docker service rm helloworld 1.8 访问服务 在管理节点上，执行 docker network ls 查看网络列表\n[root@manager ~]# docker network ls 在管理节点上，创建 overlay 网络\n[root@manager ~]# docker network create -d overlay ov_net 在管理节点上，再次部署服务\n[root@manager ~]# docker service create --network ov_net --name my-web --publish 8080:80 --replicas 2 nginx 访问 nginx 服务\n以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-swarm/","summary":"0 前言 本文参考以下链接: Docker三剑客——Swarm docker swarm 删除节点（解散集群） 截取已创建好的 swarm 集群的 token 1 部署 1.1 环境准备 准备 3 台 Ubuntu 系统主机 (即用于搭建集群的 3 个 Docker 机器)，每台机器上都需要安装 Docker 并且可以连接网络，同时要求 Docker 版本必须是 1.12 及以上，因为老版本不支持 Docker Swarm 集群管理节点 Docker 机器的","title":"docker | 三剑客之 swarm"},{"content":"1 环境准备 主机版本为 Centos7.4，docker 版本为 docker-ce-18.09.7-3.el7.x86_64\nnode1: 192.168.0.111 node2: 192.168.0.107 两台安装 docker 的环境 保证两台主机上的 docker 的 Client API 与 Server APi 版本一致 2 修改配置文件 修改 daemon.json 配置文件，添加 label，用于区别两台 docker 主机\nnode1：\n[root@localhost ~]# vim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;], \u0026#34;labels\u0026#34;: [\u0026#34;-label nodeName=node1\u0026#34;] #添加label } 查看效果\n[root@localhost ~]# systemctl restart docker [root@localhost ~]# docker info node2:\n3 修改守护进程的通信方式 修改通信方式共有三种方式：\n修改 daemon.json 文件，添加 host 键值对 添加：\u0026quot;hosts\u0026quot;: [\u0026quot;tcp://0.0.0.0:2375\u0026quot;] 开放本机 ip 的 2375 端口，可以让其他 docker 主机的 client 进行连接 修改 /lib/systemd/system/docker.service 文件，添加 -H 启动参数 修改：ExecStart=/usr/bin/docker -H \u0026lt;tcp://0.0.0.0:2375\u0026gt; 使用 dokcerd 启动 docker，添加 -H 参数 dockerd -H \u0026lt;tcp://0.0.0.0:2375\u0026gt; Centos7 中/etc/docker/daemon.json 会被 docker.service 的配置文件覆盖，直接添加 daemon.json 不起作用\n所以我使用的是第二种方式\nnode1：\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/docker -H tcp://0.0.0.0:2375 [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 5775 1 3 23:17 ? 00:00:00 /usr/bin/dockerd -H tcp://0.0.0.0:2375 root 5779 5775 0 23:17 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 5879 3919 0 23:17 pts/1 00:00:00 grep --color=auto docker 4 远程访问 node2：\n[root@localhost ~]# curl http://192.168.0.111:2375/info [root@localhost ~]# docker -H tcp://192.168.0.111:2375 info 如果频繁使用 -H 选项未免太过于麻烦，可以修改 DOCKER_HOST 这个环境变量的值，node2 就可以像使用本地的 docker 一样来远程连接 node1 的守护进程\n[root@localhost ~]# export DOCKER_HOST=\u0026#34;tcp://192.168.0.111:2375\u0026#34; [root@localhost ~]# docker info 当无需再远程连接 node1 的守护进程时，将 DOCKER_HOST 环境变量置空即可\n[root@localhost ~]# export DOCKER_HOST=\u0026#34;\u0026#34; [root@localhost ~]# docker info node1：\n因为 node1 设置了修改 Client 与守护进程的通信方式，所以本地无法再通过默认的 socket 进行连接，必须使用 -H 选项通过 tcp 来进行连接，也可以通过 DOCKER_HOST 来修改\n[root@localhost ~]# docker info Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? [root@localhost ~]# docker -H 0.0.0.0:2375 info 如果本机依旧希望使用默认的 socket 进行连接，可以在 docker.service 中再添加一个 -H 选项\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 6462 1 2 23:40 ? 00:00:00 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock root 6467 6462 0 23:40 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 6567 3919 0 23:40 pts/1 00:00:00 grep --color=auto docker [root@localhost ~]# docker info 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-remote-call-daemon/","summary":"1 环境准备 主机版本为 Centos7.4，docker 版本为 docker-ce-18.09.7-3.el7.x86_64 node1: 192.168.0.111 node2: 192.168.0.107 两台安装 docker 的环境 保证两台主机上的 docker 的 Client API 与 Server APi 版本一致 2 修改配置文件 修改 daemon.json 配置文件，添加 label，用于区别两台 docker 主机 node1： [root@localhost ~]# vim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;], \u0026#34;labels\u0026#34;: [\u0026#34;-label nodeName=node1\u0026#34;] #添加label } 查看效果 [root@localhost ~]# systemctl restart docker [root@localhost ~]# docker info node2: 3 修改守护进程的","title":"docker | 守护进程的远程调用"},{"content":"0 前言 实现跨主机的 docker 容器之间的通讯：\n使用网桥实现跨主机的连接 docker 原生的网络：overlay、macvlan 第三方网络：flaanel、weave、calic 1 网桥 2 open vswitch 3 weave 4 macvlan 4.1 简介 macvlan 是 Linux 操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以为一张物理网卡设置多个 mac 地址，相当于物理网卡施展了影分身之术，由一个变多个，同时要求物理网卡打开混杂模式。针对每个 mac 地址，都可以设置 IP 地址，本来是一块物理网卡连接到交换机，现在是多块虚拟网卡连接到交换机。\n当容器需要直连入物理网络时，可以使用 Macvlan。Macvlan 本身不创建网络，本质上首先使宿主机物理网卡工作在‘混杂模式’，这样物理网卡的 MAC 地址将会失效，所有二层网络中的流量物理网卡都能收到。接下来就是在这张物理网卡上创建虚拟网卡，并为虚拟网卡指定 MAC 地址，实现一卡多用，在物理网络看来，每张虚拟网卡都是一个单独的接口。使用 Macvlan 有几点需要注意：\n容器直接连接物理网络，由物理网络负责分配 IP 地址，可能的结果是物理网络 IP 地址被耗尽，另一个后果是网络性能问题，物理网络中接入的主机变多，广播包占比快速升高而引起的网络性能下降问题。 前边说过了，宿主机上的某张网上需要工作在‘混乱模式’下。 从长远来看 bridge 网络与 overlay 网络是更好的选择，原因就是虚拟网络应该与物理网络隔离而不是共享。 优缺点：\n优点是性能非常好 缺点是地址需要手动分配 Macvlan 网络有两种模式：bridge 模式与 802.1q trunk bridge 模式。\nbridge 模式，Macvlan 网络流量直接使用宿主机物理网卡。 802.1q trunk bridge 模式，Macvlan 网络流量使用 Docker 动态创建的 802.1q 子接口，对于路由与过虑，这种模式能够提供更细粒度的控制 4.2 部署 环境准备：\n两台 centos7 docker 版本：18.03 ip：192.168.0.53（node-1） 192.168.0.54（node-2） [root@node-1 ~]# ip link show ens33 [root@node-1 ~]# ip link set ens32 promisc on #开启混杂模式，保证多个ip可以通过 [root@node-1 ~]# docker network create -d macvlan --subnet 10.0.0.0/24 --gateway=10.0.0.1 -o parent=ens33 mac_net1 [root@node-1 ~]# docker network ls node-1 docker run -itd --name bbox-1 --ip 10.0.0.11 --network mac_net1 busybox node-2 docker run -itd --name bbox-2 --ip 10.0.0.12 --network mac_net1 busybox node-1 [root@node-1 ~]# docker exec bbox-1 ping 10.0.0.12 [root@node-1 ~]# docker exec bbox-1 ping bbox-2 可以 ping 通 ip，但是无法 ping 通主机名，因为它没有 dns 解析\n[root@node-1 ~]# brctl show 因为 macvlan 不依赖于 bridge 网络，所以查看不到新的桥接网络\n[root@node-1 ~]# docker exec bbox-1 ip link 查看到 eth0 连接到了 if2\n[root@node-1 ~]# ip link show ens33 可以查看到 ens33 的编号是 2，即 bbox-1 容器的 eth0 网卡连接到了 ens33 物理网卡\n[root@node-1 ~]# docker network create -d macvlan -o parent=ens33 mac_net2 Error response from daemon: network dm-b34ee1020a96 is already using parent interface ens33 再创建 macvlan 网络时发现已经无法再创建，即一块网卡只能添加一个 macvlan 的地址\n4.3 一块网卡绑定多个 macvlan 地址 [root@node-1 ~]# modinfo 8021q # 查看内核是否支持802.1q封装 [root@node-1 ~]# modprobe 8021q # 如果上条命令执行后没有结果，使用该命令加载该模块 node-1 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=manual 修改为不需要 ip 的 manual 模式\nnode-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32 BOOTPROTO=manual node-1 添加两块虚拟网卡，注意与实际的 ens32 网卡的网段区分开\nens32 使用的是 192.168.0.0/24 网段，虚拟网卡使用的是 192.168.1.0/24 和 192.168.2.0/24\n[root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens33.10 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.10 BOOTPROTO=none NAME=ens33.10 DEVICE=ens33.10 ONBOOT=yes IPADDR=192.168.1.10 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33.10 /etc/sysconfig/network-scripts/ifcfg-ens33.20 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.20 BOOTPROTO=none NAME=ens33.20 DEVICE=ens33.20 ONBOOT=yes IPADDR=192.168.2.10 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-1 ~]# ifup ens33.10 [root@node-1 ~]# ifup ens33.20 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.10 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.10 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.20 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.20 node-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.10 BOOTPROTO=none NAME=ens32.10 DEVICE=ens32.10 ONBOOT=yes IPADDR=192.168.1.20 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.20 BOOTPROTO=none NAME=ens32.20 DEVICE=ens32.20 ONBOOT=yes IPADDR=192.168.2.20 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-2 ~]# ifup ens32.10 [root@node-2 ~]# ifup ens32.20 node-1 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens33.10 mac_net11 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens33.20 mac_net12 node-2 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens32.10 mac_net11 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens32.20 mac_net12 node-1 [root@node-2 ~]# docker run -itd --name bbox-11 --ip=172.16.11.11 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-12 --ip=172.16.12.11 --network mac_net12 busybox node-2 [root@node-2 ~]# docker run -itd --name bbox-21 --ip=172.16.11.12 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-22 --ip=172.16.12.12 --network mac_net12 busybox node-1 [root@node-1 ~]# docker exec bbox-11 ping 172.16.11.12 PING 172.16.11.12 (172.16.11.12): 56 data bytes 64 bytes from 172.16.11.12: seq=0 ttl=64 time=0.867 ms 64 bytes from 172.16.11.12: seq=1 ttl=64 time=1.074 ms 64 bytes from 172.16.11.12: seq=2 ttl=64 time=1.145 ms 64 bytes from 172.16.11.12: seq=3 ttl=64 time=0.938 ms ^C [root@node-1 ~]# docker exec bbox-12 ping 172.16.12.12 PING 172.16.12.12 (172.16.12.12): 56 data bytes 64 bytes from 172.16.12.12: seq=0 ttl=64 time=0.858 ms 64 bytes from 172.16.12.12: seq=1 ttl=64 time=1.140 ms 64 bytes from 172.16.12.12: seq=2 ttl=64 time=0.818 ms 64 bytes from 172.16.12.12: seq=3 ttl=64 time=1.056 ms ^C 在两台系统进行修改，添加网关，修改防火墙策略 node-1 中记得将 ens32 更换为 ens33 ifconfig ens32.10 172.16.10.1 netmask 255.255.255.0 ifconfig ens32.20 172.16.20.1 netmask 255.255.255.0 iptables -t nat -A POSTROUTING -o ens32.10 -j MASQUERADE iptables -t nat -A POSTROUTING -o ens32 -j MASQUERADE iptables -A FORWARD -i ens32.10 -o ens32 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32.10 -o ens32 -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -j ACCEPT 5 overlay 5.1 简介 5.2 准备 overlay 环境 为支持容器的跨主机通信，Docker 提供了 overlay driver。Docker overlay 网络需要一个 key-value 数据库用于保存网络状态信息，包括 Network、Endpoint、IP 等。Consul、Etcd 和 ZooKeeper 都是 Docker 支持的 key-value 软件，这里我们使用 Consul\n环境描述 节点 系统版本 docker 版本 角色 IP 地址 node-1 centos7.4 docker-18.03.0 consul 192.168.0.101 node-2 centos7.4 docker-18.03.0 host 192.168.0.102 node-3 centos7.4 docker-18.03.0 host 192.168.0.103 创建 consul node-1; [root@node-1 ~]# docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 容器启动后可以通过 192.168.0.101:8500 访问到 consul\n修改 docker 配置文件 修改 node-2 和 node-3 的 docker daemon 的配置文件/etc/systemd/system/docker.service\n[root@node-2 ~]# vim /etc/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.0.101:8500 --cluster-advertise=ens32:2376 [root@node-2 ~]# systemctl daemon-reload [root@node-2 ~]# systemctl restart docker -H ：tcp：允许 tcp 连接 daemon -H：unix：默认的 socket 连接方式，支持远程的同时，本地也可以连接 \u0026ndash;cluster-store 指定 consul 的地址 \u0026ndash;cluster-advertise 告知 consul 自己的连接地址 node-2 和 node-3 会自动注册到 consul 数据库中。\n5.3 创建 overlay 网络 在 node-2 中创建网络 在 node-2 中创建 overlay 网络 ov_net1\n[root@node-2 ~]# docker network create -d overlay ov_net1 -d overlay：指定 driver 为 overlay [root@node-2 ~]# docker network ls node-3 查看创建的网络 注意到 ov_net1 的 SCOPE 为 global，而其他网络为 local 。在 node-3 上查看存在的网络:\n[root@node-3 ~]# docker network ls node-3 上也能看到 ov_net1，只是因为创建 ov_net1 时将 overlay 网络信息存入了 consul，node-3 从 consul 读取到了新网络数据。之后 ov_net1 的任何变化都会同步到 node-2 和 node-3\n查看 ov_net1 详细信息 [root@node-2 ~]# docker network inspect ov_net1 IPAM 是指 IP Address Management，docker 自动为 ov_net1 分配的 IP 空间为 10.0.0.0/24\n5.4 在 overlay 中运行容器 创建容器 bbox-1 在 node-2 上运行一个 busybox 容器并连接到 ov_net1.\n[root@node-2 ~]# docker run -itd --name bbox-1 --network ov_net1 busybox 查看 bbox-1 网络配置 [root@node-2 ~]# docker exec bbox-1 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 bbox-1 有两个网络接口，eth0 和 eth1 eth0 IP 为 10.0.0.2，连接的是 overlay 网络 ov_net1 eth1 IP 为 172.18.0.2 容器的默认路由是走 eth1，其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力 [root@node-2 ~]# docker network ls [root@node-2 ~]# docker network inspect docker_gwbridge 从 docker network inspect docker_gwbridge 输出可确认 docker_gwbridge 的 IP 地址范围是 172.18.0.0/16，当前连接的容器就是 bbox-1（172.18.0.2）\n而且此网络的网关就是网桥 docker_gwbridge 的 IP 172.18.0.1\n[root@node-2 ~]# ifconfig docker_gwbridge docker_gwbridge: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fe80::42:c5ff:fe45:937 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 02:42:c5:45:09:37 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 这样容器 bbox-1 就可以通过 docker_gwbridge 访问外网\n[root@node-2 ~]# docker exec bbox-1 ping -c 4 www.baidu.com PING www.baidu.com (182.61.200.6): 56 data bytes 64 bytes from 182.61.200.6: seq=0 ttl=53 time=6.721 ms 64 bytes from 182.61.200.6: seq=1 ttl=53 time=7.954 ms 64 bytes from 182.61.200.6: seq=2 ttl=53 time=11.723 ms 64 bytes from 182.61.200.6: seq=3 ttl=53 time=15.105 ms --- www.baidu.com ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 6.721/10.375/15.105 ms 5.5 overlay 网络连通性 node-3 中 运行 bbox-2 [root@node-3 ~]# docker run -itd --name bbox-2 --network ov_net1 busybox 查看 bbox-2 路由情况 [root@node-3 ~]# docker exec bbox-2 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.3 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 互通测试 [root@node-3 ~]# docker exec bbox-2 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=2.628 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=1.004 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=1.277 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=1.505 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 1.004/1.603/2.628 ms 可见 overlay 网络中的容器可以直接通信，同时 docker 也实现了 DNS 服务\n实现原理 docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0， veth pair 一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。\nbr0 除了连接所有的 veth pair，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：\n[root@node-2 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024217edc413 no docker_gwbridge 8000.0242c5450937 no vethc59120e virbr0 8000.525400b76fd4 yes virbr0-nic [root@node-3 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242ef3c7df7 no docker_gwbridge 8000.0242c81afaee no vethf4562a9 virbr0 8000.525400c28478 yes virbr0-nic 要查看 overlay 网络的 namespace 可以在 node-2 和 node-3 上执行 ip netns（请确保在此之前执行过 ln -s /var/run/docker/netns /var/run/netns），可以看到两个 node 上有一个相同的 namespace \u0026ldquo;1-dd91de7599\u0026rdquo;\n[root@node-2 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-2 ~]# ip netns 6889f61efc4b (id: 1) 1-dd91de7599 (id: 0) [root@node-3 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-3 ~]# ip netns 8e4722847745 (id: 1) 1-dd91de7599 (id: 0) \u0026ldquo;1-dd91de7599\u0026rdquo; 这就是 ov_net1 的 namespace，查看 namespace 中的 br0 上的设备\n[root@node-2 ~]# ip netns exec 1-dd91de7599 brctl show bridge name bridge id STP enabled interfaces br0 8000.0e7576c7c035 no veth0 vxlan0 5.6 overlay 网络隔离 不同的 overlay 网络是相互隔离的。我们创建第二个 overlay 网络 ov_net2 并运行容器 bbox-3\n创建网络 ov_net2 [root@node-2 ~]# docker network create -d overlay ov_net2 启动容器 bbox-3 [root@node-2 ~]# docker run -itd --name bbox-3 --network ov_net2 busybox 查看 bbox-3 网络 bbox-3 分配到的 IP 是 10.0.1.2，尝试 ping bbox-1（10.0.0.2）\n[root@node-2 ~]# docker exec -it bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes --- 10.0.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes --- 172.18.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss ping 失败，可见不同 overlay 网络之间是隔离的，即使通过 docker_gwbridge 也不能通信\n如果要实现 bbox-3 和 bbox-1 通信，可以将 bbox-3 也连接到 ov_net1\n这时 bbox-3 同时连接到了 ov_net1 和 ov_net2 上\n[root@node-2 ~]# docker network connect ov_net1 bbox-3 [root@node-2 ~]# docker exec bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth2 scope link src 10.0.0.4 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=0.184 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.158 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=0.162 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=0.093 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.093/0.149/0.184 ms docker 默认为 overlay 网络分配 24 位掩码的子网（10.0.X.0/24），所有主机共享这个 subnet，容器启动时会顺序从此空间分配 IP。当然我们也可以通过 \u0026ndash;subnet 指定 IP 空间。\ndocker network create -d overlay --subnet 10.22.1.0/24 ov_net 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-rong-qi-kua-zhu-ji-lian-jie/","summary":"0 前言 实现跨主机的 docker 容器之间的通讯： 使用网桥实现跨主机的连接 docker 原生的网络：overlay、macvlan 第三方网络：flaanel、weave、calic 1 网桥 2 open vswitch 3 weave 4 macvlan 4.1 简介 macvlan 是 Linux 操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以为一张物理网卡设置多","title":"docker | 容器的跨主机连接"},{"content":"1 容器操作 1.1 run 创建一个新的容器并运行一个命令\n语法: docker run [OPTIONS] IMAGE [COMMAND] [ARG…]\nOPTIONS 说明：\n-a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 1. 只指定容器端口（宿主机端口随机映射） docker run -p 80 -it ubuntu /bin/bash 2. 主机端口：容器端口 docker run -p 8080:80 -it ubuntu /bin/bash 3. IP：容器端口 docker run -p 0.0.0.0:80 -it ubuntu /bin/bash 4. IP：端口：容器端口 dokcer run -p 0.0.0.0:8080:80 -it ubuntu /bin/bash -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=\u0026#34;nginx-lb\u0026#34;: 为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \u0026#34;mars\u0026#34;: 指定容器的hostname； -e username=\u0026#34;ritchie\u0026#34;: 设置环境变量； -env-file=[]: 从指定文件读入环境变量； --cpuset=\u0026#34;0-2\u0026#34; or --cpuset=\u0026#34;0,1,2\u0026#34;: 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； --net=\u0026#34;bridge\u0026#34;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --volume , -v: 绑定一个卷 -v [hostpath]:containerpath:[ro|wo|rw] 如果没有设置 hostpath 则挂载到 /var/lib/docker/volumes/\u0026lt;containerid\u0026gt;/ 目录下 如果没有设置权限，默认 rw(读写) 实例\n使用 docker 镜像 nginx:latest 以后台模式启动一个容器,并将容器命名为 mynginx。\ndocker run --name mynginx -d nginx:latest 使用镜像 nginx:latest 以后台模式启动一个容器,并将容器的 80 端口映射到主机随机端口。\ndocker run -P -d nginx:latest 使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。\ndocker run -p 80:80 -v /data:/data -d nginx:latest 绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。\ndocker run -p 127.0.0.1:80:8080/tcp ubuntu bash 使用镜像 nginx:latest 以交互模式启动一个容器,在容器内执行/bin/bash 命令。\nrunoob@runoob:~$ docker run -it nginx:latest /bin/bash root@b8573233d675:/# 1.2 ps 列出容器\n语法: docker ps [OPTIONS]\nOPTIONS 说明：\n-a: 显示所有的容器，包括未运行的。 -f: 根据条件过滤显示的内容。 --format: 指定返回值的模板文件。 -l: 显示最近创建的容器。 -n: 列出最近创建的n个容器。 --no-trunc: 不截断输出。 -q: 静默模式，只显示容器编号。 -s: 显示总的文件大小。 实例\n列出所有在运行的容器信息。\nrunoob@runoob:~$ docker ps CONTAINER ID IMAGE COMMAND ... PORTS NAMES 09b93464c2f7 nginx:latest \u0026#34;nginx -g \u0026#39;daemon off\u0026#34; ... 80/tcp, 443/tcp myrunoob 96f7f14e99ab mysql:5.6 \u0026#34;docker-entrypoint.sh\u0026#34; ... 0.0.0.0:3306-\u0026gt;3306/tcp mymysql 列出最近创建的 5 个容器信息。\nrunoob@runoob:~$ docker ps -n 5 CONTAINER ID IMAGE COMMAND CREATED 09b93464c2f7 nginx:latest \u0026#34;nginx -g \u0026#39;daemon off\u0026#34; 2 days ago ... b8573233d675 nginx:latest \u0026#34;/bin/bash\u0026#34; 2 days ago ... b1a0703e41e7 nginx:latest \u0026#34;nginx -g \u0026#39;daemon off\u0026#34; 2 days ago ... f46fb1dec520 5c6e1090e771 \u0026#34;/bin/sh -c \u0026#39;set -x \\t\u0026#34; 2 days ago ... a63b4a5597de 860c279d2fec \u0026#34;bash\u0026#34; 2 days ago ... 列出所有创建的容器 ID。\nrunoob@runoob:~$ docker ps -a -q 09b93464c2f7 b8573233d675 b1a0703e41e7 f46fb1dec520 a63b4a5597de 6a4aa42e947b de7bb36e7968 43a432b73776 664a8ab1a585 ba52eb632bbd 1.3 inspect docker inspect : 获取容器/镜像的元数据。\n语法: docker inspect [OPTIONS] NAME|ID [NAME|ID…]\nOPTIONS 说明：\n-f :指定返回值的模板文件。 -s :显示总的文件大小。 --type json:为指定类型返回JSON。 --format :以指定格式返回数据 获取所有容器的 ip 地址\ndocker inspect --format=\u0026#39;{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; $(docker ps -aq) 获取某个容器的挂载信息\ndocker inspect --format=\u0026#39;{{json .Mounts}}\u0026#39; demo-volume-2 | python -m json.tool 1.4 start/stop/restart 启动的容器; 停止容器; 重启容器\n语法: docker start|stop|restart [OPTIONS] CONTAINER [CONTAINER…]\n实例\n# 启动已被停止的容器 myrunoob docker start myrunoob # 停止运行中的容器 myrunoob docker stop myrunoob # 重启容器 myrunoob docker restart myrunoob 1.5 rm 删除一个或多少容器\n语法: docker rm [OPTIONS] CONTAINER [CONTAINER…]\nOPTIONS 说明：\n-f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷 实例\n强制删除容器 db01、db02\ndocker rm -f db01 db02 移除容器 nginx01 对容器 db01 的连接，连接名 db\ndocker rm -l db 删除容器 nginx01,并删除容器挂载的数据卷\ndocker rm -v nginx01 1.6 attach 连接到正在运行中的容器。\n语法: docker attach [OPTIONS] CONTAINER\n要 attach 上去的容器必须正在运行，可以同时连接上同一个 container 来共享屏幕（与 screen 命令的 attach 类似）。\n官方文档中说 attach 后可以通过 CTRL-C 来 detach，但实际上经过我的测试，如果 container 当前在运行 bash，CTRL-C 自然是当前行的输入，没有退出；如果 container 当前正在前台运行进程，如输出 nginx 的 access.log 日志，CTRL-C 不仅会导致退出容器，而且还 stop 了。这不是我们想要的，detach 的意思按理应该是脱离容器终端，但容器依然运行。好在 attach 是可以带上 \u0026ndash;sig-proxy=false 来确保 CTRL-D 或 CTRL-C 不会关闭容器。\n实例\n容器 mynginx 将访问日志指到标准输出，连接到容器查看访问信息。\nrunoob@runoob:~$ docker attach --sig-proxy=false mynginx 192.168.239.1 - - [10/Jul/2016:16:54:26 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; 1.7 logs docker logs : 获取容器的日志\n语法: docker logs [OPTIONS] CONTAINER\nOPTIONS 说明：\n-f : 跟踪日志输出。类似 tail 命令的 -f 选项 --since :显示某个开始时间的所有日志 -t : 显示时间戳 --tail :仅列出最新N条容器日志 实例\n跟踪查看容器 mynginx 的日志输出。\nrunoob@runoob:~$ docker logs -f mynginx 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; 2016/07/10 16:53:33 [error] 5#5: *1 open() \u0026#34;/usr/share/nginx/html/favicon.ico\u0026#34; failed (2: No such file or directory), client: 192.168.239.1, server: localhost, request: \u0026#34;GET /favicon.ico HTTP/1.1\u0026#34;, host: \u0026#34;192.168.239.130\u0026#34;, referrer: \u0026#34;http://192.168.239.130/\u0026#34; 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \u0026#34;GET /favicon.ico HTTP/1.1\u0026#34; 404 571 \u0026#34;http://192.168.239.130/\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; 192.168.239.1 - - [10/Jul/2016:16:53:59 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; ... 查看容器 mynginx 从 2016 年 7 月 1 日后的最新 10 条日志。\ndocker logs --since=\u0026#34;2016-07-01\u0026#34; --tail=10 mynginx 1.8 top 查看容器中运行的进程信息，支持 ps 命令参数。\n语法: docker top [OPTIONS] CONTAINER [ps OPTIONS]\n容器运行时不一定有/bin/bash 终端来交互执行 top 命令，而且容器还不一定有 top 命令，可以使用 docker top 来实现查看 container 中正在运行的进程。\n实例\n查看容器 mymysql 的进程信息。\nrunoob@runoob:~/mysql$ docker top mymysql UID PID PPID C STIME TTY TIME CMD 999 40347 40331 18 00:58 ? 00:00:02 mysqld 查看所有运行容器的进程信息。\nfor i in `docker ps |grep Up|awk \u0026#39;{print $1}\u0026#39;`;do echo \\ \u0026amp;\u0026amp;docker top $i; done 1.9 exec 在运行的容器中执行命令\n语法: docker exec [OPTIONS] CONTAINER COMMAND [ARG…]\nOPTIONS 说明：\n-d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开 -t :分配一个伪终端 实例\n在容器 mynginx 中以交互模式执行容器内 /root/runoob.sh 脚本:\nrunoob@runoob:~$ docker exec -it mynginx /bin/sh /root/runoob.sh http://www.runoob.com/ 在容器 mynginx 中开启一个交互模式的终端:\nrunoob@runoob:~$ docker exec -i -t mynginx /bin/bash root@b1a0703e41e7:/# 也可以通过 docker ps -a 命令查看已经在运行的容器，然后使用容器 ID 进入容器。\n查看已经在运行的容器 ID：\ndocker ps -a ... 9df70f9a0714 openjdk \u0026#34;/usercode/script.sh…\u0026#34; ... 第一列的 9df70f9a0714 就是容器 ID。\n通过 exec 命令对指定的容器执行 bash:\ndocker exec -it 9df70f9a0714 /bin/bash 1.10 kill 杀掉一个运行中的容器。\n语法: docker kill [OPTIONS] CONTAINER [CONTAINER…]\nOPTIONS 说明：\n-s :向容器发送一个信号 实例\n杀掉运行中的容器 mynginx\nrunoob@runoob:~$ docker kill -s KILL mynginx mynginx 2 镜像操作 2.1 images docker images : 列出本地镜像。\n语法: docker images [OPTIONS] [REPOSITORY[:TAG]]\nOPTIONS 说明：\n-a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； --digests :显示镜像的摘要信息； -f :显示满足条件的镜像； --format :指定返回值的模板文件； --no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 实例\n查看本地镜像列表。\nrunoob@runoob:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 5 minutes ago 329 MB runoob/ubuntu v4 1c06aa18edee 2 days ago 142.1 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5c6e1090e771 2 days ago 165.9 MB httpd latest ed38aaffef30 11 days ago 195.1 MB alpine latest 4e38e38c8ce0 2 weeks ago 4.799 MB mongo 3.2 282fd552add6 3 weeks ago 336.1 MB redis latest 4465e4bcad80 3 weeks ago 185.7 MB php 5.6-fpm 025041cd3aa5 3 weeks ago 456.3 MB python 3.5 045767ddf24a 3 weeks ago 684.1 MB ... 列出本地镜像中 REPOSITORY 为 ubuntu 的镜像列表。\nroot@runoob:~# docker images ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 14.04 90d5884b1ee0 9 weeks ago 188 MB ubuntu 15.10 4e3b13c8a266 3 months ago 136.3 MB 2.2 inspect 获取容器/镜像的元数据。\n语法: docker inspect [OPTIONS] NAME|ID [NAME|ID…]\nOPTIONS 说明：\n-f :指定返回值的模板文件。 -s :显示总的文件大小。 --type :为指定类型返回JSON。 --format :以指定格式返回数据 实例\n获取镜像 mysql:5.6 的元信息。\nrunoob@runoob:~$ docker inspect mysql:5.6 [ { \u0026#34;Id\u0026#34;: \u0026#34;sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec\u0026#34;, \u0026#34;RepoTags\u0026#34;: [ \u0026#34;mysql:5.6\u0026#34; ], \u0026#34;RepoDigests\u0026#34;: [], \u0026#34;Parent\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2016-05-24T04:01:41.168371815Z\u0026#34;, \u0026#34;Container\u0026#34;: \u0026#34;e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54\u0026#34;, \u0026#34;ContainerConfig\u0026#34;: { \u0026#34;Hostname\u0026#34;: \u0026#34;b0cf605c7757\u0026#34;, \u0026#34;Domainname\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;AttachStdin\u0026#34;: false, \u0026#34;AttachStdout\u0026#34;: false, \u0026#34;AttachStderr\u0026#34;: false, \u0026#34;ExposedPorts\u0026#34;: { \u0026#34;3306/tcp\u0026#34;: {} }, ... 2.3 rmi 删除本地一个或多少镜像。\n语法: docker rmi [OPTIONS] IMAGE [IMAGE…]\nOPTIONS 说明：\n-f :强制删除； --no-prune :不移除该镜像的过程镜像，默认移除； 注：IMAGE 可以使用 [仓库：标签] 的格式，也可以使用镜像 ID，可以同时删除多个镜像\n1、使用 [仓库：标签] 的格式：删除一个标签。当一个镜像文件有多个标签时，删除完所有的标签，镜像文件也随之删除\n2、使用镜像 ID 的格式：先将该镜像文件的所有标签删除，再删除镜像文件\n删除所有镜像\ndocker rmi $(docker images -q) 删除某个仓库的所有镜像\ndocker rmi $(docker images -q ubuntu) 实例\n强制删除本地镜像 runoob/ubuntu:v4。\nroot@runoob:~# docker rmi -f runoob/ubuntu:v4 Untagged: runoob/ubuntu:v4 Deleted: sha256:1c06aa18edee44230f93a90a7d88139235de12cd4c089d41eed8419b503072be Deleted: sha256:85feb446e89a28d58ee7d80ea5ce367eebb7cec70f0ec18aa4faa874cbd97c73 2.4 search 语法: docker search [OPTIONS] TERM\nOPTIONS 说明：\n--automated :只列出 automated build类型的镜像； --no-trunc :显示完整的镜像描述； -s :列出收藏数不小于指定值的镜像。 实例\n从 Docker Hub 查找所有镜像名包含 java，并且收藏数大于 10 的镜像\nrunoob@runoob:~$ docker search -s 10 java NAME DESCRIPTION STARS OFFICIAL AUTOMATED java Java is a concurrent, class-based... 1037 [OK] anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC ... 115 [OK] develar/java 46 [OK] isuper/java-oracle This repository contains all java... 38 [OK] lwieske/java-8 Oracle Java 8 Container - Full + ... 27 [OK] nimmis/java-centos This is docker images of CentOS 7... 13 [OK] 2.5 pull docker pull [OPTIONS] NAME[:TAG|@DIGEST]\nOPTIONS 说明：\n-a :拉取所有 tagged 镜像 --disable-content-trust :忽略镜像的校验,默认开启 实例\n从 Docker Hub 下载 java 最新版镜像。\ndocker pull java 从 Docker Hub 下载 REPOSITORY 为 java 的所有镜像。\ndocker pull -a java 2.6 push 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库\n语法: docker push [OPTIONS] NAME[:TAG]\nOPTIONS 说明：\n--disable-content-trust :忽略镜像的校验,默认开启 实例\n上传本地镜像 myapache:v1 到镜像仓库中。\ndocker push myapache:v1 2.7 commit 从容器创建一个新的镜像。\n语法: docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\nOPTIONS 说明：\n-a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 实例\n将容器 a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。\nrunoob@runoob:~$ docker commit -a \u0026#34;runoob.com\u0026#34; -m \u0026#34;my apache\u0026#34; a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057 runoob@runoob:~$ docker images mymysql:v1 REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 15 seconds ago 329 MB 2.8 build 用于使用 Dockerfile 创建镜像。\n语法: docker build [OPTIONS] PATH | URL | -\nOPTIONS 说明：\n--build-arg=[] :设置镜像创建时的变量； --cpu-shares :设置 cpu 使用权重； --cpu-period :限制 CPU CFS周期； --cpu-quota :限制 CPU CFS配额； --cpuset-cpus :指定使用的CPU id； --cpuset-mems :指定使用的内存 id； --disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； --force-rm :设置镜像过程中删除中间容器； --isolation :使用容器隔离技术； --label=[] :设置镜像使用的元数据； -m :设置内存最大值； --memory-swap :设置Swap的最大值为内存+swap，\u0026#34;-1\u0026#34;表示不限swap； --no-cache :创建镜像的过程不使用缓存； --pull :尝试去更新镜像的新版本； --quiet, -q :安静模式，成功后只输出镜像 ID； --rm :设置镜像成功后删除中间容器； --shm-size :设置/dev/shm的大小，默认值是64M； --ulimit :Ulimit配置。 --tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 --network: 默认 default。在构建期间设置RUN指令的网络模式 实例\n使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1。\ndocker build -t runoob/ubuntu:v1 . 使用 URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像。\ndocker build github.com/creack/docker-firefox 也可以通过 -f Dockerfile 文件的位置：\n$ docker build -f /path/to/a/Dockerfile . 在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回：\n$ docker build -t test/myapp . Sending build context to Docker daemon 2.048 kB Error response from daemon: Unknown instruction: RUNCMD 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-commands/","summary":"1 容器操作 1.1 run 创建一个新的容器并运行一个命令 语法: docker run [OPTIONS] IMAGE [COMMAND] [ARG…] OPTIONS 说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)","title":"docker | 常见命令"},{"content":"0 什么是数据卷 docker 的理念之一就是将应用与其运行的环境打包。通常 docker 容器的生命周期都是与在容器中运行的程序相一致的，我们对于数据的要求就是持久化；另一方面 docker 容器之间也需要一个共享文件的渠道。\n数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS），为一个或者过个容器提供服务 数据卷设计的目的，在于数据的持久化，他完全独立于容器的生存周期，因此，docker 不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理 从图片中：\n数据卷独立于 docker 容器存在，它存在于 docker 的宿主机中 数据卷可以是目录，也可以是文件 docker 容器可以利用数据卷与宿主机共享文件 同一个数据卷可以支持多个容器的访问 1 数据卷的特点 数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中 数据卷可以在容器之间共享和重用 可以对数据卷里的内容直接进行修改 数据卷的变化不会影响镜像的更新 数据卷会一直存在，即使挂载数据卷的容器已经被删除 2 数据卷操作 2.1 添加数据卷 docker run -it -v HOST_DIRECTORY:CONTAINER_DIRETORY IMAGE [COMMADN] # HOST-DIRECTORY：指定主机目录，不存在时即创建 # CONTAINER：指定容器目录，不存在时即创建 示例:\n[root@localhost ~]# docker run -it -v /docker/data_volume:/data_volume busybox /bin/sh / # touch /data_volume/test\t#创建测试文件 / # echo \u0026#34;lvbibir\u0026#34; \u0026gt; /data_volume/test / # cat /data_volume/test lvbibir [root@localhost ~]# cat /docker/data_volume/test\t#验证测试文件 lvbibir [root@localhost ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES be3fad8d789e busybox \u0026#34;/bin/sh\u0026#34; 6 minutes ago Up 6 minutes elastic_boyd [root@localhost ~]# docker inspect elastic_boyd 2.2 访问权限 docker run -it -v HOST_DIRECTORY:CONTAINER_DIRETORY:r/w IMAGE [COMMADN] 权限可以设置为：\nro：only-read，只读 wo：only-write，只写 rw：write and read，读写 示例：\n[root@localhost ~]# docker run -itd -v /docker/data_volume:/data_volume:ro busybox /bin/sh 3ee3a2b7a97c0a10125d46ee1135bf59af1d97932572d49fdd5c0bb64bf775a5 [root@localhost ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3ee3a2b7a97c busybox \u0026#34;/bin/sh\u0026#34; 4 seconds ago Up 3 seconds confident_hopper [root@localhost ~]# docker inspect confident_hopper 2.3 dockerfile dockerfile 指令：\nVOLUME [\u0026ldquo;HOST_DIRECTORY\u0026rdquo;]\ndockerfile 中配置数据卷无法指定映射到本地的目录 构建好镜像启动容器时，数据卷会进行初始化，docker 会在/var/lib/docker/volumes/下为数据卷创建新的随机名字的目录（不同版本该目录位置可能不同，具体以 inspect 查看到的为准） 使用同一个镜像构建的多个容器，映射的本地目录也不一样 通过数据卷容器来进行容器间的数据共享 示例：\n[root@localhost ~]# cat Dockerfile #For test data_volume FROM busybox:latest VOLUME [\u0026#34;/data_volume1\u0026#34;,\u0026#34;/data_volume2\u0026#34;] CMD /bin/sh [root@localhost ~]# docker build -t test/data_volume . [root@localhost ~]# docker run -itd --name test_data_volume_1 test/data_volume /bin/sh ee8347a4bd3590e8cb65a28e1ebfc5d01e44f2ce70d33a2fa9bbc19782e34f21 [root@localhost ~]# docker exec test_data_volume_1 ls -l / | grep data_volume drwxr-xr-x 2 root root 6 Aug 14 15:20 data_volume1 drwxr-xr-x 2 root root 6 Aug 14 15:20 data_volume2 [root@localhost ~]# docker inspect test_data_volume_1 [root@localhost ~]# docker run -itd --name test_data_volume_2 test/data_volume /bin/sh b4f654706ea15e657cd61bb92d16fa6c6b8eb9129a68b1c9209ea21967175b24 [root@localhost ~]# docker exec test_data_volume_2 ls -l / | grep data_volume drwxr-xr-x 2 root root 6 Aug 14 15:24 data_volume1 drwxr-xr-x 2 root root 6 Aug 14 15:24 data_volume2 [root@localhost ~]# docker inspect test_data_volume_2 3 数据卷容器 一个命名的容器挂载了数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 使用数据卷容器而不是用数据卷直接挂载，可以不暴露宿主机的实际目录 删除数据卷容器对于已经挂载了该容器的容器没有影响，因为数据卷容器只是传递了挂载信息，任何对于目录的更改都不需要通过数据卷容器 从图片中：\n数据卷容器挂载了一个本地目录，其他容器通过连接这个数据卷容器来实现数据的共享 3.1 挂载数据卷容器 docker run -it --volumes-from [CONTAINER] IMAGE [COMMAND] CONTAINER 必须是已经挂载了卷组的容器，dockerfile 和 -v 两个方式都可以 CONTAINER 可以未运行，但必须存在 示例：\n创建数据卷容器\n[root@localhost ~]# cat Dockerfile #For test data_volume FROM busybox:latest VOLUME [\u0026#34;/data_volume1\u0026#34;,\u0026#34;/data_volume2\u0026#34;] CMD /bin/sh [root@localhost ~]# docker build -t test/data_volume . [root@localhost ~]# docker run -it --name data_volume_container test/data_volume /bin/sh / # touch /data_volume1/test1 / # touch /data_volume2/test2 / # exit 创建一个容器，挂载数据卷容器进行验证\n[root@localhost ~]# docker run -itd --name test_dvc_1 --volumes-from data_volume_container busybox /bin/sh 6c4afa29df7ef226da7f1f0d394a356d53b92e3b20fa6c4632e7197ba393612c [root@localhost ~]# docker exec test_dvc_1 ls /data_volume1/ test1 [root@localhost ~]# docker exec test_dvc_1 ls /data_volume2/ test2 使用这个新容器对挂载的目录进行更改\n[root@localhost ~]# docker exec test_dvc touch /data_volume1/test2 [root@localhost ~]# docker exec test_dvc ls /data_volume1/ test1 test2 再创建一个新容器验证上一个容器对挂载目录的更改是否生效\n[root@localhost ~]# docker run -itd --name test_dvc_2 --volumes-from data_volume_container busybox /bin/sh 276c24ecd6ee62f35abf24855ffc5416b9abe987c1bb693ec57bf27d241383d2 [root@localhost ~]# docker exec test_dvc_2 ls /data_volume1 test1 test2 [root@localhost ~]# docker inspect --format=\u0026#34;{{.Mounts}}\u0026#34; test_dvc_1 [{volume 1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be /var/lib/docker/volumes/1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be/_data /data_volume1 local true } {volume d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764 /var/lib/docker/volumes/d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764/_data /data_volume2 local true }] [root@localhost ~]# docker inspect --format=\u0026#34;{{.Mounts}}\u0026#34; test_dvc_2 [{volume d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764 /var/lib/docker/volumes/d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764/_data /data_volume2 local true } {volume 1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be /var/lib/docker/volumes/1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be/_data /data_volume1 local true }] [root@localhost ~]# docker inspect test_dvc_1 [root@localhost ~]# docker inspect test_dvc_2 3.2 删除数据卷容器 删除数据卷容器后，已经挂载了这个数据卷容器的容器不受任何影响\n数据卷容器只传递链接信息，挂载的数据并不需要通过数据卷容器来进行传输\n4 数据卷的备份和还原 4.1 数据备份 备份这个数据卷容器挂载的所有目录\ndocker run --volumes-from [container] -v $(pwd):/backup [image] tar cvf /backup/backup.tar [container data volume] -v $(pwd):/backup：挂载一个数据卷用于存放备份文件 tar 命令：将数据卷容器挂载的目录进行压缩，备份到/backup 目录 4.2 数据还原 docker run --volumes-from [container] -v $(pwd):/backup [image] tar xvf /backup/backup.tar [container data volume] 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-data-volume/","summary":"0 什么是数据卷 docker 的理念之一就是将应用与其运行的环境打包。通常 docker 容器的生命周期都是与在容器中运行的程序相一致的，我们对于数据的要求就是持久化；另一方面 docker 容器之间也需要一个共享文件的渠道。 数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS），为一个或者过个容器提供服务 数据卷设计","title":"docker | 数据卷 volume"},{"content":"1 docker 简介 Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。\nDocker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。\nDocker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容（LXC）等技术。\n在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。\n下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。\n1.1 为什么要使用 docker 作为一种新兴的虚拟化方式，Docker 跟传统的虚拟化方式相比具有众多的优势。\n首先，Docker 容器的启动可以在秒级实现，这相比传统的虚拟机方式要快得多。 其次，Docker 对系统资源的利用率很高，一台主机上可以同时运行数千个 Docker 容器。\n容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而 Docker 只需要启动 10 个隔离的应用即可。\n具体说来，Docker 在如下几个方面具有较大的优势。\n更快速的交付和部署 对开发和运维（devop）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。 开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。 更高效的虚拟化 Docker 容器的运行不需要额外的 hypervisor 支持，它是内核级的虚拟化，因此可以实现更高的性能和效率。 更轻松的迁移和扩展 Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 更简单的管理 使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。 1.2 对比传统虚拟机 1.3 应用场景 简化配置: 一次构建，多处运行 提升开发效率 应用隔离 多租户环境: 为每个容器启用多个不同的容器 快速的部署 代码流水线管理 代码调试 2 docker 镜像 docker 镜像是一套使用联合加载技术实现的层叠的只读文件系统，包含基础镜像和附加镜像层\n2.1 为什么 docker 镜像很小 Linux 操作系统分别由两部分组成\n内核空间 (kernel) 用户空间 (rootfs) 内核空间是 kernel,Linux 刚启动时会加载 bootfs 文件系统，之后 bootf 会被卸载掉，用户空间的文件系统是 rootfs,包含常见的目录，如/dev、/proc、/bin、/etc 等等\n不同的 Linux 发行版本 (红帽，centos，ubuntu 等) 主要的区别是 rootfs, 多个 Linux 发行版本的 kernel 差别不大。\n每个不同 linux 发行版的 docker 镜像只包含对应的 rootfs，所以比完整的系统镜像要小得多\n2.2 docker 镜像的存储位置 /var/lib/docker(可以使用 docker info 来进行查看)\n3 写时复制（copy on write） 当一个新容器启动时，读写层是没有任何数据的，当用户需要读取一些文件时，可以直接从只读层进行读取，只有当用户要修改只读层一些文件时，docker 才会将该文件从只读层复制出来放在读写层供使用者修改，只读层中的文件是没有改变的\n4 repository 与 registry Repository：本身是一个仓库，这个仓库里面可以放具体的镜像，是指具体的某个镜像的仓库，比如 Tomcat 下面有很多个版本的镜像，它们共同组成了 Tomcat 的 Repository。\nRegistry：镜像的仓库，比如官方的是 Docker Hub，它是开源的，也可以自己部署一个，Registry 上有很多的 Repository，Redis、Tomcat、MySQL 等等 Repository 组成了 Registry。\n5 docker 的 C/S 模式 用户在 Docker Client 中运行 Docker 的各种命令，这些命令会传送给在 docker 宿主机上运行的 docker 守护进程，docker 的守护进程来实现 docker 的各种功能\n启动 docker 服务后，docker 的守护进程会一直在后台运行\n5.1 Remote API docker 命令行接口是 docker 最常用的与守护进程进行通信的接口，docker 的二进制命令文件（例如 docker run）此时就是 docker 的 Client，docker 也提供了其他的接口：Remote API\n用户可以通过编写程序调用 Remote API，与 docker 守护进程进行通信，将自己的程序与 docker 进行集成\nRESTful 风格的 API：与大多数程序的 API 风格类似 STDIN、STDOUT、STDERR：Remote API 在某些复杂的情况下，也支持这三种方式来与 docker 守护进程进行通信 5.2 Client 与守护进程的连接方式 unix:///var/run/docker.sock 是默认的连接方式，可以通过配置修改为其他的 socket 连接方式\nunix:///var/run/docker.sock tcp://host:port fd://socketfd 用户可以通过 dokcer 的二进制命令接口或者自定义程序，自定义程序通过 Remote API 来调用 docker 守护进程，Client 与 Server 之间通过 Socket 来进行连接，这种连接意味着 Client 与 Server 既可以在同一台机器上运行，也可以在不同机器上运行，Client 可以通过远程的方式来连接 Server 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-jian-jie/","summary":"1 docker 简介 Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。 Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；G","title":"docker | 简介以及基础概念"},{"content":"1 网络概述 独立容器网络：none host none 网络最为安全，只有 localback 接口 host 网络只和物理机相连，保证跟物理机相连的网络效率\t跟物理机完全一样（网络协议栈与主机名） 容器间的网络：bridge docker bridge详解 docker 启动时默认会有一个 docker0 网桥，该网桥就是桥接模式的体现 用户也可以自建 bridge 网络，建立后 dokcer 也会创建一个网桥 跨主机的容器间的网络：macvlan overlay 第三方网络：flannel weave calic 2 docker0 安装了 docker 的系统，使用 ifconfig 可以查看到 docker0 设备，docker 守护进程就是通过 docker0 为容器提供网络连接的各种服务\ndocker0 实际上是 linux 虚拟网桥（交换机)\n网桥是数据链路层的设备，它通过 mac 地址来对网络进行划分，并且在不同的网络之间传递数据\nlinux 虚拟网桥的特点：\n可以设置 ip 地址（二层的网桥可以设置三层的 ip 地址） 相当于拥有一个隐藏的虚拟网卡 docker0 的地址划分：\nIP：172.17.0.1（各版本可能不同） 子网掩码：255.255.0.0 MAC：02:42:00:00:00:00 到 02:42:ff:ff:ff:ff（各版本可能不同） 总共提供了 65534 个地址 每当一个容器启动时，docker 守护进程会创建网络连接的两端，一端在容器内创建 eth0 网卡，另一端在 dokcer0 网桥中开启一个端口 veth*\n查看网桥设备需要预先安装 bridge-utils 软件包\n[root@localhost ~]# yum install -y bridge-utils [root@localhost ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024247d799bf no virbr0 8000.525400b76fd4 yes virbr0-nic 开启一个容器，查看网络设置：\n[root@localhost ~]# docker run -it --name nwt1 centos /bin/bash [root@0ef32e882bcf /]# ifconfig bash: ifconfig: command not found [root@0ef32e882bcf /]# yum install -y net-tools [root@0ef32e882bcf /]# ifconfig 再查看一下网桥\n[root@localhost ~]# brctl show [root@localhost ~]# ifconfig 2.1 自定义 docker0 当默认 docker0 的 ip 或者网段与主机环境发生冲突时，可以修改 docker0 的地址和网段来进行自定义\n# ifconfig docker0 IP netmask NETMASK [root@localhost ~]# ifconfig docker0 192.168.200.1 netmask 255.255.255.0 [root@localhost ~]# ifconfig [root@localhost ~]# systemctl restart docker [root@localhost ~]# docker run -it centos /bin/bash [root@a5c6ebf79340 /]# yum install -y net-tools [root@a5c6ebf79340 /]# ifconfig 2.2 自定义虚拟网桥 添加虚拟网桥\n[root@localhost ~]# brctl addbr br0 [root@localhost ~]# ifconfig br0 192.168.100.1 netmask 255.255.255.0 [root@localhost ~]# ifconfig 修改 docker 守护进程的配置\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -b=br0 [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 4156 1 1 14:06 ? 00:00:00 /usr/bin/dockerd -b=br0 root 4161 4156 0 14:06 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 4263 1558 0 14:06 pts/0 00:00:00 grep --color=auto docker 启动一个测试容器\n[root@localhost ~]# docker run -it --name nwt3 centos /bin/bash [root@d70269c9557e /]# yum install -y net-tools [root@d70269c9557e /]# ifconfig 3 同一宿主机间容器的连接 允许单台主机内所有容器互联（默认情况） 拒绝容器间连接 允许特定容器间的连接 3.1 允许容器互联 \u0026ndash;icc=true 默认为 true，即允许同一宿主机下所有容器之间网络连通\n[root@localhost ~]# docker run -itd --name test1 busybox /bin/sh 7ec641b21b66b6472f4e92cfaa7f9c0674c4322a5265a05e272ae180b0d4687c [root@localhost ~]# docker exec test1 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) [root@localhost ~]# docker run -itd --name test2 busybox /bin/sh fee0ff3e7f82cd1fa06eea11d850251931dff4dff2f0c7ee3e5a9904532beeb6 [root@localhost ~]# docker exec test2 ping 172.17.0.2 -c 4 PING 172.17.0.2 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.133 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.136 ms 64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.264 ms 64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.163 ms --- 172.17.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.133/0.174/0.264 ms 容器的 ip 是不可靠的连接\n可以使用 \u0026ndash;link 选项来连接两个容器\ndocker run --link=[CONTAINER_NAME]:[ALIAS] [IMAGE] [COMMAND] \u0026ndash;link 后面的 test3 指连接到 test3 容器，nt 是为 test3 创建了一个别名\n新建两个容器进行测试\n[root@localhost ~]# docker run -itd --name test3 busybox /bin/sh 1fd4e373dba17fdf1fa93121e08ea2f1f32d8f4116339c072a72a73574b0926f [root@localhost ~]# docker run -itd --name test4 --link=test3:nt busybox /bin/sh c04b9b759bd4cc9af54000a742df58c8369a7f1bfc8862a8325481f1d61db135 [root@localhost ~]# [root@localhost ~]# docker exec test4 ping nt -c 4 PING nt (172.17.0.4): 56 data bytes 64 bytes from 172.17.0.4: seq=0 ttl=64 time=0.256 ms 64 bytes from 172.17.0.4: seq=1 ttl=64 time=0.196 ms 64 bytes from 172.17.0.4: seq=2 ttl=64 time=0.164 ms 64 bytes from 172.17.0.4: seq=3 ttl=64 time=0.148 ms --- nt ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.148/0.191/0.256 ms \u0026ndash;link 选项对容器做了如下改变：\n修改了 env 环境变量 修改了 hosts 文件 [root@localhost ~]# docker exec test4 env [root@localhost ~]# docker exec test4 cat /etc/hosts 删除之前使用的 test1 与 test2 容器，这两个容器占用的 ip 释放，重启 test3 后，使用最新的 ip 地址\n[root@localhost ~]# docker rm -f test1 test1 [root@localhost ~]# docker rm -f test2 test2 [root@localhost ~]# docker restart test3 test3 [root@localhost ~]# docker exec test3 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) 可以看到随着 test3 的 ip 地址发生改变，test4 容器中的 hosts 文件也随之改变\n[root@localhost ~]# docker exec test4 cat /etc/hosts 3.2 拒绝容器互联 修改守护进程的启动选项：\u0026ndash;icc=false\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --icc=false [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker 新建两个容器进行测试，可以看到无法 ping 通\n[root@localhost ~]# docker run -itd --name test10 busybox /bin/sh 700f026459206531b0fda811a43bc12af2f0815dc695f317a1f52939bfada2a1 [root@localhost ~]# docker run -itd --name test11 busybox /bin/sh 792cc31481739e1b2537597bc54c76737333bf95412dac2209e050f35d276dd4 [root@localhost ~]# docker exec test10 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:06 inet addr:172.17.0.6 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) [root@localhost ~]# docker exec test11 ping 172.16.0.6 ^C 3.3 允许特定容器间的连接 修改守护进程选项：\n\u0026ndash;icc=false \u0026ndash;iptables=true\t# 允许 docker 容器配置添加到 linux 的 iptables 设置中 \u0026ndash;link 只有设置了 \u0026ndash;link 的两个容器间才可以互通\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --icc=false --iptables=true [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker 新建两个容器进行验证\n[root@localhost ~]# docker run -itd --name test21 busybox /bin/sh 77f56db227acaa590f729c12a4852d3131f1729851ea8c613a670effbfa512ad [root@localhost ~]# docker run -itd --name test22 --link=test21:nt busybox /bin/sh f4e346387588198cafcfd1d6a2c330a20375b746d05c08bf06e100f9af294a9e [root@localhost ~]# docker exec test22 ping nt -c 4 PING nt (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.201 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.164 ms 64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.195 ms 64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.188 ms --- nt ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.164/0.187/0.201 ms [root@localhost ~]# docker exec test22 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:14 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1124 (1.0 KiB) TX bytes:476 (476.0 B) [root@localhost ~]# docker exec test21 ping 172.17.0.3 -c 4 PING 172.17.0.3 (172.17.0.3): 56 data bytes 64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.181 ms 64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.168 ms 64 bytes from 172.17.0.3: seq=2 ttl=64 time=0.109 ms 64 bytes from 172.17.0.3: seq=3 ttl=64 time=0.226 ms --- 172.17.0.3 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.109/0.171/0.226 ms 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/docker-network/","summary":"1 网络概述 独立容器网络：none host none 网络最为安全，只有 localback 接口 host 网络只和物理机相连，保证跟物理机相连的网络效率 跟物理机完全一样（网络协议栈与主机名） 容器间的网络：bridge docker bridge详解 docker 启动时默认会有一个 docker0 网桥，该网桥就是桥接模式的体现 用户也可以自建 bridge 网络，建立后 dokcer 也会创建","title":"docker | 网络简介"},{"content":"0 前言 查看硬件信息，并将信息整合成 json 数值，然后传给前段进行分析，最后再进行相应的处理。在装系统的时候，或是进行监控时，都是一个标准的自动化运维流程。使用 shell 直接生成好 json 数据再进行传输，会变得非常方便。\n1 环境 [root@sys-idc-pxe01 ~]# yum install jq lsscsi MegaCli 2 脚本内容 #!/bin/sh #description: get server hardware info #author: lvbibir #date: 20180122 #需要安装jq工具 yum install jq #用于存放该服务器的所有信息，个人喜欢把全局变量写到外面 #写到函数里面，没有加local的变量也是全局变量 INFO=\u0026#34;{}\u0026#34; #定义一个工具函数，用于生成json数值，后面将会频繁用到 function create_json() { #utility function local key=$1 local value=\u0026#34;$2\u0026#34; local json=\u0026#34;\u0026#34; #if value is string if [ -z \u0026#34;$(echo \u0026#34;$value\u0026#34; |egrep \u0026#34;\\[|\\]|\\{|\\}\u0026#34;)\u0026#34; ] then json=$(jq -n {\u0026#34;$key\u0026#34;:\u0026#34;\\\u0026#34;$value\\\u0026#34;\u0026#34;}) #if value is json, object elif [ \u0026#34;$(echo \u0026#34;$value\u0026#34; |jq -r type)\u0026#34; == \u0026#34;object\u0026#34; ] then json=$(jq -n {\u0026#34;$key\u0026#34;:\u0026#34;$value\u0026#34;}) #if value is array elif [ \u0026#34;$(echo \u0026#34;$value\u0026#34; |jq -r type)\u0026#34; == \u0026#34;array\u0026#34; ] then json=$(jq -n \u0026#34;{$key:$value}\u0026#34;) else echo \u0026#34;value type error...\u0026#34; exit 1 return 0 fi echo $json return 0 } #获取CPU信息 function get_cpu() { #获取cpu信息，去掉空格和制表符和空行，以便于for循环 local cpu_model_1=$(dmidecode -s processor-version |grep \u0026#39;@\u0026#39; |tr -d \u0026#34; \u0026#34; |tr -s \u0026#34;\\n\u0026#34; |tr -d \u0026#34;\\t\u0026#34;) local cpu_info=\u0026#34;{}\u0026#34; local i=0 #因为去掉了空格和制表符，以下默认使用换行符分隔 for line in $(echo \u0026#34;$cpu_model_1\u0026#34;) do local cpu_model=\u0026#34;$line\u0026#34; local cpu1=$(create_json \u0026#34;cpu_model\u0026#34; \u0026#34;$cpu_model\u0026#34;) #获取每块cpu的信息，这里只记录了型号 local cpu=$(create_json \u0026#34;cpu_$i\u0026#34; \u0026#34;$cpu1\u0026#34;) local cpu_info=$(jq -n \u0026#34;$cpu_info + $cpu\u0026#34;) i=$[ $i + 1] done #将cpu的信息整合成一个json，key是cpu local info=$(create_json \u0026#34;cpu\u0026#34; \u0026#34;$cpu_info\u0026#34;) #将信息加入到全局变量中 INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_mem() { #generate json {Locator:{sn:sn,size:size}} local mem_info=\u0026#34;{}\u0026#34; #获取每个内存的信息，包括Size:|Locator:|Serial Number: local mem_info_1=$(dmidecode -t memory |egrep \u0026#39;Size:|Locator:|Serial Number:\u0026#39; |grep -v \u0026#39;Bank Locator:\u0026#39; |awk \u0026#39; { if (NR%3==1 \u0026amp;\u0026amp; $NF==\u0026#34;MB\u0026#34;) { size=$2; getline (NR+1); locator=$2; getline (NR+2); sn=$NF; printf(\u0026#34;%s,%s,%s\\n\u0026#34;,locator,size,sn) } }\u0026#39;) #根据上面的信息，将信息过滤并整合成json local i=0 for line in $(echo \u0026#34;$mem_info_1\u0026#34;) do local locator=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39;) local sn=$(echo $line |awk -F , \u0026#39;{print $3}\u0026#39;) local size=$(echo $line |awk -F , \u0026#39;{print $2}\u0026#39;) local mem1=$(create_json \u0026#34;locator\u0026#34; \u0026#34;$locator\u0026#34;) local mem2=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local mem3=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local mem4=$(jq -n \u0026#34;$mem1 + $mem2 + $mem3\u0026#34;) #每条内存的信息，key是内存从0开始的序号 local mem=$(create_json \u0026#34;mem_$i\u0026#34; \u0026#34;$mem4\u0026#34;) #将这些内存的信息组合到一个json中 mem_info=$(jq -n \u0026#34;$mem_info + $mem\u0026#34;) i=$[ $i + 1 ] done #给这些内存的信息设置key，mem local info=$(create_json \u0026#34;mem\u0026#34; \u0026#34;$mem_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_megacli_disk() { #设置megacli工具的路径，此条可以根据情况更改 local raid_tool=\u0026#34;/opt/MegaRAID/MegaCli/MegaCli64\u0026#34; #将硬盘信息获取，保存下来，省去每次都执行的操作 $raid_tool pdlist aall \u0026gt;/tmp/megacli_pdlist.txt local disk_info=\u0026#34;{}\u0026#34; #获取硬盘的必要信息 local disk_info_1=$(cat /tmp/megacli_pdlist.txt |egrep \u0026#39;Enclosure Device ID:|Slot Number:|PD Type:|Raw Size:|Inquiry Data:|Media Type:\u0026#39;|awk \u0026#39; { if(NR%6==1 \u0026amp;\u0026amp; $1$2==\u0026#34;EnclosureDevice\u0026#34;) { E=$NF; getline (NR+1); S=$NF; getline (NR+2); pdtype=$NF; getline (NR+3); size=$3$4; getline (NR+4); sn=$3$4$5$6; getline (NR+5); mediatype=$3$4$5$6; printf(\u0026#34;%s,%s,%s,%s,%s,%s\\n\u0026#34;,E,S,pdtype,size,sn,mediatype) } }\u0026#39;) #将获取到的硬盘信息进行整合，生成json local i=0 for line in $(echo $disk_info_1) do #local key=$(echo $line |awk -F , \u0026#39;{printf(\u0026#34;ES%s_%s\\n\u0026#34;,$1,$2)}\u0026#39;) local E=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39;) local S=$(echo $line |awk -F , \u0026#39;{print $2}\u0026#39;) local pdtype=$(echo $line |awk -F , \u0026#39;{print $3}\u0026#39;) local size=$(echo $line |awk -F , \u0026#39;{print $4}\u0026#39;) local sn=$(echo $line |awk -F , \u0026#39;{print $5}\u0026#39;) local mediatype=$(echo $line |awk -F , \u0026#39;{print $6}\u0026#39;) local disk1=$(create_json \u0026#34;pdtype\u0026#34; \u0026#34;$pdtype\u0026#34;) local disk1_1=$(create_json \u0026#34;enclosure_id\u0026#34; \u0026#34;$E\u0026#34;) local disk1_2=$(create_json \u0026#34;slot_id\u0026#34; \u0026#34;$S\u0026#34;) local disk2=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local disk3=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local disk4=$(create_json \u0026#34;mediatype\u0026#34; \u0026#34;$mediatype\u0026#34;) local disk5=$(jq -n \u0026#34;$disk1 + $disk1_1 + $disk1_2 + $disk2 + $disk3 + $disk4\u0026#34;) local disk=$(create_json \u0026#34;disk_$i\u0026#34; \u0026#34;$disk5\u0026#34;) disk_info=$(jq -n \u0026#34;$disk_info + $disk\u0026#34;) i=$[ $i + 1 ] done #echo $disk_info local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;$disk_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_hba_disk() { #对于hba卡的硬盘，使用smartctl获取硬盘信息 local disk_tool=\u0026#34;smartctl\u0026#34; local disk_info=\u0026#34;{}\u0026#34; #lsscsi 需要使用yum install lsscsi 安装 local disk_info_1=$(lsscsi -g |grep -v \u0026#39;enclosu\u0026#39; |awk \u0026#39;{printf(\u0026#34;%s,%s,%s,%s\\n\u0026#34;,$1,$2,$(NF-1),$NF)}\u0026#39;) local i=0 for line in $(echo $disk_info_1) do local E=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39; |awk -F \u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) local S=$(echo $line |awk -F , \u0026#39;{print $NF}\u0026#39; |egrep -o [0-9]*) local sd=$(echo $line |awk -F , \u0026#39;{print $(NF-1)}\u0026#39;) $disk_tool -i $sd \u0026gt;/tmp/disk_info.txt local pdtype=\u0026#34;SATA\u0026#34; if [ \u0026#34;$(cat /tmp/disk_info.txt |grep \u0026#39;Transport protocol:\u0026#39; |awk \u0026#39;{print $NF}\u0026#39;)\u0026#34; == \u0026#34;SAS\u0026#34; ] then local pdtype=\u0026#34;SAS\u0026#34; fi local size=$(cat /tmp/disk_info.txt |grep \u0026#39;User Capacity:\u0026#39; |awk \u0026#39;{printf(\u0026#34;%s%s\\n\u0026#34;,$(NF-1),$NF)}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) local sn=$(cat /tmp/disk_info.txt |grep \u0026#39;Serial Number:\u0026#39; |awk \u0026#39;{print $NF}\u0026#39;) local mediatype=\u0026#34;disk\u0026#34; local disk1=$(create_json \u0026#34;pdtype\u0026#34; \u0026#34;$pdtype\u0026#34;) local disk1_1=$(create_json \u0026#34;enclosure_id\u0026#34; \u0026#34;$E\u0026#34;) local disk1_2=$(create_json \u0026#34;slot_id\u0026#34; \u0026#34;$S\u0026#34;) local disk2=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local disk3=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local disk4=$(create_json \u0026#34;mediatype\u0026#34; \u0026#34;$mediatype\u0026#34;) local disk5=$(jq -n \u0026#34;$disk1 + $disk1_1 + $disk1_2 + $disk2 + $disk3 + $disk4\u0026#34;) local disk=$(create_json \u0026#34;disk_$i\u0026#34; \u0026#34;$disk5\u0026#34;) disk_info=$(jq -n \u0026#34;$disk_info + $disk\u0026#34;) i=$[ $i + 1 ] done #echo $disk_info local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;$disk_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_disk() { #根据获取到的硬盘控制器类型，来判断使用什么工具采集硬盘信息 if [ \u0026#34;$(echo \u0026#34;$INFO\u0026#34; |jq -r .disk_ctrl.disk_ctrl_0.type)\u0026#34; == \u0026#34;raid\u0026#34; ] then get_megacli_disk elif [ \u0026#34;$(echo \u0026#34;$INFO\u0026#34; |jq -r .disk_ctrl.disk_ctrl_0.type)\u0026#34; == \u0026#34;hba\u0026#34; ] then get_hba_disk else local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;error\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) fi #hp机器比较特殊，这里我没有做hp机器硬盘信息采集，有兴趣的朋友可以自行添加上 #if hp machine return 0 } function get_diskController() { local disk_ctrl=\u0026#34;{}\u0026#34; #if LSI Controller local disk_ctrl_1=\u0026#34;$(lspci -nn |grep LSI)\u0026#34; local i=0 #以换行符分隔 IFS_OLD=$IFS \u0026amp;\u0026amp; IFS=$\u0026#39;\\n\u0026#39; for line in $(echo \u0026#34;$disk_ctrl_1\u0026#34;) do #echo $line local ctrl_id=$(echo \u0026#34;$line\u0026#34; |awk -F \u0026#39;]:\u0026#39; \u0026#39;{print $1}\u0026#39; |awk \u0026#39;{print $NF}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) case \u0026#34;$ctrl_id\u0026#34; in #根据控制器的id或进行判断是raid卡还是hba卡，因为品牌比较多，后续可以在此处进行扩展添加 0104) # 获取Logic以后的字符串，并进行拼接 local ctrl_name=$(echo \u0026#34;${line##*\u0026#34;Logic\u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s\\n\u0026#34;,$1,$2,$3)}\u0026#39;) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$ctrl_id\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;raid\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;$ctrl_name\u0026#34;) ;; 0100|0107) local ctrl_name=$(echo \u0026#34;${line##*\u0026#34;Logic\u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s\\n\u0026#34;,$1,$3,$4)}\u0026#39;) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$ctrl_id\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;hba\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;$ctrl_name\u0026#34;) ;; *) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;----\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;----\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;----\u0026#34;) ;; esac local ctrl_tmp=$(jq -n \u0026#34;$ctrl1 + $ctrl2 + $ctrl3\u0026#34;) local ctrl=$(create_json \u0026#34;disk_ctrl_$i\u0026#34; \u0026#34;$ctrl_tmp\u0026#34;) disk_ctrl=$(jq -n \u0026#34;$disk_ctrl + $ctrl\u0026#34;) i=$[ $i + 1 ] done IFS=$IFS_OLD local info=$(create_json \u0026#34;disk_ctrl\u0026#34; \u0026#34;$disk_ctrl\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_netcard() { local netcard_info=\u0026#34;{}\u0026#34; local netcard_info_1=\u0026#34;$(lspci -nn |grep Ether)\u0026#34; local i=0 #echo \u0026#34;$netcard_info_1\u0026#34; IFS_OLD=$IFS \u0026amp;\u0026amp; IFS=$\u0026#39;\\n\u0026#39; for line in $(echo \u0026#34;$netcard_info_1\u0026#34;) do local net_id=$(echo $line |egrep -o \u0026#39;[0-9a-z]{4}:[0-9a-z]{4}\u0026#39;) local net_id_1=$(echo $net_id |awk -F : \u0026#39;{print $1}\u0026#39;) case \u0026#34;$net_id_1\u0026#34; in 8086) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; 14e4) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; *) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; esac local net1=$(jq -n \u0026#34;$net1 + $net2 + $net3\u0026#34;) #echo $net local net2=$(create_json \u0026#34;net_$i\u0026#34; \u0026#34;$net1\u0026#34;) netcard_info=$(jq -n \u0026#34;$netcard_info + $net2\u0026#34;) i=$[ $i + 1 ] done IFS=$IFS_OLD local info=$(create_json \u0026#34;net\u0026#34; \u0026#34;$netcard_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_server() { local product=$(dmidecode -s system-product-name |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) local manufacturer=$(dmidecode -s system-manufacturer |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) local server1=$(create_json \u0026#34;manufacturer\u0026#34; \u0026#34;$manufacturer\u0026#34;) local server2=$(create_json \u0026#34;product\u0026#34; \u0026#34;$product\u0026#34;) local server3=$(jq -n \u0026#34;$server1 + $server2\u0026#34;) local info=$(create_json \u0026#34;basic_info\u0026#34; \u0026#34;$server3\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } ALL_INFO=\u0026#34;\u0026#34; function get_all() { #因为硬盘信息的获取依赖硬盘控制器的信息，所以get_diskController要放到get_disk前面 get_server get_cpu get_mem get_diskController get_disk get_netcard local sn=$(dmidecode -s system-serial-number |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) ALL_INFO=$(create_json \u0026#34;$sn\u0026#34; \u0026#34;$INFO\u0026#34;) return 0 } function main() { get_all echo $ALL_INFO return 0 } #------------------------------------------------- main 以上\n","permalink":"https://www.lvbibir.cn/en/posts/tech/shell-get-server-hardware-information/","summary":"0 前言 查看硬件信息，并将信息整合成 json 数值，然后传给前段进行分析，最后再进行相应的处理。在装系统的时候，或是进行监控时，都是一个标准的自动化运维流程。使用 shell 直接生成好 json 数据再进行传输，会变得非常方便。 1 环境 [root@sys-idc-pxe01 ~]# yum install jq lsscsi MegaCli 2 脚本内容 #!/bin/sh #description: get server hardware info #author: lvbibir #date: 20180122 #需要安装jq工具 yum install jq #用于存","title":"shell | 获取服务器硬件信息整合为 json 格式 "}]