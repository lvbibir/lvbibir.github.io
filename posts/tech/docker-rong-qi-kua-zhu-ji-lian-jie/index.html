<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>docker | 容器的跨主机连接 | lvbibir's Blog</title><meta name=keywords content="linux,docker,network"><meta name=description content="介绍docker容器在不同宿主机下实现通信的几种方案"><meta name=author content="
作者:&nbsp;lvbibir"><link rel=canonical href=https://www.lvbibir.cn/posts/tech/docker-rong-qi-kua-zhu-ji-lian-jie/><link crossorigin=anonymous href=/assets/css/stylesheet.27ec2ab56d27168f4436b50fcf541dd26c35482c551d49559610d6fcbac73a35.css integrity="sha256-J+wqtW0nFo9ENrUPz1Qd0mw1SCxVHUlVlhDW/LrHOjU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.lvbibir.cn/img/avatar.gif><link rel=icon type=image/png sizes=16x16 href=https://www.lvbibir.cn/img/avatar.gif><link rel=icon type=image/png sizes=32x32 href=https://www.lvbibir.cn/img/avatar.gif><link rel=apple-touch-icon href=https://www.lvbibir.cn/img/avatar.gif><link rel=mask-icon href=https://www.lvbibir.cn/img/avatar.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="docker | 容器的跨主机连接"><meta property="og:description" content="介绍docker容器在不同宿主机下实现通信的几种方案"><meta property="og:type" content="article"><meta property="og:url" content="https://www.lvbibir.cn/posts/tech/docker-rong-qi-kua-zhu-ji-lian-jie/"><meta property="og:image" content="https://image.lvbibir.cn/blog/docker.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-08-01T00:00:00+00:00"><meta property="article:modified_time" content="2019-08-01T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://image.lvbibir.cn/blog/docker.png"><meta name=twitter:title content="docker | 容器的跨主机连接"><meta name=twitter:description content="介绍docker容器在不同宿主机下实现通信的几种方案"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"📚 文章","item":"https://www.lvbibir.cn/posts/"},{"@type":"ListItem","position":3,"name":"👨🏻‍💻 技术","item":"https://www.lvbibir.cn/posts/tech/"},{"@type":"ListItem","position":4,"name":"docker | 容器的跨主机连接","item":"https://www.lvbibir.cn/posts/tech/docker-rong-qi-kua-zhu-ji-lian-jie/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"docker | 容器的跨主机连接","name":"docker | 容器的跨主机连接","description":"介绍docker容器在不同宿主机下实现通信的几种方案","keywords":["linux","docker","network"],"articleBody":"实现跨主机的docker容器之间的通讯：\n使用网桥实现跨主机的连接 docker原生的网络：overlay、macvlan 第三方网络：flaanel、weave、calic 网桥 open vswitch weave macvlan macvlan是Linux操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以为一张物理网卡设置多个mac地址，相当于物理网卡施展了影分身之术，由一个变多个，同时要求物理网卡打开混杂模式。针对每个mac地址，都可以设置IP地址，本来是一块物理网卡连接到交换机，现在是多块虚拟网卡连接到交换机。\n当容器需要直连入物理网络时，可以使用Macvlan。Macvlan本身不创建网络，本质上首先使宿主机物理网卡工作在‘混杂模式’，这样物理网卡的MAC地址将会失效，所有二层网络中的流量物理网卡都能收到。接下来就是在这张物理网卡上创建虚拟网卡，并为虚拟网卡指定MAC地址，实现一卡多用，在物理网络看来，每张虚拟网卡都是一个单独的接口。使用Macvlan有几点需要注意：\n容器直接连接物理网络，由物理网络负责分配IP地址，可能的结果是物理网络IP地址被耗尽，另一个后果是网络性能问题，物理网络中接入的主机变多，广播包占比快速升高而引起的网络性能下降问题。 前边说过了，宿主机上的某张网上需要工作在‘混乱模式’下。 从长远来看bridge网络与overlay网络是更好的选择，原因就是虚拟网络应该与物理网络隔离而不是共享。 优缺点：\n优点是性能非常好 缺点是地址需要手动分配 Macvlan网络有两种模式：bridge模式与802.1q trunk bridge模式。\nbridge模式，Macvlan网络流量直接使用宿主机物理网卡。 802.1q trunk bridge模式，Macvlan网络流量使用Docker动态创建的802.1q子接口，对于路由与过虑，这种模式能够提供更细粒度的控制 环境准备：\n两台centos7 docker版本：18.03 ip：192.168.0.53（node-1） 192.168.0.54（node-2） node-1 node-2 注意：node-1使用的物理网卡是ens33，node-2使用的是ens32 [root@node-1 ~]# ip link show ens33 [root@node-1 ~]# ip link set ens32 promisc on #开启混杂模式，保证多个ip可以通过 [root@node-1 ~]# docker network create -d macvlan --subnet 10.0.0.0/24 --gateway=10.0.0.1 -o parent=ens33 mac_net1 [root@node-1 ~]# docker network ls node-1 docker run -itd --name bbox-1 --ip 10.0.0.11 --network mac_net1 busybox node-2 docker run -itd --name bbox-2 --ip 10.0.0.12 --network mac_net1 busybox node-1 [root@node-1 ~]# docker exec bbox-1 ping 10.0.0.12 [root@node-1 ~]# docker exec bbox-1 ping bbox-2 可以ping通ip，但是无法ping通主机名，因为它没有dns解析 [root@node-1 ~]# brctl show 因为macvlan不依赖于bridge网络，所以查看不到新的桥接网络 [root@node-1 ~]# docker exec bbox-1 ip link 查看到eth0连接到了if2 [root@node-1 ~]# ip link show ens33 可以查看到ens33的编号是2，即bbox-1容器的eth0网卡连接到了ens33物理网卡 [root@node-1 ~]# docker network create -d macvlan -o parent=ens33 mac_net2 Error response from daemon: network dm-b34ee1020a96 is already using parent interface ens33 再创建macvlan网络时发现已经无法再创建，即一块网卡只能添加一个macvlan的地址\n一块网卡绑定多个macvlan地址 [root@node-1 ~]# modinfo 8021q # 查看内核是否支持802.1q封装 [root@node-1 ~]# modprobe 8021q # 如果上条命令执行后没有结果，使用该命令加载该模块 node-1 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=manual 修改为不需要ip的manual模式\nnode-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32 BOOTPROTO=manual node-1 添加两块虚拟网卡，注意与实际的ens32网卡的网段区分开 ens32使用的是192.168.0.0/24网段，虚拟网卡使用的是192.168.1.0/24和192.168.2.0/24\n[root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens33.10 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.10 BOOTPROTO=none NAME=ens33.10 DEVICE=ens33.10 ONBOOT=yes IPADDR=192.168.1.10 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33.10 /etc/sysconfig/network-scripts/ifcfg-ens33.20 [root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.20 BOOTPROTO=none NAME=ens33.20 DEVICE=ens33.20 ONBOOT=yes IPADDR=192.168.2.10 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-1 ~]# ifup ens33.10 [root@node-1 ~]# ifup ens33.20 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.10 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.10 [root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.20 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.20 node-2 [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.10 BOOTPROTO=none NAME=ens32.10 DEVICE=ens32.10 ONBOOT=yes IPADDR=192.168.1.20 PREFIX=24 NETWORK=192.168.1.0 VLAN=yes [root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.20 BOOTPROTO=none NAME=ens32.20 DEVICE=ens32.20 ONBOOT=yes IPADDR=192.168.2.20 PREFIX=24 NETWORK=192.168.2.0 VLAN=yes [root@node-2 ~]# ifup ens32.10 [root@node-2 ~]# ifup ens32.20 node-1 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens33.10 mac_net11 [root@node-1 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens33.20 mac_net12 node-2 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens32.10 mac_net11 [root@node-2 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens32.20 mac_net12 node-1 [root@node-2 ~]# docker run -itd --name bbox-11 --ip=172.16.11.11 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-12 --ip=172.16.12.11 --network mac_net12 busybox node-2 [root@node-2 ~]# docker run -itd --name bbox-21 --ip=172.16.11.12 --network mac_net11 busybox [root@node-2 ~]# docker run -itd --name bbox-22 --ip=172.16.12.12 --network mac_net12 busybox node-1 [root@node-1 ~]# docker exec bbox-11 ping 172.16.11.12 PING 172.16.11.12 (172.16.11.12): 56 data bytes 64 bytes from 172.16.11.12: seq=0 ttl=64 time=0.867 ms 64 bytes from 172.16.11.12: seq=1 ttl=64 time=1.074 ms 64 bytes from 172.16.11.12: seq=2 ttl=64 time=1.145 ms 64 bytes from 172.16.11.12: seq=3 ttl=64 time=0.938 ms ^C [root@node-1 ~]# docker exec bbox-12 ping 172.16.12.12 PING 172.16.12.12 (172.16.12.12): 56 data bytes 64 bytes from 172.16.12.12: seq=0 ttl=64 time=0.858 ms 64 bytes from 172.16.12.12: seq=1 ttl=64 time=1.140 ms 64 bytes from 172.16.12.12: seq=2 ttl=64 time=0.818 ms 64 bytes from 172.16.12.12: seq=3 ttl=64 time=1.056 ms ^C 在两台系统进行修改，添加网关，修改防火墙策略\nnode-1中记得将ens32更换为ens33\nifconfig ens32.10 172.16.10.1 netmask 255.255.255.0 ifconfig ens32.20 172.16.20.1 netmask 255.255.255.0 iptables -t nat -A POSTROUTING -o ens32.10 -j MASQUERADE iptables -t nat -A POSTROUTING -o ens32 -j MASQUERADE iptables -A FORWARD -i ens32.10 -o ens32 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -m state --state RELATE,ESTABLISHED -j ACCEPT iptables -A FORWARD -i ens32.10 -o ens32 -j ACCEPT iptables -A FORWARD -i ens32 -o ens32.10 -j ACCEPT overlay 一、跨主机网络概述 二、准备overlay环境 为支持容器的跨主机通信，Docker提供了overlay driver。Docker overlay网络需要一个key-value数据库用于保存网络状态信息，包括Network、Endpoint、IP等。Consul、Etcd和ZooKeeper都是Docker支持的key-value软件，这里我们使用Consul\n1. 环境描述\n节点 系统版本 docker版本 角色 IP地址 node-1 centos7.4 docker-18.03.0 consul 192.168.0.101 node-2 centos7.4 docker-18.03.0 host 192.168.0.102 node-3 centos7.4 docker-18.03.0 host 192.168.0.103 2. 创建consul\nnode-1; [root@node-1 ~]# docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 容器启动后可以通过192.168.0.101:8500访问到consul 3. 修改docker配置文件 修改node-2和node-3的docker daemon的配置文件/etc/systemd/system/docker.service\n[root@node-2 ~]# vim /etc/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.0.101:8500 --cluster-advertise=ens32:2376 [root@node-2 ~]# systemctl daemon-reload [root@node-2 ~]# systemctl restart docker -H ：tcp：允许tcp连接daemon -H：unix：默认的socket连接方式，支持远程的同时，本地也可以连接 –cluster-store 指定consul的地址 –cluster-advertise 告知consul自己的连接地址 node-2和node-3会自动注册到consul数据库中。 三、创建overlay网络 1、在node-2中创建网络 在node-2中创建overlay网络ov_net1\n[root@node-2 ~]# docker network create -d overlay ov_net1 -d overlay：指定driver为overlay [root@node-2 ~]# docker network ls 2、node-3查看创建的网络 注意到ov_net1的 SCOPE 为 global，而其他网络为 local 。在node-3上查看存在的网络:\n[root@node-3 ~]# docker network ls node-3上也能看到ov_net1，只是因为创建ov_net1时将overlay网络信息存入了consul，node-3从consul读取到了新网络数据。之后ov_net1的任何变化都会同步到node-2和node-3. 3、查看ov_net1详细信息\n[root@node-2 ~]# docker network inspect ov_net1 IPAM 是指 IP Address Management，docker自动为 ov_net1 分配的 IP 空间为 10.0.0.0/24 四、在overlay中运行容器 1、创建容器 bbox-1 在 node-2 上运行一个 busybox 容器并连接到 ov_net1.\n[root@node-2 ~]# docker run -itd --name bbox-1 --network ov_net1 busybox 2、查看 bbox-1 网络配置\n[root@node-2 ~]# docker exec bbox-1 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 bbox-1 有两个网络接口，eth0 和 eth1 eth0 IP 为 10.0.0.2，连接的是overlay网络 ov_net1 eth1 IP 为 172.18.0.2 容器的默认路由是走 eth1，其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力 [root@node-2 ~]# docker network ls [root@node-2 ~]# docker network inspect docker_gwbridge 从 docker network inspect docker_gwbridge 输出可确认 docker_gwbridge 的 IP 地址范围是 172.18.0.0/16，当前连接的容器就是 bbox-1（172.18.0.2） 而且此网络的网关就是网桥 docker_gwbridge 的 IP 172.18.0.1\n[root@node-2 ~]# ifconfig docker_gwbridge docker_gwbridge: flags=4163 mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fe80::42:c5ff:fe45:937 prefixlen 64 scopeid 0x20 ether 02:42:c5:45:09:37 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 这样容器 bbox-1 就可以通过docker_gwbridge 访问外网\n[root@node-2 ~]# docker exec bbox-1 ping -c 4 www.baidu.com PING www.baidu.com (182.61.200.6): 56 data bytes 64 bytes from 182.61.200.6: seq=0 ttl=53 time=6.721 ms 64 bytes from 182.61.200.6: seq=1 ttl=53 time=7.954 ms 64 bytes from 182.61.200.6: seq=2 ttl=53 time=11.723 ms 64 bytes from 182.61.200.6: seq=3 ttl=53 time=15.105 ms --- www.baidu.com ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 6.721/10.375/15.105 ms 五、overlay网络连通性 1、node-3 中 运行 bbox-2\n[root@node-3 ~]# docker run -itd --name bbox-2 --network ov_net1 busybox 2、查看 bbox-2 路由情况\n[root@node-3 ~]# docker exec bbox-2 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth0 scope link src 10.0.0.3 172.18.0.0/16 dev eth1 scope link src 172.18.0.2 3、互通测试\n[root@node-3 ~]# docker exec bbox-2 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=2.628 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=1.004 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=1.277 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=1.505 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 1.004/1.603/2.628 ms 可见 overlay 网络中的容器可以直接通信，同时docker也实现了DNS服务 4、实现原理 docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0， veth pair 一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。 br0 除了连接所有的 veth pair，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示： [root@node-2 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.024217edc413 no docker_gwbridge 8000.0242c5450937 no vethc59120e virbr0 8000.525400b76fd4 yes virbr0-nic [root@node-3 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242ef3c7df7 no docker_gwbridge 8000.0242c81afaee no vethf4562a9 virbr0 8000.525400c28478 yes virbr0-nic 要查看 overlay 网络的 namespace 可以在 node-2 和 node-3 上执行 ip netns（请确保在此之前执行过 ln -s /var/run/docker/netns /var/run/netns），可以看到两个 node 上有一个相同的 namespace “1-dd91de7599”\n[root@node-2 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-2 ~]# ip netns 6889f61efc4b (id: 1) 1-dd91de7599 (id: 0) [root@node-3 ~]# ln -s /var/run/docker/netns /var/run/netns [root@node-3 ~]# ip netns 8e4722847745 (id: 1) 1-dd91de7599 (id: 0) “1-dd91de7599” 这就是 ov_net1 的 namespace，查看 namespace 中的 br0 上的设备\n[root@node-2 ~]# ip netns exec 1-dd91de7599 brctl show bridge name bridge id STP enabled interfaces br0 8000.0e7576c7c035 no veth0 vxlan0 六、overlay网络隔离 不同的 overlay 网络是相互隔离的。我们创建第二个 overlay 网络 ov_net2 并运行容器 bbox-3 1、创建网络 ov_net2\n[root@node-2 ~]# docker network create -d overlay ov_net2 2、启动容器 bbox-3\n[root@node-2 ~]# docker run -itd --name bbox-3 --network ov_net2 busybox 3、查看 bbox-3 网络 bbox-3 分配到的 IP 是 10.0.1.2，尝试 ping bbox-1（10.0.0.2）\n[root@node-2 ~]# docker exec -it bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes --- 10.0.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss [root@node-2 ~]# docker exec -it bbox-3 ping -c 4 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes --- 172.18.0.2 ping statistics --- 4 packets transmitted, 0 packets received, 100% packet loss ping 失败，可见不同 overlay 网络之间是隔离的，即使通过 docker_gwbridge 也不能通信 如果要实现 bbox-3 和 bbox-1 通信，可以将 bbox-3 也连接到 ov_net1 这时 bbox-3 同时连接到了 ov_net1 和 ov_net2 上\n[root@node-2 ~]# docker network connect ov_net1 bbox-3 [root@node-2 ~]# docker exec bbox-3 ip r default via 172.18.0.1 dev eth1 10.0.0.0/24 dev eth2 scope link src 10.0.0.4 10.0.1.0/24 dev eth0 scope link src 10.0.1.2 172.18.0.0/16 dev eth1 scope link src 172.18.0.3 [root@node-2 ~]# docker exec bbox-3 ping -c 4 10.0.0.2 PING 10.0.0.2 (10.0.0.2): 56 data bytes 64 bytes from 10.0.0.2: seq=0 ttl=64 time=0.184 ms 64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.158 ms 64 bytes from 10.0.0.2: seq=2 ttl=64 time=0.162 ms 64 bytes from 10.0.0.2: seq=3 ttl=64 time=0.093 ms --- 10.0.0.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.093/0.149/0.184 ms docker 默认为 overlay 网络分配 24 位掩码的子网（10.0.X.0/24），所有主机共享这个 subnet，容器启动时会顺序从此空间分配 IP。当然我们也可以通过 –subnet 指定 IP 空间。\ndocker network create -d overlay --subnet 10.22.1.0/24 ov_net ","wordCount":"3979","inLanguage":"en","image":"https://image.lvbibir.cn/blog/docker.png","datePublished":"2019-08-01T00:00:00Z","dateModified":"2019-08-01T00:00:00Z","author":{"@type":"Person","name":"lvbibir"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.lvbibir.cn/posts/tech/docker-rong-qi-kua-zhu-ji-lian-jie/"},"publisher":{"@type":"Organization","name":"lvbibir's Blog","logo":{"@type":"ImageObject","url":"https://www.lvbibir.cn/img/avatar.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.lvbibir.cn accesskey=h title="lvbibir's Blog (Alt + H)"><img src=https://www.lvbibir.cn/img/avatar.gif alt aria-label=logo height=35>lvbibir's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li class=menu-item><a href=https://www.lvbibir.cn/search title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li class=menu-item><a href=https://www.lvbibir.cn/ title="🏡 主页"><span>🏡 主页</span></a></li><li class=menu-item><a href=https://www.lvbibir.cn/posts title="📚 文章"><span>📚 文章</span></a></li><li class=menu-item><a href=https://www.lvbibir.cn/tags title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li class=menu-item><a href=https://www.lvbibir.cn/archives title="📈 归档"><span>📈 归档</span></a></li><li class=menu-item><a href=https://www.lvbibir.cn/links title="🤝 友链"><span>🤝 友链</span></a></li><li class=menu-item><a href=https://www.lvbibir.cn/about title="🙋🏻‍♂️ 关于"><span>🙋🏻‍♂️ 关于</span></a></li></ul></nav></header><main class="main page"><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://www.lvbibir.cn>主页</a>&nbsp;»&nbsp;<a href=https://www.lvbibir.cn/posts/>📚 文章</a>&nbsp;»&nbsp;<a href=https://www.lvbibir.cn/posts/tech/>👨🏻‍💻 技术</a></div><h1 class=post-title>docker | 容器的跨主机连接</h1><div class=post-description>介绍docker容器在不同宿主机下实现通信的几种方案</div><div class=post-meta>创建:&nbsp;<span title='2019-08-01 00:00:00 +0000 UTC'>2019-08-01</span>&nbsp;|&nbsp;更新:&nbsp;2019-08-01&nbsp;|&nbsp;字数:&nbsp;3979字&nbsp;|&nbsp;时长:&nbsp;8分钟&nbsp;|&nbsp;
作者:&nbsp;lvbibir
&nbsp;|&nbsp;标签: &nbsp;<ul class=post-tags-meta><a href=https://www.lvbibir.cn/tags/docker/>docker</a></ul><span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv></span></span></div></header><figure class=entry-cover1><img loading=lazy src=https://image.lvbibir.cn/blog/docker.png alt></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>文章目录</span></summary><div class=inner><ul><li><a href=#%e7%bd%91%e6%a1%a5 aria-label=网桥>网桥</a></li><li><a href=#open-vswitch aria-label="open vswitch">open vswitch</a></li><li><a href=#weave aria-label=weave>weave</a></li><li><a href=#macvlan aria-label=macvlan>macvlan</a><ul><li><a href=#%e4%b8%80%e5%9d%97%e7%bd%91%e5%8d%a1%e7%bb%91%e5%ae%9a%e5%a4%9a%e4%b8%aamacvlan%e5%9c%b0%e5%9d%80 aria-label=一块网卡绑定多个macvlan地址>一块网卡绑定多个macvlan地址</a></li></ul></li><li><a href=#overlay aria-label=overlay>overlay</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>实现跨主机的docker容器之间的通讯：</p><ol><li>使用网桥实现跨主机的连接</li><li>docker原生的网络：overlay、macvlan</li><li>第三方网络：flaanel、weave、calic</li></ol><h1 id=网桥>网桥<a hidden class=anchor aria-hidden=true href=#网桥>#</a></h1><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190815125207352.png alt=在这里插入图片描述></p><h1 id=open-vswitch>open vswitch<a hidden class=anchor aria-hidden=true href=#open-vswitch>#</a></h1><h1 id=weave>weave<a hidden class=anchor aria-hidden=true href=#weave>#</a></h1><h1 id=macvlan>macvlan<a hidden class=anchor aria-hidden=true href=#macvlan>#</a></h1><p>macvlan是Linux操作系统内核提供的网络虚拟化方案之一，更准确的说法是网卡虚拟化方案。它可以为一张物理网卡设置多个mac地址，相当于物理网卡施展了影分身之术，由一个变多个，同时要求物理网卡打开混杂模式。针对每个mac地址，都可以设置IP地址，本来是一块物理网卡连接到交换机，现在是多块虚拟网卡连接到交换机。</p><p>当容器需要直连入物理网络时，可以使用Macvlan。Macvlan本身不创建网络，本质上首先使宿主机物理网卡工作在‘混杂模式’，这样物理网卡的MAC地址将会失效，所有二层网络中的流量物理网卡都能收到。接下来就是在这张物理网卡上创建虚拟网卡，并为虚拟网卡指定MAC地址，实现一卡多用，在物理网络看来，每张虚拟网卡都是一个单独的接口。使用Macvlan有几点需要注意：</p><ul><li>容器直接连接物理网络，由物理网络负责分配IP地址，可能的结果是物理网络IP地址被耗尽，另一个后果是网络性能问题，物理网络中接入的主机变多，广播包占比快速升高而引起的网络性能下降问题。</li><li>前边说过了，宿主机上的某张网上需要工作在‘混乱模式’下。</li><li>从长远来看bridge网络与overlay网络是更好的选择，原因就是虚拟网络应该与物理网络隔离而不是共享。</li></ul><p>优缺点：</p><ul><li>优点是性能非常好</li><li>缺点是地址需要手动分配</li></ul><p>Macvlan网络有两种模式：bridge模式与802.1q trunk bridge模式。</p><ul><li>bridge模式，Macvlan网络流量直接使用宿主机物理网卡。</li><li>802.1q trunk bridge模式，Macvlan网络流量使用Docker动态创建的802.1q子接口，对于路由与过虑，这种模式能够提供更细粒度的控制</li></ul><hr><p>环境准备：</p><ol><li>两台centos7</li><li>docker版本：18.03</li><li>ip：192.168.0.53（node-1） 192.168.0.54（node-2）</li></ol><ul><li>node-1 node-2</li><li>注意：node-1使用的物理网卡是ens33，node-2使用的是ens32</li></ul><pre tabindex=0><code>[root@node-1 ~]# ip link show ens33
[root@node-1 ~]# ip link set ens32 promisc on
#开启混杂模式，保证多个ip可以通过
[root@node-1 ~]# docker network create -d macvlan --subnet 10.0.0.0/24 --gateway=10.0.0.1 -o parent=ens33 mac_net1
[root@node-1 ~]# docker network ls
</code></pre><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190818182057577.png alt=在这里插入图片描述></p><ul><li>node-1</li></ul><pre tabindex=0><code>docker run -itd --name bbox-1 --ip 10.0.0.11 --network mac_net1 busybox
</code></pre><ul><li>node-2</li></ul><pre tabindex=0><code>docker run -itd --name bbox-2 --ip 10.0.0.12 --network mac_net1 busybox
</code></pre><ul><li>node-1</li></ul><pre tabindex=0><code>[root@node-1 ~]# docker exec bbox-1 ping 10.0.0.12
[root@node-1 ~]# docker exec bbox-1 ping bbox-2
</code></pre><p>可以ping通ip，但是无法ping通主机名，因为它没有dns解析
<img loading=lazy src=https://image.lvbibir.cn/blog/20190818182937702.png alt=在这里插入图片描述></p><pre tabindex=0><code>[root@node-1 ~]# brctl show
</code></pre><p>因为macvlan不依赖于bridge网络，所以查看不到新的桥接网络
<img loading=lazy src=https://image.lvbibir.cn/blog/20190818183048914.png alt=在这里插入图片描述></p><pre><code>[root@node-1 ~]# docker exec bbox-1  ip link
</code></pre><p>查看到eth0连接到了if2
<img loading=lazy src=https://image.lvbibir.cn/blog/20190818183317728.png alt=在这里插入图片描述></p><pre><code>[root@node-1 ~]# ip link show ens33
</code></pre><p>可以查看到ens33的编号是2，即bbox-1容器的eth0网卡连接到了ens33物理网卡
<img loading=lazy src=https://image.lvbibir.cn/blog/20190818183502889.png alt=在这里插入图片描述></p><pre tabindex=0><code>[root@node-1 ~]# docker network create  -d macvlan -o parent=ens33 mac_net2
Error response from daemon: network dm-b34ee1020a96 is already using parent interface ens33
</code></pre><p>再创建macvlan网络时发现已经无法再创建，即一块网卡只能添加一个macvlan的地址</p><h2 id=一块网卡绑定多个macvlan地址>一块网卡绑定多个macvlan地址<a hidden class=anchor aria-hidden=true href=#一块网卡绑定多个macvlan地址>#</a></h2><pre tabindex=0><code>[root@node-1 ~]# modinfo 8021q
# 查看内核是否支持802.1q封装
[root@node-1 ~]# modprobe 8021q
# 如果上条命令执行后没有结果，使用该命令加载该模块
</code></pre><ul><li>node-1</li></ul><pre tabindex=0><code>[root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33
BOOTPROTO=manual
</code></pre><p>修改为不需要ip的manual模式</p><ul><li>node-2</li></ul><pre tabindex=0><code>[root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32
BOOTPROTO=manual
</code></pre><ul><li>node-1</li></ul><p>添加两块虚拟网卡，注意与实际的ens32网卡的网段区分开
ens32使用的是192.168.0.0/24网段，虚拟网卡使用的是192.168.1.0/24和192.168.2.0/24</p><pre tabindex=0><code>[root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens33.10
[root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.10
BOOTPROTO=none
NAME=ens33.10
DEVICE=ens33.10
ONBOOT=yes
IPADDR=192.168.1.10
PREFIX=24
NETWORK=192.168.1.0
VLAN=yes
[root@node-1 ~]# cp -p /etc/sysconfig/network-scripts/ifcfg-ens33.10 /etc/sysconfig/network-scripts/ifcfg-ens33.20
[root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33.20
BOOTPROTO=none
NAME=ens33.20
DEVICE=ens33.20
ONBOOT=yes
IPADDR=192.168.2.10
PREFIX=24
NETWORK=192.168.2.0
VLAN=yes
[root@node-1 ~]# ifup ens33.10
[root@node-1 ~]# ifup ens33.20
[root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.10 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.10
[root@node-1 ~]# scp /etc/sysconfig/network-scripts/ifcfg-ens33.20 192.168.0.54:/etc/sysconfig/network-scripts/ifcfg-ens32.20
</code></pre><ul><li>node-2</li></ul><pre tabindex=0><code>[root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.10
BOOTPROTO=none
NAME=ens32.10
DEVICE=ens32.10
ONBOOT=yes
IPADDR=192.168.1.20
PREFIX=24
NETWORK=192.168.1.0
VLAN=yes
[root@node-2 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32.20
BOOTPROTO=none
NAME=ens32.20
DEVICE=ens32.20
ONBOOT=yes
IPADDR=192.168.2.20
PREFIX=24
NETWORK=192.168.2.0
VLAN=yes
[root@node-2 ~]# ifup ens32.10
[root@node-2 ~]# ifup ens32.20
</code></pre><ul><li>node-1</li></ul><pre tabindex=0><code>[root@node-1 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens33.10 mac_net11
[root@node-1 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens33.20 mac_net12
</code></pre><ul><li>node-2</li></ul><pre tabindex=0><code>[root@node-2 ~]# docker network create -d macvlan --subnet 172.16.11.0/24 --gateway 172.16.11.1 -o parent=ens32.10 mac_net11
[root@node-2 ~]# docker network create -d macvlan --subnet 172.16.12.0/24 --gateway 172.16.12.1 -o parent=ens32.20 mac_net12
</code></pre><ul><li>node-1</li></ul><pre tabindex=0><code>[root@node-2 ~]# docker run -itd --name bbox-11 --ip=172.16.11.11 --network mac_net11 busybox
[root@node-2 ~]# docker run -itd --name bbox-12 --ip=172.16.12.11 --network mac_net12 busybox
</code></pre><ul><li>node-2</li></ul><pre tabindex=0><code>[root@node-2 ~]# docker run -itd --name bbox-21 --ip=172.16.11.12 --network mac_net11 busybox
[root@node-2 ~]# docker run -itd --name bbox-22 --ip=172.16.12.12 --network mac_net12 busybox
</code></pre><ul><li>node-1</li></ul><pre tabindex=0><code>[root@node-1 ~]# docker exec bbox-11 ping 172.16.11.12
PING 172.16.11.12 (172.16.11.12): 56 data bytes
64 bytes from 172.16.11.12: seq=0 ttl=64 time=0.867 ms
64 bytes from 172.16.11.12: seq=1 ttl=64 time=1.074 ms
64 bytes from 172.16.11.12: seq=2 ttl=64 time=1.145 ms
64 bytes from 172.16.11.12: seq=3 ttl=64 time=0.938 ms
^C
[root@node-1 ~]# docker exec bbox-12 ping 172.16.12.12
PING 172.16.12.12 (172.16.12.12): 56 data bytes
64 bytes from 172.16.12.12: seq=0 ttl=64 time=0.858 ms
64 bytes from 172.16.12.12: seq=1 ttl=64 time=1.140 ms
64 bytes from 172.16.12.12: seq=2 ttl=64 time=0.818 ms
64 bytes from 172.16.12.12: seq=3 ttl=64 time=1.056 ms
^C
</code></pre><ul><li><p>在两台系统进行修改，添加网关，修改防火墙策略</p></li><li><p>node-1中记得将ens32更换为ens33</p><pre><code> ifconfig ens32.10 172.16.10.1 netmask 255.255.255.0
  ifconfig ens32.20 172.16.20.1 netmask 255.255.255.0
  iptables -t nat -A POSTROUTING -o ens32.10 -j MASQUERADE
  iptables -t nat -A POSTROUTING -o ens32 -j MASQUERADE

  iptables -A FORWARD -i ens32.10 -o ens32 -m state --state RELATE,ESTABLISHED -j ACCEPT
  iptables -A FORWARD -i ens32 -o ens32.10 -m state --state RELATE,ESTABLISHED -j ACCEPT
  iptables -A FORWARD -i ens32.10 -o ens32 -j ACCEPT
  iptables -A FORWARD -i ens32 -o ens32.10 -j ACCEPT
</code></pre></li></ul><h1 id=overlay>overlay<a hidden class=anchor aria-hidden=true href=#overlay>#</a></h1><p><strong>一、跨主机网络概述</strong>
<img loading=lazy src=https://image.lvbibir.cn/blog/20190819130602900.png alt=在这里插入图片描述>
<strong>二、准备overlay环境</strong>
为支持容器的跨主机通信，Docker提供了overlay driver。Docker overlay网络需要一个key-value数据库用于保存网络状态信息，包括Network、Endpoint、IP等。Consul、Etcd和ZooKeeper都是Docker支持的key-value软件，这里我们使用Consul</p><p><strong>1. 环境描述</strong></p><table><thead><tr><th>节点</th><th>系统版本</th><th>docker版本</th><th>角色</th><th>IP地址</th></tr></thead><tbody><tr><td>node-1</td><td>centos7.4</td><td>docker-18.03.0</td><td>consul</td><td>192.168.0.101</td></tr><tr><td>node-2</td><td>centos7.4</td><td>docker-18.03.0</td><td>host</td><td>192.168.0.102</td></tr><tr><td>node-3</td><td>centos7.4</td><td>docker-18.03.0</td><td>host</td><td>192.168.0.103</td></tr></tbody></table><p><strong>2. 创建consul</strong></p><ul><li>node-1;</li></ul><pre tabindex=0><code>[root@node-1 ~]# docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap
</code></pre><p>容器启动后可以通过192.168.0.101:8500访问到consul
<img loading=lazy src=https://image.lvbibir.cn/blog/20190819224043952.png alt=在这里插入图片描述>
<strong>3. 修改docker配置文件</strong>
修改node-2和node-3的docker daemon的配置文件/etc/systemd/system/docker.service</p><pre tabindex=0><code>[root@node-2 ~]# vim  /etc/systemd/system/docker.service
ExecStart=/usr/bin/dockerd  -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.0.101:8500 --cluster-advertise=ens32:2376
[root@node-2 ~]# systemctl daemon-reload
[root@node-2 ~]# systemctl restart docker
</code></pre><ul><li>-H ：tcp：允许tcp连接daemon
-H：unix：默认的socket连接方式，支持远程的同时，本地也可以连接</li><li>&ndash;cluster-store 指定consul的地址</li><li>&ndash;cluster-advertise 告知consul自己的连接地址</li></ul><p>node-2和node-3会自动注册到consul数据库中。
<img loading=lazy src=https://image.lvbibir.cn/blog/20190819225334857.png alt=在这里插入图片描述></p><p><strong>三、创建overlay网络
1、在node-2中创建网络</strong>
在node-2中创建overlay网络ov_net1</p><pre tabindex=0><code>[root@node-2 ~]# docker network create -d overlay ov_net1
</code></pre><ul><li>-d overlay：指定driver为overlay</li></ul><pre tabindex=0><code>[root@node-2 ~]# docker network ls
</code></pre><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190819225928983.png alt=在这里插入图片描述></p><p><strong>2、node-3查看创建的网络</strong>
注意到ov_net1的 SCOPE 为 global，而其他网络为 local 。在node-3上查看存在的网络:</p><pre tabindex=0><code>[root@node-3 ~]# docker network ls
</code></pre><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190819230148207.png alt=在这里插入图片描述>
node-3上也能看到ov_net1，只是因为创建ov_net1时将overlay网络信息存入了consul，node-3从consul读取到了新网络数据。之后ov_net1的任何变化都会同步到node-2和node-3.
<strong>3、查看ov_net1详细信息</strong></p><pre tabindex=0><code>[root@node-2 ~]# docker network inspect ov_net1
</code></pre><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190819230439425.png alt=在这里插入图片描述>
IPAM 是指 IP Address Management，docker自动为 ov_net1 分配的 IP 空间为 10.0.0.0/24
<strong>四、在overlay中运行容器</strong>
<strong>1、创建容器 bbox-1</strong>
在 node-2 上运行一个 busybox 容器并连接到 ov_net1.</p><pre tabindex=0><code>[root@node-2 ~]# docker run -itd --name bbox-1 --network ov_net1 busybox
</code></pre><p><strong>2、查看 bbox-1 网络配置</strong></p><pre tabindex=0><code>[root@node-2 ~]# docker exec bbox-1 ip r
default via 172.18.0.1 dev eth1
10.0.0.0/24 dev eth0 scope link  src 10.0.0.2
172.18.0.0/16 dev eth1 scope link  src 172.18.0.2
</code></pre><ul><li>bbox-1 有两个网络接口，eth0 和 eth1</li><li>eth0 IP 为 10.0.0.2，连接的是overlay网络 ov_net1</li><li>eth1 IP 为 172.18.0.2</li><li>容器的默认路由是走 eth1，其实，docker 会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力</li></ul><pre tabindex=0><code>[root@node-2 ~]# docker network ls
</code></pre><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190819231543466.png alt=在这里插入图片描述></p><pre tabindex=0><code>[root@node-2 ~]# docker network inspect docker_gwbridge
</code></pre><p><img loading=lazy src=https://image.lvbibir.cn/blog/20190819232009799.png alt=在这里插入图片描述>
从 docker network inspect docker_gwbridge 输出可确认 docker_gwbridge 的 IP 地址范围是 172.18.0.0/16，当前连接的容器就是 bbox-1（172.18.0.2）
而且此网络的网关就是网桥 docker_gwbridge 的 IP 172.18.0.1</p><pre tabindex=0><code>[root@node-2 ~]# ifconfig docker_gwbridge
docker_gwbridge: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 172.18.0.1  netmask 255.255.0.0  broadcast 172.18.255.255
        inet6 fe80::42:c5ff:fe45:937  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 02:42:c5:45:09:37  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre><p>这样容器 bbox-1 就可以通过docker_gwbridge 访问外网</p><pre tabindex=0><code>[root@node-2 ~]# docker exec bbox-1 ping -c 4 www.baidu.com
PING www.baidu.com (182.61.200.6): 56 data bytes
64 bytes from 182.61.200.6: seq=0 ttl=53 time=6.721 ms
64 bytes from 182.61.200.6: seq=1 ttl=53 time=7.954 ms
64 bytes from 182.61.200.6: seq=2 ttl=53 time=11.723 ms
64 bytes from 182.61.200.6: seq=3 ttl=53 time=15.105 ms

--- www.baidu.com ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 6.721/10.375/15.105 ms
</code></pre><p><strong>五、overlay网络连通性</strong>
<strong>1、node-3 中 运行 bbox-2</strong></p><pre tabindex=0><code>[root@node-3 ~]# docker run -itd --name bbox-2 --network ov_net1 busybox
</code></pre><p><strong>2、查看 bbox-2 路由情况</strong></p><pre tabindex=0><code>[root@node-3 ~]# docker exec bbox-2 ip r
default via 172.18.0.1 dev eth1
10.0.0.0/24 dev eth0 scope link  src 10.0.0.3
172.18.0.0/16 dev eth1 scope link  src 172.18.0.2
</code></pre><p><strong>3、互通测试</strong></p><pre tabindex=0><code>[root@node-3 ~]# docker exec bbox-2 ping -c 4 10.0.0.2
PING 10.0.0.2 (10.0.0.2): 56 data bytes
64 bytes from 10.0.0.2: seq=0 ttl=64 time=2.628 ms
64 bytes from 10.0.0.2: seq=1 ttl=64 time=1.004 ms
64 bytes from 10.0.0.2: seq=2 ttl=64 time=1.277 ms
64 bytes from 10.0.0.2: seq=3 ttl=64 time=1.505 ms

--- 10.0.0.2 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 1.004/1.603/2.628 ms
</code></pre><p>可见 overlay 网络中的容器可以直接通信，同时docker也实现了DNS服务
<strong>4、实现原理</strong>
docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0， veth pair 一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。
br0 除了连接所有的 veth pair，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的。逻辑网络拓扑结构如图所示：
<img loading=lazy src=https://image.lvbibir.cn/blog/20190819233352547.png alt=在这里插入图片描述></p><pre tabindex=0><code>[root@node-2 ~]# brctl show
bridge name     bridge id               STP enabled     interfaces
docker0         8000.024217edc413       no
docker_gwbridge         8000.0242c5450937       no              vethc59120e
virbr0          8000.525400b76fd4       yes             virbr0-nic
[root@node-3 ~]# brctl show
bridge name     bridge id               STP enabled     interfaces
docker0         8000.0242ef3c7df7       no
docker_gwbridge         8000.0242c81afaee       no              vethf4562a9
virbr0          8000.525400c28478       yes             virbr0-nic
</code></pre><p>要查看 overlay 网络的 namespace 可以在 node-2 和 node-3 上执行 ip netns（请确保在此之前执行过 ln -s /var/run/docker/netns /var/run/netns），可以看到两个 node 上有一个相同的 namespace &ldquo;1-dd91de7599&rdquo;</p><pre tabindex=0><code>[root@node-2 ~]# ln -s /var/run/docker/netns /var/run/netns
[root@node-2 ~]# ip netns
6889f61efc4b (id: 1)
1-dd91de7599 (id: 0)
</code></pre><pre tabindex=0><code>[root@node-3 ~]# ln -s /var/run/docker/netns /var/run/netns
[root@node-3 ~]# ip netns
8e4722847745 (id: 1)
1-dd91de7599 (id: 0)
</code></pre><p>&ldquo;1-dd91de7599&rdquo; 这就是 ov_net1 的 namespace，查看 namespace 中的 br0 上的设备</p><pre tabindex=0><code>[root@node-2 ~]# ip netns exec 1-dd91de7599 brctl show
bridge name     bridge id               STP enabled     interfaces
br0             8000.0e7576c7c035       no              veth0
                                                        vxlan0
</code></pre><p><strong>六、overlay网络隔离</strong>
不同的 overlay 网络是相互隔离的。我们创建第二个 overlay 网络 ov_net2 并运行容器 bbox-3
<strong>1、创建网络 ov_net2</strong></p><pre tabindex=0><code>[root@node-2 ~]# docker network create -d overlay ov_net2
</code></pre><p><strong>2、启动容器 bbox-3</strong></p><pre tabindex=0><code>[root@node-2 ~]# docker run -itd --name bbox-3 --network ov_net2 busybox
</code></pre><p><strong>3、查看 bbox-3 网络</strong>
bbox-3 分配到的 IP 是 10.0.1.2，尝试 ping bbox-1（10.0.0.2）</p><pre tabindex=0><code>[root@node-2 ~]# docker exec -it bbox-3 ip r
default via 172.18.0.1 dev eth1
10.0.1.0/24 dev eth0 scope link  src 10.0.1.2
172.18.0.0/16 dev eth1 scope link  src 172.18.0.3
</code></pre><pre tabindex=0><code>[root@node-2 ~]# docker exec -it bbox-3 ping -c 4 10.0.0.2
PING 10.0.0.2 (10.0.0.2): 56 data bytes

--- 10.0.0.2 ping statistics ---
4 packets transmitted, 0 packets received, 100% packet loss
[root@node-2 ~]# docker exec -it bbox-3 ping -c 4 172.18.0.2
PING 172.18.0.2 (172.18.0.2): 56 data bytes

--- 172.18.0.2 ping statistics ---
4 packets transmitted, 0 packets received, 100% packet loss
</code></pre><p>ping 失败，可见不同 overlay 网络之间是隔离的，即使通过 docker_gwbridge 也不能通信
如果要实现 bbox-3 和 bbox-1 通信，可以将 bbox-3 也连接到 ov_net1
这时 bbox-3 同时连接到了 ov_net1 和 ov_net2 上</p><pre tabindex=0><code>[root@node-2 ~]# docker network connect ov_net1 bbox-3
[root@node-2 ~]# docker exec bbox-3 ip r
default via 172.18.0.1 dev eth1
10.0.0.0/24 dev eth2 scope link  src 10.0.0.4
10.0.1.0/24 dev eth0 scope link  src 10.0.1.2
172.18.0.0/16 dev eth1 scope link  src 172.18.0.3
[root@node-2 ~]# docker exec bbox-3 ping -c 4 10.0.0.2
PING 10.0.0.2 (10.0.0.2): 56 data bytes
64 bytes from 10.0.0.2: seq=0 ttl=64 time=0.184 ms
64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.158 ms
64 bytes from 10.0.0.2: seq=2 ttl=64 time=0.162 ms
64 bytes from 10.0.0.2: seq=3 ttl=64 time=0.093 ms

--- 10.0.0.2 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 0.093/0.149/0.184 ms
</code></pre><p>docker 默认为 overlay 网络分配 24 位掩码的子网（10.0.X.0/24），所有主机共享这个 subnet，容器启动时会顺序从此空间分配 IP。当然我们也可以通过 &ndash;subnet 指定 IP 空间。</p><pre tabindex=0><code>docker network create -d overlay --subnet 10.22.1.0/24 ov_net
</code></pre></div><div class=post-reward><div style=padding:0;margin:0;width:100%;font-size:16px;text-align:center><div id=QR style=opacity:0><div id=wechat style=display:inline-block><a class=fancybox rel=group><img id=wechat_qr src=https://www.lvbibir.cn/img/wechat_pay.png alt=wechat_pay></a><p>微信</p></div><div id=alipay style=display:inline-block><a class=fancybox rel=group><img id=alipay_qr src=https://www.lvbibir.cn/img/alipay.png alt=alipay></a><p>支付宝</p></div></div><button id=rewardButton onclick='var qr=document.getElementById("QR");qr.style.opacity==="0"?qr.style.opacity="1":qr.style.opacity="0"'>
<span>🧧 鼓励</span></button></div></div><footer class=post-footer><nav class=paginav><a class=prev href=https://www.lvbibir.cn/posts/tech/docker-command/><span class=title>« 上一页</span><br><span>docker | 命令大全</span></a>
<a class=next href=https://www.lvbibir.cn/posts/tech/docker-data-volume/><span class=title>下一页 »</span><br><span>docker | 数据卷（data volume）</span></a></nav></footer></div><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>💬评论</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.7/twikoo.all.min.js></script><script>twikoo.init({envId:"https://twikoo.lvbibir.cn/",el:"#tcomment",lang:"zh-CN",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><a href=https://gohugo.io/ target=_blank><img style=display:unset src=https://image.lvbibir.cn/blog/frame-hugo-blue.svg></a>
<a href=https://github.com/adityatelange/hugo-PaperMod target=_blank><img style=display:unset src=https://image.lvbibir.cn/blog/theme-papermod-lightgrey.svg></a>
<a href=https://cn.aliyun.com/ target=_blank><img style=display:unset src=https://image.lvbibir.cn/blog/图床-阿里云-orange.svg></a><br><span id=runtime_span></span>
<script type=text/javascript>function show_runtime(){window.setTimeout("show_runtime()",1e3),X=new Date("7/13/2021 1:00:00"),Y=new Date,T=Y.getTime()-X.getTime(),M=24*60*60*1e3,a=T/M,A=Math.floor(a),b=(a-A)*24,B=Math.floor(b),c=(b-B)*60,C=Math.floor((b-B)*60),D=Math.floor((c-C)*60),runtime_span.innerHTML="网站已运行"+A+"天"+B+"小时"+C+"分"+D+"秒"}show_runtime()</script>|
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<span id=busuanzi_container>总访客数: <span id=busuanzi_value_site_uv></span>
|
总访问量: <span id=busuanzi_value_site_pv></span></span><br><a href=https://beian.miit.gov.cn/ target=_blank style="border-bottom:1px solid">京ICP备2021023168号-1</a>&nbsp;
|
<span>Copyright
&copy;
2020-2022
<a href=https://www.lvbibir.cn style="border-bottom:1px solid">lvbibir's Blog</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg><span id=read_progress></span></span></a>
<script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>400||document.documentElement.scrollTop>400?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="📄复制";function s(){t.innerText="👌🏻已复制!",setTimeout(()=>{t.innerText="📄复制"},2e3)}t.addEventListener("click",t=>{const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild===n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName==="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>